/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40/DDPGHER_CNN.py
episode index:0
target Thresh 15.199999999999996
target distance 5.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([31.7444593 ,  3.77482624,  3.03072691]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 6.639517126861043}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6964132180495735
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([37.41736374,  5.11702109,  0.97348129]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 1.0578831581508092}
episode index:1
target Thresh 15.80497010805058
target distance 15.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'dynamicTrap': False, 'previousTarget': array([15.35363499,  5.37889464]), 'currentState': array([27.3305424 , 20.52056091,  1.94502163]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 19.82246419735011}
done in step count: 99
reward sum = -0.2333631298603784
running average episode reward sum: 0.2315250440945975
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'dynamicTrap': False, 'previousTarget': array([15.,  5.]), 'currentState': array([33.54221139,  6.53382106,  5.17092723]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 18.605542457338757}
episode index:2
target Thresh 16.403920662949282
target distance 9.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([32.8053645 , 16.63227417,  0.36014402]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 10.634055533412054}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15435002939639833
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([33.60011401, 20.57562622,  1.52236915]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 14.58797506504665}
episode index:3
target Thresh 16.9969115602507
target distance 11.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([11.38476522, 10.82279987,  3.43856192]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 10.184470864504194}
done in step count: 99
reward sum = -0.1111018112027953
running average episode reward sum: 0.08798706924659994
{'scaleFactor': 20, 'currentTarget': array([18.60588116, 14.78834013]), 'dynamicTrap': False, 'previousTarget': array([18.23193106, 15.36348074]), 'currentState': array([34.09633386,  2.13741625,  5.09717642]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 20.0}
episode index:4
target Thresh 17.58400209953875
target distance 6.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'dynamicTrap': False, 'previousTarget': array([14., 16.]), 'currentState': array([10.98199252, 21.41954567,  4.80209622]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 6.203212424672765}
done in step count: 99
reward sum = -0.07986059248591038
running average episode reward sum: 0.054417536900097874
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'dynamicTrap': False, 'previousTarget': array([14., 16.]), 'currentState': array([10.90726375,  1.18959455,  5.59425073]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 15.129875312314919}
episode index:5
target Thresh 18.165250990356583
target distance 15.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([30.85620599,  3.54228884,  2.7063235 ]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 13.820037284737193}
done in step count: 99
reward sum = -0.021750142382105257
running average episode reward sum: 0.04172292368639735
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([37.80500262, 11.86963064,  1.88946993]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 6.387388727328568}
episode index:6
target Thresh 18.740716358077673
target distance 11.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 21.]), 'dynamicTrap': False, 'previousTarget': array([32., 21.]), 'currentState': array([25.82990045,  8.33611513,  5.52827346]), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 14.087019147186993}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.11962490343655265
{'scaleFactor': 20, 'currentTarget': array([32., 21.]), 'dynamicTrap': False, 'previousTarget': array([32., 21.]), 'currentState': array([31.86890924, 21.51341201,  5.63478838]), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 0.5298836516322718}
episode index:7
target Thresh 19.310455749718336
target distance 9.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([1.74015655, 5.19203071, 6.27197313]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 11.789694432965746}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.20898601068824452
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([12.92064908, 11.38446136,  1.2563865 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.997699985545007}
episode index:8
target Thresh 19.87452613969254
target distance 17.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([ 8.02895226, 20.10807879,  3.96607935]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 17.562903516193135}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.23599241508582575
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([11.71447223,  2.16258423,  4.15538826]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.884754929175273}
episode index:9
target Thresh 20.43298393550932
target distance 2.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'dynamicTrap': False, 'previousTarget': array([9., 8.]), 'currentState': array([6.25410354, 8.32814681, 3.5634557 ]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 2.7654344537255056}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.29926775485414103
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'dynamicTrap': False, 'previousTarget': array([9., 8.]), 'currentState': array([9.58664708, 7.56223225, 2.63564109]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.7319804631174736}
episode index:10
target Thresh 20.985884983413655
target distance 13.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.98480296, 14.19372581]), 'dynamicTrap': True, 'previousTarget': array([32., 14.]), 'currentState': array([19.      , 22.      ,  4.762049], dtype=float32), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 15.150677365133863}
done in step count: 60
reward sum = 0.4475087027775488
running average episode reward sum: 0.3127442046653599
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([31.85690599, 13.70667499,  5.82132321]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 0.3263670613076152}
episode index:11
target Thresh 21.533284573971084
target distance 12.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'dynamicTrap': False, 'previousTarget': array([21., 22.]), 'currentState': array([ 9.46723143, 17.86823343,  4.99830848]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 12.250561040290057}
done in step count: 80
reward sum = 0.23858933126139897
running average episode reward sum: 0.3065646318816965
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'dynamicTrap': False, 'previousTarget': array([21., 22.]), 'currentState': array([21.66576655, 22.04608143,  2.6758917 ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 0.667359423070307}
episode index:12
target Thresh 22.075237447596823
target distance 13.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'dynamicTrap': False, 'previousTarget': array([16.,  5.]), 'currentState': array([13.2844112 , 16.97971524,  3.78806475]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.283647660075871}
done in step count: 77
reward sum = 0.25634984612563144
running average episode reward sum: 0.3027019560543069
{'scaleFactor': 20, 'currentTarget': array([14.28926082,  5.95803926]), 'dynamicTrap': True, 'previousTarget': array([16.,  5.]), 'currentState': array([14.7091817 ,  6.88748785,  5.43186442]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 1.019905988235239}
episode index:13
target Thresh 22.611797800029873
target distance 14.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'dynamicTrap': False, 'previousTarget': array([19., 22.]), 'currentState': array([ 4.02605463, 19.01185466,  4.13942146]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 15.269186378268039}
done in step count: 99
reward sum = -0.10623948074365511
running average episode reward sum: 0.27349185342588106
{'scaleFactor': 20, 'currentTarget': array([19.03579487, 21.49275886]), 'dynamicTrap': True, 'previousTarget': array([19.03847921, 21.48518336]), 'currentState': array([21.15612868, 16.72172504,  1.40180094]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 5.220974925371374}
episode index:14
target Thresh 23.143019287752598
target distance 18.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'dynamicTrap': False, 'previousTarget': array([26.,  8.]), 'currentState': array([ 9.20238209, 13.69035492,  4.67279243]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 17.735278589957876}
done in step count: 79
reward sum = 0.29312116192346954
running average episode reward sum: 0.27480047399238694
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'dynamicTrap': False, 'previousTarget': array([26.,  8.]), 'currentState': array([25.26848713,  8.86128352,  4.8429947 ]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 1.1300090213280496}
episode index:15
target Thresh 23.66895503335648
target distance 9.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([6.69364121, 7.25281313, 2.8722775 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 9.390306418422467}
done in step count: 88
reward sum = 0.37007538776674465
running average episode reward sum: 0.28075515610328433
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([15.10227283,  6.04836398,  4.55265713]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.899028999609363}
episode index:16
target Thresh 24.18965763085434
target distance 17.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.12012723,  3.08783503]), 'dynamicTrap': False, 'previousTarget': array([20.20859686,  3.13497444]), 'currentState': array([36.26476429, 14.89252452,  2.10797375]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1918429246939598
running average episode reward sum: 0.2529552689975641
{'scaleFactor': 20, 'currentTarget': array([19.48828897,  2.99416895]), 'dynamicTrap': True, 'previousTarget': array([19.55184907,  3.02743479]), 'currentState': array([20.15176788,  9.84817613,  5.8032958 ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 6.886045217369829}
episode index:17
target Thresh 24.705179150939866
target distance 21.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.72266573,  8.00806335]), 'dynamicTrap': False, 'previousTarget': array([31.3829006 ,  8.12161403]), 'currentState': array([13.43427957, 16.10342817,  4.38539743]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.16730537058233713
running average episode reward sum: 0.22960745568756957
{'scaleFactor': 20, 'currentTarget': array([32.00128641,  6.63113104]), 'dynamicTrap': False, 'previousTarget': array([31.04288052,  6.42563   ]), 'currentState': array([12.33342444,  3.00136447,  1.22184954]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:18
target Thresh 25.215571146194662
target distance 14.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([37.09646529,  1.97693613,  1.03552073]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 13.251799517308532}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.25684769982672884
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([24.70876607,  3.98973137,  0.23070648]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.7088404486188602}
episode index:19
target Thresh 25.720884656243566
target distance 17.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.28191761, 16.61460972]), 'dynamicTrap': True, 'previousTarget': array([36.33935727, 16.53366395]), 'currentState': array([20.      ,  5.      ,  5.260673], dtype=float32), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.45504637803264547
running average episode reward sum: 0.26675763373702466
{'scaleFactor': 20, 'currentTarget': array([37., 17.]), 'dynamicTrap': False, 'previousTarget': array([37., 17.]), 'currentState': array([36.67527287, 17.12003001,  5.96000358]), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 0.3462006803819952}
episode index:20
target Thresh 26.2211702128587
target distance 25.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.09007143, 11.58233545]), 'dynamicTrap': False, 'previousTarget': array([12.76931317, 11.31390548]), 'currentState': array([29.28902398, 23.31237964,  1.83421391]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1670864379069267
running average episode reward sum: 0.24609839223016983
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'dynamicTrap': False, 'previousTarget': array([4., 5.]), 'currentState': array([1.1598723 , 2.62827535, 3.77736492]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 3.700189617209406}
episode index:21
target Thresh 26.71647784501262
target distance 8.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 15.]), 'dynamicTrap': False, 'previousTarget': array([30., 15.]), 'currentState': array([38.63858465, 19.36156728,  1.52519339]), 'targetState': array([30, 15], dtype=int32), 'currentDistance': 9.677211060825417}
done in step count: 56
reward sum = 0.5404540563611712
running average episode reward sum: 0.25947819514521536
{'scaleFactor': 20, 'currentTarget': array([30., 15.]), 'dynamicTrap': False, 'previousTarget': array([30., 15.]), 'currentState': array([29.28868623, 15.95523945,  0.88861958]), 'targetState': array([30, 15], dtype=int32), 'currentDistance': 1.1909868583560725}
episode index:22
target Thresh 27.206857083881303
target distance 26.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.53372746, 14.16082385]), 'dynamicTrap': False, 'previousTarget': array([28.73939228, 14.94498726]), 'currentState': array([10.79129084,  3.22049405,  4.72437978]), 'targetState': array([38, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2609505403096852
running average episode reward sum: 0.23685085882108925
{'scaleFactor': 20, 'currentTarget': array([38., 21.]), 'dynamicTrap': False, 'previousTarget': array([38., 21.]), 'currentState': array([29.21987772, 22.16901547,  4.83606368]), 'targetState': array([38, 21], dtype=int32), 'currentDistance': 8.857603763992588}
episode index:23
target Thresh 27.692356967797288
target distance 13.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.98278373,  4.0067146 ]), 'dynamicTrap': True, 'previousTarget': array([13.,  4.]), 'currentState': array([18.      , 17.      ,  5.932827], dtype=float32), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 13.928313770981566}
done in step count: 81
reward sum = 0.19822675167185366
running average episode reward sum: 0.23524152102320442
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'dynamicTrap': False, 'previousTarget': array([13.,  4.]), 'currentState': array([12.4833626 ,  4.19004501,  3.26543082]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.550482801959194}
episode index:24
target Thresh 28.17302604715355
target distance 10.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 19.]), 'dynamicTrap': False, 'previousTarget': array([31., 19.]), 'currentState': array([25.00432273,  9.35769911,  0.63593972]), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 11.35438736788538}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.25822097491112656
{'scaleFactor': 20, 'currentTarget': array([31., 19.]), 'dynamicTrap': False, 'previousTarget': array([31., 19.]), 'currentState': array([30.75675214, 18.33054355,  2.30991393]), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 0.7122790590335434}
episode index:25
target Thresh 28.648912389258577
target distance 18.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.13318081,  6.93668853]), 'dynamicTrap': False, 'previousTarget': array([26.28727678,  8.05181363]), 'currentState': array([13.2531738 , 21.33618177,  5.87339497]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4488832683010906
running average episode reward sum: 0.2655541400415098
{'scaleFactor': 20, 'currentTarget': array([29.,  5.]), 'dynamicTrap': False, 'previousTarget': array([29.,  5.]), 'currentState': array([28.1688218 ,  5.53207505,  3.69648396]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 0.9868946583905484}
episode index:26
target Thresh 29.120063583143168
target distance 26.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.31358002,  4.79241584]), 'dynamicTrap': False, 'previousTarget': array([16.05891029,  4.53392998]), 'currentState': array([34.29046155,  3.83106027,  3.24418378]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5817871092760893
running average episode reward sum: 0.2772664722353831
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'dynamicTrap': False, 'previousTarget': array([10.,  5.]), 'currentState': array([10.86782066,  4.71437187,  3.28212528]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.9136170565944275}
episode index:27
target Thresh 29.586526744319322
target distance 5.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([ 6.54810839, 14.17324459,  0.35772823]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 5.202199761516513}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.2917410480487761
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([5.24548939, 8.05892146, 1.37478816]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.206198606260227}
episode index:28
target Thresh 30.048348519491885
target distance 24.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.60037106, 18.58421865]), 'dynamicTrap': False, 'previousTarget': array([ 9.02633404, 18.32455532]), 'currentState': array([26.213025  , 11.26512626,  3.07380319]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4152637070488566
running average episode reward sum: 0.29600045008326165
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.02484044, 20.18288618,  3.36642427]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.1845654403754017}
episode index:29
target Thresh 30.505575091223232
target distance 20.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 11.]), 'dynamicTrap': False, 'previousTarget': array([36.40285  , 10.8507125]), 'currentState': array([18.40866707,  7.37876924,  0.47676414]), 'targetState': array([37, 11], dtype=int32), 'currentDistance': 18.940722590567006}
done in step count: 53
reward sum = 0.43196628771863044
running average episode reward sum: 0.30053264467110724
{'scaleFactor': 20, 'currentTarget': array([37., 11.]), 'dynamicTrap': False, 'previousTarget': array([37., 11.]), 'currentState': array([36.20582901, 10.67642338,  0.51991813]), 'targetState': array([37, 11], dtype=int32), 'currentDistance': 0.857560134337034}
episode index:30
target Thresh 30.95825218255155
target distance 4.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([29.32422663,  2.70354082,  0.26930874]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 4.308675484984868}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3137591150775971
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([29.96941612,  6.66916888,  3.80336392]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 1.0243128634103729}
episode index:31
target Thresh 31.406425061563212
target distance 17.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([ 6.97902912, 21.63109896,  0.060054  ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 18.04741450224184}
done in step count: 21
reward sum = 0.8016305895390459
running average episode reward sum: 0.3290050986545174
{'scaleFactor': 20, 'currentTarget': array([14.50373546,  4.67303194]), 'dynamicTrap': True, 'previousTarget': array([16.,  6.]), 'currentState': array([13.88467993,  4.2644084 ,  0.74150332]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.7417566547629869}
episode index:32
target Thresh 31.850138545919584
target distance 30.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.75832025, 12.22774484]), 'dynamicTrap': False, 'previousTarget': array([26.56953382, 12.57218647]), 'currentState': array([ 8.03837763, 19.26789743,  5.77476215]), 'targetState': array([38,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.4227313513364101
running average episode reward sum: 0.33184528812972625
{'scaleFactor': 20, 'currentTarget': array([38.,  8.]), 'dynamicTrap': False, 'previousTarget': array([38.,  8.]), 'currentState': array([3.76405492e+01, 8.06226986e+00, 2.81970829e-02]), 'targetState': array([38,  8], dtype=int32), 'currentDistance': 0.3648046096519631}
episode index:33
target Thresh 32.28943700733888
target distance 24.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.63251764, 10.9194666 ]), 'dynamicTrap': False, 'previousTarget': array([ 9.81870354, 11.66690579]), 'currentState': array([27.10924097, 18.5752288 ,  4.70572722]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5630541541109331
running average episode reward sum: 0.3386455488938794
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'dynamicTrap': False, 'previousTarget': array([4., 9.]), 'currentState': array([4.32573996, 8.77858984, 1.53435545]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.39386416813256675}
episode index:34
target Thresh 32.72436437603333
target distance 17.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'dynamicTrap': False, 'previousTarget': array([23., 14.]), 'currentState': array([ 7.82005154, 15.82260598,  0.51515543]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 15.288974057883381}
done in step count: 56
reward sum = 0.3590247338747352
running average episode reward sum: 0.3392278113219038
{'scaleFactor': 20, 'currentTarget': array([22.54806879, 14.53242059]), 'dynamicTrap': True, 'previousTarget': array([23., 14.]), 'currentState': array([23.16773855, 15.16731595,  4.49632421]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.8871768394146667}
episode index:35
target Thresh 33.15496414510223
target distance 11.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([29.87161667, 10.37396064,  4.35841215]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 14.878817794964412}
done in step count: 24
reward sum = 0.7778213593991465
running average episode reward sum: 0.35141096543516054
{'scaleFactor': 20, 'currentTarget': array([23.90773619, 23.11844919]), 'dynamicTrap': True, 'previousTarget': array([22., 23.]), 'currentState': array([24.30893997, 22.3994552 ,  2.68975293]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.82335704817673}
episode index:36
target Thresh 33.58127937488131
target distance 21.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.88885482, 16.12088734]), 'dynamicTrap': False, 'previousTarget': array([31.36486284, 15.92277877]), 'currentState': array([14.7692349 ,  5.78074163,  0.73232907]), 'targetState': array([35, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3977624403275073
running average episode reward sum: 0.3526637079998186
{'scaleFactor': 20, 'currentTarget': array([35., 18.]), 'dynamicTrap': False, 'previousTarget': array([35., 18.]), 'currentState': array([35.9172559 , 18.41895029,  2.87995327]), 'targetState': array([35, 18], dtype=int32), 'currentDistance': 1.008403556930278}
episode index:37
target Thresh 34.003352697248836
target distance 14.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'dynamicTrap': False, 'previousTarget': array([15., 20.]), 'currentState': array([29.56672714,  6.38899565,  1.51721042]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 19.936122466589648}
done in step count: 35
reward sum = 0.6754847930822101
running average episode reward sum: 0.36115899971251314
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'dynamicTrap': False, 'previousTarget': array([15., 20.]), 'currentState': array([14.76106447, 20.86953359,  5.53746708]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.901764300858379}
episode index:38
target Thresh 34.42122631988877
target distance 13.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'dynamicTrap': False, 'previousTarget': array([22., 18.]), 'currentState': array([28.59307294,  5.18101951,  0.34334212]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 14.415091802748965}
done in step count: 38
reward sum = 0.5439067007075205
running average episode reward sum: 0.3658448381995646
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'dynamicTrap': False, 'previousTarget': array([22., 18.]), 'currentState': array([21.79083987, 18.23964173,  2.55222797]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.3180819382207253}
episode index:39
target Thresh 34.83494203051159
target distance 15.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([19.73202821,  3.6652516 ,  0.4019717 ]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 18.22228142225197}
done in step count: 40
reward sum = 0.5560484117018792
running average episode reward sum: 0.37059992753712245
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([34.05980475, 15.98206284,  5.67348686]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 0.9838821221041022}
episode index:40
target Thresh 35.24454120103312
target distance 30.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.87001963, 14.05622019]), 'dynamicTrap': False, 'previousTarget': array([25., 14.]), 'currentState': array([ 6.87049781, 14.19452025,  5.77671451]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.3596473505060264
running average episode reward sum: 0.3703327915119738
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([35.9748005 , 14.84177047,  5.43545674]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 1.2879493546608978}
episode index:41
target Thresh 35.650064791711785
target distance 21.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.56754114,  7.82450147]), 'dynamicTrap': False, 'previousTarget': array([26.36486284,  8.07722123]), 'currentState': array([ 8.90737532, 17.21164925,  5.60543346]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2500556288241911
running average episode reward sum: 0.3674690495432171
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([30.,  6.]), 'currentState': array([30.7702143 ,  5.82507281,  2.51991863]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 0.7898288409876838}
episode index:42
target Thresh 36.05155335524454
target distance 17.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 22.]), 'currentState': array([6.86635669, 4.58030866, 4.80424476]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 17.6539414025954}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.3747966203680349
{'scaleFactor': 20, 'currentTarget': array([ 4., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 22.]), 'currentState': array([ 3.0916757 , 21.23937603,  0.60427975]), 'targetState': array([ 4, 22], dtype=int32), 'currentDistance': 1.1847370412719342}
episode index:43
target Thresh 36.44904704082236
target distance 23.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.33196031,  5.32426216]), 'dynamicTrap': False, 'previousTarget': array([16.65859887,  6.02547777]), 'currentState': array([33.91784064, 12.71107828,  5.02397251]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4434508493875483
running average episode reward sum: 0.3763569437548421
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'dynamicTrap': False, 'previousTarget': array([12.,  4.]), 'currentState': array([12.95704413,  3.78030524,  1.95442273]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.9819364821409308}
episode index:44
target Thresh 36.842585598145
target distance 10.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 21.]), 'dynamicTrap': False, 'previousTarget': array([36., 21.]), 'currentState': array([35.72803847, 11.12402484,  1.17873495]), 'targetState': array([36, 21], dtype=int32), 'currentDistance': 9.879719051007907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.38891523721364557
{'scaleFactor': 20, 'currentTarget': array([36., 21.]), 'dynamicTrap': False, 'previousTarget': array([36., 21.]), 'currentState': array([36.32086609, 20.69140498,  2.80905974]), 'targetState': array([36, 21], dtype=int32), 'currentDistance': 0.44518078828052143}
episode index:45
target Thresh 37.23220838139618
target distance 11.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.82465197, 6.91055455]), 'dynamicTrap': True, 'previousTarget': array([10.,  7.]), 'currentState': array([21.     ,  8.     ,  5.36914], dtype=float32), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 12.223992437979872}
done in step count: 53
reward sum = 0.5215552303570863
running average episode reward sum: 0.3917987153254595
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([10.81171427,  6.36845779,  1.69554069]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.0284578868443972}
episode index:46
target Thresh 37.6179543531789
target distance 11.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 17.]), 'currentState': array([16.39991752,  6.00544234,  0.79041046]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 14.465087188372037}
done in step count: 38
reward sum = 0.625199289438307
running average episode reward sum: 0.39676468498743495
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.03649413, 17.71603664,  0.53451694]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 1.200438269493987}
episode index:47
target Thresh 37.99986208841179
target distance 13.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.01438468,  3.70060324]), 'dynamicTrap': True, 'previousTarget': array([33.,  3.]), 'currentState': array([26.      , 16.      ,  3.141141], dtype=float32), 'targetState': array([33,  3], dtype=int32), 'currentDistance': 14.6801063543892}
done in step count: 36
reward sum = 0.6579651176262243
running average episode reward sum: 0.4022063606674098
{'scaleFactor': 20, 'currentTarget': array([33.,  3.]), 'dynamicTrap': False, 'previousTarget': array([33.,  3.]), 'currentState': array([32.06141626,  3.1239677 ,  3.44208996]), 'targetState': array([33,  3], dtype=int32), 'currentDistance': 0.9467351432034506}
episode index:48
target Thresh 38.37796977818664
target distance 10.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'dynamicTrap': False, 'previousTarget': array([20.,  3.]), 'currentState': array([26.69425625, 13.06609683,  3.1644243 ]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 12.088811861805654}
done in step count: 84
reward sum = 0.18319552666812794
running average episode reward sum: 0.3977367518102816
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'dynamicTrap': False, 'previousTarget': array([20.,  3.]), 'currentState': array([19.94539845,  3.84273115,  3.63513854]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.8444981477673499}
episode index:49
target Thresh 38.752315233587495
target distance 16.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'dynamicTrap': False, 'previousTarget': array([26., 12.]), 'currentState': array([10.00479135, 12.00944949,  4.76998711]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 15.995211439335028}
done in step count: 30
reward sum = 0.6701357574503
running average episode reward sum: 0.40318473192308196
{'scaleFactor': 20, 'currentTarget': array([26., 12.]), 'dynamicTrap': False, 'previousTarget': array([26., 12.]), 'currentState': array([26.53939836, 12.95205496,  0.03979779]), 'targetState': array([26, 12], dtype=int32), 'currentDistance': 1.0942391119681023}
episode index:50
target Thresh 39.12293588947188
target distance 8.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([25.89438888, 11.25157355,  5.88981516]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 10.264840631193302}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.4103780341560638
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([32.06094494,  3.4422141 ,  4.18770209]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.4463939915307953}
episode index:51
target Thresh 39.48986880821423
target distance 33.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.98577846,  8.10632421]), 'dynamicTrap': False, 'previousTarget': array([21.29527642,  8.73765188]), 'currentState': array([ 3.62252692, 13.11276901,  5.09565311]), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 59
reward sum = 0.5014747365870901
running average episode reward sum: 0.4121298938181989
{'scaleFactor': 20, 'currentTarget': array([35.,  5.]), 'dynamicTrap': False, 'previousTarget': array([35.,  5.]), 'currentState': array([34.00039273,  4.09739303,  1.67521461]), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 1.3468162545942655}
episode index:52
target Thresh 39.85315068341218
target distance 14.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.,  4.]), 'dynamicTrap': False, 'previousTarget': array([31.,  4.]), 'currentState': array([28.30849941, 17.21219669,  6.09562826]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 13.483557274672926}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.4172322466708817
{'scaleFactor': 20, 'currentTarget': array([31.,  4.]), 'dynamicTrap': False, 'previousTarget': array([31.,  4.]), 'currentState': array([31.91657383,  3.72380924,  2.70479969]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 0.957282050762308}
episode index:53
target Thresh 40.212817843556
target distance 10.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 20.]), 'dynamicTrap': False, 'previousTarget': array([37., 20.]), 'currentState': array([27.95801378,  9.86639454,  4.75700247]), 'targetState': array([37, 20], dtype=int32), 'currentDistance': 13.581144074041543}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.4251157827211233
{'scaleFactor': 20, 'currentTarget': array([37., 20.]), 'dynamicTrap': False, 'previousTarget': array([37., 20.]), 'currentState': array([36.13769315, 19.5842089 ,  3.28358566]), 'targetState': array([37, 20], dtype=int32), 'currentDistance': 0.9573167427059213}
episode index:54
target Thresh 40.56890625566143
target distance 3.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'dynamicTrap': False, 'previousTarget': array([20., 20.]), 'currentState': array([21.89702641, 23.05479826,  5.83577383]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 3.5959006664790474}
done in step count: 14
reward sum = 0.8402151506759682
running average episode reward sum: 0.43266304395666594
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'dynamicTrap': False, 'previousTarget': array([20., 20.]), 'currentState': array([19.3401804 , 20.67479688,  3.62757758]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.9437757914943555}
episode index:55
target Thresh 40.92145152886641
target distance 31.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.35916553,  9.4997782 ]), 'dynamicTrap': False, 'previousTarget': array([21.03417236,  9.8599444 ]), 'currentState': array([ 2.20605215, 15.25809808,  5.99329615]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4657932897530035
running average episode reward sum: 0.43325465548874337
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([32.01651399,  5.78501935,  5.39196459]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 1.0067082043059674}
episode index:56
target Thresh 41.270488917992054
target distance 15.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 22.]), 'dynamicTrap': False, 'previousTarget': array([28., 22.]), 'currentState': array([30.25782722,  8.66310349,  0.40699291]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 13.526662276061327}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.440742439684402
{'scaleFactor': 20, 'currentTarget': array([28., 22.]), 'dynamicTrap': False, 'previousTarget': array([28., 22.]), 'currentState': array([27.85577368, 22.68679823,  4.71620488]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 0.7017784837979387}
episode index:57
target Thresh 41.61605332706815
target distance 4.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 14.]), 'dynamicTrap': False, 'previousTarget': array([34., 14.]), 'currentState': array([30.12398484, 11.02883837,  0.87435001]), 'targetState': array([34, 14], dtype=int32), 'currentDistance': 4.883778757458674}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.44970543227605025
{'scaleFactor': 20, 'currentTarget': array([34., 14.]), 'dynamicTrap': False, 'previousTarget': array([34., 14.]), 'currentState': array([33.21364793, 13.63383627,  0.98695847]), 'targetState': array([34, 14], dtype=int32), 'currentDistance': 0.8674246093752657}
episode index:58
target Thresh 41.95817931282356
target distance 25.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.86924472,  8.4503052 ]), 'dynamicTrap': False, 'previousTarget': array([28.04848294,  8.90448546]), 'currentState': array([10.99855475, 15.07579086,  0.0327544 ]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6971304222583654
running average episode reward sum: 0.4538990761740556
{'scaleFactor': 20, 'currentTarget': array([34.,  7.]), 'dynamicTrap': False, 'previousTarget': array([34.,  7.]), 'currentState': array([33.7302016 ,  7.08718837,  1.38319353]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 0.28353657373465807}
episode index:59
target Thresh 42.296901088141965
target distance 7.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([33.62715859, 19.36346881,  1.20958107]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 8.449728239905685}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4621839257361547
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([26.8983533 , 22.65220121,  3.54751375]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.963328941901676}
episode index:60
target Thresh 42.63225252548319
target distance 26.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.88482645,  6.0641439 ]), 'dynamicTrap': False, 'previousTarget': array([25.88854382,  6.05572809]), 'currentState': array([ 8.00206613, 15.01997346,  5.61931205]), 'targetState': array([34,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.47037561934904504
running average episode reward sum: 0.4623182157953824
{'scaleFactor': 20, 'currentTarget': array([34.,  2.]), 'dynamicTrap': False, 'previousTarget': array([34.,  2.]), 'currentState': array([3.38348609e+01, 1.24755413e+00, 2.29091128e-02]), 'targetState': array([34,  2], dtype=int32), 'currentDistance': 0.7703542652135728}
episode index:61
target Thresh 42.96426716027041
target distance 12.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([37.1345286 , 14.6010029 ,  2.81907928]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 12.842308638408687}
done in step count: 38
reward sum = 0.6554222192428754
running average episode reward sum: 0.46543279649614844
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([26.98101632, 20.47222581,  1.22965869]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 1.113974240325171}
episode index:62
target Thresh 43.29297819424379
target distance 31.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.33446329, 20.68520285]), 'dynamicTrap': False, 'previousTarget': array([23.50882004, 20.40521743]), 'currentState': array([ 5.88446747, 16.02712786,  0.61378961]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.47063730610480287
running average episode reward sum: 0.46551540775977784
{'scaleFactor': 20, 'currentTarget': array([35., 23.]), 'dynamicTrap': False, 'previousTarget': array([35., 23.]), 'currentState': array([34.04579082, 23.38548603,  3.78741272]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 1.0291329488829877}
episode index:63
target Thresh 43.61841849878065
target distance 20.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.89890463,  5.00838105]), 'dynamicTrap': True, 'previousTarget': array([34.9007438 ,  4.99007438]), 'currentState': array([15.       ,  3.       ,  2.6352165], dtype=float32), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6565403530847341
running average episode reward sum: 0.46850017253048026
{'scaleFactor': 20, 'currentTarget': array([35.,  5.]), 'dynamicTrap': False, 'previousTarget': array([35.,  5.]), 'currentState': array([34.82704834,  5.7161376 ,  6.23145474]), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 0.7367260919163013}
episode index:64
target Thresh 43.94062061818265
target distance 8.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([ 2.78637349, 11.87890366,  4.66728806]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 9.553395274715987}
done in step count: 50
reward sum = 0.5345262417727213
running average episode reward sum: 0.4695159582111301
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([11.74849532,  6.48471504,  2.43962183]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9087154846094239}
episode index:65
target Thresh 44.25961677293022
target distance 32.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.60858407, 12.47621535]), 'dynamicTrap': False, 'previousTarget': array([21.65744374, 12.6857707 ]), 'currentState': array([2.01093164, 8.484722  , 5.04253447]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.625126294716529
running average episode reward sum: 0.47187369058242407
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([33.50999424, 15.22375239,  6.02214035]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 0.5386750157627336}
episode index:66
target Thresh 44.57543886290469
target distance 21.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.88580793, 22.91917853]), 'dynamicTrap': False, 'previousTarget': array([27.97736275, 22.95130299]), 'currentState': array([ 7.93821907, 21.4722167 ,  5.6058619 ]), 'targetState': array([29, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6975994209010986
running average episode reward sum: 0.4752427313334491
{'scaleFactor': 20, 'currentTarget': array([29., 23.]), 'dynamicTrap': False, 'previousTarget': array([29., 23.]), 'currentState': array([28.09620931, 23.65034515,  1.11428073]), 'targetState': array([29, 23], dtype=int32), 'currentDistance': 1.1134569695064973}
episode index:67
target Thresh 44.88811847057822
target distance 31.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.04325831, 13.15334573]), 'dynamicTrap': False, 'previousTarget': array([24.03417236, 13.1400556 ]), 'currentState': array([5.00427232, 7.02823248, 5.57220221]), 'targetState': array([36, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5691520379972788
running average episode reward sum: 0.4766237505490936
{'scaleFactor': 20, 'currentTarget': array([36., 17.]), 'dynamicTrap': False, 'previousTarget': array([36., 17.]), 'currentState': array([35.94971629, 16.64875445,  6.10101335]), 'targetState': array([36, 17], dtype=int32), 'currentDistance': 0.35482656564808857}
episode index:68
target Thresh 45.197686864172155
target distance 14.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'dynamicTrap': False, 'previousTarget': array([3., 5.]), 'currentState': array([ 7.12814897, 17.04108544,  4.44072562]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 12.729075081199062}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.48308927147487374
{'scaleFactor': 20, 'currentTarget': array([3., 5.]), 'dynamicTrap': False, 'previousTarget': array([3., 5.]), 'currentState': array([3.68830105, 5.98512137, 4.14433382]), 'targetState': array([3, 5], dtype=int32), 'currentDistance': 1.201758068446385}
episode index:69
target Thresh 45.50417500078382
target distance 17.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([22.7800844 , 20.84060618,  2.8915596 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 16.505833271958164}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.48897854265750007
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.37983913, 15.3383567 ,  4.0095157 ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.7629217643824149}
episode index:70
target Thresh 45.8076135294823
target distance 1.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 23.]), 'currentState': array([ 7.16127042, 23.07866319,  3.57808399]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.8404114652441348}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.4958957462820423
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 23.]), 'currentState': array([ 8.36697715, 22.14756583,  0.51247507]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 1.061773015835816}
episode index:71
target Thresh 46.10803279437332
target distance 15.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.00116686, 15.14951062]), 'dynamicTrap': True, 'previousTarget': array([38., 15.]), 'currentState': array([23.       ,  4.       ,  4.6055846], dtype=float32), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 18.690815771804424}
done in step count: 49
reward sum = 0.562107289432865
running average episode reward sum: 0.496815351048026
{'scaleFactor': 20, 'currentTarget': array([38., 15.]), 'dynamicTrap': False, 'previousTarget': array([38., 15.]), 'currentState': array([37.23254945, 14.63032839,  3.12474354]), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 0.8518435563220145}
episode index:72
target Thresh 46.40546283763372
target distance 7.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 16.]), 'dynamicTrap': False, 'previousTarget': array([32., 16.]), 'currentState': array([34.36265752,  9.75368986,  1.92070866]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 6.678213902787952}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5023984568557078
{'scaleFactor': 20, 'currentTarget': array([32., 16.]), 'dynamicTrap': False, 'previousTarget': array([32., 16.]), 'currentState': array([31.99482041, 15.73860108,  0.70374927]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 0.2614502265868084}
episode index:73
target Thresh 46.6999334025157
target distance 8.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 14.]), 'dynamicTrap': False, 'previousTarget': array([34., 14.]), 'currentState': array([25.69582413,  6.12335264,  2.78503829]), 'targetState': array([34, 14], dtype=int32), 'currentDistance': 11.445562921930488}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5072316987176752
{'scaleFactor': 20, 'currentTarget': array([34., 14.]), 'dynamicTrap': False, 'previousTarget': array([34., 14.]), 'currentState': array([33.30760904, 13.92856529,  3.18774482]), 'targetState': array([34, 14], dtype=int32), 'currentDistance': 0.6960662065456671}
episode index:74
target Thresh 46.99147393632111
target distance 9.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([ 7.56540293, 11.79143288,  3.08296895]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 8.898147629259347}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5121688897080924
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([10.10607337, 19.94258482,  1.15986099]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.8957685690259483}
episode index:75
target Thresh 47.2801135933463
target distance 11.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 14.]), 'currentState': array([15.02511916, 18.01854675,  1.6460039 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 11.734648291349473}
done in step count: 11
reward sum = 0.8659342642587163
running average episode reward sum: 0.5168236972679691
{'scaleFactor': 20, 'currentTarget': array([ 4., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 14.]), 'currentState': array([ 4.18570989, 14.09494655,  4.8392826 ]), 'targetState': array([ 4, 14], dtype=int32), 'currentDistance': 0.20857375103730047}
episode index:76
target Thresh 47.56588123779751
target distance 1.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'dynamicTrap': False, 'previousTarget': array([23.,  2.]), 'currentState': array([24.3005893 ,  1.64108862,  5.819089  ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 1.3492034332600973}
done in step count: 21
reward sum = 0.800592695746422
running average episode reward sum: 0.5205090089365204
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'dynamicTrap': False, 'previousTarget': array([23.,  2.]), 'currentState': array([23.55725261,  2.53476325,  4.8434446 ]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.7723355498308063}
episode index:77
target Thresh 47.84880544667733
target distance 14.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'dynamicTrap': False, 'previousTarget': array([17., 18.]), 'currentState': array([3.24592321, 6.46002378, 6.14975214]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 17.953987840475747}
done in step count: 40
reward sum = 0.45831593591383435
running average episode reward sum: 0.519711661846486
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'dynamicTrap': False, 'previousTarget': array([17., 18.]), 'currentState': array([17.59548716, 17.71441316,  1.15167581]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.6604277401165004}
episode index:78
target Thresh 48.12891451264241
target distance 15.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'dynamicTrap': False, 'previousTarget': array([20., 21.]), 'currentState': array([34.97765604, 21.03735549,  3.11985004]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 14.977702620816624}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5222182298306859
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'dynamicTrap': False, 'previousTarget': array([20., 21.]), 'currentState': array([19.12622495, 20.57244501,  0.80972984]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.972772376457764}
episode index:79
target Thresh 48.406236446832764
target distance 31.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.65216727, 13.42070481]), 'dynamicTrap': False, 'previousTarget': array([17.79307546, 13.5762039 ]), 'currentState': array([35.61052218,  7.05040036,  4.75110555]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.4337841340474818
running average episode reward sum: 0.5211128036333958
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 17.]), 'currentState': array([ 5.27156218, 16.24870654,  0.58377429]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 1.0464528271143254}
episode index:80
target Thresh 48.68079898167293
target distance 11.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'dynamicTrap': False, 'previousTarget': array([21., 21.]), 'currentState': array([11.63568095, 10.17919701,  0.82442587]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 14.31014495380553}
done in step count: 22
reward sum = 0.7202087947793344
running average episode reward sum: 0.5235707788327284
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'dynamicTrap': False, 'previousTarget': array([21., 21.]), 'currentState': array([20.31079523, 20.07014599,  0.44684435]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 1.1574245989332559}
episode index:81
target Thresh 48.952629573645176
target distance 29.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.16970218, 16.94199902]), 'dynamicTrap': False, 'previousTarget': array([17.74981351, 16.18111808]), 'currentState': array([34.4588066 ,  8.84825702,  3.51317763]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6029739484575041
running average episode reward sum: 0.5245391101696159
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.11616082, 20.45299615,  3.92159412]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.5592017048085656}
episode index:82
target Thresh 49.221755406035236
target distance 13.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'dynamicTrap': False, 'previousTarget': array([14., 19.]), 'currentState': array([1.8400487 , 5.53154218, 5.39335823]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 18.14562678923029}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5261986635529216
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'dynamicTrap': False, 'previousTarget': array([14., 19.]), 'currentState': array([13.07587811, 19.85085031,  0.2154354 ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 1.2561638148720156}
episode index:83
target Thresh 49.48820339165063
target distance 7.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([12.27405942,  6.03399931,  3.91244751]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 6.084476529529496}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5300707957855639
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([7.88236123, 3.47137975, 3.82832866]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.0003800334205366}
episode index:84
target Thresh 49.75200017551195
target distance 4.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'dynamicTrap': False, 'previousTarget': array([25., 14.]), 'currentState': array([27.24630655, 12.17892831,  3.13610637]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 2.891746046761111}
done in step count: 33
reward sum = 0.5715520225064696
running average episode reward sum: 0.5305588102175746
{'scaleFactor': 20, 'currentTarget': array([25., 14.]), 'dynamicTrap': False, 'previousTarget': array([25., 14.]), 'currentState': array([25.97676462, 13.50099333,  3.82996507]), 'targetState': array([25, 14], dtype=int32), 'currentDistance': 1.096848563566972}
episode index:85
target Thresh 50.01317213751742
target distance 23.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.78103983, 16.26796109]), 'dynamicTrap': False, 'previousTarget': array([26.92481176, 16.26740767]), 'currentState': array([ 6.8499784 , 17.92711457,  4.6038425 ]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5037149640585589
running average episode reward sum: 0.5302466724715394
{'scaleFactor': 20, 'currentTarget': array([30., 16.]), 'dynamicTrap': False, 'previousTarget': array([30., 16.]), 'currentState': array([29.22630878, 16.96729627,  3.94606203]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 1.2386525630839613}
episode index:86
target Thresh 50.27174539508087
target distance 6.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'dynamicTrap': False, 'previousTarget': array([21.,  9.]), 'currentState': array([26.49241286, 11.17021925,  6.11362928]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 5.905628729091751}
done in step count: 49
reward sum = 0.455315230939084
running average episode reward sum: 0.5293853915343848
{'scaleFactor': 20, 'currentTarget': array([21.,  9.]), 'dynamicTrap': False, 'previousTarget': array([21.,  9.]), 'currentState': array([21.20937409,  8.25433946,  3.77020489]), 'targetState': array([21,  9], dtype=int32), 'currentDistance': 0.7744980009965193}
episode index:87
target Thresh 50.52774580574355
target distance 14.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([37.06518321, 22.15230155,  2.17639768]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 15.817852348602337}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5332417599575051
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([29.80102358,  7.05019728,  3.48711441]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 0.9704209501283219}
episode index:88
target Thresh 50.78119896975985
target distance 17.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'dynamicTrap': False, 'previousTarget': array([20., 22.]), 'currentState': array([36.09454295, 21.52511716,  5.08910978]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 16.101547331278802}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5361672939315069
{'scaleFactor': 20, 'currentTarget': array([20., 22.]), 'dynamicTrap': False, 'previousTarget': array([20., 22.]), 'currentState': array([20.39907637, 21.27912692,  4.05919044]), 'targetState': array([20, 22], dtype=int32), 'currentDistance': 0.823965990307602}
episode index:89
target Thresh 51.032130232657394
target distance 33.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.53848471, 12.5473956 ]), 'dynamicTrap': False, 'previousTarget': array([23.96336993, 12.79009879]), 'currentState': array([ 5.56125541, 13.50149696,  0.70093554]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5639878203144263
running average episode reward sum: 0.536476410891317
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([37.31902991, 12.69906223,  4.5239503 ]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 0.7684192130221895}
episode index:90
target Thresh 51.28056468777158
target distance 19.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([8.97845989, 1.25659226, 6.12848055]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 19.973278491032882}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5403215588124688
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([12.65654449, 20.41608257,  1.9059101 ]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.8786411240724872}
episode index:91
target Thresh 51.52652717875493
target distance 5.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 18.]), 'dynamicTrap': False, 'previousTarget': array([32., 18.]), 'currentState': array([36.47146266, 20.15648055,  3.928424  ]), 'targetState': array([32, 18], dtype=int32), 'currentDistance': 4.96431129980545}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5440831165614217
{'scaleFactor': 20, 'currentTarget': array([32., 18.]), 'dynamicTrap': False, 'previousTarget': array([32., 18.]), 'currentState': array([31.22149137, 17.34884744,  2.69510673]), 'targetState': array([32, 18], dtype=int32), 'currentDistance': 1.0149262744717287}
episode index:92
target Thresh 51.77004230206154
target distance 25.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.24268556,  2.776928  ]), 'dynamicTrap': False, 'previousTarget': array([11.55225396,  3.33254095]), 'currentState': array([29.9155546 ,  6.37945816,  3.72964489]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.680673875639088
running average episode reward sum: 0.5455518344009664
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'dynamicTrap': False, 'previousTarget': array([6., 2.]), 'currentState': array([6.87324893, 2.28285009, 3.9969826 ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.9179149514236237}
episode index:93
target Thresh 52.01113440940665
target distance 27.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.47825527,  5.68323482]), 'dynamicTrap': False, 'previousTarget': array([18.12232531,  5.79136948]), 'currentState': array([36.32450185,  8.15841093,  2.03732729]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5476172443901931
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'dynamicTrap': False, 'previousTarget': array([11.,  5.]), 'currentState': array([11.83272954,  5.70822399,  4.13822439]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.0931695720753707}
episode index:94
target Thresh 52.24982761020191
target distance 12.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'dynamicTrap': False, 'previousTarget': array([19.,  9.]), 'currentState': array([29.46344494,  7.31342399,  4.57180572]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 10.598500771593214}
done in step count: 44
reward sum = 0.4936330181515066
running average episode reward sum: 0.5470489893771543
{'scaleFactor': 20, 'currentTarget': array([19.0870446 ,  9.33063206]), 'dynamicTrap': True, 'previousTarget': array([19.,  9.]), 'currentState': array([20.06019272,  9.15130694,  1.24731471]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.9895325912325192}
episode index:95
target Thresh 52.48614577396632
target distance 6.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([8.01183042, 4.7334452 , 1.1264078 ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 6.606216469879723}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5503095035986557
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([12.36919499,  9.17671724,  2.25832945]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.9022745982826184}
episode index:96
target Thresh 52.72011253271318
target distance 25.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.02126665, 19.85880838]), 'dynamicTrap': False, 'previousTarget': array([27.74881264, 19.84018998]), 'currentState': array([ 8.31233355, 23.2585078 ,  4.84297681]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5532394444012478
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([32.75959976, 18.45941726,  5.50890426]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 0.5916265524754749}
episode index:97
target Thresh 52.95175128331335
target distance 20.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.79299114, 21.58520934]), 'dynamicTrap': False, 'previousTarget': array([13.20729355, 20.2384301 ]), 'currentState': array([4.86206049, 3.6900011 , 2.64084083]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5552183489923739
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'dynamicTrap': False, 'previousTarget': array([14., 22.]), 'currentState': array([13.03349986, 21.85961822,  2.82018527]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.976641983284683}
episode index:98
target Thresh 53.1810851898349
target distance 2.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  3.]), 'dynamicTrap': False, 'previousTarget': array([35.,  3.]), 'currentState': array([33.55882511,  3.8131525 ,  0.63054376]), 'targetState': array([35,  3], dtype=int32), 'currentDistance': 1.654751360298835}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5570070865748186
{'scaleFactor': 20, 'currentTarget': array([35.,  3.]), 'dynamicTrap': False, 'previousTarget': array([35.,  3.]), 'currentState': array([35.03915407,  2.52624074,  4.8455294 ]), 'targetState': array([35,  3], dtype=int32), 'currentDistance': 0.4753744626358849}
episode index:99
target Thresh 53.408137185859616
target distance 8.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  5.]), 'dynamicTrap': False, 'previousTarget': array([34.,  5.]), 'currentState': array([30.51207465, 13.0379967 ,  4.22565842]), 'targetState': array([34,  5], dtype=int32), 'currentDistance': 8.762135258867481}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5604808364591585
{'scaleFactor': 20, 'currentTarget': array([34.,  5.]), 'dynamicTrap': False, 'previousTarget': array([34.,  5.]), 'currentState': array([34.34170222,  5.74347966,  5.11434221]), 'targetState': array([34,  5], dtype=int32), 'currentDistance': 0.8182434918381859}
episode index:100
target Thresh 53.63292997677631
target distance 11.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.97451407, 13.95724923]), 'dynamicTrap': True, 'previousTarget': array([26., 14.]), 'currentState': array([15.      ,  3.      ,  4.075804], dtype=float32), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 15.508103358267835}
done in step count: 44
reward sum = 0.458275469012629
running average episode reward sum: 0.5594689021280047
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'dynamicTrap': False, 'previousTarget': array([26., 14.]), 'currentState': array([25.47624364, 13.89065606,  4.50367871]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.5350484240621262}
episode index:101
target Thresh 53.85548604205138
target distance 9.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'dynamicTrap': False, 'previousTarget': array([11.,  9.]), 'currentState': array([2.92842787, 3.82187725, 0.76411396]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 9.589746188416798}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5630304295034941
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'dynamicTrap': False, 'previousTarget': array([11.,  9.]), 'currentState': array([11.14240042,  8.07253946,  1.06933647]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.9383287986955012}
episode index:102
target Thresh 54.075827637476834
target distance 31.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.23014807, 13.40748119]), 'dynamicTrap': False, 'previousTarget': array([13.04149382, 13.28764556]), 'currentState': array([31.18906648, 12.12624122,  2.30939555]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5215599345517704
running average episode reward sum: 0.5626278033389143
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.36298144, 13.91952175,  2.50438128]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.3717960093213497}
episode index:103
target Thresh 54.293976797395835
target distance 7.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'dynamicTrap': False, 'previousTarget': array([10., 14.]), 'currentState': array([ 9.01400302, 21.07375504,  1.69839263]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 7.142142567962397}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.566270614358742
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'dynamicTrap': False, 'previousTarget': array([10., 14.]), 'currentState': array([ 9.74575833, 14.44567008,  4.39362317]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.5130893215000505}
episode index:104
target Thresh 54.50995533690616
target distance 17.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 19.]), 'currentState': array([8.52642535, 1.1551843 , 4.60807973]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 19.0008335415085}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5683602098487276
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 19.]), 'currentState': array([ 1.31070517, 18.3371757 ,  1.92092919]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.9562758053397078}
episode index:105
target Thresh 54.72378485404175
target distance 17.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.96140138,  8.51512191]), 'dynamicTrap': False, 'previousTarget': array([13.85786438,  8.85786438]), 'currentState': array([29.20544275, 21.46190539,  5.027354  ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5742445952844233
running average episode reward sum: 0.5684157229188757
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([11.,  6.]), 'currentState': array([11.24169389,  5.06894321,  0.99326807]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.9619161480766055}
episode index:106
target Thresh 54.93548673193251
target distance 13.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'dynamicTrap': False, 'previousTarget': array([12.,  6.]), 'currentState': array([13.28362884, 17.17714389,  4.16924822]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 11.250611028797392}
done in step count: 29
reward sum = 0.6622133385903126
running average episode reward sum: 0.5692923361494499
{'scaleFactor': 20, 'currentTarget': array([12.,  6.]), 'dynamicTrap': False, 'previousTarget': array([12.,  6.]), 'currentState': array([11.84051625,  6.42780058,  4.21912433]), 'targetState': array([12,  6], dtype=int32), 'currentDistance': 0.4565615010470828}
episode index:107
target Thresh 55.145082140942634
target distance 27.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.06650238,  8.48995872]), 'dynamicTrap': False, 'previousTarget': array([16.05464491,  8.52256629]), 'currentState': array([36.01860105,  9.87334533,  2.72661817]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5966412760264239
running average episode reward sum: 0.5695455670742366
{'scaleFactor': 20, 'currentTarget': array([9., 8.]), 'dynamicTrap': False, 'previousTarget': array([9., 8.]), 'currentState': array([9.35127681, 7.45300694, 3.25331992]), 'targetState': array([9, 8], dtype=int32), 'currentDistance': 0.6500744590119613}
episode index:108
target Thresh 55.3525920407877
target distance 3.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 23.]), 'dynamicTrap': False, 'previousTarget': array([29., 23.]), 'currentState': array([2.71065113e+01, 2.32991041e+01, 2.63004303e-02]), 'targetState': array([29, 23], dtype=int32), 'currentDistance': 1.9169671144370997}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5733121215047483
{'scaleFactor': 20, 'currentTarget': array([29., 23.]), 'dynamicTrap': False, 'previousTarget': array([29., 23.]), 'currentState': array([28.82866531, 23.28557632,  5.85506675]), 'targetState': array([29, 23], dtype=int32), 'currentDistance': 0.33303064594150017}
episode index:109
target Thresh 55.55803718263061
target distance 16.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'dynamicTrap': False, 'previousTarget': array([7., 7.]), 'currentState': array([ 7.91981268, 21.54012949,  4.33806407]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 14.569194239847281}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5764048953772837
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'dynamicTrap': False, 'previousTarget': array([7., 7.]), 'currentState': array([7.0737113 , 6.84811598, 4.27074879]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.16882568190654523}
episode index:110
target Thresh 55.761438111156764
target distance 27.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.70171633, 12.42999421]), 'dynamicTrap': False, 'previousTarget': array([21.27623097, 12.87723068]), 'currentState': array([ 4.60105219, 20.93681253,  6.26874603]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7024335482541532
running average episode reward sum: 0.5775402886464447
{'scaleFactor': 20, 'currentTarget': array([30.,  9.]), 'dynamicTrap': False, 'previousTarget': array([30.,  9.]), 'currentState': array([30.13685369,  9.85472672,  5.88963865]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 0.8656134807736098}
episode index:111
target Thresh 55.9628151666285
target distance 13.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.17481894, 11.03689891]), 'dynamicTrap': True, 'previousTarget': array([ 7., 11.]), 'currentState': array([20.       ,  5.       ,  3.4224348], dtype=float32), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 14.174957411160443}
done in step count: 18
reward sum = 0.8146137614500875
running average episode reward sum: 0.5796570160821916
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 11.]), 'currentState': array([ 7.4988964 , 11.95227547,  5.81217729]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0750470603148587}
episode index:112
target Thresh 56.1621884869192
target distance 19.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.80425699,  7.01739996]), 'dynamicTrap': False, 'previousTarget': array([17.85786438,  7.85786438]), 'currentState': array([30.55590942, 21.53952293,  4.46027017]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 32
reward sum = 0.6852956135417502
running average episode reward sum: 0.5805918709269664
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.,  3.]), 'currentState': array([13.88931995,  3.48031348,  3.61646903]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 1.0107378551602504}
episode index:113
target Thresh 56.35957800952702
target distance 8.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([33.98798171, 20.97685925,  1.88618025]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 8.240203291325463}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5836749716022297
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([26.36441924, 23.84638   ,  2.35745195]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.9214990404483716}
episode index:114
target Thresh 56.55500347356872
target distance 5.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([ 7.55180911, 19.12023003,  4.94031111]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 3.9272834800332457}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5863072316032202
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([10.58209966, 21.03490574,  0.7997528 ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.4193555852209912}
episode index:115
target Thresh 56.74848442175355
target distance 10.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 3.51780729, 14.74544242,  1.06820905]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 8.619686853526781}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5889712921433538
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 5.98131396, 22.00918944,  1.10874838]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.9909867429585986}
episode index:116
target Thresh 56.9400402023376
target distance 15.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 17.]), 'dynamicTrap': False, 'previousTarget': array([31., 17.]), 'currentState': array([15.31067592, 17.4018779 ,  2.84157622]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 15.694470232930604}
done in step count: 47
reward sum = 0.5240430996359327
running average episode reward sum: 0.5884163503270511
{'scaleFactor': 20, 'currentTarget': array([31., 17.]), 'dynamicTrap': False, 'previousTarget': array([31., 17.]), 'currentState': array([31.26022561, 17.21691848,  1.54526271]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 0.33877867888761476}
episode index:117
target Thresh 57.12968997105851
target distance 9.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'dynamicTrap': False, 'previousTarget': array([25., 18.]), 'currentState': array([17.7521716 , 21.68148154,  0.6531431 ]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 8.129226460955389}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5907920237375759
{'scaleFactor': 20, 'currentTarget': array([25., 18.]), 'dynamicTrap': False, 'previousTarget': array([25., 18.]), 'currentState': array([24.08146949, 17.68658409,  0.21148186]), 'targetState': array([25, 18], dtype=int32), 'currentDistance': 0.9705296632250848}
episode index:118
target Thresh 57.31745269305122
target distance 13.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([35.122503  ,  5.43610135,  1.10928464]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 14.729012623475947}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.593201511126327
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([26.46551571, 16.1987601 ,  2.5575988 ]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.9266554088175182}
episode index:119
target Thresh 57.50334714474439
target distance 5.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 13.]), 'currentState': array([7.2987562 , 9.86839159, 1.81446153]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 4.5484902645079215}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5962631319502744
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.97209939, 12.15920538,  3.11596614]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.8412574132248697}
episode index:120
target Thresh 57.68739191573811
target distance 26.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.31108494, 11.43249293]), 'dynamicTrap': False, 'previousTarget': array([21.609422  , 11.48199646]), 'currentState': array([3.60801517, 2.12650988, 3.12641585]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 51
reward sum = 0.547144913026904
running average episode reward sum: 0.5958571962566926
{'scaleFactor': 20, 'currentTarget': array([30., 16.]), 'dynamicTrap': False, 'previousTarget': array([30., 16.]), 'currentState': array([29.26296922, 15.85727326,  5.84446578]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 0.7507231834627924}
episode index:121
target Thresh 57.86960541066285
target distance 26.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.63016505, 17.39576465]), 'dynamicTrap': False, 'previousTarget': array([16.51217609, 17.49719013]), 'currentState': array([36.06923119, 12.69228642,  4.99446496]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5976772761037462
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'dynamicTrap': False, 'previousTarget': array([10., 19.]), 'currentState': array([10.7281229 , 19.19277342,  2.24457511]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.7532095003224082}
episode index:122
target Thresh 58.05000585101993
target distance 25.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.27424694, 20.35870087]), 'dynamicTrap': False, 'previousTarget': array([16.38838649, 19.9223227 ]), 'currentState': array([35.05286396, 17.3911583 ,  3.17853725]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5860655963249807
running average episode reward sum: 0.5975828722031059
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([10.06311519, 21.61318298,  1.22983782]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 1.1197082292135379}
episode index:123
target Thresh 58.22861127700375
target distance 6.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'dynamicTrap': False, 'previousTarget': array([8., 6.]), 'currentState': array([3.62351671, 3.14620822, 0.41786912]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 5.224723296174879}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5999119205862755
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'dynamicTrap': False, 'previousTarget': array([8., 6.]), 'currentState': array([8.35284507, 5.31570638, 3.0849718 ]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.7699073958583631}
episode index:124
target Thresh 58.405439549305726
target distance 19.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([ 5.0573165 , 20.68753613,  0.11331313]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 17.793902141813998}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6022037041953143
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([6.3265247 , 3.98895975, 4.56470354]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 1.1964992149095497}
episode index:125
target Thresh 58.58050835090044
target distance 19.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.57753879,  6.26764537]), 'dynamicTrap': False, 'previousTarget': array([27.30852571,  6.97927459]), 'currentState': array([11.68272287, 16.97115858,  5.25604725]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6855292383864182
running average episode reward sum: 0.6028650179587358
{'scaleFactor': 20, 'currentTarget': array([29.,  6.]), 'dynamicTrap': False, 'previousTarget': array([29.,  6.]), 'currentState': array([28.56801346,  6.8293309 ,  0.24253531]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 0.9350947062956402}
episode index:126
target Thresh 58.75383518881396
target distance 4.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'dynamicTrap': False, 'previousTarget': array([19.,  8.]), 'currentState': array([14.92310231, 10.04468169,  2.69783037]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 4.56090100338985}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6056817974236277
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'dynamicTrap': False, 'previousTarget': array([19.,  8.]), 'currentState': array([18.50426859,  7.61563356,  5.63214681]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 0.6272855762476021}
episode index:127
target Thresh 58.92543739587451
target distance 9.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'dynamicTrap': False, 'previousTarget': array([25., 12.]), 'currentState': array([32.19194925, 14.08860374,  2.31923866]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 7.489085361459019}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6084545647093806
{'scaleFactor': 20, 'currentTarget': array([25., 12.]), 'dynamicTrap': False, 'previousTarget': array([25., 12.]), 'currentState': array([25.44814843, 12.51173209,  2.82673848]), 'targetState': array([25, 12], dtype=int32), 'currentDistance': 0.6802255079412914}
episode index:128
target Thresh 59.0953321324458
target distance 17.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.74890409, 22.08775169]), 'dynamicTrap': False, 'previousTarget': array([16.76756726, 20.99675711]), 'currentState': array([28.43921238,  6.62954754,  3.76819956]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5576596428755654
running average episode reward sum: 0.6080608056253975
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'dynamicTrap': False, 'previousTarget': array([15., 23.]), 'currentState': array([14.92942062, 22.24424331,  0.87519301]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.7590452034223354}
episode index:129
target Thresh 59.26353638814306
target distance 13.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'dynamicTrap': False, 'previousTarget': array([15., 23.]), 'currentState': array([28.66346664, 17.06117212,  1.10194081]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 14.898321957075575}
done in step count: 19
reward sum = 0.7707908925959951
running average episode reward sum: 0.6093125755251714
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'dynamicTrap': False, 'previousTarget': array([15., 23.]), 'currentState': array([15.00815862, 22.67295759,  1.6145467 ]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.32714415889401466}
episode index:130
target Thresh 59.43006698353203
target distance 26.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.29117102, 15.23224121]), 'dynamicTrap': False, 'previousTarget': array([23.41934502, 15.20720018]), 'currentState': array([ 4.84686229, 22.96577052,  4.37149882]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6097169225897425
{'scaleFactor': 20, 'currentTarget': array([31., 12.]), 'dynamicTrap': False, 'previousTarget': array([31., 12.]), 'currentState': array([31.20612019, 12.26005605,  3.01089936]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 0.3318353238691178}
episode index:131
target Thresh 59.59494057181104
target distance 22.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.63363077,  8.19864474]), 'dynamicTrap': True, 'previousTarget': array([10.17429997,  8.77104998]), 'currentState': array([26.       , 21.       ,  2.6065912], dtype=float32), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6135062315302364
running average episode reward sum: 0.6097456294756554
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([4.74544671, 4.97982259, 2.71682517]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 1.2311551946984938}
episode index:132
target Thresh 59.75817364047629
target distance 11.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([27.00056881,  9.00141621,  4.95651555]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 13.038685341381386}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6117589782991388
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([37.19747701,  1.30426775,  5.86672974]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 1.0621141691343323}
episode index:133
target Thresh 59.919782512970706
target distance 9.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.88490835, 12.88262094]), 'dynamicTrap': True, 'previousTarget': array([25., 13.]), 'currentState': array([16.       , 10.       ,  0.8942374], dtype=float32), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 9.340829717076636}
done in step count: 45
reward sum = 0.3345879544755361
running average episode reward sum: 0.6096905378228433
{'scaleFactor': 20, 'currentTarget': array([25., 13.]), 'dynamicTrap': False, 'previousTarget': array([25., 13.]), 'currentState': array([25.94121222, 12.27559994,  2.10549594]), 'targetState': array([25, 13], dtype=int32), 'currentDistance': 1.1877019320418964}
episode index:134
target Thresh 60.079783350316184
target distance 25.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.00091523, 13.65567378]), 'dynamicTrap': False, 'previousTarget': array([31.25928039, 13.60740149]), 'currentState': array([11.72168405, 18.97650502,  4.05815399]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6107648248552936
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([36.36079779, 12.81362209,  3.55126571]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 1.034678873096317}
episode index:135
target Thresh 60.238192152729795
target distance 17.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.82520853, 12.9034868 ]), 'dynamicTrap': True, 'previousTarget': array([32., 13.]), 'currentState': array([15.        , 13.        ,  0.60350156], dtype=float32), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 16.825485335586823}
done in step count: 13
reward sum = 0.8478200229989679
running average episode reward sum: 0.6125078777828206
{'scaleFactor': 20, 'currentTarget': array([32., 13.]), 'dynamicTrap': False, 'previousTarget': array([32., 13.]), 'currentState': array([31.96669047, 12.38889953,  0.97996236]), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 0.6120076059558188}
episode index:136
target Thresh 60.3950247612238
target distance 25.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.30769536, 3.41981208]), 'dynamicTrap': False, 'previousTarget': array([9.95151706, 3.90448546]), 'currentState': array([27.30253384,  9.68049183,  4.26099503]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.755369235704461
running average episode reward sum: 0.6135506614172852
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([3.51165355, 2.41260214, 4.80871348]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6393143043882542}
episode index:137
target Thresh 60.55029685918974
target distance 21.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.8305109, 15.0892385]), 'dynamicTrap': False, 'previousTarget': array([13.28336929, 14.28013989]), 'currentState': array([29.73660411,  6.18015179,  3.32077813]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = -0.11440011990637039
running average episode reward sum: 0.6082756557555196
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'dynamicTrap': False, 'previousTarget': array([10., 16.]), 'currentState': array([ 9.65611433, 15.18639123,  0.91766225]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.8832986926576346}
episode index:138
target Thresh 60.704023973966805
target distance 10.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([30.91905865,  8.10147461,  5.701612  ]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 10.08283645140695}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6103408543059023
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([28.72992503, 17.51442206,  0.68971596]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.5556315556382733}
episode index:139
target Thresh 60.85622147839457
target distance 10.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'dynamicTrap': False, 'previousTarget': array([20.,  2.]), 'currentState': array([27.15355328, 12.44541838,  3.66718864]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 12.660177311442721}
done in step count: 9
reward sum = 0.9043820750088044
running average episode reward sum: 0.6124411487394944
{'scaleFactor': 20, 'currentTarget': array([20.13353648,  2.0233492 ]), 'dynamicTrap': True, 'previousTarget': array([20.,  2.]), 'currentState': array([21.10681324,  1.18740039,  4.59309417]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 1.2829957286003322}
episode index:140
target Thresh 61.006904592350324
target distance 19.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.04952341, 21.645171  ]), 'dynamicTrap': False, 'previousTarget': array([14.02072541, 21.30852571]), 'currentState': array([2.56328067, 5.27244283, 1.91636217]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6140759150135684
{'scaleFactor': 20, 'currentTarget': array([15., 23.]), 'dynamicTrap': False, 'previousTarget': array([15., 23.]), 'currentState': array([14.04372191, 23.1434873 ,  0.45916175]), 'targetState': array([15, 23], dtype=int32), 'currentDistance': 0.9669831379307362}
episode index:141
target Thresh 61.15608838427102
target distance 11.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  4.]), 'dynamicTrap': False, 'previousTarget': array([35.,  4.]), 'currentState': array([30.67276598, 15.92710256,  6.26832933]), 'targetState': array([35,  4], dtype=int32), 'currentDistance': 12.687818159959642}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6156876564105428
{'scaleFactor': 20, 'currentTarget': array([35.,  4.]), 'dynamicTrap': False, 'previousTarget': array([35.,  4.]), 'currentState': array([35.9084683 ,  3.30023109,  3.17329055]), 'targetState': array([35,  4], dtype=int32), 'currentDistance': 1.1467306464110407}
episode index:142
target Thresh 61.30378777266018
target distance 12.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'dynamicTrap': False, 'previousTarget': array([24., 14.]), 'currentState': array([34.75739197,  1.79110877,  4.30491352]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 16.27201607091456}
done in step count: 51
reward sum = 0.4636638970596557
running average episode reward sum: 0.6146245531982989
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'dynamicTrap': False, 'previousTarget': array([24., 14.]), 'currentState': array([24.92837574, 13.03375149,  2.43401783]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.3399692951720998}
episode index:143
target Thresh 61.450017527579725
target distance 2.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  8.]), 'dynamicTrap': False, 'previousTarget': array([32.,  8.]), 'currentState': array([33.34367633, 10.27976975,  5.35353925]), 'targetState': array([32,  8], dtype=int32), 'currentDistance': 2.6462834709146668}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6167001969086138
{'scaleFactor': 20, 'currentTarget': array([32.,  8.]), 'dynamicTrap': False, 'previousTarget': array([32.,  8.]), 'currentState': array([31.96379825,  8.91162921,  2.93912705]), 'targetState': array([32,  8], dtype=int32), 'currentDistance': 0.9123477284835445}
episode index:144
target Thresh 61.594792272126995
target distance 20.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'dynamicTrap': False, 'previousTarget': array([11.59715  , 14.8507125]), 'currentState': array([29.46401178, 11.006839  ,  3.27755868]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 18.890872555888052}
done in step count: 43
reward sum = 0.5508629327539558
running average episode reward sum: 0.6162461468109954
{'scaleFactor': 20, 'currentTarget': array([11., 15.]), 'dynamicTrap': False, 'previousTarget': array([11., 15.]), 'currentState': array([11.55577006, 15.23511132,  5.39873279]), 'targetState': array([11, 15], dtype=int32), 'currentDistance': 0.6034547983291222}
episode index:145
target Thresh 61.738126483897105
target distance 21.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.57243434, 1.9915308 ]), 'dynamicTrap': False, 'previousTarget': array([7., 2.]), 'currentState': array([26.57024576,  1.69566199,  1.62618655]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7838132836436554
running average episode reward sum: 0.6173938669262876
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'dynamicTrap': False, 'previousTarget': array([6., 2.]), 'currentState': array([6.55315158, 1.16873411, 3.79351718]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.9984886792666239}
episode index:146
target Thresh 61.88003449643067
target distance 13.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.03665183, 12.19627405]), 'dynamicTrap': True, 'previousTarget': array([37., 12.]), 'currentState': array([24.      , 14.      ,  4.428777], dtype=float32), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 13.160840332201362}
done in step count: 17
reward sum = 0.8329431933839267
running average episode reward sum: 0.6188601888749791
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([36.68543201, 11.59922625,  3.07294508]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 0.5094826981731315}
episode index:147
target Thresh 62.02053050064719
target distance 9.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 13.]), 'currentState': array([4.99801423, 5.54401041, 1.70886633]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 7.522487162153545}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6211692146933914
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 13.]), 'currentState': array([ 3.60994686, 12.92751285,  1.05926085]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.39673144046750797}
episode index:148
target Thresh 62.15962854626419
target distance 11.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 20.]), 'dynamicTrap': False, 'previousTarget': array([30., 20.]), 'currentState': array([19.18618567, 10.64022084,  5.78952551]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 14.301889607943505}
done in step count: 22
reward sum = 0.7722265995390458
running average episode reward sum: 0.6221830226453756
{'scaleFactor': 20, 'currentTarget': array([30., 20.]), 'dynamicTrap': False, 'previousTarget': array([30., 20.]), 'currentState': array([29.73726747, 20.81749211,  4.95637122]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 0.8586744047643894}
episode index:149
target Thresh 62.29734254320212
target distance 24.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.14767194, 19.92524231]), 'dynamicTrap': False, 'previousTarget': array([27.58583933, 19.52566297]), 'currentState': array([10.44256035, 10.62314451,  4.95658779]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6548660169964367
running average episode reward sum: 0.6224009092743826
{'scaleFactor': 20, 'currentTarget': array([34., 23.]), 'dynamicTrap': False, 'previousTarget': array([34., 23.]), 'currentState': array([33.41188829, 23.07796445,  0.0692146 ]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 0.5932569770615133}
episode index:150
target Thresh 62.43368626297547
target distance 10.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'dynamicTrap': False, 'previousTarget': array([24., 21.]), 'currentState': array([32.38902758, 17.51301341,  2.4251523 ]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 9.084869794092004}
done in step count: 12
reward sum = 0.8402429765738457
running average episode reward sum: 0.6238435719717301
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'dynamicTrap': False, 'previousTarget': array([24., 21.]), 'currentState': array([24.57723432, 20.41253117,  1.95498618]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 0.8236012928113428}
episode index:151
target Thresh 62.56867334006981
target distance 26.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.30556765, 15.23141021]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 14.77694787]), 'currentState': array([10.56835074, 11.99996024,  6.18916351]), 'targetState': array([35, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7825954924537468
running average episode reward sum: 0.6248879925012171
{'scaleFactor': 20, 'currentTarget': array([35., 16.]), 'dynamicTrap': False, 'previousTarget': array([35., 16.]), 'currentState': array([34.1377746 , 15.01507651,  5.32912284]), 'targetState': array([35, 16], dtype=int32), 'currentDistance': 1.3090099019644619}
episode index:152
target Thresh 62.70231727330534
target distance 12.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.76171448, 8.11461441]), 'dynamicTrap': True, 'previousTarget': array([8., 7.]), 'currentState': array([20.       , 18.       ,  3.3066707], dtype=float32), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 14.967294673993205}
done in step count: 29
reward sum = 0.5960063438672967
running average episode reward sum: 0.6246992235558974
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'dynamicTrap': False, 'previousTarget': array([8., 7.]), 'currentState': array([8.83783775, 7.60976274, 3.90055672]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 1.0362348685835028}
episode index:153
target Thresh 62.83463142718677
target distance 17.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'dynamicTrap': False, 'previousTarget': array([22., 18.]), 'currentState': array([ 6.64277826, 14.20836416,  5.39934808]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 15.818367864980283}
done in step count: 41
reward sum = 0.40770520913355723
running average episode reward sum: 0.6232901715141939
{'scaleFactor': 20, 'currentTarget': array([22., 18.]), 'dynamicTrap': False, 'previousTarget': array([22., 18.]), 'currentState': array([22.9535167 , 18.16963278,  1.01380118]), 'targetState': array([22, 18], dtype=int32), 'currentDistance': 0.9684881936707994}
episode index:154
target Thresh 62.96562903323974
target distance 26.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.29136402,  9.10068383]), 'dynamicTrap': False, 'previousTarget': array([25.64012894,  8.77694787]), 'currentState': array([7.646461  , 5.34864631, 1.21867245]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5529725297356085
running average episode reward sum: 0.6228365093091708
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([32.74908676,  9.56793513,  3.65643403]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 0.8647606729828897}
episode index:155
target Thresh 63.095323191334025
target distance 6.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 20.]), 'currentState': array([ 7.49120151, 22.15453138,  4.58165908]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 4.9812545043486205}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6250638329674454
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.90632195, 19.75597188,  3.48632991]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.9385995939100166}
episode index:156
target Thresh 63.22372687099351
target distance 24.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.32210583, 8.41095501]), 'dynamicTrap': False, 'previousTarget': array([9.15444247, 8.48069469]), 'currentState': array([28.01493901,  4.91920451,  3.11154127]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6263447552022743
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([5.36304915, 8.61880529, 2.22615778]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.526416268190287}
episode index:157
target Thresh 63.350852912693156
target distance 11.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'dynamicTrap': False, 'previousTarget': array([2., 9.]), 'currentState': array([11.31727796,  2.02888975,  4.13442588]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 11.636496324100337}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6282207041847151
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'dynamicTrap': False, 'previousTarget': array([2., 9.]), 'currentState': array([2.71292551, 8.61369788, 1.75435835]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.8108588718980915}
episode index:158
target Thresh 63.47671402914309
target distance 12.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'dynamicTrap': False, 'previousTarget': array([10.,  5.]), 'currentState': array([ 2.6044203 , 16.77084461,  3.5136764 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 13.901344608228456}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6300150220670983
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'dynamicTrap': False, 'previousTarget': array([10.,  5.]), 'currentState': array([9.58316366, 5.96068435, 4.6071372 ]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 1.0472186726824666}
episode index:159
target Thresh 63.60132280655983
target distance 3.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([20.99478228,  1.99459799,  2.93434238]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 3.1590432240886166}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6322030531791788
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([18.8568975 ,  2.63942907,  3.68535718]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.9296691420737158}
episode index:160
target Thresh 63.724691705924954
target distance 5.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([32.32852817, 10.65059302,  0.36432743]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 4.2843072753872935}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6342427609855193
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([34.44824016, 14.60270019,  5.88384108]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 0.8171208187587075}
episode index:161
target Thresh 63.84683306423122
target distance 21.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.53814116, 12.88152209]), 'dynamicTrap': False, 'previousTarget': array([ 6.09009055, 12.89618185]), 'currentState': array([26.47907277, 11.34553826,  1.35297173]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5842880916761712
running average episode reward sum: 0.6339343988292888
{'scaleFactor': 20, 'currentTarget': array([ 5., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 13.]), 'currentState': array([ 4.75828575, 13.11064359,  2.99975311]), 'targetState': array([ 5, 13], dtype=int32), 'currentDistance': 0.26583413021474095}
episode index:162
target Thresh 63.967759095716225
target distance 20.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.04714887, 9.34794373]), 'dynamicTrap': False, 'previousTarget': array([9.47568183, 9.361625  ]), 'currentState': array([25.75123589, 20.34673806,  3.03458226]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7121905629953127
running average episode reward sum: 0.6344144979959515
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'dynamicTrap': False, 'previousTarget': array([7., 8.]), 'currentState': array([7.19569151, 7.48814449, 1.90833313]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.5479883527511159}
episode index:163
target Thresh 64.08748189308392
target distance 18.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.18258225, 18.65619562]), 'dynamicTrap': False, 'previousTarget': array([14.72118773, 17.78704435]), 'currentState': array([27.39535675,  3.64211589,  2.33431453]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6491390876233969
running average episode reward sum: 0.6345042820790456
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'dynamicTrap': False, 'previousTarget': array([13., 20.]), 'currentState': array([13.95932502, 20.81303382,  5.32929912]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 1.2575088419953941}
episode index:164
target Thresh 64.20601342871377
target distance 4.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([5.93826219, 5.9776268 , 4.17266178]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 4.9451895455809876}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6365394015815968
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([2.65841915, 1.93489504, 3.69848427]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.34772997008331313}
episode index:165
target Thresh 64.32336555585815
target distance 13.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([12.98336288,  2.12527188,  5.48119617]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 15.754162658690074}
done in step count: 18
reward sum = 0.7765791093570775
running average episode reward sum: 0.6373830142790395
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([25.49416015, 11.74685149,  5.81578449]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.9020316576280728}
episode index:166
target Thresh 64.43955000982754
target distance 12.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([9.660344  , 7.41605708, 2.54614961]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 10.668389094233628}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6386139135551168
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([10.67629562, 18.76240317,  5.61565841]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.8282771960229034}
episode index:167
target Thresh 64.5545784091642
target distance 7.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 13.]), 'dynamicTrap': False, 'previousTarget': array([29., 13.]), 'currentState': array([22.97674169,  6.52437126,  1.50269717]), 'targetState': array([29, 13], dtype=int32), 'currentDistance': 8.843834472344332}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6403051682031692
{'scaleFactor': 20, 'currentTarget': array([29., 13.]), 'dynamicTrap': False, 'previousTarget': array([29., 13.]), 'currentState': array([28.49711168, 13.00493191,  0.50537055]), 'targetState': array([29, 13], dtype=int32), 'currentDistance': 0.5029125079077185}
episode index:168
target Thresh 64.66846225680388
target distance 2.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'dynamicTrap': False, 'previousTarget': array([25., 19.]), 'currentState': array([23.41805363, 20.09657514,  2.00335575]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.9248458002856956}
done in step count: 9
reward sum = 0.865962941911561
running average episode reward sum: 0.6416404213020354
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'dynamicTrap': False, 'previousTarget': array([25., 19.]), 'currentState': array([24.06725904, 19.41004125,  6.06530186]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.0188913165953302}
episode index:169
target Thresh 64.78121294122626
target distance 25.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.34168675, 13.78336294]), 'dynamicTrap': False, 'previousTarget': array([19.43046618, 13.42781353]), 'currentState': array([36.8143365 ,  6.11777676,  2.53081834]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7267064521072774
running average episode reward sum: 0.6421408097185368
{'scaleFactor': 20, 'currentTarget': array([13., 16.]), 'dynamicTrap': False, 'previousTarget': array([13., 16.]), 'currentState': array([12.25654981, 15.19884593,  2.46617495]), 'targetState': array([13, 16], dtype=int32), 'currentDistance': 1.0929620454253048}
episode index:170
target Thresh 64.89284173759373
target distance 21.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5.36292088, 3.26559148]), 'dynamicTrap': False, 'previousTarget': array([5.35322867, 3.25775784]), 'currentState': array([24.99366428,  7.09102135,  2.50227433]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 30
reward sum = 0.6540824483970847
running average episode reward sum: 0.6422106438628559
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'dynamicTrap': False, 'previousTarget': array([4., 3.]), 'currentState': array([4.28222653, 3.27199289, 3.20070995]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.3919591112918564}
episode index:171
target Thresh 65.00335980887897
target distance 10.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 15.]), 'dynamicTrap': False, 'previousTarget': array([36., 15.]), 'currentState': array([26.9552536 ,  4.92617172,  4.98093486]), 'targetState': array([36, 15], dtype=int32), 'currentDistance': 13.538443546641586}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6435277087983565
{'scaleFactor': 20, 'currentTarget': array([36., 15.]), 'dynamicTrap': False, 'previousTarget': array([36., 15.]), 'currentState': array([36.41426675, 15.98588694,  5.5433445 ]), 'targetState': array([36, 15], dtype=int32), 'currentDistance': 1.0693876737555823}
episode index:172
target Thresh 65.1127782069812
target distance 20.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 16.]), 'dynamicTrap': False, 'previousTarget': array([34.1565257 , 15.74695771]), 'currentState': array([15.82558631, 11.46655901,  2.06807186]), 'targetState': array([35, 16], dtype=int32), 'currentDistance': 19.703051221232716}
done in step count: 22
reward sum = 0.7442636976216868
running average episode reward sum: 0.6441099977510926
{'scaleFactor': 20, 'currentTarget': array([35., 16.]), 'dynamicTrap': False, 'previousTarget': array([35., 16.]), 'currentState': array([34.93463321, 16.13606807,  5.55261635]), 'targetState': array([35, 16], dtype=int32), 'currentDistance': 0.15095475314971216}
episode index:173
target Thresh 65.22110787383143
target distance 8.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'dynamicTrap': False, 'previousTarget': array([16., 14.]), 'currentState': array([10.65958318,  7.7878609 ,  2.51160204]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 8.192113525575659}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6458190216111496
{'scaleFactor': 20, 'currentTarget': array([16., 14.]), 'dynamicTrap': False, 'previousTarget': array([16., 14.]), 'currentState': array([15.28930564, 14.43994072,  1.37865925]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 0.8358434728501093}
episode index:174
target Thresh 65.32835964248659
target distance 11.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'dynamicTrap': False, 'previousTarget': array([23., 12.]), 'currentState': array([32.88814881,  8.48214883,  3.47249413]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 10.495273402061933}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6474547149042686
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'dynamicTrap': False, 'previousTarget': array([23., 12.]), 'currentState': array([23.3045318 , 11.83425981,  1.70913976]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 0.3467123095772244}
episode index:175
target Thresh 65.43454423821294
target distance 15.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'dynamicTrap': False, 'previousTarget': array([23.,  7.]), 'currentState': array([ 7.48012549, 15.24934537,  3.45591128]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 17.576069064737023}
done in step count: 16
reward sum = 0.8158215088550493
running average episode reward sum: 0.6484113444153526
{'scaleFactor': 20, 'currentTarget': array([23.,  7.]), 'dynamicTrap': False, 'previousTarget': array([23.,  7.]), 'currentState': array([22.93214252,  7.78253494,  4.32694586]), 'targetState': array([23,  7], dtype=int32), 'currentDistance': 0.7854715574382883}
episode index:176
target Thresh 65.53967227955853
target distance 8.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 18.]), 'dynamicTrap': False, 'previousTarget': array([37., 18.]), 'currentState': array([30.17489556, 15.20499316,  1.80804413]), 'targetState': array([37, 18], dtype=int32), 'currentDistance': 7.375236531129476}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6501208286271303
{'scaleFactor': 20, 'currentTarget': array([37., 18.]), 'dynamicTrap': False, 'previousTarget': array([37., 18.]), 'currentState': array([36.07752277, 18.82491687,  0.07346677]), 'targetState': array([37, 18], dtype=int32), 'currentDistance': 1.237518519453659}
episode index:177
target Thresh 65.6437542794151
target distance 29.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.14982902,  1.88369635]), 'dynamicTrap': False, 'previousTarget': array([15.,  2.]), 'currentState': array([34.14779281,  1.59831169,  4.59205914]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6508382473393326
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'dynamicTrap': False, 'previousTarget': array([6., 2.]), 'currentState': array([6.1364508 , 1.67655058, 2.75523399]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.3510531927563211}
episode index:178
target Thresh 65.74680064606939
target distance 15.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([9.21107399, 6.11214777, 2.2690239 ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 15.230204859768962}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6521046315608947
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.02362101, 21.02927608,  0.78901288]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.9768177994923674}
episode index:179
target Thresh 65.84882168424393
target distance 10.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([28.02308834,  3.43107341,  0.54466414]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 12.410134802699538}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6535061729133831
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([37.26385966, 12.93925253,  6.09762475]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 0.9756112127434957}
episode index:180
target Thresh 65.94982759612753
target distance 34.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.24644203, 10.05087339]), 'dynamicTrap': False, 'previousTarget': array([23.99135509,  9.58798103]), 'currentState': array([ 5.24660114, 10.13065188,  1.74666231]), 'targetState': array([38, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.660414235920198
running average episode reward sum: 0.6535443390073434
{'scaleFactor': 20, 'currentTarget': array([38., 10.]), 'dynamicTrap': False, 'previousTarget': array([38., 10.]), 'currentState': array([38.61235895, 10.10593025,  4.99131943]), 'targetState': array([38, 10], dtype=int32), 'currentDistance': 0.6214537019930704}
episode index:181
target Thresh 66.0498284823956
target distance 15.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([26.07274511, 20.0295136 ,  1.3954212 ]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 15.072774004991674}
done in step count: 22
reward sum = 0.7478976322950015
running average episode reward sum: 0.6540627636957372
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([10.85842032, 19.73186237,  1.52069748]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.30322037130262736}
episode index:182
target Thresh 66.14883434322005
target distance 24.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.97595816, 18.58402854]), 'dynamicTrap': False, 'previousTarget': array([27.97366596, 18.32455532]), 'currentState': array([ 8.7258979 , 13.15860877,  2.8131038 ]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.5537531977845835
running average episode reward sum: 0.6535146239913048
{'scaleFactor': 20, 'currentTarget': array([33., 20.]), 'dynamicTrap': False, 'previousTarget': array([33., 20.]), 'currentState': array([32.46738745, 19.18611445,  5.5693465 ]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 0.9726694316515209}
episode index:183
target Thresh 66.2468550792695
target distance 21.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([12.02263725,  6.04869701]), 'currentState': array([30.69253872,  6.74776605,  2.32217026]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 19.706730712925282}
done in step count: 31
reward sum = 0.6126896104529009
running average episode reward sum: 0.6532927489177265
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([11.,  6.]), 'currentState': array([11.59146027,  5.5534412 ,  3.16863543]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.7411072892712933}
episode index:184
target Thresh 66.34390049269922
target distance 17.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.94261624,  7.04724258]), 'dynamicTrap': False, 'previousTarget': array([29.33935727,  7.46633605]), 'currentState': array([14.50207984, 19.75904165,  1.4779002 ]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6967349804857735
running average episode reward sum: 0.6535275717910672
{'scaleFactor': 20, 'currentTarget': array([30.,  7.]), 'dynamicTrap': False, 'previousTarget': array([30.,  7.]), 'currentState': array([30.18663797,  7.94498078,  4.25823968]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 0.9632353802732218}
episode index:185
target Thresh 66.43998028813144
target distance 14.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'dynamicTrap': False, 'previousTarget': array([13.,  9.]), 'currentState': array([ 3.33137899, 23.03897496,  5.30245108]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 17.046262059631243}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6545917126475393
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'dynamicTrap': False, 'previousTarget': array([13.,  9.]), 'currentState': array([12.68282338,  8.50870156,  4.86248601]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.5847864328844363}
episode index:186
target Thresh 66.53510407362576
target distance 8.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.,  3.]), 'currentState': array([11.13651008,  9.32257546,  3.78359079]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 6.591475945349742}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6562280992643974
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.,  3.]), 'currentState': array([12.52145478,  3.9897292 ,  5.27354282]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 1.0993495420416401}
episode index:187
target Thresh 66.62928136164001
target distance 20.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([29.9007438 ,  6.00992562]), 'currentState': array([11.3097241 ,  8.92600743,  1.33246201]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 18.91792623113891}
done in step count: 29
reward sum = 0.6660664549770446
running average episode reward sum: 0.65628043094372
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([30.,  6.]), 'currentState': array([30.16383381,  6.07538035,  6.18874496]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 0.18034332663621866}
episode index:188
target Thresh 66.72252156998145
target distance 2.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([35.90265707, 23.25363248,  2.56907511]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 2.4276839029622788}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6579937619969278
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([35.34484282, 21.30326565,  4.40682015]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 0.45922393490785124}
episode index:189
target Thresh 66.81483402274864
target distance 27.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.31049356, 15.22217095]), 'dynamicTrap': False, 'previousTarget': array([27.6656401, 15.3582148]), 'currentState': array([ 9.75655381, 19.42258049,  5.73846215]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6568043470672731
running average episode reward sum: 0.6579875019183506
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([35.35231257, 14.36820562,  2.88659044]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 0.5096072220547072}
episode index:190
target Thresh 66.90622795126379
target distance 4.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([32.57462547, 12.19844582,  0.73359555]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 3.1433101021904934}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6596226406517623
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([33.95256685, 14.62601381,  5.51677823]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 0.3769821904518998}
episode index:191
target Thresh 66.99671249499588
target distance 24.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.06056708, 2.30830454]), 'dynamicTrap': False, 'previousTarget': array([8.06908484, 2.3390904 ]), 'currentState': array([26.95985859,  4.31284873,  3.85942483]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 23
reward sum = 0.7667532316338189
running average episode reward sum: 0.6601806124797939
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([4.00958195, 1.66862613, 2.22888192]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.33151237351826746}
episode index:192
target Thresh 67.08629670247471
target distance 17.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.97064535, 7.16622128]), 'dynamicTrap': True, 'previousTarget': array([7., 7.]), 'currentState': array([24.       , 15.       ,  4.9861865], dtype=float32), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 18.744786176052223}
done in step count: 99
reward sum = -0.5626931696497414
running average episode reward sum: 0.6538444788936304
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'dynamicTrap': False, 'previousTarget': array([7., 7.]), 'currentState': array([9.15233646, 7.19274877, 1.77239978]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 2.1609498639472964}
episode index:193
target Thresh 67.17498953219567
target distance 22.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.02012314,  8.07006581]), 'dynamicTrap': False, 'previousTarget': array([25.13646016,  8.92760259]), 'currentState': array([10.76519805, 21.00402346,  0.09047013]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.707929532598275
running average episode reward sum: 0.6541232678302523
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([32.34054769,  2.4509497 ,  2.85379009]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.6460874235072901}
episode index:194
target Thresh 67.26279985351566
target distance 20.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.17204955, 15.02154655]), 'dynamicTrap': False, 'previousTarget': array([13.76121364, 14.9529684 ]), 'currentState': array([27.64601601,  1.21921723,  2.99002385]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.65460044129949
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 19.]), 'currentState': array([8.80556888e+00, 1.92934630e+01, 1.72338148e-02]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.35202843011726825}
episode index:195
target Thresh 67.34973644753997
target distance 10.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'dynamicTrap': False, 'previousTarget': array([21., 21.]), 'currentState': array([31.34143707, 13.04623719,  1.14460057]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 13.046365913117766}
done in step count: 23
reward sum = 0.6687928575792421
running average episode reward sum: 0.6546728515866316
{'scaleFactor': 20, 'currentTarget': array([21., 21.]), 'dynamicTrap': False, 'previousTarget': array([21., 21.]), 'currentState': array([20.98937616, 21.63714819,  1.98293651]), 'targetState': array([21, 21], dtype=int32), 'currentDistance': 0.6372367568403253}
episode index:196
target Thresh 67.43580800800046
target distance 6.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([30.37103505,  4.57708007,  4.40560937]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 4.9976482834294185}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6559867825302712
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([26.46550763,  6.01339401,  2.05038962]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 1.0909118796586434}
episode index:197
target Thresh 67.5210231421249
target distance 24.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.8881199 , 22.18425874]), 'dynamicTrap': False, 'previousTarget': array([21.57960839, 22.07908508]), 'currentState': array([ 3.54178066, 17.11285281,  0.22155612]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7008795682008904
running average episode reward sum: 0.6562135137710319
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([25.65215728, 23.26268711,  5.14827958]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.43588883179830645}
episode index:198
target Thresh 67.60539037149772
target distance 14.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.,  2.]), 'dynamicTrap': False, 'previousTarget': array([36.,  2.]), 'currentState': array([25.81760703, 14.32694265,  5.61380005]), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 15.988578480062447}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6572378597050533
{'scaleFactor': 20, 'currentTarget': array([36.,  2.]), 'dynamicTrap': False, 'previousTarget': array([36.,  2.]), 'currentState': array([36.20524074,  1.64294285,  3.74482858]), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 0.41184168049071895}
episode index:199
target Thresh 67.68891813291215
target distance 6.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([32.06702405,  6.50719808,  5.85756564]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 6.892070948253786}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.658754650456528
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([37.79969655,  2.24474506,  4.33890665]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 0.7813651520242823}
episode index:200
target Thresh 67.77161477921395
target distance 2.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'dynamicTrap': False, 'previousTarget': array([14.,  9.]), 'currentState': array([12.89203368,  9.72565857,  5.95925725]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.3244507246320256}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6604026372701772
{'scaleFactor': 20, 'currentTarget': array([14.,  9.]), 'dynamicTrap': False, 'previousTarget': array([14.,  9.]), 'currentState': array([14.82482556,  9.90689342,  0.51935094]), 'targetState': array([14,  9], dtype=int32), 'currentDistance': 1.2258845313848923}
episode index:201
target Thresh 67.85348858013666
target distance 20.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.22945294, 24.14927522]), 'dynamicTrap': True, 'previousTarget': array([ 6.38838649, 22.9223227 ]), 'currentState': array([26.       , 19.       ,  4.8832974], dtype=float32), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 19.464030212359997}
done in step count: 18
reward sum = 0.7759939108510875
running average episode reward sum: 0.6609748712978055
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.32059151, 22.07187933,  1.47960635]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.9819301857643875}
episode index:202
target Thresh 67.9345477231286
target distance 30.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.90217891, 12.22730684]), 'dynamicTrap': False, 'previousTarget': array([15.11145618, 12.94427191]), 'currentState': array([32.28497607,  2.33597918,  3.53508937]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6616282674177357
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.81246808, 18.35073034,  2.69802861]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 1.040026670957088}
episode index:203
target Thresh 68.01480031417161
target distance 26.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.08677929, 13.74127336]), 'dynamicTrap': False, 'previousTarget': array([13.88441983, 13.88171698]), 'currentState': array([31.83737238,  6.78316595,  2.93976784]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.662084595946098
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.85769229, 15.03011611,  0.65065832]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.9802684545082916}
episode index:204
target Thresh 68.0942543785917
target distance 18.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'dynamicTrap': False, 'previousTarget': array([19.,  5.]), 'currentState': array([15.4517476 , 22.02356821,  4.32135081]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 17.389421197777477}
done in step count: 28
reward sum = 0.640305790771478
running average episode reward sum: 0.6619783578720755
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'dynamicTrap': False, 'previousTarget': array([19.,  5.]), 'currentState': array([18.78692366,  4.81734093,  3.96436977]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.28065256898962193}
episode index:205
target Thresh 68.1729178618615
target distance 10.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'dynamicTrap': False, 'previousTarget': array([10., 12.]), 'currentState': array([9.98243378, 3.68287834, 2.59123415]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 8.317140208779813}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6633351626853227
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'dynamicTrap': False, 'previousTarget': array([10., 12.]), 'currentState': array([ 9.55020077, 12.31701195,  0.98395663]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.5502871264759202}
episode index:206
target Thresh 68.2507986303949
target distance 13.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([29.52805501, 15.86487581,  5.28842611]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 15.403859303384905}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6642439675568664
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([37.37223869,  3.05276413,  3.22009707]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 0.62997485194433}
episode index:207
target Thresh 68.32790447233364
target distance 11.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.0005135 , 11.00122918]), 'dynamicTrap': True, 'previousTarget': array([28., 11.]), 'currentState': array([17.      ,  4.      ,  4.232051], dtype=float32), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 13.039497970625925}
done in step count: 29
reward sum = 0.6230839991572399
running average episode reward sum: 0.6640460830934067
{'scaleFactor': 20, 'currentTarget': array([28., 11.]), 'dynamicTrap': False, 'previousTarget': array([28., 11.]), 'currentState': array([27.88171868, 10.23126868,  4.56654531]), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 0.7777778077048599}
episode index:208
target Thresh 68.40424309832619
target distance 3.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([28.55269043, 15.07331823,  2.12803447]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 2.9606674984801518}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6655583027915244
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([29.07327418, 17.25056436,  1.02526295]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.7530092177328244}
episode index:209
target Thresh 68.47982214229874
target distance 5.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 17.]), 'dynamicTrap': False, 'previousTarget': array([32., 17.]), 'currentState': array([2.71300325e+01, 1.48037844e+01, 2.44566759e-02]), 'targetState': array([32, 17], dtype=int32), 'currentDistance': 5.342279117660597}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6669175015872791
{'scaleFactor': 20, 'currentTarget': array([32., 17.]), 'dynamicTrap': False, 'previousTarget': array([32., 17.]), 'currentState': array([32.23876505, 17.28988588,  5.97890767]), 'targetState': array([32, 17], dtype=int32), 'currentDistance': 0.37555635369866114}
episode index:210
target Thresh 68.5546491622187
target distance 8.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([29.27791168, 12.94575798,  3.9588387 ]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 8.595334486535226}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6682187463636474
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([26.41251801,  5.54247357,  2.78031647]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.6815047222491821}
episode index:211
target Thresh 68.62873164085039
target distance 13.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'dynamicTrap': False, 'previousTarget': array([18., 19.]), 'currentState': array([7.80677929, 7.35149599, 0.73363876]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 15.47867546109226}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6689637929554962
{'scaleFactor': 20, 'currentTarget': array([18., 19.]), 'dynamicTrap': False, 'previousTarget': array([18., 19.]), 'currentState': array([17.0958315 , 19.53341158,  5.94641865]), 'targetState': array([18, 19], dtype=int32), 'currentDistance': 1.0497850203048575}
episode index:212
target Thresh 68.70207698650344
target distance 12.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.15805683,  7.22118486]), 'dynamicTrap': True, 'previousTarget': array([13.,  6.]), 'currentState': array([25.       ,  5.       ,  2.9635174], dtype=float32), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 11.06713123692634}
done in step count: 23
reward sum = 0.7289428489289927
running average episode reward sum: 0.6692453847675784
{'scaleFactor': 20, 'currentTarget': array([13.,  6.]), 'dynamicTrap': False, 'previousTarget': array([13.,  6.]), 'currentState': array([12.89005967,  5.99731645,  0.86108056]), 'targetState': array([13,  6], dtype=int32), 'currentDistance': 0.10997307692613066}
episode index:213
target Thresh 68.7746925337735
target distance 26.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.14380082, 18.28392976]), 'dynamicTrap': False, 'previousTarget': array([19.88854382, 17.94427191]), 'currentState': array([2.06433941, 9.73214193, 5.63473582]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6605847542203817
running average episode reward sum: 0.6692049145313766
{'scaleFactor': 20, 'currentTarget': array([28., 22.]), 'dynamicTrap': False, 'previousTarget': array([28., 22.]), 'currentState': array([27.4204554 , 22.77324372,  5.07493918]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 0.9663217872714106}
episode index:214
target Thresh 68.84658554427583
target distance 12.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 16.]), 'dynamicTrap': False, 'previousTarget': array([19., 16.]), 'currentState': array([30.60176474, 15.50837176,  3.13316727]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 11.612176517256593}
done in step count: 8
reward sum = 0.9039112873836408
running average episode reward sum: 0.6702965720795266
{'scaleFactor': 20, 'currentTarget': array([20.13291915, 15.40124394]), 'dynamicTrap': True, 'previousTarget': array([19., 16.]), 'currentState': array([20.38969163, 15.02456276,  2.84640808]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.4558736821629561}
episode index:215
target Thresh 68.91776320737138
target distance 7.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([ 5.20067619, 20.67592123,  0.65233237]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 5.948557914265182}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6714653133866952
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([10.15240082, 22.16155168,  5.6703175 ]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.8628576477038408}
episode index:216
target Thresh 68.9882326408858
target distance 10.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 15.]), 'dynamicTrap': False, 'previousTarget': array([33., 15.]), 'currentState': array([24.40487335, 23.61009504,  0.73450439]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 12.165933529576286}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6721782318680266
{'scaleFactor': 20, 'currentTarget': array([33., 15.]), 'dynamicTrap': False, 'previousTarget': array([33., 15.]), 'currentState': array([32.18693943, 15.70071301,  3.73528853]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 1.073343473511528}
episode index:217
target Thresh 69.05800089182114
target distance 17.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.06462813,  5.9250451 ]), 'dynamicTrap': False, 'previousTarget': array([18.43600015,  6.29270602]), 'currentState': array([32.69259188, 19.56405813,  1.34263951]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6988248298951562
running average episode reward sum: 0.6723004639690685
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'dynamicTrap': False, 'previousTarget': array([16.,  4.]), 'currentState': array([16.82867874,  3.87404891,  3.54841189]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.8381957567883516}
episode index:218
target Thresh 69.12707493706064
target distance 4.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 22.]), 'dynamicTrap': False, 'previousTarget': array([29., 22.]), 'currentState': array([29.17860167, 19.84051178,  0.78668058]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 2.1668613060612327}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.673705941302543
{'scaleFactor': 20, 'currentTarget': array([29., 22.]), 'dynamicTrap': False, 'previousTarget': array([29., 22.]), 'currentState': array([28.81684677, 21.97327679,  1.08923066]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 0.18509250304847102}
episode index:219
target Thresh 69.1954616840664
target distance 12.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.98890625, 10.1569155 ]), 'dynamicTrap': True, 'previousTarget': array([32., 10.]), 'currentState': array([20.       , 16.       ,  4.6839705], dtype=float32), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 13.336997769031717}
done in step count: 12
reward sum = 0.8572690111171293
running average episode reward sum: 0.6745403188926093
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([32.2154935 , 10.22047901,  4.53690871]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 0.3082992742437816}
episode index:220
target Thresh 69.2631679715701
target distance 22.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5.19658139, 8.71662595]), 'dynamicTrap': False, 'previousTarget': array([5.18339664, 8.70226409]), 'currentState': array([25.03220267,  6.15769487,  2.27346441]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8162686238355866
running average episode reward sum: 0.6751816234398625
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([3.0942907 , 9.37293669, 1.91597576]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.3846719566920441}
episode index:221
target Thresh 69.3302005702569
target distance 16.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'dynamicTrap': False, 'previousTarget': array([3., 4.]), 'currentState': array([ 4.78253734, 19.13851968,  6.0820505 ]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 15.243103934659379}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6760144015083375
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'dynamicTrap': False, 'previousTarget': array([3., 4.]), 'currentState': array([2.99397002, 3.89402007, 2.64766246]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.10615133898353492}
episode index:222
target Thresh 69.39656618344256
target distance 18.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([28.02729168, 23.00118114,  4.19484401]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 18.108950839379872}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6768786679265467
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([30.04441624,  5.97953265,  5.3908512 ]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 0.9805391438354123}
episode index:223
target Thresh 69.46227144774366
target distance 9.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'dynamicTrap': False, 'previousTarget': array([16., 11.]), 'currentState': array([15.03134875,  1.91758739,  5.14974668]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 9.133920524907502}
done in step count: 19
reward sum = 0.7640410783129323
running average episode reward sum: 0.6772677858300574
{'scaleFactor': 20, 'currentTarget': array([15.82754931, 11.33587464]), 'dynamicTrap': True, 'previousTarget': array([16., 11.]), 'currentState': array([15.59715203, 12.19712164,  5.37153749]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 0.8915319951614491}
episode index:224
target Thresh 69.52732293374143
target distance 5.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 16.]), 'dynamicTrap': False, 'previousTarget': array([28., 16.]), 'currentState': array([33.09858867, 16.33194078,  2.08809569]), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 5.109382652960444}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6785270223819238
{'scaleFactor': 20, 'currentTarget': array([28., 16.]), 'dynamicTrap': False, 'previousTarget': array([28., 16.]), 'currentState': array([27.90039826, 15.60000728,  4.05011994]), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 0.41220708456024574}
episode index:225
target Thresh 69.59172714663865
target distance 21.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.28346853,  6.19041496]), 'dynamicTrap': False, 'previousTarget': array([17.20101013,  6.17157288]), 'currentState': array([37.06693075,  9.12548269,  2.09079775]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6902912152363442
running average episode reward sum: 0.6785790763326071
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([15.89874938,  6.34240349,  3.03593849]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.3570599885368369}
episode index:226
target Thresh 69.65549052691028
target distance 17.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'dynamicTrap': False, 'previousTarget': array([15.,  6.]), 'currentState': array([31.9811568 ,  3.21911805,  2.66658092]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 17.207352811253195}
done in step count: 17
reward sum = 0.8160821413740903
running average episode reward sum: 0.6791848167072392
{'scaleFactor': 20, 'currentTarget': array([15.,  6.]), 'dynamicTrap': False, 'previousTarget': array([15.,  6.]), 'currentState': array([14.57063321,  5.64177575,  1.47824292]), 'targetState': array([15,  6], dtype=int32), 'currentDistance': 0.5591783748835706}
episode index:227
target Thresh 69.7186194509475
target distance 19.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.93887705, 18.88686281]), 'dynamicTrap': False, 'previousTarget': array([26.88271492, 18.29822396]), 'currentState': array([12.91637756,  4.62609467,  2.53481197]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7485687845273008
running average episode reward sum: 0.6794891323555727
{'scaleFactor': 20, 'currentTarget': array([30., 22.]), 'dynamicTrap': False, 'previousTarget': array([30., 22.]), 'currentState': array([29.87792272, 22.15558458,  5.46885184]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 0.1977610243807951}
episode index:228
target Thresh 69.78112023169533
target distance 6.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([36.21873418, 10.03031291,  1.14770621]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 6.368669043832949}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6805513837183342
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([34.05140482, 16.09125178,  5.31602597]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 0.10473463263544465}
episode index:229
target Thresh 69.84299911928389
target distance 13.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([16.8158042 , 23.12099294,  0.81951075]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 15.772213527569031}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6813318488093035
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([28.15528508, 11.47308669,  3.72920951]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 0.5493187488528269}
episode index:230
target Thresh 69.90426230165353
target distance 20.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.98351985,  4.9226078 ]), 'dynamicTrap': False, 'previousTarget': array([15.28991511,  5.85014149]), 'currentState': array([ 6.63561519, 22.60357718,  0.77221602]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7146552206420721
running average episode reward sum: 0.6814761058302247
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'dynamicTrap': False, 'previousTarget': array([17.,  3.]), 'currentState': array([17.22267188,  3.33137041,  2.60304306]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.3992356616248171}
episode index:231
target Thresh 69.96491590517354
target distance 18.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.54025145, 17.73248294]), 'dynamicTrap': True, 'previousTarget': array([31.54026305, 17.73247066]), 'currentState': array([17.       ,  4.       ,  3.8269444], dtype=float32), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.68183483209177
running average episode reward sum: 0.6814776520641106
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([35.43682805, 21.61144506,  0.16730778]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 0.7514544600608798}
episode index:232
target Thresh 70.02496599525482
target distance 18.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.20651905,  3.88504481,  2.67352262]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 17.116201132462137}
done in step count: 24
reward sum = 0.7051606927384252
running average episode reward sum: 0.6815792960155025
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([27.40855846, 21.5209891 ,  5.37739028]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.7881831897667646}
episode index:233
target Thresh 70.08441857695641
target distance 8.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 10.]), 'dynamicTrap': False, 'previousTarget': array([30., 10.]), 'currentState': array([37.65495616, 12.28891878,  5.41364622]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 7.9898374818524145}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6826899834231328
{'scaleFactor': 20, 'currentTarget': array([30., 10.]), 'dynamicTrap': False, 'previousTarget': array([30., 10.]), 'currentState': array([30.36241129, 10.55163844,  2.91640142]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 0.6600355352002814}
episode index:234
target Thresh 70.14327959558602
target distance 22.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4.41291659, 4.0490139 ]), 'dynamicTrap': False, 'previousTarget': array([6.18339664, 4.29773591]), 'currentState': array([24.27348738,  6.40649725,  4.21301436]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.683519051676647
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([3.895664  , 3.66655121, 1.67866215]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.34939103771212576}
episode index:235
target Thresh 70.20155493729459
target distance 11.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([33.88222793, 16.44570429,  2.83913004]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 13.355493549923665}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6844165906706389
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([27.54140615,  4.44930261,  2.19868478]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.7722617639164585}
episode index:236
target Thresh 70.25925042966482
target distance 14.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 23.]), 'dynamicTrap': False, 'previousTarget': array([34., 23.]), 'currentState': array([20.84450146, 22.68327429,  1.70893419]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 13.159310658801756}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6852687775104932
{'scaleFactor': 20, 'currentTarget': array([34., 23.]), 'dynamicTrap': False, 'previousTarget': array([34., 23.]), 'currentState': array([33.46686517, 23.77480597,  6.12371058]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 0.9405089298072962}
episode index:237
target Thresh 70.31637184229405
target distance 20.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.03157084, 9.8388943 ]), 'dynamicTrap': False, 'previousTarget': array([ 7.11145618, 10.05572809]), 'currentState': array([25.51754666, 17.4722879 ,  5.96927345]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6854051714394335
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([4.33359315, 8.02833034, 1.17157813]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.178235975715838}
episode index:238
target Thresh 70.37292488737113
target distance 7.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.69009374, 14.86945475,  1.40868252]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 7.137276617608291}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6864766148618668
{'scaleFactor': 20, 'currentTarget': array([ 3., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 22.]), 'currentState': array([ 2.70835077, 22.68899592,  0.76225143]), 'targetState': array([ 3, 22], dtype=int32), 'currentDistance': 0.7481808909857943}
episode index:239
target Thresh 70.4289152202477
target distance 27.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.10183537, 15.58173515]), 'dynamicTrap': False, 'previousTarget': array([27.0200334 , 14.67631238]), 'currentState': array([9.74710186, 7.63794778, 0.34886515]), 'targetState': array([36, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6384728108440842
running average episode reward sum: 0.6862765990117927
{'scaleFactor': 20, 'currentTarget': array([36., 19.]), 'dynamicTrap': False, 'previousTarget': array([36., 19.]), 'currentState': array([36.76742714, 18.60156997,  5.50090425]), 'targetState': array([36, 19], dtype=int32), 'currentDistance': 0.8646912193175089}
episode index:240
target Thresh 70.48434844000371
target distance 17.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.93916043, 21.9945683 ]), 'dynamicTrap': True, 'previousTarget': array([ 5., 22.]), 'currentState': array([22.       , 21.       ,  6.2809086], dtype=float32), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 17.08980436027143}
done in step count: 15
reward sum = 0.8401583546412884
running average episode reward sum: 0.6869151125206289
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.76859695, 21.46437935,  2.91669474]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.9368194823397266}
episode index:241
target Thresh 70.53923009000735
target distance 35.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.72900069, 19.05807231]), 'dynamicTrap': False, 'previousTarget': array([17.00815827, 18.57119548]), 'currentState': array([35.72882177, 19.14266956,  3.37448609]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6521971890213402
running average episode reward sum: 0.68677165002683
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.27154271, 18.7444767 ,  0.70139846]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.37286405152565}
episode index:242
target Thresh 70.5935656584693
target distance 33.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.77484813,  2.3188823 ]), 'dynamicTrap': False, 'previousTarget': array([23.99082358,  2.39421747]), 'currentState': array([4.78164847, 2.84038728, 0.80856913]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6157311025031807
running average episode reward sum: 0.686479302094634
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([37.00664946,  2.13769989,  0.47710505]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 0.13786034970768438}
episode index:243
target Thresh 70.64736057899175
target distance 19.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.53277565, 16.2129135 ]), 'dynamicTrap': False, 'previousTarget': array([27.90977806, 15.67985983]), 'currentState': array([10.9085464 ,  6.75846782,  1.43338063]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7380155231805523
running average episode reward sum: 0.686690516115478
{'scaleFactor': 20, 'currentTarget': array([30., 17.]), 'dynamicTrap': False, 'previousTarget': array([30., 17.]), 'currentState': array([29.81821034, 17.93756044,  4.66417552]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 0.955022016633956}
episode index:244
target Thresh 70.70062023111153
target distance 18.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'dynamicTrap': False, 'previousTarget': array([10., 11.]), 'currentState': array([28.0508462 , 11.95615088,  5.12979692]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 18.07615205293528}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6873981399461957
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'dynamicTrap': False, 'previousTarget': array([10., 11.]), 'currentState': array([ 9.07524081, 10.73460811,  2.3310017 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.9620875330797619}
episode index:245
target Thresh 70.75334994083828
target distance 19.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.22853989, 18.37769432]), 'dynamicTrap': False, 'previousTarget': array([15.70177604, 17.88271492]), 'currentState': array([30.75289761,  5.76839944,  4.88159531]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7841994821496454
running average episode reward sum: 0.6877916413372666
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([11.39865352, 21.45462268,  0.93634319]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.7538563290020447}
episode index:246
target Thresh 70.80555498118687
target distance 15.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.91392024,  5.56120541]), 'dynamicTrap': True, 'previousTarget': array([29.,  4.]), 'currentState': array([23.      , 19.      ,  3.422272], dtype=float32), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 15.113023952129286}
done in step count: 16
reward sum = 0.8024478209948755
running average episode reward sum: 0.688255836396609
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([29.27247304,  4.10959815,  4.13750391]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 0.29368914555203046}
episode index:247
target Thresh 70.85724057270487
target distance 10.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([36.51270066, 15.09467911,  4.68771803]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 11.043753941101999}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.689164148538089
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([28.60698367,  7.09054154,  3.74345517]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 0.4033108025795749}
episode index:248
target Thresh 70.90841188399449
target distance 15.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([23.18727225, 20.28885526,  4.70053236]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 17.301975609312013}
done in step count: 99
reward sum = -0.5352653384757674
running average episode reward sum: 0.6842467610400415
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([21.66445743,  6.97449264,  5.4375063 ]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 14.55929672449689}
episode index:249
target Thresh 70.9590740322295
target distance 13.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([19.87229411,  3.15364225,  2.1834621 ]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 14.805717691300355}
done in step count: 16
reward sum = 0.80465863759769
running average episode reward sum: 0.684728408546272
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([32.92523702,  9.95804525,  4.50610475]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 0.08573041532573931}
episode index:250
target Thresh 71.00923208366696
target distance 25.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.9021285 , 10.05342448]), 'dynamicTrap': False, 'previousTarget': array([27.56953382,  9.42781353]), 'currentState': array([8.84934824, 3.9713554 , 1.91224617]), 'targetState': array([34, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.34992152727752235
running average episode reward sum: 0.6833945165890261
{'scaleFactor': 20, 'currentTarget': array([34., 12.]), 'dynamicTrap': False, 'previousTarget': array([34., 12.]), 'currentState': array([34.65875161, 12.90919253,  3.52889968]), 'targetState': array([34, 12], dtype=int32), 'currentDistance': 1.1227576499672913}
episode index:251
target Thresh 71.05889105415379
target distance 10.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'dynamicTrap': False, 'previousTarget': array([23., 13.]), 'currentState': array([13.85334942, 14.72779759,  1.23591341]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 9.308410248719747}
done in step count: 14
reward sum = 0.8033570119901141
running average episode reward sum: 0.683870558237443
{'scaleFactor': 20, 'currentTarget': array([23., 13.]), 'dynamicTrap': False, 'previousTarget': array([23., 13.]), 'currentState': array([22.69446437, 12.75797426,  5.11701757]), 'targetState': array([23, 13], dtype=int32), 'currentDistance': 0.3897800427766107}
episode index:252
target Thresh 71.10805590962842
target distance 2.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  9.]), 'dynamicTrap': False, 'previousTarget': array([28.,  9.]), 'currentState': array([29.80317726, 11.12811037,  3.56650317]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 2.7893192712471984}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6850414255961883
{'scaleFactor': 20, 'currentTarget': array([28.,  9.]), 'dynamicTrap': False, 'previousTarget': array([28.,  9.]), 'currentState': array([28.29422649,  8.1757324 ,  3.24855888]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 0.8752064355659784}
episode index:253
target Thresh 71.1567315666174
target distance 14.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([ 4.93891597, 19.67023957,  5.01790798]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 12.704980677672662}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6859409367059813
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([4.02883753, 7.22095011, 2.90651557]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.22282404167975112}
episode index:254
target Thresh 71.20492289272697
target distance 17.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.53660518, 17.3254504 ]), 'dynamicTrap': False, 'previousTarget': array([13.43600015, 16.70729398]), 'currentState': array([26.05875356,  2.58938908,  3.12985593]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6670456838648434
running average episode reward sum: 0.6858668376752318
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([10.53021796, 19.99367498,  6.28231655]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 1.0991292568770659}
episode index:255
target Thresh 71.25263470712989
target distance 14.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'dynamicTrap': False, 'previousTarget': array([16., 16.]), 'currentState': array([29.88309898, 20.24704008,  5.2209177 ]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 14.51818813802016}
done in step count: 18
reward sum = 0.7812187495138618
running average episode reward sum: 0.6862393060808515
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'dynamicTrap': False, 'previousTarget': array([16., 16.]), 'currentState': array([16.5859233 , 15.7676168 ,  1.29265768]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.6303237801643258}
episode index:256
target Thresh 71.29987178104737
target distance 5.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 10.]), 'dynamicTrap': False, 'previousTarget': array([30., 10.]), 'currentState': array([33.53689466, 12.60326268,  3.76051378]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 4.3916512128714995}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6873827329054396
{'scaleFactor': 20, 'currentTarget': array([30., 10.]), 'dynamicTrap': False, 'previousTarget': array([30., 10.]), 'currentState': array([30.73398549, 10.42766833,  3.02605951]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 0.8494909619208151}
episode index:257
target Thresh 71.34663883822618
target distance 19.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.29192078, 15.46660435]), 'dynamicTrap': False, 'previousTarget': array([ 9.89888325, 14.86398076]), 'currentState': array([25.91466586,  4.34525794,  2.5068779 ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7135067800376435
running average episode reward sum: 0.6874839889020761
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 17.]), 'currentState': array([ 6.33341504, 16.89005506,  0.41738587]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.6755911470636368}
episode index:258
target Thresh 71.39294055541099
target distance 22.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.10049239,  7.37918641]), 'dynamicTrap': False, 'previousTarget': array([15.12677025,  7.26249016]), 'currentState': array([31.77332637, 18.42529917,  3.07047427]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7370535670878147
running average episode reward sum: 0.6876753772348395
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'dynamicTrap': False, 'previousTarget': array([10.,  4.]), 'currentState': array([10.64594904,  4.41688207,  1.66426436]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.7687917949664963}
episode index:259
target Thresh 71.43878156281212
target distance 11.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'dynamicTrap': False, 'previousTarget': array([25., 17.]), 'currentState': array([35.19565601, 20.88017908,  3.0795002 ]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 10.909041715238248}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6886515494354787
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'dynamicTrap': False, 'previousTarget': array([25., 17.]), 'currentState': array([25.16673283, 17.67148926,  4.30786419]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.6918798057962713}
episode index:260
target Thresh 71.4841664445685
target distance 23.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.81306197,  4.02734809]), 'dynamicTrap': False, 'previousTarget': array([11.64765455,  3.95156206]), 'currentState': array([31.12441759,  9.23038031,  1.97941089]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6892104084853431
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'dynamicTrap': False, 'previousTarget': array([8., 3.]), 'currentState': array([8.17234329, 2.81525886, 3.70767957]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.25264896495102523}
episode index:261
target Thresh 71.52909973920613
target distance 28.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.94496883, 10.12164021]), 'dynamicTrap': False, 'previousTarget': array([16.11145618, 11.05572809]), 'currentState': array([33.10941234, 18.49140675,  3.29058766]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7316840674928897
running average episode reward sum: 0.6893725216876619
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'dynamicTrap': False, 'previousTarget': array([6., 6.]), 'currentState': array([5.09871334, 6.66242715, 0.76333063]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 1.1185380504928712}
episode index:262
target Thresh 71.57358594009192
target distance 7.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.16347005, 19.11500716]), 'dynamicTrap': True, 'previousTarget': array([15., 19.]), 'currentState': array([22.       , 17.       ,  3.8158712], dtype=float32), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 7.156213875141475}
done in step count: 20
reward sum = 0.6693647086921064
running average episode reward sum: 0.6892964463530782
{'scaleFactor': 20, 'currentTarget': array([15., 19.]), 'dynamicTrap': False, 'previousTarget': array([15., 19.]), 'currentState': array([15.09166606, 18.12035478,  2.67682268]), 'targetState': array([15, 19], dtype=int32), 'currentDistance': 0.8844084909670561}
episode index:263
target Thresh 71.61762949588302
target distance 10.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'dynamicTrap': False, 'previousTarget': array([14., 18.]), 'currentState': array([22.28398826, 16.97307747,  3.65716445]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 8.347396682423106}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6902877100028771
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'dynamicTrap': False, 'previousTarget': array([14., 18.]), 'currentState': array([13.3464491 , 18.71508902,  3.56444845]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.968752329290245}
episode index:264
target Thresh 71.66123481097173
target distance 4.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([36.61720074,  9.50611374,  2.8296898 ]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 3.6507622213334963}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6913443563802248
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([32.79070347,  9.79759404,  1.10678071]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 0.2911583988432266}
episode index:265
target Thresh 71.70440624592588
target distance 8.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([3.34109566, 9.9832156 , 0.36128777]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 7.296614442700293}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6923204680099985
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([10.27809797,  6.95418997,  4.50717849]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.28184577499665203}
episode index:266
target Thresh 71.74714811792495
target distance 34.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.78715261, 21.02423636]), 'dynamicTrap': False, 'previousTarget': array([23.86301209, 20.66317505]), 'currentState': array([ 3.98696107, 23.84423904,  0.76703382]), 'targetState': array([38, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7354816623372671
running average episode reward sum: 0.6924821204232092
{'scaleFactor': 20, 'currentTarget': array([38., 19.]), 'dynamicTrap': False, 'previousTarget': array([38., 19.]), 'currentState': array([38.09765206, 19.70792283,  0.39578042]), 'targetState': array([38, 19], dtype=int32), 'currentDistance': 0.7146262386253114}
episode index:267
target Thresh 71.78946470119176
target distance 8.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'dynamicTrap': False, 'previousTarget': array([21., 17.]), 'currentState': array([25.58079942,  7.81408525,  3.15102088]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 10.264733466872617}
done in step count: 15
reward sum = 0.7954419898843544
running average episode reward sum: 0.6928662990406015
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'dynamicTrap': False, 'previousTarget': array([21., 17.]), 'currentState': array([20.66484583, 17.02440063,  1.44582657]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.33604123043077394}
episode index:268
target Thresh 71.8313602274199
target distance 14.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 17.]), 'dynamicTrap': False, 'previousTarget': array([37., 17.]), 'currentState': array([30.54758476,  4.00529147,  1.67506158]), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 14.50848415146852}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6936526030404834
{'scaleFactor': 20, 'currentTarget': array([37., 17.]), 'dynamicTrap': False, 'previousTarget': array([37., 17.]), 'currentState': array([36.78449106, 16.80489024,  5.9829852 ]), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 0.2907093435831592}
episode index:269
target Thresh 71.8728388861969
target distance 4.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'dynamicTrap': False, 'previousTarget': array([27.,  8.]), 'currentState': array([31.04849612, 11.20723403,  2.24770364]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 5.164946365479941}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6946772193255186
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'dynamicTrap': False, 'previousTarget': array([27.,  8.]), 'currentState': array([27.571031  ,  7.83882236,  5.20020548]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.5933419231896588}
episode index:270
target Thresh 71.91390482542322
target distance 10.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 13.]), 'dynamicTrap': False, 'previousTarget': array([33., 13.]), 'currentState': array([34.53752261, 22.10706771,  4.64741302]), 'targetState': array([33, 13], dtype=int32), 'currentDistance': 9.235943810491104}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6955187967244205
{'scaleFactor': 20, 'currentTarget': array([33., 13.]), 'dynamicTrap': False, 'previousTarget': array([33., 13.]), 'currentState': array([33.88693096, 12.722431  ,  3.73600161]), 'targetState': array([33, 13], dtype=int32), 'currentDistance': 0.9293498117972111}
episode index:271
target Thresh 71.95456215172698
target distance 29.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.94090924, 11.87452273]), 'dynamicTrap': False, 'previousTarget': array([22.81242258, 11.73274794]), 'currentState': array([3.09349428, 9.40873285, 5.49751592]), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6957927833019232
{'scaleFactor': 20, 'currentTarget': array([32., 13.]), 'dynamicTrap': False, 'previousTarget': array([32., 13.]), 'currentState': array([32.24336966, 13.82808821,  5.84076002]), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 0.863109997500308}
episode index:272
target Thresh 71.9948149308747
target distance 18.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'dynamicTrap': False, 'previousTarget': array([13., 11.]), 'currentState': array([29.96497001,  3.64665024,  3.1169765 ]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 18.49005030254622}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6964263108823887
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'dynamicTrap': False, 'previousTarget': array([13., 11.]), 'currentState': array([12.97568478, 10.18396334,  2.57896003]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 0.816398839715218}
episode index:273
target Thresh 72.03466718817785
target distance 13.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([2.07389501, 7.14727932, 2.38261491]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 14.837868374611293}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.69711959030149
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([10.23765035, 19.32489323,  0.94276789]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8286933136816872}
episode index:274
target Thresh 72.07412290889535
target distance 11.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 16.]), 'dynamicTrap': False, 'previousTarget': array([32., 16.]), 'currentState': array([24.4800287 ,  5.49091036,  2.24591458]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 12.922497186331661}
done in step count: 12
reward sum = 0.8572749216161293
running average episode reward sum: 0.6977019733244523
{'scaleFactor': 20, 'currentTarget': array([32., 16.]), 'dynamicTrap': False, 'previousTarget': array([32., 16.]), 'currentState': array([31.84873612, 15.93678459,  5.24835116]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 0.16394191295815017}
episode index:275
target Thresh 72.11318603863218
target distance 4.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'dynamicTrap': False, 'previousTarget': array([8., 8.]), 'currentState': array([ 8.99335051, 10.57303717,  6.23330915]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 2.75812718072295}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.698725154580523
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'dynamicTrap': False, 'previousTarget': array([8., 8.]), 'currentState': array([8.9928959 , 7.76614075, 3.43066907]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 1.0200649066441059}
episode index:276
target Thresh 72.15186048373384
target distance 28.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.88439606, 14.33951253]), 'dynamicTrap': False, 'previousTarget': array([15.11381692, 14.86933753]), 'currentState': array([34.86587903, 15.19994164,  3.85383737]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6283963289332003
running average episode reward sum: 0.6984712599030959
{'scaleFactor': 20, 'currentTarget': array([ 7., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 14.]), 'currentState': array([ 6.41329775, 13.19363571,  2.37942817]), 'targetState': array([ 7, 14], dtype=int32), 'currentDistance': 0.9972175771518184}
episode index:277
target Thresh 72.19015011167708
target distance 28.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.1147938 , 10.35817747]), 'dynamicTrap': False, 'previousTarget': array([17., 11.]), 'currentState': array([33.57705049, 21.71573463,  4.25364149]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6867439971323929
running average episode reward sum: 0.6984290755046402
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.68439039, 1.17535086, 6.09455682]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.8829811070215522}
episode index:278
target Thresh 72.22805875145662
target distance 25.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.64327546, 14.38952734]), 'dynamicTrap': False, 'previousTarget': array([14.14246323, 14.38290441]), 'currentState': array([32.36828425, 11.08437552,  3.01852107]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.69877024112521
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.38978792, 15.64054424,  1.04038084]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.7498210072867101}
episode index:279
target Thresh 72.265590193968
target distance 27.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.59365155, 14.9591028 ]), 'dynamicTrap': False, 'previousTarget': array([11.79416933, 14.19604781]), 'currentState': array([29.29839565,  5.65630556,  2.40255097]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6979375346872735
running average episode reward sum: 0.698767267173646
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.9591085 , 19.50785315,  0.71438584]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 1.0780063243812794}
episode index:280
target Thresh 72.30274819238674
target distance 11.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 13.]), 'dynamicTrap': False, 'previousTarget': array([16., 13.]), 'currentState': array([5.86415921, 8.70849732, 1.27053002]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 11.00691890109843}
done in step count: 12
reward sum = 0.8589729225756189
running average episode reward sum: 0.6993373940611974
{'scaleFactor': 20, 'currentTarget': array([15.91221594, 13.85334315]), 'dynamicTrap': True, 'previousTarget': array([16., 13.]), 'currentState': array([16.1768696 , 14.08445789,  4.8480423 ]), 'targetState': array([16, 13], dtype=int32), 'currentDistance': 0.3513624671666519}
episode index:281
target Thresh 72.33953646254369
target distance 9.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'dynamicTrap': False, 'previousTarget': array([21.,  4.]), 'currentState': array([28.30413671,  3.1094842 ,  4.0592742 ]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 7.358222026832926}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7002638430538882
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'dynamicTrap': False, 'previousTarget': array([21.,  4.]), 'currentState': array([21.86020554,  4.3279923 ,  3.29123473]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.9206152943871763}
episode index:282
target Thresh 72.37595868329649
target distance 3.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([26.0129692 , 16.9586724 ,  0.86487842]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 3.1633393965108603}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.701252663396454
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([29.05991055, 17.06391608,  0.62332976]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.9379991402595422}
episode index:283
target Thresh 72.41201849689757
target distance 26.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.25891817, 19.64127722]), 'dynamicTrap': False, 'previousTarget': array([22.88854382, 18.94427191]), 'currentState': array([ 4.91148601, 11.68064076,  0.61341453]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.7016634178830764
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([31.04508629, 23.35251163,  5.30224854]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 0.3553832085141242}
episode index:284
target Thresh 72.44771950935835
target distance 18.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'dynamicTrap': False, 'previousTarget': array([19.,  9.]), 'currentState': array([36.49271313, 15.36272156,  3.45520854]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 18.613952782145684}
done in step count: 22
reward sum = 0.7510420769251654
running average episode reward sum: 0.7018366763358557
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'dynamicTrap': False, 'previousTarget': array([19.,  9.]), 'currentState': array([18.42447515,  8.82530084,  0.56283966]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.601455438932682}
episode index:285
target Thresh 72.48306529080982
target distance 17.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'dynamicTrap': False, 'previousTarget': array([20., 10.]), 'currentState': array([ 3.62921414, 15.56092205,  0.17761683]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 17.289490556479745}
done in step count: 18
reward sum = 0.7812187495138618
running average episode reward sum: 0.7021142360322823
{'scaleFactor': 20, 'currentTarget': array([20., 10.]), 'dynamicTrap': False, 'previousTarget': array([20., 10.]), 'currentState': array([20.28863858,  9.6132396 ,  3.16969897]), 'targetState': array([20, 10], dtype=int32), 'currentDistance': 0.48259283192691993}
episode index:286
target Thresh 72.5180593758596
target distance 22.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.28251688, 10.6315604 ]), 'dynamicTrap': False, 'previousTarget': array([ 6.08213587, 10.81071492]), 'currentState': array([26.02694246,  7.44445223,  3.71970749]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7908031662282484
running average episode reward sum: 0.702423256694986
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.51687664, 11.19073725,  2.3666214 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.5194120532281405}
episode index:287
target Thresh 72.55270526394531
target distance 9.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 15.]), 'dynamicTrap': False, 'previousTarget': array([33., 15.]), 'currentState': array([31.62377803,  7.45723723,  2.83250189]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 7.667284855735348}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.703286335838059
{'scaleFactor': 20, 'currentTarget': array([33., 15.]), 'dynamicTrap': False, 'previousTarget': array([33., 15.]), 'currentState': array([32.71132337, 14.42976776,  0.16217598]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 0.6391392644985084}
episode index:288
target Thresh 72.58700641968467
target distance 31.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.97462467, 20.52745081]), 'dynamicTrap': False, 'previousTarget': array([24.50882004, 20.40521743]), 'currentState': array([ 6.55647086, 15.73836922,  0.60420483]), 'targetState': array([36, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.7036266273733566
{'scaleFactor': 20, 'currentTarget': array([36., 23.]), 'dynamicTrap': False, 'previousTarget': array([36., 23.]), 'currentState': array([36.69304556, 22.30830017,  1.2024154 ]), 'targetState': array([36, 23], dtype=int32), 'currentDistance': 0.9791633164348317}
episode index:289
target Thresh 72.62096627322182
target distance 7.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'dynamicTrap': False, 'previousTarget': array([23., 12.]), 'currentState': array([27.6300613 , 17.96971213,  3.49116862]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 7.554795204299256}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7045127286927587
{'scaleFactor': 20, 'currentTarget': array([23., 12.]), 'dynamicTrap': False, 'previousTarget': array([23., 12.]), 'currentState': array([23.94007754, 12.553749  ,  3.2039988 ]), 'targetState': array([23, 12], dtype=int32), 'currentDistance': 1.091047084075576}
episode index:290
target Thresh 72.65458822057043
target distance 20.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.26620776,  5.01300496]), 'dynamicTrap': False, 'previousTarget': array([10.02495322,  5.00124766]), 'currentState': array([30.24238455,  5.98889484,  0.20442152]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8243253953273639
running average episode reward sum: 0.70492445606951
{'scaleFactor': 20, 'currentTarget': array([10.,  5.]), 'dynamicTrap': False, 'previousTarget': array([10.,  5.]), 'currentState': array([9.28259015, 5.19108713, 3.77493467]), 'targetState': array([10,  5], dtype=int32), 'currentDistance': 0.7424225108587675}
episode index:291
target Thresh 72.68787562395326
target distance 2.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([26.93000267, 20.46153242,  2.01050824]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 2.6313583584468905}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7058332730007789
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([26.23087883, 17.92725598,  5.53667325]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.2420676085415362}
episode index:292
target Thresh 72.72083181213836
target distance 32.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.99596794, 19.82897181]), 'dynamicTrap': False, 'previousTarget': array([22.84555753, 19.51930531]), 'currentState': array([ 4.26662796, 23.10817043,  5.99195761]), 'targetState': array([35, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.7061878620629648
{'scaleFactor': 20, 'currentTarget': array([35., 18.]), 'dynamicTrap': False, 'previousTarget': array([35., 18.]), 'currentState': array([35.55809573, 18.00878767,  0.2599377 ]), 'targetState': array([35, 18], dtype=int32), 'currentDistance': 0.5581649147479708}
episode index:293
target Thresh 72.75346008077202
target distance 24.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.77514333, 11.25089447]), 'dynamicTrap': False, 'previousTarget': array([25., 11.]), 'currentState': array([ 5.83539863, 12.80221044,  0.9326396 ]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.7062766903200784
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([28.64234638, 11.16239026,  1.0034643 ]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.39279346297815887}
episode index:294
target Thresh 72.7857636927083
target distance 22.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.21443893, 17.26560622]), 'dynamicTrap': False, 'previousTarget': array([19.52085402, 17.66475581]), 'currentState': array([33.91287867,  3.70257303,  5.00857747]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7544859329390313
running average episode reward sum: 0.7064401114814987
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'dynamicTrap': False, 'previousTarget': array([13., 23.]), 'currentState': array([13.65649633, 23.84790583,  2.17696485]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.0723486944810827}
episode index:295
target Thresh 72.81774587833532
target distance 3.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'dynamicTrap': False, 'previousTarget': array([3., 7.]), 'currentState': array([2.44449643, 5.49605299, 0.2913152 ]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 1.6032594384420795}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.707398084077845
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'dynamicTrap': False, 'previousTarget': array([3., 7.]), 'currentState': array([3.04304636, 6.73212933, 1.96510482]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.27130735761795616}
episode index:296
target Thresh 72.84940983589827
target distance 11.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 11.]), 'dynamicTrap': False, 'previousTarget': array([35., 11.]), 'currentState': array([27.45380615, 21.95454833,  3.73791587]), 'targetState': array([35, 11], dtype=int32), 'currentDistance': 13.302149105497987}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7080613298385553
{'scaleFactor': 20, 'currentTarget': array([35., 11.]), 'dynamicTrap': False, 'previousTarget': array([35., 11.]), 'currentState': array([35.802783  , 10.64892007,  3.43346882]), 'targetState': array([35, 11], dtype=int32), 'currentDistance': 0.8761949880258396}
episode index:297
target Thresh 72.88075873181931
target distance 7.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 14.]), 'dynamicTrap': False, 'previousTarget': array([38., 14.]), 'currentState': array([31.64729111, 20.69334842,  0.52426565]), 'targetState': array([38, 14], dtype=int32), 'currentDistance': 9.22809965839713}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7088446144679594
{'scaleFactor': 20, 'currentTarget': array([38., 14.]), 'dynamicTrap': False, 'previousTarget': array([38., 14.]), 'currentState': array([38.31074086, 14.08525786,  5.51283616]), 'targetState': array([38, 14], dtype=int32), 'currentDistance': 0.3222247470634676}
episode index:298
target Thresh 72.91179570101417
target distance 6.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  7.]), 'dynamicTrap': False, 'previousTarget': array([33.,  7.]), 'currentState': array([28.61363823,  7.60764894,  6.23954427]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 4.428250987361015}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7097518231152238
{'scaleFactor': 20, 'currentTarget': array([33.,  7.]), 'dynamicTrap': False, 'previousTarget': array([33.,  7.]), 'currentState': array([32.18302502,  7.10216841,  6.24323303]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 0.8233386300689255}
episode index:299
target Thresh 72.94252384720562
target distance 2.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([10.01523868, 19.85200856,  2.10315394]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 2.112023973286287}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7106529837048396
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([ 9.0311803 , 18.27710575,  5.49739683]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.2788544589931228}
episode index:300
target Thresh 72.97294624323388
target distance 13.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([33.15802813,  3.45721359,  3.10472918]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 11.573453879883111}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7113576073285044
{'scaleFactor': 20, 'currentTarget': array([34., 15.]), 'dynamicTrap': False, 'previousTarget': array([34., 15.]), 'currentState': array([34.26766529, 15.52516339,  0.1367977 ]), 'targetState': array([34, 15], dtype=int32), 'currentDistance': 0.5894415064389983}
episode index:301
target Thresh 73.00306593136389
target distance 22.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.54537908, 21.04638341]), 'dynamicTrap': False, 'previousTarget': array([23.9793708 , 20.90815322]), 'currentState': array([ 4.5555391 , 21.68379824,  0.40389562]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.753268576256815
running average episode reward sum: 0.7114963853713133
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([26.66839875, 20.99376036,  5.34559554]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.6684278742558164}
episode index:302
target Thresh 73.03288592358962
target distance 15.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'dynamicTrap': False, 'previousTarget': array([13., 10.]), 'currentState': array([27.05514889,  5.60728881,  5.12628961]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 14.725594110665632}
done in step count: 15
reward sum = 0.8329259788737768
running average episode reward sum: 0.7118971431056449
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'dynamicTrap': False, 'previousTarget': array([13., 10.]), 'currentState': array([13.35621553,  9.15155305,  0.86794987]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.9201911409584607}
episode index:303
target Thresh 73.06240920193507
target distance 5.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([10.02955929,  6.97545603,  4.5802219 ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 7.074717612430454}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7127152314835868
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.96145226, 2.93005637, 4.28036833]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.9308548656763602}
episode index:304
target Thresh 73.09163871875273
target distance 22.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.76970701, 12.05588822]), 'dynamicTrap': False, 'previousTarget': array([34.20732955, 12.72394111]), 'currentState': array([17.69317472, 20.61386582,  0.65449816]), 'targetState': array([38, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7131983236906613
{'scaleFactor': 20, 'currentTarget': array([38., 11.]), 'dynamicTrap': False, 'previousTarget': array([38., 11.]), 'currentState': array([37.99573046, 11.86307408,  5.97926563]), 'targetState': array([38, 11], dtype=int32), 'currentDistance': 0.8630846442448191}
episode index:305
target Thresh 73.12057739701862
target distance 13.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 19.]), 'currentState': array([2.82025471, 7.67334387, 2.68780285]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 11.534488655594169}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7139135754037865
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.76854644, 18.65482143,  6.232301  ]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.4155947515650485}
episode index:306
target Thresh 73.1492281306247
target distance 15.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'dynamicTrap': False, 'previousTarget': array([6., 6.]), 'currentState': array([ 6.94996763, 19.40215664,  5.69108677]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 13.435782113102851}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7145938070618456
{'scaleFactor': 20, 'currentTarget': array([6., 6.]), 'dynamicTrap': False, 'previousTarget': array([6., 6.]), 'currentState': array([6.30119839, 6.40768062, 5.180894  ]), 'targetState': array([6, 6], dtype=int32), 'currentDistance': 0.5068766728705761}
episode index:307
target Thresh 73.17759378466818
target distance 17.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'dynamicTrap': False, 'previousTarget': array([14., 21.]), 'currentState': array([30.3290903 , 19.86251063,  4.74816799]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 16.368661276593382}
done in step count: 25
reward sum = 0.6792844033820735
running average episode reward sum: 0.7144791661408074
{'scaleFactor': 20, 'currentTarget': array([14., 21.]), 'dynamicTrap': False, 'previousTarget': array([14., 21.]), 'currentState': array([14.30247205, 21.17602298,  5.85591787]), 'targetState': array([14, 21], dtype=int32), 'currentDistance': 0.3499620417579499}
episode index:308
target Thresh 73.2056771957381
target distance 12.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 19.]), 'dynamicTrap': False, 'previousTarget': array([30., 19.]), 'currentState': array([25.2782667 ,  8.50933738,  2.24385023]), 'targetState': array([30, 19], dtype=int32), 'currentDistance': 11.504293442093543}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7151531646142285
{'scaleFactor': 20, 'currentTarget': array([30., 19.]), 'dynamicTrap': False, 'previousTarget': array([30., 19.]), 'currentState': array([30.17551166, 19.17331643,  0.16021708]), 'targetState': array([30, 19], dtype=int32), 'currentDistance': 0.24666359437164037}
episode index:309
target Thresh 73.233481172199
target distance 13.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([32.64494374, 15.71555751,  5.45269537]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 12.60276914872402}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7157930487525169
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([27.69356032,  4.09682124,  2.74907374]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 0.32137148516754555}
episode index:310
target Thresh 73.26100849447168
target distance 16.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'dynamicTrap': False, 'previousTarget': array([23.,  5.]), 'currentState': array([8.64096252, 3.69127837, 1.23218315]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 14.418554354512265}
done in step count: 16
reward sum = 0.7772466328110048
running average episode reward sum: 0.7159906487012581
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'dynamicTrap': False, 'previousTarget': array([23.,  5.]), 'currentState': array([23.62902682,  5.11507534,  0.25605965]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 0.6394662453409918}
episode index:311
target Thresh 73.28826191531131
target distance 6.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 11.]), 'dynamicTrap': False, 'previousTarget': array([38., 11.]), 'currentState': array([33.49084751,  8.33310204,  0.71684021]), 'targetState': array([38, 11], dtype=int32), 'currentDistance': 5.23877857196508}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7168371530323437
{'scaleFactor': 20, 'currentTarget': array([38., 11.]), 'dynamicTrap': False, 'previousTarget': array([38., 11.]), 'currentState': array([37.06664026, 10.05303181,  0.29520726]), 'targetState': array([38, 11], dtype=int32), 'currentDistance': 1.329627449588468}
episode index:312
target Thresh 73.31524416008267
target distance 22.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.03748579, 18.16900638]), 'dynamicTrap': False, 'previousTarget': array([ 6.9414844 , 18.06407315]), 'currentState': array([26.32858318, 12.89135904,  2.2276205 ]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6834448814051547
running average episode reward sum: 0.716730468458455
{'scaleFactor': 20, 'currentTarget': array([ 4., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 19.]), 'currentState': array([ 4.28343847, 19.5873058 ,  3.20177242]), 'targetState': array([ 4, 19], dtype=int32), 'currentDistance': 0.6521238125983786}
episode index:313
target Thresh 73.34195792703274
target distance 28.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.84158612, 12.38255002]), 'dynamicTrap': False, 'previousTarget': array([21.88618308, 12.86933753]), 'currentState': array([ 2.87008408, 13.44983931,  6.0631671 ]), 'targetState': array([30, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7171055744870908
{'scaleFactor': 20, 'currentTarget': array([30., 12.]), 'dynamicTrap': False, 'previousTarget': array([30., 12.]), 'currentState': array([29.30902114, 11.48816752,  5.6546424 ]), 'targetState': array([30, 12], dtype=int32), 'currentDistance': 0.8598978294154014}
episode index:314
target Thresh 73.36840588756047
target distance 30.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.41205128, 17.95934088]), 'dynamicTrap': False, 'previousTarget': array([17.0992562 , 18.00992562]), 'currentState': array([37.32769472, 19.79431945,  5.3900671 ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7048324943693189
running average episode reward sum: 0.7170666123279867
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.45695826, 17.46193906,  2.79624203]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.6497680647528521}
episode index:315
target Thresh 73.39459068648395
target distance 25.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.24453361, 14.48558592]), 'dynamicTrap': False, 'previousTarget': array([18.06369443, 14.40509555]), 'currentState': array([38.15935373, 16.32947842,  1.76169264]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.702795580213738
running average episode reward sum: 0.7170214508339543
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'dynamicTrap': False, 'previousTarget': array([13., 14.]), 'currentState': array([12.82627816, 14.67618255,  5.94331833]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.6981419060219437}
episode index:316
target Thresh 73.4205149423049
target distance 7.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'dynamicTrap': False, 'previousTarget': array([10.,  4.]), 'currentState': array([8.70229335, 9.32469886, 4.34933925]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 5.48055293563977}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7177898248376327
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'dynamicTrap': False, 'previousTarget': array([10.,  4.]), 'currentState': array([9.2092238 , 3.64121453, 4.62464797]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.8683628348238894}
episode index:317
target Thresh 73.4461812474705
target distance 28.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.75804702, 14.17841133]), 'dynamicTrap': False, 'previousTarget': array([23.6295994 , 13.44442825]), 'currentState': array([5.66292977, 5.65980043, 0.77924049]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 35
reward sum = 0.663762972583466
running average episode reward sum: 0.7176199290758272
{'scaleFactor': 20, 'currentTarget': array([34., 19.]), 'dynamicTrap': False, 'previousTarget': array([34., 19.]), 'currentState': array([34.81669876, 18.84339496,  4.57410844]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 0.8315780243757562}
episode index:318
target Thresh 73.47159216863268
target distance 18.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.47387627, 21.36794544]), 'dynamicTrap': False, 'previousTarget': array([21.80368799, 20.36442559]), 'currentState': array([8.81377459, 6.75967305, 2.53135329]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7180664445164713
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'dynamicTrap': False, 'previousTarget': array([24., 23.]), 'currentState': array([23.85676816, 22.79279087,  5.39055396]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.2518947858881146}
episode index:319
target Thresh 73.49675024690468
target distance 21.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.46187165, 20.55587431]), 'dynamicTrap': True, 'previousTarget': array([21.87838597, 20.3829006 ]), 'currentState': array([14.       ,  2.       ,  6.1705337], dtype=float32), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.756441442279757
running average episode reward sum: 0.7181863663844815
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'dynamicTrap': False, 'previousTarget': array([23., 23.]), 'currentState': array([22.2077488 , 22.28003095,  5.29868803]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 1.070522017238227}
episode index:320
target Thresh 73.52165799811533
target distance 21.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.49067303, 13.5335218 ]), 'dynamicTrap': False, 'previousTarget': array([25.23047895, 13.50557744]), 'currentState': array([ 6.63407656, 20.19901273,  0.07434988]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.43267604526222125
running average episode reward sum: 0.7172969261317642
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'dynamicTrap': False, 'previousTarget': array([27., 13.]), 'currentState': array([27.56224251, 12.62813264,  3.82497394]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.6740934435828003}
episode index:321
target Thresh 73.5463179130605
target distance 13.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([29.19533521, 22.36605726,  2.0803635 ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 13.200411706538326}
done in step count: 15
reward sum = 0.7942327813748413
running average episode reward sum: 0.7175358573592271
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([16.5260753 , 21.23176765,  3.60391655]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.9310940651551375}
episode index:322
target Thresh 73.57073245775224
target distance 12.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 16.]), 'dynamicTrap': False, 'previousTarget': array([16., 16.]), 'currentState': array([23.01867444,  4.27035681,  3.03698272]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 13.669174086824032}
done in step count: 24
reward sum = 0.6889554018579328
running average episode reward sum: 0.7174473729768701
{'scaleFactor': 20, 'currentTarget': array([17.65258025, 15.28080407]), 'dynamicTrap': True, 'previousTarget': array([16., 16.]), 'currentState': array([17.74099904, 15.11806931,  2.06464103]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.18520390544740753}
episode index:323
target Thresh 73.59490407366536
target distance 23.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.6678813 , 3.37180982]), 'dynamicTrap': False, 'previousTarget': array([7.65859887, 4.02547777]), 'currentState': array([25.40057914, 10.37795224,  5.28820407]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7979232683635068
running average episode reward sum: 0.7176957553700387
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([3.70358328, 1.95886826, 3.71114173]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7047845384319109}
episode index:324
target Thresh 73.61883517798158
target distance 5.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 17.]), 'dynamicTrap': False, 'previousTarget': array([38., 17.]), 'currentState': array([36.92048523, 20.52682886,  6.18651187]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 3.688342980972216}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7185031530458232
{'scaleFactor': 20, 'currentTarget': array([38., 17.]), 'dynamicTrap': False, 'previousTarget': array([38., 17.]), 'currentState': array([37.84032773, 17.16185   ,  5.39570606]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 0.22735579015183802}
episode index:325
target Thresh 73.6425281638313
target distance 6.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 10.]), 'dynamicTrap': False, 'previousTarget': array([34., 10.]), 'currentState': array([32.84023575, 14.76090662,  4.9459976 ]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 4.900131118971973}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7193055973616336
{'scaleFactor': 20, 'currentTarget': array([34., 10.]), 'dynamicTrap': False, 'previousTarget': array([34., 10.]), 'currentState': array([33.73610925, 10.94627593,  4.43973209]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 0.9823830534948172}
episode index:326
target Thresh 73.66598540053283
target distance 28.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.67061009, 11.77004566]), 'dynamicTrap': False, 'previousTarget': array([11.6170994 , 11.87838597]), 'currentState': array([28.64566168,  3.00092253,  2.76716661]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7196579159062466
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.18850996, 15.34417361,  2.12801807]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.6823813195713436}
episode index:327
target Thresh 73.6892092338294
target distance 8.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  6.]), 'dynamicTrap': False, 'previousTarget': array([35.,  6.]), 'currentState': array([27.25782773,  3.29890497,  5.98244309]), 'targetState': array([35,  6], dtype=int32), 'currentDistance': 8.199825963454005}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7203631968025691
{'scaleFactor': 20, 'currentTarget': array([35.,  6.]), 'dynamicTrap': False, 'previousTarget': array([35.,  6.]), 'currentState': array([35.29250114,  5.36355144,  0.61557084]), 'targetState': array([35,  6], dtype=int32), 'currentDistance': 0.700445345093013}
episode index:328
target Thresh 73.71220198612369
target distance 35.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.68438729,  7.56830409]), 'dynamicTrap': False, 'previousTarget': array([22.61161351,  7.0776773 ]), 'currentState': array([ 4.27816127, 12.40549248,  1.39093301]), 'targetState': array([38,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.032919130909374705
running average episode reward sum: 0.7182737011615563
{'scaleFactor': 20, 'currentTarget': array([38.,  4.]), 'dynamicTrap': False, 'previousTarget': array([38.,  4.]), 'currentState': array([38.60034698,  4.98702893,  5.55755485]), 'targetState': array([38,  4], dtype=int32), 'currentDistance': 1.1552673321715512}
episode index:329
target Thresh 73.73496595671007
target distance 10.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'dynamicTrap': False, 'previousTarget': array([8., 5.]), 'currentState': array([ 1.8389743 , 13.73659559,  4.92277312]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 10.690478949489547}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7189500843380394
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'dynamicTrap': False, 'previousTarget': array([8., 5.]), 'currentState': array([7.11862139, 5.30295076, 4.11290631]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.931991099464355}
episode index:330
target Thresh 73.7575034220046
target distance 19.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.15072657, 15.31373742]), 'dynamicTrap': False, 'previousTarget': array([25.29822396, 14.88271492]), 'currentState': array([11.59844079,  1.59400789,  0.76124543]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7516885024779356
running average episode reward sum: 0.7190489919457129
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([28.61884708, 18.90935775,  5.00212462]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.9860066226289467}
episode index:331
target Thresh 73.77981663577259
target distance 7.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'dynamicTrap': False, 'previousTarget': array([23.,  3.]), 'currentState': array([30.34004538,  4.3757983 ,  3.90876961]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 7.467870318644786}
done in step count: 7
reward sum = 0.88451104233491
running average episode reward sum: 0.7195473716155599
{'scaleFactor': 20, 'currentTarget': array([25.71393497,  2.51281763]), 'dynamicTrap': True, 'previousTarget': array([25.53192911,  2.49017459]), 'currentState': array([26.60644207,  3.34862087,  3.04099458]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.2227575385935694}
episode index:332
target Thresh 73.80190782935398
target distance 13.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.86222284, 14.62846571]), 'dynamicTrap': True, 'previousTarget': array([26., 16.]), 'currentState': array([13.        , 16.        ,  0.06402928], dtype=float32), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 11.941249390347041}
done in step count: 10
reward sum = 0.8844820750088044
running average episode reward sum: 0.7200426710251493
{'scaleFactor': 20, 'currentTarget': array([26., 16.]), 'dynamicTrap': False, 'previousTarget': array([26., 16.]), 'currentState': array([26.38149626, 16.64441837,  5.65944762]), 'targetState': array([26, 16], dtype=int32), 'currentDistance': 0.7488754446569935}
episode index:333
target Thresh 73.82377921188656
target distance 5.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 11.]), 'dynamicTrap': False, 'previousTarget': array([33., 11.]), 'currentState': array([34.04109754, 16.02893249,  0.15582824]), 'targetState': array([33, 11], dtype=int32), 'currentDistance': 5.135566770275477}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7207919414711818
{'scaleFactor': 20, 'currentTarget': array([33., 11.]), 'dynamicTrap': False, 'previousTarget': array([33., 11.]), 'currentState': array([33.82512696, 11.13310518,  4.9396739 ]), 'targetState': array([33, 11], dtype=int32), 'currentDistance': 0.8357939318440017}
episode index:334
target Thresh 73.8454329705268
target distance 16.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  7.]), 'dynamicTrap': False, 'previousTarget': array([30.,  7.]), 'currentState': array([27.33050311, 21.30427009,  5.83758187]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 14.551232129970467}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7213672408921145
{'scaleFactor': 20, 'currentTarget': array([30.,  7.]), 'dynamicTrap': False, 'previousTarget': array([30.,  7.]), 'currentState': array([30.18945794,  7.64345493,  4.70457658]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 0.6707671405184039}
episode index:335
target Thresh 73.86687127066863
target distance 15.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.85786438,  3.85786438]), 'currentState': array([26.20307804, 16.36379534,  5.01185131]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 19.50175509366058}
done in step count: 18
reward sum = 0.797415687057892
running average episode reward sum: 0.7215935755533222
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([12.62540097,  2.34254914,  3.79042181]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.9073962830818596}
episode index:336
target Thresh 73.8880962561599
target distance 5.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 16.]), 'currentState': array([ 4.74986421, 12.75685011,  0.86739147]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 3.475753260493257}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7223315738454488
{'scaleFactor': 20, 'currentTarget': array([ 6., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 16.]), 'currentState': array([ 6.09494725, 16.41048994,  0.55107933]), 'targetState': array([ 6, 16], dtype=int32), 'currentDistance': 0.4213276297598746}
episode index:337
target Thresh 73.90911004951687
target distance 29.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.83490935, 13.84933123]), 'dynamicTrap': False, 'previousTarget': array([25.76435286, 13.18845838]), 'currentState': array([7.67420569, 5.47145306, 0.80874205]), 'targetState': array([37, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7310129152605904
running average episode reward sum: 0.7223572582875054
{'scaleFactor': 20, 'currentTarget': array([37., 19.]), 'dynamicTrap': False, 'previousTarget': array([37., 19.]), 'currentState': array([37.18682377, 19.33106705,  5.96306676]), 'targetState': array([37, 19], dtype=int32), 'currentDistance': 0.38014275303327294}
episode index:338
target Thresh 73.92991475213636
target distance 16.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'dynamicTrap': False, 'previousTarget': array([15., 10.]), 'currentState': array([30.92665164, 10.95113976,  1.34443241]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 15.955027404048229}
done in step count: 19
reward sum = 0.7738305521849311
running average episode reward sum: 0.7225090969125716
{'scaleFactor': 20, 'currentTarget': array([14.93947752,  8.71766466]), 'dynamicTrap': True, 'previousTarget': array([15., 10.]), 'currentState': array([14.73924904,  8.89117507,  2.09400764]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.26494774735345483}
episode index:339
target Thresh 73.95051244450599
target distance 4.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 3.81330819, 19.3559896 ,  2.36522448]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 3.206068866675356}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.72326671721577
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.97942865, 22.29212142,  2.13107879]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.29284484335851996}
episode index:340
target Thresh 73.97090518641218
target distance 2.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'dynamicTrap': False, 'previousTarget': array([26.,  9.]), 'currentState': array([26.76809075,  9.17790768,  4.41087493]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7884253577441153}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7240782517693893
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'dynamicTrap': False, 'previousTarget': array([26.,  9.]), 'currentState': array([26.76809075,  9.17790768,  4.41087493]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.7884253577441153}
episode index:341
target Thresh 73.99109501714608
target distance 26.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.49796889, 15.14071321]), 'dynamicTrap': False, 'previousTarget': array([19.10027853, 14.54221128]), 'currentState': array([36.44392455,  8.73362619,  3.64671457]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.724376761629232
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'dynamicTrap': False, 'previousTarget': array([12., 17.]), 'currentState': array([11.70471191, 16.26769793,  0.99418814]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.7895957091945216}
episode index:342
target Thresh 74.01108395570759
target distance 18.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.10997653, 22.1666494 ]), 'dynamicTrap': True, 'previousTarget': array([18., 22.]), 'currentState': array([19.       ,  4.       ,  4.2280664], dtype=float32), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 18.188438417156515}
done in step count: 16
reward sum = 0.8217567710948755
running average episode reward sum: 0.724660668362368
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'dynamicTrap': False, 'previousTarget': array([18., 22.]), 'currentState': array([17.41906868, 21.97094227,  1.30549241]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.5816575925345885}
episode index:343
target Thresh 74.03087400100725
target distance 11.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.81922733, 22.96448552]), 'dynamicTrap': True, 'previousTarget': array([35., 23.]), 'currentState': array([24.       , 16.       ,  0.2562338], dtype=float32), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 12.867001924528504}
done in step count: 8
reward sum = 0.9127446944279201
running average episode reward sum: 0.7252074242520934
{'scaleFactor': 20, 'currentTarget': array([35., 23.]), 'dynamicTrap': False, 'previousTarget': array([35., 23.]), 'currentState': array([35.54173144, 22.82915676,  0.16368665]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 0.5680320090249363}
episode index:344
target Thresh 74.05046713206606
target distance 10.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 16.]), 'currentState': array([5.5242319 , 5.71468475, 0.82495516]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 10.579089637069291}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7258070124366003
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.05292117, 16.01063028,  0.40677547]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.9471384875843132}
episode index:345
target Thresh 74.06986530821347
target distance 14.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 14.]), 'dynamicTrap': False, 'previousTarget': array([28., 14.]), 'currentState': array([15.32688842, 22.87434548,  5.20910144]), 'targetState': array([28, 14], dtype=int32), 'currentDistance': 15.471320716605318}
done in step count: 12
reward sum = 0.8668808817161292
running average episode reward sum: 0.7262147403824948
{'scaleFactor': 20, 'currentTarget': array([28., 14.]), 'dynamicTrap': False, 'previousTarget': array([28., 14.]), 'currentState': array([27.56944808, 13.16423654,  4.77522463]), 'targetState': array([28, 14], dtype=int32), 'currentDistance': 0.94014653935439}
episode index:346
target Thresh 74.08907046928324
target distance 3.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'dynamicTrap': False, 'previousTarget': array([24.,  6.]), 'currentState': array([26.50226892,  6.6574165 ,  2.79003465]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 2.587188858568922}
done in step count: 1
reward sum = 0.9801
running average episode reward sum: 0.7269463981911907
{'scaleFactor': 20, 'currentTarget': array([25.62502342,  6.7613988 ]), 'dynamicTrap': True, 'previousTarget': array([24.,  6.]), 'currentState': array([26.50226892,  6.6574165 ,  2.79003465]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.8833866578990228}
episode index:347
target Thresh 74.1080845358075
target distance 2.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'dynamicTrap': False, 'previousTarget': array([12., 14.]), 'currentState': array([11.49088811, 14.12163673,  4.04002374]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.5234409313928438}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7277310349779975
{'scaleFactor': 20, 'currentTarget': array([12., 14.]), 'dynamicTrap': False, 'previousTarget': array([12., 14.]), 'currentState': array([11.49088811, 14.12163673,  4.04002374]), 'targetState': array([12, 14], dtype=int32), 'currentDistance': 0.5234409313928438}
episode index:348
target Thresh 74.12690940920874
target distance 26.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.42062836, 10.78687763]), 'dynamicTrap': False, 'previousTarget': array([11.35987106, 10.77694787]), 'currentState': array([31.07292223,  7.07374614,  1.80101555]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7322192479648664
running average episode reward sum: 0.727743895187129
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.16906301, 11.03972981,  1.62831346]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.9750390416257281}
episode index:349
target Thresh 74.14554697198997
target distance 18.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.93651556,  6.04738412]), 'dynamicTrap': False, 'previousTarget': array([22.94818637,  6.71272322]), 'currentState': array([ 9.73910083, 20.13402371,  0.97948485]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7306147079670628
running average episode reward sum: 0.7277520975093573
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'dynamicTrap': False, 'previousTarget': array([26.,  4.]), 'currentState': array([26.50985551,  3.86773881,  4.66907169]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.5267311114407682}
episode index:350
target Thresh 74.16399908792303
target distance 30.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.15980901,  9.75052281]), 'dynamicTrap': False, 'previousTarget': array([22.1565257 ,  9.74695771]), 'currentState': array([3.00202878, 4.00774853, 5.44885349]), 'targetState': array([33, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.7278947449791289
{'scaleFactor': 20, 'currentTarget': array([33., 13.]), 'dynamicTrap': False, 'previousTarget': array([33., 13.]), 'currentState': array([32.30726365, 12.89720511,  3.90196764]), 'targetState': array([33, 13], dtype=int32), 'currentDistance': 0.7003216685437734}
episode index:351
target Thresh 74.18226760223489
target distance 3.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([30.63593209,  2.25606527,  0.37363761]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 3.0642876895245763}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7286112371808927
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([32.87981491,  5.2852032 ,  0.30748552]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 0.9248865549862333}
episode index:352
target Thresh 74.20035434179216
target distance 8.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 15.]), 'dynamicTrap': False, 'previousTarget': array([38., 15.]), 'currentState': array([34.46453282, 22.0810163 ,  4.49684346]), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 7.914563794082509}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.729268417840437
{'scaleFactor': 20, 'currentTarget': array([38., 15.]), 'dynamicTrap': False, 'previousTarget': array([38., 15.]), 'currentState': array([37.29536387, 15.61144941,  4.06740129]), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 0.9329429011773878}
episode index:353
target Thresh 74.21826111528392
target distance 22.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.28960969, 13.86287542]), 'dynamicTrap': False, 'previousTarget': array([16.18339664, 13.70226409]), 'currentState': array([36.25383785, 12.66721905,  2.26676577]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6930332453359002
running average episode reward sum: 0.729166058596074
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'dynamicTrap': False, 'previousTarget': array([14., 14.]), 'currentState': array([13.59659297, 14.15991802,  5.46122846]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.43394815845182105}
episode index:354
target Thresh 74.23598971340242
target distance 17.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'dynamicTrap': False, 'previousTarget': array([22.,  4.]), 'currentState': array([ 6.1216425 , 15.2556096 ,  6.11599857]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 19.463067191531852}
done in step count: 25
reward sum = 0.6719213973013626
running average episode reward sum: 0.7290048060290467
{'scaleFactor': 20, 'currentTarget': array([22.,  4.]), 'dynamicTrap': False, 'previousTarget': array([22.,  4.]), 'currentState': array([22.28905323,  4.51842456,  4.57020117]), 'targetState': array([22,  4], dtype=int32), 'currentDistance': 0.5935619535464087}
episode index:355
target Thresh 74.25354190902225
target distance 13.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'dynamicTrap': False, 'previousTarget': array([19., 13.]), 'currentState': array([6.90588355, 9.58287688, 1.67546165]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 12.567592573571687}
done in step count: 18
reward sum = 0.7892628178586257
running average episode reward sum: 0.7291740701072197
{'scaleFactor': 20, 'currentTarget': array([19., 13.]), 'dynamicTrap': False, 'previousTarget': array([19., 13.]), 'currentState': array([19.30562304, 13.30300303,  2.18253873]), 'targetState': array([19, 13], dtype=int32), 'currentDistance': 0.43036760856842726}
episode index:356
target Thresh 74.27091945737759
target distance 16.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 18.]), 'dynamicTrap': False, 'previousTarget': array([37.52228  , 17.6118525]), 'currentState': array([22.4511505 ,  6.92227561,  1.62013352]), 'targetState': array([38, 18], dtype=int32), 'currentDistance': 19.091429975535405}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7295650273695775
{'scaleFactor': 20, 'currentTarget': array([38., 18.]), 'dynamicTrap': False, 'previousTarget': array([38., 18.]), 'currentState': array([38.41982519, 18.22112434,  4.25306189]), 'targetState': array([38, 18], dtype=int32), 'currentDistance': 0.4744988583590251}
episode index:357
target Thresh 74.28812409623775
target distance 16.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'dynamicTrap': False, 'previousTarget': array([18., 15.]), 'currentState': array([1.48667601, 4.80320735, 2.47298551]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 19.407845057961488}
done in step count: 15
reward sum = 0.8511945059241272
running average episode reward sum: 0.7299047745163779
{'scaleFactor': 20, 'currentTarget': array([18., 15.]), 'dynamicTrap': False, 'previousTarget': array([18., 15.]), 'currentState': array([18.59782335, 15.58347804,  0.45225984]), 'targetState': array([18, 15], dtype=int32), 'currentDistance': 0.8353678150885439}
episode index:358
target Thresh 74.30515754608098
target distance 14.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([36.97598822, 16.29816589,  5.52223635]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 14.658146962046493}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7304162298728327
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([28.98057095,  4.12055354,  2.79270256]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 0.12210914778011414}
episode index:359
target Thresh 74.32202151026645
target distance 2.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 22.]), 'dynamicTrap': False, 'previousTarget': array([36., 22.]), 'currentState': array([34.8889183 , 21.37066885,  1.8300944 ]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 1.27693392208438}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.7311372959009638
{'scaleFactor': 20, 'currentTarget': array([36., 22.]), 'dynamicTrap': False, 'previousTarget': array([36., 22.]), 'currentState': array([35.61023513, 22.42864197,  0.09736997]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 0.5793535971041556}
episode index:360
target Thresh 74.3387176752046
target distance 17.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.07706459, 18.90950127]), 'dynamicTrap': False, 'previousTarget': array([22.85099785, 17.88715666]), 'currentState': array([35.04376852,  3.68240079,  2.55478668]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6221795907688862
running average episode reward sum: 0.730835474003091
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'dynamicTrap': False, 'previousTarget': array([22., 19.]), 'currentState': array([22.36862443, 18.55099582,  6.03233361]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.5809377986027553}
episode index:361
target Thresh 74.3552477105259
target distance 19.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'dynamicTrap': False, 'previousTarget': array([26., 20.]), 'currentState': array([ 6.91726766, 19.78645967,  5.35276389]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 19.083927083872386}
done in step count: 13
reward sum = 0.8595238197062927
running average episode reward sum: 0.7311909666707793
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'dynamicTrap': False, 'previousTarget': array([26., 20.]), 'currentState': array([25.11511171, 20.73970663,  0.11803361]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 1.153340010000905}
episode index:362
target Thresh 74.3716132692476
target distance 14.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4.98619962, 9.02870068]), 'dynamicTrap': True, 'previousTarget': array([5., 9.]), 'currentState': array([19.       , 17.       ,  1.0521194], dtype=float32), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 16.122289351020527}
done in step count: 16
reward sum = 0.7840852015757599
running average episode reward sum: 0.731336680816523
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([4.50479106, 9.44828113, 3.60804009]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6679729531269156}
episode index:363
target Thresh 74.38781598793923
target distance 10.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 20.]), 'currentState': array([11.3219269 , 13.56975108,  2.94247532]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 11.324593687859753}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.731913998037909
{'scaleFactor': 20, 'currentTarget': array([ 2., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 20.]), 'currentState': array([ 2.4449786 , 19.31320333,  3.21881962]), 'targetState': array([ 2, 20], dtype=int32), 'currentDistance': 0.8183493309159954}
episode index:364
target Thresh 74.40385748688617
target distance 7.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  8.]), 'dynamicTrap': False, 'previousTarget': array([28.,  8.]), 'currentState': array([33.61069587,  9.03617664,  4.55323935]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 5.705573606369243}
Traceback (most recent call last):
  File "/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40/DDPGHER_CNN.py", line 212, in <module>
    agent.train()
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 238, in train
    self.update_net(state, action, nextState, reward, info)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 195, in update_net
    self.actor_optimizer.step()
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py", line 93, in step
    exp_avg.mul_(beta1).add_(1 - beta1, grad)
KeyboardInterrupt

Process finished with exit code 1
