/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40/DDPGHER_CNN.py
episode index:0
target Thresh 15.199999999999996
target distance 9.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.8524805 ,  5.2652225 ,  0.14375251]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.469360008681909}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.68455017, 10.72323621,  1.7266253 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.411369152087844}
episode index:1
target Thresh 15.80497010805058
target distance 13.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.10572947,  1.14849165,  5.16217852]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.010652384185848}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.18673344,  3.66644973,  0.40616489]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.3569853778557}
episode index:2
target Thresh 16.403920662949282
target distance 2.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.59737396, 14.52989057,  4.47189593]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.6279611775466036}
done in step count: 99
reward sum = -0.059469191009059605
running average episode reward sum: -0.019823063669686534
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35.69606987, 14.90996101]), 'currentState': array([24.20768639,  8.20195158,  4.29169882]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.75490084176682}
episode index:3
target Thresh 16.9969115602507
target distance 7.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.95231866,  8.08143526,  6.25208759]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.983799027151309}
done in step count: 99
reward sum = -0.08756799886233657
running average episode reward sum: -0.03675929746784905
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.95161122,  6.93902271,  4.99964282]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.118279353038766}
episode index:4
target Thresh 17.58400209953875
target distance 2.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.00316559, 14.40154601,  5.86558378]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.0622133732308883}
done in step count: 99
reward sum = 0.0
running average episode reward sum: -0.02940743797427924
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.84261741, 11.7069213 ,  4.29403515]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.797002788328417}
episode index:5
target Thresh 18.165250990356583
target distance 3.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.01273377, 11.89088591,  4.99407208]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.2698960390291485}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.12471684406455337
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.63055311, 14.53437676,  2.18250333]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.594387082115039}
episode index:6
target Thresh 18.740716358077673
target distance 11.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.21309153,  5.26672274,  2.8257252 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.846712816236707}
done in step count: 99
reward sum = -0.18987080945229404
running average episode reward sum: 0.07977575070500374
{'scaleFactor': 20, 'currentTarget': array([36.35296865, 16.15809101]), 'dynamicTrap': True, 'previousTarget': array([36.38193943, 15.96053709]), 'currentState': array([18.51673891,  8.55384898,  2.75899854]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.389574217826066}
episode index:7
target Thresh 19.310455749718336
target distance 6.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([3.88533980e+01, 1.95869649e+01, 1.34388765e-02]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.990736467805219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06980378186687827
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.23482291, 19.60256554,  4.52539921]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.3693401734088}
episode index:8
target Thresh 19.87452613969254
target distance 5.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.56710243, 10.61365945,  1.3832342 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.653689341003026}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.1426011767658755
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.67001097, 14.35081741,  2.16797581]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.7282381421288271}
episode index:9
target Thresh 20.43298393550932
target distance 12.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.93597484,  1.48039822,  4.7749536 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.657511886494472}
done in step count: 99
reward sum = -0.023820705694111963
running average episode reward sum: 0.12595898851987675
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.85044903,  4.74178575,  1.26465474]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.430673736576823}
episode index:10
target Thresh 20.985884983413655
target distance 12.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.83811675,  4.23136823,  3.3202405 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.512208127260896}
done in step count: 99
reward sum = -0.1064622893933941
running average episode reward sum: 0.10482978143685212
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.10419864,  3.39075915,  4.70114235]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.88542748665248}
episode index:11
target Thresh 21.533284573971084
target distance 15.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.03900742, 14.97615335]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([20.      ,  3.      ,  2.547536], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.224983564897244}
done in step count: 99
reward sum = -0.20897204975367079
running average episode reward sum: 0.07867962883764187
{'scaleFactor': 20, 'currentTarget': array([35.00238963, 15.32744931]), 'dynamicTrap': True, 'previousTarget': array([35.03248171, 15.1743937 ]), 'currentState': array([22.71031453, 12.10771953,  2.62385423]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.706760801541684}
episode index:12
target Thresh 22.075237447596823
target distance 12.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.04932904, 14.83025233]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([23.       ,  6.       ,  1.7614567], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.938530263029769}
done in step count: 99
reward sum = -0.3373939841568322
running average episode reward sum: 0.04667396629960541
{'scaleFactor': 20, 'currentTarget': array([33.78301821, 16.13066097]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.55648942, 10.0482091 ,  4.54453973]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.65592278148452}
episode index:13
target Thresh 22.611797800029873
target distance 18.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.69684964, 16.06498226]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([17.      , 23.      ,  4.608022], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.079802510993936}
done in step count: 99
reward sum = -0.21164911111623408
running average episode reward sum: 0.02822231791275973
{'scaleFactor': 20, 'currentTarget': array([33.6427952 , 16.03979701]), 'dynamicTrap': True, 'previousTarget': array([33.76344897, 16.108429  ]), 'currentState': array([19.77809695, 21.5996743 ,  4.91543657]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.937941394305849}
episode index:14
target Thresh 23.143019287752598
target distance 14.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.55221272, 15.71980703]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.       ,  2.       ,  3.4100692], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10680440301484918
running average episode reward sum: 0.019220536517585804
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.06744579, 18.81733093,  5.08353835]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.897064334295097}
episode index:15
target Thresh 23.66895503335648
target distance 7.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.73704208, 23.54883176,  2.18677235]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.552875031262474}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.07397789387640646
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.47853632, 15.32717223,  0.1547848 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5796884272197328}
episode index:16
target Thresh 24.18965763085434
target distance 10.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.79304457,  6.87218728,  2.28311455]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.281260846129312}
done in step count: 70
reward sum = 0.36140639319410395
running average episode reward sum: 0.09088545265980044
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.0268225 , 15.81877447,  0.22108085]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.2717963971652684}
episode index:17
target Thresh 24.705179150939866
target distance 5.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.51804326,  9.94603075,  4.26310754]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.630516354452394}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.13313947035063795
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.36944181, 14.69890171,  3.61241523]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.698758758961226}
episode index:18
target Thresh 25.215571146194662
target distance 9.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.53263209, 17.30471087,  3.72570181]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.814939267384959}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.1683232134658173
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.09871507, 15.89133855,  5.43472884]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.2675957312180863}
episode index:19
target Thresh 25.720884656243566
target distance 7.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.19567404,  8.70924536,  5.4013865 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.293797126564346}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.20378310394247484
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.72303111, 14.04801175,  5.70240329]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9914602333499246}
episode index:20
target Thresh 26.2211702128587
target distance 21.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.3804685 , 13.70177164]), 'dynamicTrap': False, 'previousTarget': array([31.71663071, 13.28013989]), 'currentState': array([14.46046558,  4.82069652,  4.98602223]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.18482312015920363
running average episode reward sum: 0.20288024757184286
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.51242213, 15.25857467,  2.02304944]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5518994857433015}
episode index:21
target Thresh 26.71647784501262
target distance 24.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.98976473, 12.73575694]), 'dynamicTrap': False, 'previousTarget': array([28.88854382, 11.94427191]), 'currentState': array([11.76447672,  4.49932099,  2.10927444]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.2284411954459757
running average episode reward sum: 0.2040421088388489
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.78343607, 14.47874571,  2.15721704]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5644519211433194}
episode index:22
target Thresh 27.206857083881303
target distance 26.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.75926999, 16.18176692]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 16.22305213]), 'currentState': array([ 9.10849126, 19.90290856,  3.30500376]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.26021230486885566
running average episode reward sum: 0.1838571343298183
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.50369184, 12.29201039,  2.01316238]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.038126703874638}
episode index:23
target Thresh 27.692356967797288
target distance 26.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.84285811, 14.88880253]), 'dynamicTrap': False, 'previousTarget': array([29., 15.]), 'currentState': array([ 9.84750562, 14.45766594,  3.34641755]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.38528078124590337
running average episode reward sum: 0.19224978628465517
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.36510393, 14.57648255,  1.27502807]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5591671562194377}
episode index:24
target Thresh 28.17302604715355
target distance 1.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.65383726, 13.88654943,  4.8136121 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.746976329188368}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.22221900080930898
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.74191617, 14.43023567,  2.83249735]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.6254907290705564}
episode index:25
target Thresh 28.648912389258577
target distance 19.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([17.49818681, 19.12842914,  1.095514  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.98214092610581}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.234929172976735
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.71925979, 14.57826017,  2.36124771]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5066355187990671}
episode index:26
target Thresh 29.120063583143168
target distance 28.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.44614779, 13.84002808]), 'dynamicTrap': False, 'previousTarget': array([26.79898987, 13.82842712]), 'currentState': array([ 6.62754538, 11.15246442,  3.38494515]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5256963012782757
running average episode reward sum: 0.24569832587679205
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.39062252, 14.85305963,  0.21744281]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.6268431885204088}
episode index:27
target Thresh 29.586526744319322
target distance 12.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.98439941, 14.96581261]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([23.       , 20.       ,  1.0436944], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.99880271365785}
done in step count: 56
reward sum = 0.5050739199341856
running average episode reward sum: 0.25496173995027044
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.01576058, 15.50062729,  2.27863415]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.5008753172309562}
episode index:28
target Thresh 30.048348519491885
target distance 13.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.91301062,  9.8770477 ,  5.82124996]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.053964965154616}
done in step count: 39
reward sum = 0.5932399018705056
running average episode reward sum: 0.2666265041544165
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.60492811, 14.16047793,  2.84846213]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9278357104448205}
episode index:29
target Thresh 30.505575091223232
target distance 10.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.22575257,  5.02587009,  4.26568985]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.976684399969619}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.2836663326625742
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.10451978, 14.16455304,  2.67848155]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8419596232134817}
episode index:30
target Thresh 30.95825218255155
target distance 2.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.78251585, 16.44916584,  4.57647085]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.528779661219689}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.3045824299285231
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.27906815, 14.53411509,  5.57054245]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8583655888380929}
episode index:31
target Thresh 31.406425061563212
target distance 10.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.45894686,  4.90109142,  4.33240366]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.10933166512041}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.3203682248751711
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.51194578, 15.04660993,  2.91889505]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.4902748322043323}
episode index:32
target Thresh 31.850138545919584
target distance 12.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.99044184,  2.74265863,  5.68526411]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.475824354599727}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.3313435694247231
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.33352819, 14.45099679,  2.81789084]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.6423749484435339}
episode index:33
target Thresh 32.28943700733888
target distance 11.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.87647923,  3.19533314,  0.2672922 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.210918956289389}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.34335406365894533
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.09735111, 14.86621839,  5.55281808]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.16545319539453007}
episode index:34
target Thresh 32.72436437603333
target distance 19.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.4456642 , 14.86250945]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([16.      , 21.      ,  5.762959], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.43994130961797}
done in step count: 54
reward sum = 0.46868993451424595
running average episode reward sum: 0.34693508854052535
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.13002871, 14.91907347,  1.24116041]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8737271618970264}
episode index:35
target Thresh 33.15496414510223
target distance 14.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.71568258, 13.91238019]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.      , 22.      ,  5.995846], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.069776960585148}
done in step count: 53
reward sum = 0.5220558823200565
running average episode reward sum: 0.3517995550344012
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.43520609, 15.90967515,  1.69903637]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0707478846936904}
episode index:36
target Thresh 33.58127937488131
target distance 8.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.70247235,  6.83090217,  4.66840982]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.953906572909911}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.3641759959313433
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.41145529, 14.81232496,  1.40970078]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.6177433118466076}
episode index:37
target Thresh 34.003352697248836
target distance 9.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.0809103 ,  5.98466426,  0.82268208]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.252376240616423}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.37547700350271995
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.63783722, 14.05692943,  1.1099523 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.1385158858705569}
episode index:38
target Thresh 34.42122631988877
target distance 24.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.10943857, 14.2050893 ]), 'dynamicTrap': False, 'previousTarget': array([30.40285  , 13.8507125]), 'currentState': array([11.51426432, 10.2014477 ,  1.75696254]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.28860020790033225
running average episode reward sum: 0.373249393359069
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.13487121, 15.05627639,  1.7800766 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8669572382905324}
episode index:39
target Thresh 34.83494203051159
target distance 18.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.9955621 , 14.99098002]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([17.       , 14.       ,  1.0517572], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.022827105421783}
done in step count: 81
reward sum = 0.18404155414569362
running average episode reward sum: 0.3685191973787346
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.51958159, 14.61034921,  1.8560852 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.6185705966591768}
episode index:40
target Thresh 35.24454120103312
target distance 29.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.01224243, 16.0970134 ]), 'dynamicTrap': False, 'previousTarget': array([25.70920231, 16.60186167]), 'currentState': array([ 7.19822962, 18.81820735,  0.23150795]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.266881146536986
running average episode reward sum: 0.3530216280149366
{'scaleFactor': 20, 'currentTarget': array([35.37987512, 16.20457954]), 'dynamicTrap': True, 'previousTarget': array([35.39112449, 16.11318075]), 'currentState': array([20.90147976, 12.30067737,  3.92693957]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.99547879183051}
episode index:41
target Thresh 35.650064791711785
target distance 24.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.97562325, 14.48314796]), 'dynamicTrap': False, 'previousTarget': array([30.57960839, 14.07908508]), 'currentState': array([12.26143031, 11.11408333,  1.73345011]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 71
reward sum = 0.3074670734618261
running average episode reward sum: 0.35193699576367204
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.16350979, 14.8285885 ,  1.25492764]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8538722255490605}
episode index:42
target Thresh 36.05155335524454
target distance 7.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.67663339,  7.1458902 ,  0.10917586]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.48816488547695}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.36239498631658773
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.28935273, 15.09007354,  1.86602238]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.30304825880528274}
episode index:43
target Thresh 36.44904704082236
target distance 7.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.98920178, 21.99813894,  0.96144801]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.609814447673217}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.3699863097650647
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.08445692, 15.84903029,  0.4398447 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8532206114643317}
episode index:44
target Thresh 36.842585598145
target distance 27.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.70778559, 12.10700421]), 'dynamicTrap': False, 'previousTarget': array([26.75497521, 11.94628712]), 'currentState': array([7.82403896, 5.5188244 , 6.04936838]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.268324217653127
running average episode reward sum: 0.3677271521625772
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.37280761, 14.0841686 ,  1.1416855 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9888036552740168}
episode index:45
target Thresh 37.23220838139618
target distance 27.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.77241213, 15.32795307]), 'dynamicTrap': False, 'previousTarget': array([27.94535509, 15.52256629]), 'currentState': array([ 7.79296946, 16.23452381,  3.43824863]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.06534351227461549
running average episode reward sum: 0.35831257250089904
{'scaleFactor': 20, 'currentTarget': array([31.7836083 , 12.98144604]), 'dynamicTrap': False, 'previousTarget': array([32.88781621, 13.52219178]), 'currentState': array([14.8433402 ,  2.35001543,  1.95641152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:46
target Thresh 37.6179543531789
target distance 20.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([34.77872706, 14.96680906]), 'currentState': array([16.54104271, 12.67644325,  5.68681354]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.604623629452895}
done in step count: 99
reward sum = -0.20875308128306305
running average episode reward sum: 0.34624734582464456
{'scaleFactor': 20, 'currentTarget': array([34.18247845, 14.89878134]), 'dynamicTrap': False, 'previousTarget': array([34.22510465, 14.8506475 ]), 'currentState': array([14.33403141, 12.44131309,  1.43572343]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:47
target Thresh 37.99986208841179
target distance 29.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.00688244, 16.18551408]), 'dynamicTrap': False, 'previousTarget': array([25.95260657, 15.62395817]), 'currentState': array([ 5.14615213, 18.54165322,  1.30441773]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3390338594532978
{'scaleFactor': 20, 'currentTarget': array([22.58862159, 17.24967517]), 'dynamicTrap': False, 'previousTarget': array([23.19801571, 17.22319058]), 'currentState': array([ 2.90928983, 20.81673297,  2.7453634 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:48
target Thresh 38.37796977818664
target distance 7.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.57609603, 22.64502349,  6.20383567]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.832740824925658}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.332114801097108
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.55954929, 22.59942479,  1.01255826]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.612178004580197}
episode index:49
target Thresh 38.752315233587495
target distance 7.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.73888757, 21.48091314,  4.66076684]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.710138983161335}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.3254725050751659
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.55388966, 18.42231002,  1.95304209]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.4668428930557567}
episode index:50
target Thresh 39.12293588947188
target distance 12.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.41601684, 21.88114439,  1.35759008]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.342481664982559}
done in step count: 99
reward sum = -0.07496074574128354
running average episode reward sum: 0.3176208727062159
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.85171855, 21.95836569,  3.24789683]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.101054997328786}
episode index:51
target Thresh 39.48986880821423
target distance 13.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.93570484,  5.38815895,  0.72494721]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.21922613847497}
done in step count: 99
reward sum = -0.2564034372559461
running average episode reward sum: 0.306581943668482
{'scaleFactor': 20, 'currentTarget': array([35.646662  , 13.36734189]), 'dynamicTrap': True, 'previousTarget': array([35.73926039, 13.48808659]), 'currentState': array([21.3835232 ,  7.18380268,  3.01557043]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.545844636402284}
episode index:52
target Thresh 39.85315068341218
target distance 30.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.6700677 , 16.60739239]), 'dynamicTrap': False, 'previousTarget': array([24.72787848, 16.71202025]), 'currentState': array([ 5.9604352 , 20.00303474,  5.34623427]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.300797378693605
{'scaleFactor': 20, 'currentTarget': array([25.70517779, 17.56028264]), 'dynamicTrap': False, 'previousTarget': array([25.3691049, 17.6661398]), 'currentState': array([ 6.42330267, 22.87152465,  4.89565122]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:53
target Thresh 40.212817843556
target distance 9.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.31808577,  8.30026343,  4.12904453]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.96640801868622}
done in step count: 99
reward sum = -0.2053012434875297
running average episode reward sum: 0.29142518198654693
{'scaleFactor': 20, 'currentTarget': array([36.60167302, 14.48324897]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([19.55532245,  4.26898972,  3.81950399]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.87232145054174}
episode index:54
target Thresh 40.56890625566143
target distance 25.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.89955628, 16.52942257]), 'dynamicTrap': False, 'previousTarget': array([29.04848294, 16.90448546]), 'currentState': array([ 9.49993673, 21.39303864,  3.400702  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28612654231406426
{'scaleFactor': 20, 'currentTarget': array([29.76577882, 15.79292878]), 'dynamicTrap': False, 'previousTarget': array([29.7004268 , 16.16571526]), 'currentState': array([ 9.99139319, 18.78853769,  3.59762238]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:55
target Thresh 40.92145152886641
target distance 6.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.11255955, 22.26277446,  6.12173886]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.347494916371726}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.2895910943543742
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.95514536, 15.73456105,  2.36625315]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.204940909930618}
episode index:56
target Thresh 41.270488917992054
target distance 29.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.52946852, 11.44953906]), 'dynamicTrap': False, 'previousTarget': array([24.90745963, 11.51981367]), 'currentState': array([4.42377816, 5.53577582, 1.95077693]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28451054883938515
{'scaleFactor': 20, 'currentTarget': array([24.19042535, 11.65238333]), 'dynamicTrap': False, 'previousTarget': array([22.74010619, 11.58527388]), 'currentState': array([5.08560433, 5.7358122 , 4.98199808]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:57
target Thresh 41.61605332706815
target distance 32.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.13435579, 17.83249658]), 'dynamicTrap': False, 'previousTarget': array([22.65744374, 17.3142293 ]), 'currentState': array([ 1.53904284, 21.83545931,  1.61212325]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2796051945490509
{'scaleFactor': 20, 'currentTarget': array([26.33838733, 15.71699458]), 'dynamicTrap': False, 'previousTarget': array([27.58579859, 15.96338642]), 'currentState': array([ 6.40655984, 17.3669197 ,  3.03530876]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:58
target Thresh 41.95817931282356
target distance 8.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.1467749 , 23.46545325,  2.369416  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.5471660251408}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2748661234549992
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.60893172, 21.72789367,  0.56923215]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.75521176765386}
episode index:59
target Thresh 42.296901088141965
target distance 32.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.63833731, 11.98106666]), 'dynamicTrap': False, 'previousTarget': array([22.2530188 , 11.41491154]), 'currentState': array([3.2093402 , 7.23616715, 0.74911535]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2702850213974159
{'scaleFactor': 20, 'currentTarget': array([25.93050782, 13.29693188]), 'dynamicTrap': False, 'previousTarget': array([24.43673407, 12.78908866]), 'currentState': array([6.27406128, 9.60584678, 5.84730408]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:60
target Thresh 42.63225252548319
target distance 31.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.43882058, 14.87017656]), 'dynamicTrap': False, 'previousTarget': array([23.95850618, 14.28764556]), 'currentState': array([ 4.44033147, 14.62434488,  0.29607582]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2658541194072943
{'scaleFactor': 20, 'currentTarget': array([24.5725104 , 10.50644419]), 'dynamicTrap': False, 'previousTarget': array([25.96540973, 10.59326485]), 'currentState': array([6.20535531, 2.59141961, 2.31239323]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:61
target Thresh 42.96426716027041
target distance 3.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.17823156, 18.56700445,  0.57753932]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.00529162673654}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.2772161336104025
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.50028068, 15.6188912 ,  3.81221754]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.7958059318990964}
episode index:62
target Thresh 43.29297819424379
target distance 24.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.85917725, 12.30160843]), 'dynamicTrap': False, 'previousTarget': array([29.72658355, 13.02246883]), 'currentState': array([10.5489543 ,  4.25575637,  3.66807652]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2728158775213485
{'scaleFactor': 20, 'currentTarget': array([29.39115513, 12.97518257]), 'dynamicTrap': False, 'previousTarget': array([29.32563838, 13.3185568 ]), 'currentState': array([10.57943568,  6.18406992,  4.41189092]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:63
target Thresh 43.61841849878065
target distance 21.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.54537654, 16.1109875 ]), 'dynamicTrap': False, 'previousTarget': array([32.6897547 , 15.88009345]), 'currentState': array([12.50572526, 22.23403242,  2.72639465]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1967158391503084
running average episode reward sum: 0.26547944444835386
{'scaleFactor': 20, 'currentTarget': array([34.66507962, 15.0032885 ]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([14.66604363, 15.19965422,  3.46084209]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:64
target Thresh 43.94062061818265
target distance 6.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.83165536, 19.67433152,  3.31840372]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.262944374947033}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.26139514530299457
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.0431539 , 20.9150123 ,  1.28494756]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.131543035048365}
episode index:65
target Thresh 44.25961677293022
target distance 8.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.68273991,  6.59769886,  5.12033582]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.429993968449061}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25743461279840374
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.56341591,  4.63611459,  1.80377494]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.646457755208187}
episode index:66
target Thresh 44.57543886290469
target distance 29.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.04198419, 17.09766081]), 'dynamicTrap': False, 'previousTarget': array([25.44164417, 17.30718934]), 'currentState': array([ 7.70255616, 22.19535867,  5.44011337]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2535923051446962
{'scaleFactor': 20, 'currentTarget': array([22.69865603, 17.82203257]), 'dynamicTrap': False, 'previousTarget': array([24.33577594, 17.5547817 ]), 'currentState': array([ 3.20503651, 22.2940341 ,  2.08416956]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:67
target Thresh 44.88811847057822
target distance 11.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.69629948, 11.46782667,  4.12893915]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.892313474813298}
done in step count: 99
reward sum = -0.28720007528085617
running average episode reward sum: 0.24563947602079103
{'scaleFactor': 20, 'currentTarget': array([36.32258831, 16.31437384]), 'dynamicTrap': True, 'previousTarget': array([36.1244066 , 16.33868157]), 'currentState': array([24.0793766 , 13.84740678,  4.89125799]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.489281790640186}
episode index:68
target Thresh 45.197686864172155
target distance 23.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.94956104, 16.1587495 ]), 'dynamicTrap': False, 'previousTarget': array([31.54352728, 15.75140711]), 'currentState': array([11.72094065, 21.65967285,  0.72737932]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.007778213593991467
running average episode reward sum: 0.24196675588144637
{'scaleFactor': 20, 'currentTarget': array([31.278456  , 15.58325494]), 'dynamicTrap': False, 'previousTarget': array([32.1035382 , 15.24915925]), 'currentState': array([11.51964569, 18.67993304,  1.41633246]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:69
target Thresh 45.50417500078382
target distance 8.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.70655587,  9.24489126,  3.71097922]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.094676426122831}
done in step count: 99
reward sum = -0.012863974561144072
running average episode reward sum: 0.23832631687512368
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.54871835,  8.69532442,  5.51805368]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.543865314578339}
episode index:70
target Thresh 45.8076135294823
target distance 13.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.21946778,  1.76376077,  5.55145443]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.355460455139804}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23496960818674165
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.52372197, 10.09741721,  2.05479131]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.492474024545466}
episode index:71
target Thresh 46.10803279437332
target distance 6.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.50477577,  8.91416725,  2.18893433]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.106730621315051}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.23170614140637025
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.30230928, 10.45534261,  0.36259806]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.554701057385655}
episode index:72
target Thresh 46.40546283763372
target distance 31.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.71138205, 16.44218504]), 'dynamicTrap': False, 'previousTarget': array([23.74482241, 16.81535122]), 'currentState': array([ 2.84770775, 18.77337429,  2.94822741]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22853208467477612
{'scaleFactor': 20, 'currentTarget': array([22.26474801, 17.52132955]), 'dynamicTrap': False, 'previousTarget': array([22.02954935, 16.88576124]), 'currentState': array([ 2.64555149, 21.4055448 ,  0.33283989]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:73
target Thresh 46.6999334025157
target distance 9.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.33516879,  5.75357532,  2.27854347]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.946223259748335}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22544381326025212
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.12892631,  4.53550352,  2.3067833 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.718660100123868}
episode index:74
target Thresh 46.99147393632111
target distance 24.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.03354615, 14.26443681]), 'dynamicTrap': False, 'previousTarget': array([30.40285  , 13.8507125]), 'currentState': array([12.62141609,  9.45099661,  5.54447753]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22243789575011544
{'scaleFactor': 20, 'currentTarget': array([31.72141193, 13.82498839]), 'dynamicTrap': False, 'previousTarget': array([32.51316429, 14.25274036]), 'currentState': array([12.89401959,  7.07744827,  3.71809256]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:75
target Thresh 47.2801135933463
target distance 24.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.61511057, 13.87970165]), 'dynamicTrap': False, 'previousTarget': array([30.2, 13.6]), 'currentState': array([11.23755328,  8.92891649,  0.77838016]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.21951108133235075
{'scaleFactor': 20, 'currentTarget': array([29.69858144, 13.69957125]), 'dynamicTrap': False, 'previousTarget': array([28.60527797, 13.77003064]), 'currentState': array([10.27443305,  8.93486146,  4.59368621]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:76
target Thresh 47.56588123779751
target distance 17.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([34.33935727, 14.53366395]), 'currentState': array([18.24663669,  4.65386082,  0.4855144 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.690550481660033}
done in step count: 99
reward sum = -0.20653602626189976
running average episode reward sum: 0.2139780020129449
{'scaleFactor': 20, 'currentTarget': array([34.27817651, 14.40463823]), 'dynamicTrap': False, 'previousTarget': array([34.53133311, 14.67210293]), 'currentState': array([18.84921474,  1.67879293,  4.13482677]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:77
target Thresh 47.84880544667733
target distance 9.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.19178177,  6.90920136,  6.10893518]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.697614239280034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.21123469429483022
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.87822685,  3.27831682,  4.67549704]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.069881789135959}
episode index:78
target Thresh 48.12891451264241
target distance 30.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.83911911, 16.44875938]), 'dynamicTrap': False, 'previousTarget': array([24.61161351, 17.0776773 ]), 'currentState': array([ 5.03936745, 19.27184885,  3.80418372]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20856083740502224
{'scaleFactor': 20, 'currentTarget': array([24.32559531, 16.10879577]), 'dynamicTrap': False, 'previousTarget': array([25.44371374, 16.49569277]), 'currentState': array([ 4.43262853, 18.17516276,  3.39637076]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:79
target Thresh 48.406236446832764
target distance 4.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.10522226, 17.22412928,  3.93487096]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.819575410177488}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.21474692312495405
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.64212145, 15.74139351,  2.08850691]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.823250504189235}
episode index:80
target Thresh 48.68079898167293
target distance 30.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.40614336, 11.82382229]), 'dynamicTrap': False, 'previousTarget': array([23.97366596, 11.32455532]), 'currentState': array([4.11688377, 6.53946193, 1.62964821]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2120957265431645
{'scaleFactor': 20, 'currentTarget': array([23.33813401, 12.50456591]), 'dynamicTrap': False, 'previousTarget': array([22.05400856, 11.80576974]), 'currentState': array([3.78087122, 8.31965585, 6.2158237 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:81
target Thresh 48.952629573645176
target distance 2.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.66334537, 14.38415968,  5.08156798]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.701851576890291}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.2217043152438576
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.66334537, 14.38415968,  5.08156798]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.701851576890291}
episode index:82
target Thresh 49.221755406035236
target distance 24.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.84247673, 13.8802083 ]), 'dynamicTrap': False, 'previousTarget': array([30.57960839, 14.07908508]), 'currentState': array([ 9.16521466, 10.30175091,  2.31557226]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.21903317891561835
{'scaleFactor': 20, 'currentTarget': array([31.2312369 , 13.98789577]), 'dynamicTrap': False, 'previousTarget': array([32.37656712, 14.38734772]), 'currentState': array([11.91562838,  8.80067421,  2.94340997]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:83
target Thresh 49.48820339165063
target distance 9.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.06987703,  4.89637617,  3.0020957 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.166529452867483}
done in step count: 99
reward sum = -0.14220912425146678
running average episode reward sum: 0.2147326753064864
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.58373409,  5.26144165,  2.25031648]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.259667401965064}
episode index:84
target Thresh 49.75200017551195
target distance 30.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.16226948, 12.02280323]), 'dynamicTrap': False, 'previousTarget': array([24.1565257 , 11.74695771]), 'currentState': array([3.76628736, 7.14470137, 1.38360167]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2122064085381748
{'scaleFactor': 20, 'currentTarget': array([24.63349471, 12.10940007]), 'dynamicTrap': False, 'previousTarget': array([24.9299817 , 12.76096998]), 'currentState': array([5.36842481, 6.7375213 , 3.67961282]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:85
target Thresh 50.01317213751742
target distance 22.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.42878703, 16.25424035]), 'dynamicTrap': True, 'previousTarget': array([32.29527642, 15.73765188]), 'currentState': array([13.        , 21.        ,  0.37717956], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15798800990058828
running average episode reward sum: 0.20790182227725895
{'scaleFactor': 20, 'currentTarget': array([32.90140428, 15.54796027]), 'dynamicTrap': False, 'previousTarget': array([33.83396895, 15.3646567 ]), 'currentState': array([13.55018657, 20.60071924,  2.68193092]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:86
target Thresh 50.27174539508087
target distance 4.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.26001179, 18.48509202,  4.88524151]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.43321573799863}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20551214615912955
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.29792182, 21.04801102,  0.32401603]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.282953720469307}
episode index:87
target Thresh 50.52774580574355
target distance 27.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.00407421, 16.14551268]), 'dynamicTrap': False, 'previousTarget': array([27.5237412 , 16.66139084]), 'currentState': array([ 9.35937244, 19.89860368,  4.70946426]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20317678086186672
{'scaleFactor': 20, 'currentTarget': array([28.01127503, 16.18415946]), 'dynamicTrap': False, 'previousTarget': array([29.38226375, 16.17354877]), 'currentState': array([ 8.29233181, 19.5253087 ,  2.58551292]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:88
target Thresh 50.78119896975985
target distance 27.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.20261411, 16.60260805]), 'dynamicTrap': False, 'previousTarget': array([27.5237412 , 16.66139084]), 'currentState': array([ 6.52643077, 20.18699212,  2.63576078]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2008938956836435
{'scaleFactor': 20, 'currentTarget': array([25.8163685 , 17.00952172]), 'dynamicTrap': False, 'previousTarget': array([26.35432057, 17.40394507]), 'currentState': array([ 6.2786357 , 21.28468221,  3.20250463]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:89
target Thresh 51.032130232657394
target distance 3.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.33193523, 15.59826232,  3.25763798]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.4074550310278}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19866174128715855
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.3660826 , 19.60726051,  0.49541639]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.805520892919977}
episode index:90
target Thresh 51.28056468777158
target distance 26.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.94451756, 13.64644007]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 13.77694787]), 'currentState': array([ 7.22101581, 10.3323069 ,  2.18256009]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19647864522905792
{'scaleFactor': 20, 'currentTarget': array([29.26152425, 15.50388939]), 'dynamicTrap': False, 'previousTarget': array([27.76729406, 15.36859093]), 'currentState': array([ 9.33818529, 17.25333638,  5.9432879 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:91
target Thresh 51.52652717875493
target distance 2.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.05807929, 16.71084201,  0.5762229 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.6763166366794797}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19434300778091598
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.5562572 , 17.18752343,  1.84528035]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.2571399681872055}
episode index:92
target Thresh 51.77004230206154
target distance 27.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.32799348, 15.45498021]), 'dynamicTrap': False, 'previousTarget': array([27.87767469, 15.79136948]), 'currentState': array([ 9.39202914, 17.05414433,  4.67636446]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19225329801983085
{'scaleFactor': 20, 'currentTarget': array([27.51995701, 15.98399796]), 'dynamicTrap': False, 'previousTarget': array([26.11593853, 16.05985383]), 'currentState': array([ 7.69079629, 18.59251956,  5.59652624]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:93
target Thresh 52.01113440940665
target distance 4.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.45534575, 12.14123682,  4.93203196]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.827243420465417}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.19020805016855608
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.69173289, 12.19140806,  0.29803666]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.1428935591146745}
episode index:94
target Thresh 52.24982761020191
target distance 7.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.17070978,  7.15386439,  2.92704487]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.847992467869767}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18820586016678179
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.12763159,  7.75994973,  5.06087558]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.241175158182743}
episode index:95
target Thresh 52.48614577396632
target distance 33.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.21293683, 17.30474411]), 'dynamicTrap': False, 'previousTarget': array([21.85467564, 16.59337265]), 'currentState': array([ 1.48666168, 20.60232938,  0.87161231]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18624538245671116
{'scaleFactor': 20, 'currentTarget': array([23.9770641 , 15.48643129]), 'dynamicTrap': False, 'previousTarget': array([23.25316771, 16.10484365]), 'currentState': array([ 3.9965094 , 16.36815346,  4.23667584]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:96
target Thresh 52.72011253271318
target distance 14.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([19.77320623, 11.77387975,  3.00630474]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.564803253166776}
done in step count: 99
reward sum = -0.29584184357400967
running average episode reward sum: 0.1812754110543326
{'scaleFactor': 20, 'currentTarget': array([33.78644223, 13.69655208]), 'dynamicTrap': True, 'previousTarget': array([33.98261465, 13.65936018]), 'currentState': array([18.90479186, 12.91905492,  1.91163592]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.901946830301066}
episode index:97
target Thresh 52.95175128331335
target distance 8.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.19250148, 21.52340452,  3.2019558 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.101890906410598}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17942566196194143
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.82008227, 22.01753535,  4.58389544]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.116038801567942}
episode index:98
target Thresh 53.1810851898349
target distance 1.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.17248787, 14.34146728,  4.06536984]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0575640280045593}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.18771429163909353
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.17248787, 14.34146728,  4.06536984]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0575640280045593}
episode index:99
target Thresh 53.408137185859616
target distance 11.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.77963282,  3.01843102,  4.50002885]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.113013179245307}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1858371487227026
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.28477629,  1.74527477,  3.12060559]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.316846137187593}
episode index:100
target Thresh 53.63292997677631
target distance 13.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.65016521,  1.15158709,  4.13933849]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.0463612242833}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1839971769531709
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.40157689,  1.91232554,  4.30289048]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.18492242385914}
episode index:101
target Thresh 53.85548604205138
target distance 13.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.46335728,  3.72113707,  1.08432984]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.434817421271958}
done in step count: 99
reward sum = -0.03913283242272639
running average episode reward sum: 0.18180962784164248
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.54622108,  3.39437689,  1.15173448]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.96878163357496}
episode index:102
target Thresh 54.075827637476834
target distance 8.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.30832419, 13.65448611,  0.3765533 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.808475162387676}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18004448582376245
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.86689851, 11.80474696,  4.26520127]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.046352032637123}
episode index:103
target Thresh 54.293976797395835
target distance 13.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.02969692,  1.14802452,  4.74344683]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.999889083247384}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17831328884468783
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.89809349,  3.48137352,  5.11678898]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.553585065051163}
episode index:104
target Thresh 54.50995533690616
target distance 28.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.11371653, 11.3860418 ]), 'dynamicTrap': False, 'previousTarget': array([25.83483823, 11.72672794]), 'currentState': array([5.13232163, 5.08472071, 2.45817673]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17661506704616697
{'scaleFactor': 20, 'currentTarget': array([24.48515444, 12.13220154]), 'dynamicTrap': False, 'previousTarget': array([25.68654516, 12.04260917]), 'currentState': array([5.18993079, 6.86965989, 1.58135598]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:105
target Thresh 54.72378485404175
target distance 10.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.63247155, 16.35765079,  3.482234  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.456044269967538}
done in step count: 99
reward sum = -0.2585348972524938
running average episode reward sum: 0.17250987870372678
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.73477046, 16.09262904,  2.3863804 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.318093245456199}
episode index:106
target Thresh 54.93548673193251
target distance 11.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.89431028, 14.83059947]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([24.       , 15.       ,  1.1119853], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.895627243350265}
done in step count: 99
reward sum = -0.26788408554222337
running average episode reward sum: 0.16839404726217583
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.68553673, 15.77517714,  5.99645992]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.346663876872293}
episode index:107
target Thresh 55.145082140942634
target distance 6.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.4759647 , 21.71393591,  1.69350123]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.582595874011735}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1668348431208594
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.09820125, 23.12034561,  0.59211366]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.62325047815089}
episode index:108
target Thresh 55.3525920407877
target distance 24.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.47368387, 15.37283818]), 'dynamicTrap': False, 'previousTarget': array([30.84555753, 15.51930531]), 'currentState': array([12.68799311, 18.29284527,  5.57465429]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16530424822984233
{'scaleFactor': 20, 'currentTarget': array([32.63770873, 15.35249433]), 'dynamicTrap': False, 'previousTarget': array([31.27519508, 15.39234059]), 'currentState': array([12.8567157 , 18.30415738,  5.77990818]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:109
target Thresh 55.55803718263061
target distance 12.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.72324857,  1.83918815,  4.25958776]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.273151632864629}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16380148233684375
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.45859853,  2.60746864,  1.64273113]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.478074501073468}
episode index:110
target Thresh 55.761438111156764
target distance 13.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.43600184,  1.14795915,  4.7245836 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.832976462200936}
done in step count: 99
reward sum = -0.13847220647989703
running average episode reward sum: 0.16107829595110737
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.70605684,  3.46719122,  5.65168322]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.141834791841884}
episode index:111
target Thresh 55.9628151666285
target distance 7.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.32913978, 12.79190716,  2.25683677]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.947596933265194}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15964009688011532
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.49085981, 14.33559063,  5.44692761]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.538476384489888}
episode index:112
target Thresh 56.1621884869192
target distance 28.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.81139557, 11.60116832]), 'dynamicTrap': False, 'previousTarget': array([25.61502987, 11.31304745]), 'currentState': array([5.83919943, 5.27220525, 1.36804533]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15822735265993731
{'scaleFactor': 20, 'currentTarget': array([25.99385349, 11.46813508]), 'dynamicTrap': False, 'previousTarget': array([26.63967812, 12.19329638]), 'currentState': array([7.37441957, 4.16630787, 3.53186065]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:113
target Thresh 56.35957800952702
target distance 30.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.70516643, 13.38149897]), 'dynamicTrap': False, 'previousTarget': array([24.9007438 , 13.99007438]), 'currentState': array([ 4.94784219, 10.2753536 ,  3.7455194 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15683939342607822
{'scaleFactor': 20, 'currentTarget': array([22.2013687, 13.5944079]), 'dynamicTrap': False, 'previousTarget': array([23.28540727, 13.22774106]), 'currentState': array([ 2.32090079, 11.41106282,  1.39655912]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:114
target Thresh 56.55500347356872
target distance 12.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.69279767,  1.947536  ,  2.80945206]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.25481041462127}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15547557261367753
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.0549909 ,  2.99789247,  4.82265592]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.039253600256766}
episode index:115
target Thresh 56.74848442175355
target distance 23.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.41386937, 15.98277057]), 'dynamicTrap': False, 'previousTarget': array([31.7042351, 15.5731765]), 'currentState': array([10.8578458, 20.1734677,  1.3326726]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1541352659532148
{'scaleFactor': 20, 'currentTarget': array([32.91411993, 15.44600498]), 'dynamicTrap': False, 'previousTarget': array([31.72040832, 15.87372135]), 'currentState': array([13.35621151, 19.62789671,  4.32470614]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:116
target Thresh 56.9400402023376
target distance 27.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.99665343, 14.46196309]), 'dynamicTrap': False, 'previousTarget': array([27.98629668, 14.74023321]), 'currentState': array([ 9.07649529, 12.67666212,  4.42433168]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15281787051771725
{'scaleFactor': 20, 'currentTarget': array([28.22350401, 14.55681913]), 'dynamicTrap': False, 'previousTarget': array([27.04435806, 14.24308868]), 'currentState': array([ 8.26613851, 13.25161321,  5.9464713 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:117
target Thresh 57.12968997105851
target distance 5.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.35239917,  8.69648298,  3.24129629]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.4469613215476205}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.15152280381841454
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.29647079,  8.79458051,  2.0467788 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.21249755904177}
episode index:118
target Thresh 57.31745269305122
target distance 12.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.89936195,  1.25890068,  3.75178123]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.33991084066452}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1502495029459909
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.14305825,  1.40836149,  3.64741832]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.128292080401046}
episode index:119
target Thresh 57.50334714474439
target distance 23.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.99578183, 14.00597805]), 'dynamicTrap': False, 'previousTarget': array([31.54352728, 14.24859289]), 'currentState': array([10.37904262, 10.10937148,  2.06422138]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14899742375477432
{'scaleFactor': 20, 'currentTarget': array([30.07179017, 13.93936258]), 'dynamicTrap': False, 'previousTarget': array([31.65836342, 14.20828538]), 'currentState': array([10.51948258,  9.73136213,  2.05193633]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:120
target Thresh 57.68739191573811
target distance 23.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.05689356, 12.75782042]), 'dynamicTrap': False, 'previousTarget': array([30.62485559, 13.28798697]), 'currentState': array([10.34433704,  5.69805922,  2.31197882]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14776604008737948
{'scaleFactor': 20, 'currentTarget': array([30.84033993, 13.69952431]), 'dynamicTrap': False, 'previousTarget': array([29.13192368, 13.17137268]), 'currentState': array([11.75150079,  7.73159164,  5.75445998]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:121
target Thresh 57.86960541066285
target distance 22.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.12868402, 14.43909496]), 'dynamicTrap': False, 'previousTarget': array([32.29527642, 14.26234812]), 'currentState': array([12.49970564, 10.60461873,  1.1009469 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1821340534033652
running average episode reward sum: 0.14506194096040614
{'scaleFactor': 20, 'currentTarget': array([31.96743791, 14.1773892 ]), 'dynamicTrap': False, 'previousTarget': array([33.56859767, 14.58990263]), 'currentState': array([12.66498686,  8.94141902,  2.214928  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:122
target Thresh 58.05000585101993
target distance 6.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.48337645, 17.61205933,  0.26947689]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.103768453456037}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1438825755867443
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.29720153, 17.6950892 ,  0.03543491]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.4203154175854245}
episode index:123
target Thresh 58.22861127700375
target distance 25.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.71630976, 12.84666652]), 'dynamicTrap': False, 'previousTarget': array([29.04848294, 13.09551454]), 'currentState': array([8.53691045, 7.1765129 , 2.01152897]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1427222322352383
{'scaleFactor': 20, 'currentTarget': array([27.75135919, 12.45767138]), 'dynamicTrap': False, 'previousTarget': array([28.94265257, 13.07447735]), 'currentState': array([8.87850227, 5.83836086, 3.01060874]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:124
target Thresh 58.405439549305726
target distance 6.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.69538   , 11.48052931,  3.52677202]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.22045062797165}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1415804543773564
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.35710193, 13.08420039,  0.41854715]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.913637457487092}
episode index:125
target Thresh 58.58050835090044
target distance 5.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.70236074, 21.35668631,  0.08308673]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.395371100296599}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1404567999775361
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.1103867 , 19.76296267,  2.0792237 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.764241661029977}
episode index:126
target Thresh 58.75383518881396
target distance 26.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.54816882, 14.695529  ]), 'dynamicTrap': False, 'previousTarget': array([28.94108971, 14.53392998]), 'currentState': array([ 7.56484216, 13.87903928,  1.58203006]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1393508409225949
{'scaleFactor': 20, 'currentTarget': array([28.94752733, 14.60550899]), 'dynamicTrap': False, 'previousTarget': array([28.02839598, 14.93811319]), 'currentState': array([ 8.9898748 , 13.30469939,  4.39314241]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:127
target Thresh 58.92543739587451
target distance 11.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.83255958, 15.10876839]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([24.      ,  7.      ,  5.806095], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.531314493953824}
done in step count: 99
reward sum = -0.3244712413031297
running average episode reward sum: 0.13572723090520641
{'scaleFactor': 20, 'currentTarget': array([35.51361691, 13.24570046]), 'dynamicTrap': True, 'previousTarget': array([35.66756394, 13.33788322]), 'currentState': array([23.261152  ,  5.49656371,  3.23846884]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.497310673402158}
episode index:128
target Thresh 59.0953321324458
target distance 7.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.07916249,  6.82714958,  4.44620926]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.243790128060645}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13467508182842186
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.74344009,  7.96678875,  5.58186639]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.246077822917889}
episode index:129
target Thresh 59.26353638814306
target distance 10.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.6615924 , 16.73260744,  5.1136278 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.516511600899255}
done in step count: 99
reward sum = -0.017736895487447387
running average episode reward sum: 0.13350268200291518
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.2041394 , 17.11687592,  1.3094612 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.021978279718665}
episode index:130
target Thresh 59.43006698353203
target distance 3.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.49203282, 16.7472771 ,  1.67150927]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.834743568334059}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13248357756014484
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.63416502, 16.42554539,  2.99357134]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.592678387425002}
episode index:131
target Thresh 59.59494057181104
target distance 4.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.97581745, 19.95784443,  6.04928893]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.807400584811541}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1314799140937801
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.52259565, 19.44010463,  5.56593794]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.084492245805628}
episode index:132
target Thresh 59.75817364047629
target distance 26.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.20640232, 16.7086325 ]), 'dynamicTrap': False, 'previousTarget': array([28.31231517, 16.80053053]), 'currentState': array([ 7.6703837 , 20.99161934,  2.78044796]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13049134331112008
{'scaleFactor': 20, 'currentTarget': array([26.5879439 , 16.77690421]), 'dynamicTrap': False, 'previousTarget': array([28.23646908, 16.4417626 ]), 'currentState': array([ 7.01974056, 20.91035535,  1.95302314]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:133
target Thresh 59.919782512970706
target distance 33.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.17477033, 13.17567725]), 'dynamicTrap': False, 'previousTarget': array([21.91786413, 13.81071492]), 'currentState': array([ 2.37408666, 10.35913216,  3.92653894]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12951752731626098
{'scaleFactor': 20, 'currentTarget': array([22.52667202, 13.93623979]), 'dynamicTrap': False, 'previousTarget': array([21.68373694, 13.38879753]), 'currentState': array([2.59900921e+00, 1.22367531e+01, 8.54717876e-03]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:134
target Thresh 60.079783350316184
target distance 21.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.69375331, 14.10940133]), 'dynamicTrap': True, 'previousTarget': array([32.6897547 , 14.11990655]), 'currentState': array([14.       ,  7.       ,  1.3619452], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.16306128224236224
running average episode reward sum: 0.127350276875086
{'scaleFactor': 20, 'currentTarget': array([32.43667876, 13.93761382]), 'dynamicTrap': False, 'previousTarget': array([34.00657606, 14.09453681]), 'currentState': array([13.96067808,  6.28010778,  2.06620188]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:135
target Thresh 60.238192152729795
target distance 2.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.82967282, 13.4943032 ,  3.589746  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.36956229246913}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.1336932895451221
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.53693061, 14.2724598 ,  1.589746  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9042174602649815}
episode index:136
target Thresh 60.3950247612238
target distance 25.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.55988517, 14.30882364]), 'dynamicTrap': False, 'previousTarget': array([29.61161351, 13.9223227 ]), 'currentState': array([10.79788838, 11.23254623,  6.26947183]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13271742611778545
{'scaleFactor': 20, 'currentTarget': array([29.44402848, 14.07037629]), 'dynamicTrap': False, 'previousTarget': array([30.44164917, 13.9963171 ]), 'currentState': array([ 9.71824301, 10.76986302,  1.39780993]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:137
target Thresh 60.55029685918974
target distance 9.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.4736258, 15.0359662]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([26.       , 13.       ,  4.1559944], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.669676457975928}
done in step count: 99
reward sum = -0.280287061015588
running average episode reward sum: 0.1297246399791378
{'scaleFactor': 20, 'currentTarget': array([36.64287829, 13.93179486]), 'dynamicTrap': True, 'previousTarget': array([36.70154794, 14.12271182]), 'currentState': array([25.98304745, 13.08828165,  3.95837376]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.693152390463254}
episode index:138
target Thresh 60.704023973966805
target distance 24.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.09847322, 14.29034524]), 'dynamicTrap': False, 'previousTarget': array([30.57960839, 14.07908508]), 'currentState': array([12.67109968,  9.53880262,  5.0918687 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12879136918792097
{'scaleFactor': 20, 'currentTarget': array([31.66258844, 14.31505798]), 'dynamicTrap': False, 'previousTarget': array([30.89724709, 13.89283336]), 'currentState': array([12.07093293, 10.29423292,  0.26974124]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:139
target Thresh 60.85622147839457
target distance 31.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.42622341, 15.21311292]), 'dynamicTrap': False, 'previousTarget': array([23.98960229, 14.64482588]), 'currentState': array([ 3.42961309, 15.5813191 ,  0.90697432]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12787143083657868
{'scaleFactor': 20, 'currentTarget': array([23.40341154, 15.34216311]), 'dynamicTrap': False, 'previousTarget': array([23.88423362, 14.75633848]), 'currentState': array([ 3.4121116 , 15.93201636,  0.84811634]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:140
target Thresh 61.006904592350324
target distance 28.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.59506923, 14.97654853]), 'dynamicTrap': False, 'previousTarget': array([26.98725709, 14.71383061]), 'currentState': array([ 5.59513141, 14.92667811,  1.54848504]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1269645412561774
{'scaleFactor': 20, 'currentTarget': array([25.38540224, 14.4835945 ]), 'dynamicTrap': False, 'previousTarget': array([26.78737403, 14.82931968]), 'currentState': array([ 5.41418823, 13.41092927,  2.79641754]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:141
target Thresh 61.15608838427102
target distance 4.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.32594786, 18.27066261,  1.99298525]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.549206182203941}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12607042476845784
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.06392285, 19.91166181,  0.11037099]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.9634243998195196}
episode index:142
target Thresh 61.30378777266018
target distance 9.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.69854701,  7.65943382,  0.7465862 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.08142741804224}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12518881340644067
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.45794486,  7.67295316,  5.39584893]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.515141996280782}
episode index:143
target Thresh 61.450017527579725
target distance 29.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.00574156, 15.94281091]), 'dynamicTrap': False, 'previousTarget': array([25.95260657, 15.62395817]), 'currentState': array([ 7.14339626, 18.28529658,  6.18114514]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12431944664667371
{'scaleFactor': 20, 'currentTarget': array([28.13289605, 15.44691563]), 'dynamicTrap': False, 'previousTarget': array([27.99059887, 15.81268205]), 'currentState': array([ 8.17511689, 16.745781  ,  3.73990566]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:144
target Thresh 61.594792272126995
target distance 29.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.16039846, 14.6277606 ]), 'dynamicTrap': False, 'previousTarget': array([25.95260657, 14.37604183]), 'currentState': array([ 7.18290574, 13.67919073,  5.7943967 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12346207115255872
{'scaleFactor': 20, 'currentTarget': array([28.33374363, 14.27569571]), 'dynamicTrap': False, 'previousTarget': array([28.37493506, 14.70053494]), 'currentState': array([ 8.45076186, 12.11536341,  3.73532838]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:145
target Thresh 61.738126483897105
target distance 4.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.64704428, 15.35281273,  3.49400282]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.367230284703942}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12261644052822612
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.32666614, 17.55746628,  6.16417771]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.327352359688749}
episode index:146
target Thresh 61.88003449643067
target distance 12.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.68688873,  1.61468634,  3.48010206]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.652325550092916}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12178231508245588
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.19436363,  1.98570901,  4.23361224]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.400586886691627}
episode index:147
target Thresh 62.02053050064719
target distance 21.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.08718928, 15.41361271]), 'dynamicTrap': False, 'previousTarget': array([33.79898987, 15.17157288]), 'currentState': array([13.53897356, 19.6405811 ,  0.8707391 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20970299073010273
running average episode reward sum: 0.11954254950264129
{'scaleFactor': 20, 'currentTarget': array([32.91318725, 13.86619731]), 'dynamicTrap': True, 'previousTarget': array([32.90940926, 13.85093059]), 'currentState': array([13.49692591, 18.66294576,  2.12899176]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:148
target Thresh 62.15962854626419
target distance 3.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.65541136, 13.6871426 ,  0.82947922]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.112813607489646}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11874025051269067
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.08461076, 12.50940761,  5.10987133]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.9645774029876204}
episode index:149
target Thresh 62.29734254320212
target distance 28.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.99495416, 14.02284373]), 'dynamicTrap': False, 'previousTarget': array([26.68855151, 13.51581277]), 'currentState': array([ 8.18674326, 11.25973298,  6.1711784 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11794864884260607
{'scaleFactor': 20, 'currentTarget': array([27.54016788, 13.52117914]), 'dynamicTrap': False, 'previousTarget': array([29.22412141, 13.89024634]), 'currentState': array([7.92193429, 9.63210326, 2.49795753]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:150
target Thresh 62.43368626297547
target distance 6.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.13160167, 22.50624407,  1.18094206]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.735283601607996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11716753196285372
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.04215917, 21.31063736,  2.65961838]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.382914924851515}
episode index:151
target Thresh 62.56867334006981
target distance 8.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.32043701, 22.89296611,  2.19523382]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.899467947335325}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1163966929367823
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.908296  , 22.51538837,  1.44653833]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.515947847802816}
episode index:152
target Thresh 62.70231727330534
target distance 33.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.89046961, 17.53277511]), 'dynamicTrap': False, 'previousTarget': array([21.43700211, 18.28799949]), 'currentState': array([ 2.25360126, 21.32663879,  3.86882699]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11563593023784909
{'scaleFactor': 20, 'currentTarget': array([21.09507234, 17.45120172]), 'dynamicTrap': False, 'previousTarget': array([22.80352814, 17.21019115]), 'currentState': array([ 1.398769  , 20.92332425,  2.22364795]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:153
target Thresh 62.83463142718677
target distance 2.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.34931957, 13.12954997,  2.13031185]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.494660140057356}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.12106030763825266
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.93714274, 14.38328483,  5.6650802 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.619910185157145}
episode index:154
target Thresh 62.96562903323974
target distance 1.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.68326771, 13.46540678,  5.11406649]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.277798610570945}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.12635333887543168
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([3.57182244e+01, 1.40972272e+01, 3.24325045e-02]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.1536225411309136}
episode index:155
target Thresh 63.095323191334025
target distance 8.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.28047695, 21.26012174,  4.00341785]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.491986117973027}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12554338157494813
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.94156559, 21.83535054,  0.48389268]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.48839355303802}
episode index:156
target Thresh 63.22372687099351
target distance 29.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.52101791, 13.19843958]), 'dynamicTrap': False, 'previousTarget': array([25.81242258, 13.73274794]), 'currentState': array([4.81019253, 9.80972849, 2.91718578]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1247437422018593
{'scaleFactor': 20, 'currentTarget': array([26.05897759, 13.53014063]), 'dynamicTrap': False, 'previousTarget': array([24.69356708, 13.62456693]), 'currentState': array([ 6.32387735, 10.28578886,  4.82847971]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:157
target Thresh 63.350852912693156
target distance 26.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.32768245, 13.08791291]), 'dynamicTrap': False, 'previousTarget': array([28.11558017, 12.88171698]), 'currentState': array([7.92127135, 8.25146709, 1.27219939]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12395422484615132
{'scaleFactor': 20, 'currentTarget': array([27.24244696, 13.12333577]), 'dynamicTrap': False, 'previousTarget': array([27.0136801 , 12.64653008]), 'currentState': array([7.80317806, 8.42069558, 0.57911569]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:158
target Thresh 63.47671402914309
target distance 13.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.31471513, 16.04532323,  2.10961878]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.722441791163286}
done in step count: 99
reward sum = -0.35454420203298587
running average episode reward sum: 0.12094480077772907
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([33.41612263, 14.89797938]), 'currentState': array([21.8016415 , 15.90226584,  4.28360406]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.229162888716766}
episode index:159
target Thresh 63.60132280655983
target distance 13.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.31678537, 19.90646977,  5.87608081]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.671659309691796}
done in step count: 99
reward sum = -0.32610333588226836
running average episode reward sum: 0.11815074992360408
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.60195043, 18.19273319,  2.59036847]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.836768110039333}
episode index:160
target Thresh 63.724691705924954
target distance 10.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.18152691,  6.57697917,  1.2084235 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.703863292648714}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11741689433401648
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.4440387 ,  6.09573573,  1.11172051]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.002135047564098}
episode index:161
target Thresh 63.84683306423122
target distance 2.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.33008383, 14.20920817,  2.00696135]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.7541492304083888}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11669209868997935
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.52403222, 14.01579652,  4.1289503 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.7740172993302854}
episode index:162
target Thresh 63.967759095716225
target distance 5.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.24981021,  8.49347972,  3.24037313]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.68328790324471}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11597619624402855
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.00061358,  9.32315616,  4.32940334]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.564418059924115}
episode index:163
target Thresh 64.08748189308392
target distance 32.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.56297564, 11.33767828]), 'dynamicTrap': False, 'previousTarget': array([22.2530188 , 11.41491154]), 'currentState': array([4.51568903, 5.23842647, 4.83006239]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1152690243157113
{'scaleFactor': 20, 'currentTarget': array([23.31659696, 11.63487821]), 'dynamicTrap': False, 'previousTarget': array([21.80424524, 11.01659174]), 'currentState': array([4.0978967 , 6.09939624, 5.90331788]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:164
target Thresh 64.20601342871377
target distance 12.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.60378241, 22.67185501,  5.11087066]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.920475980267696}
done in step count: 99
reward sum = -0.1716339559639534
running average episode reward sum: 0.11353021837462242
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.8788015 , 22.87004667,  0.82166244]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.452027117200961}
episode index:165
target Thresh 64.32336555585815
target distance 24.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.40417622, 14.95022228]), 'dynamicTrap': False, 'previousTarget': array([31., 15.]), 'currentState': array([ 9.40496747, 14.77231907,  2.36840785]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11284630139646204
{'scaleFactor': 20, 'currentTarget': array([28.86689206, 15.24367714]), 'dynamicTrap': False, 'previousTarget': array([29.3001377 , 14.86882577]), 'currentState': array([ 8.88265926, 16.03767925,  0.81472259]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:166
target Thresh 64.43955000982754
target distance 10.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.13273491, 12.44230349,  1.10217524]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.16419553559}
done in step count: 99
reward sum = -0.13952977197548114
running average episode reward sum: 0.11133506742417494
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.99304162, 12.26986966,  4.41549588]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.411636990537174}
episode index:167
target Thresh 64.5545784091642
target distance 12.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.54348682,  3.98320923,  1.66950393]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.888207011176917}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11067235868950724
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.92847421,  3.34159148,  1.63897509]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.179845477821658}
episode index:168
target Thresh 64.66846225680388
target distance 25.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.7820551 , 14.25071616]), 'dynamicTrap': False, 'previousTarget': array([29.61161351, 13.9223227 ]), 'currentState': array([ 9.98512239, 11.40792612,  0.57136297]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11001749266175867
{'scaleFactor': 20, 'currentTarget': array([31.02093384, 14.27217945]), 'dynamicTrap': False, 'previousTarget': array([30.14467665, 14.34968187]), 'currentState': array([11.34733547, 10.6736343 ,  4.49006731]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:169
target Thresh 64.78121294122626
target distance 25.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.3289672 , 14.95456356]), 'dynamicTrap': False, 'previousTarget': array([29.98401917, 14.79936077]), 'currentState': array([11.33049893, 14.70704215,  5.77237732]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10937033094021892
{'scaleFactor': 20, 'currentTarget': array([31.1056891 , 14.69679002]), 'dynamicTrap': False, 'previousTarget': array([31.94819109, 14.951426  ]), 'currentState': array([11.16603623, 13.14429412,  3.21733994]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:170
target Thresh 64.89284173759373
target distance 9.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.24569624, 22.66493887,  0.41428304]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.635683141371283}
done in step count: 99
reward sum = -0.011900734877313704
running average episode reward sum: 0.10866114342081813
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.37710229, 20.2370368 ,  3.71741388]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.088652994252175}
episode index:171
target Thresh 65.00335980887897
target distance 23.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.10519751, 14.23747304]), 'dynamicTrap': False, 'previousTarget': array([31.83200822, 14.58678368]), 'currentState': array([11.47781974, 10.39481225,  3.45230305]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10802939258697616
{'scaleFactor': 20, 'currentTarget': array([30.21259387, 14.31769819]), 'dynamicTrap': False, 'previousTarget': array([30.90577735, 14.13870148]), 'currentState': array([10.41267092, 11.49581031,  1.21658068]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:172
target Thresh 65.1127782069812
target distance 6.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.45843934, 10.68445352,  0.40519929]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.264959275051579}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10740494523098208
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.17161384,  8.63783629,  4.28576208]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.962535125400928}
episode index:173
target Thresh 65.22110787383143
target distance 31.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.04070867, 16.6614291 ]), 'dynamicTrap': False, 'previousTarget': array([23.90700027, 16.0735161 ]), 'currentState': array([ 3.23095721, 19.41347323,  1.05908322]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10678767543080403
{'scaleFactor': 20, 'currentTarget': array([24.63403718, 15.99689202]), 'dynamicTrap': False, 'previousTarget': array([24.7439043 , 16.57719964]), 'currentState': array([ 4.72588676, 17.9114538 ,  3.64030964]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:174
target Thresh 65.32835964248659
target distance 21.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.39327236, 15.13795443]), 'dynamicTrap': False, 'previousTarget': array([33.23047895, 15.50557744]), 'currentState': array([14.8910445 , 19.57226481,  4.26033831]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1856190764217361
running average episode reward sum: 0.10511677970593236
{'scaleFactor': 20, 'currentTarget': array([32.59639243, 15.66953476]), 'dynamicTrap': False, 'previousTarget': array([32.78863629, 15.45300176]), 'currentState': array([13.32989638, 21.03629635,  0.52874809]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:175
target Thresh 65.43454423821294
target distance 23.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.91160324, 16.11906885]), 'dynamicTrap': False, 'previousTarget': array([31.54352728, 15.75140711]), 'currentState': array([10.37840911, 20.41491881,  1.91019523]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10451952527578502
{'scaleFactor': 20, 'currentTarget': array([29.6981869 , 16.53544999]), 'dynamicTrap': False, 'previousTarget': array([30.43990188, 16.04779929]), 'currentState': array([10.48759336, 22.09900065,  0.84554702]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:176
target Thresh 65.53967227955853
target distance 5.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.55798142, 13.88629212,  1.66418076]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.537579724045586}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10392901948326645
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.96859842, 14.24909963,  5.38205105]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.087126224240266}
episode index:177
target Thresh 65.6437542794151
target distance 33.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.32875438, 16.88288083]), 'dynamicTrap': False, 'previousTarget': array([21.91786413, 16.18928508]), 'currentState': array([ 1.51578132, 19.6116349 ,  0.85299253]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10334514858729305
{'scaleFactor': 20, 'currentTarget': array([21.86480095, 17.13592196]), 'dynamicTrap': False, 'previousTarget': array([22.0368106 , 16.44608094]), 'currentState': array([ 2.12409188, 20.34596955,  0.58219445]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:178
target Thresh 65.74680064606939
target distance 33.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.76874319, 13.33793176]), 'dynamicTrap': False, 'previousTarget': array([21.91786413, 13.81071492]), 'currentState': array([ 2.95087732, 10.64494233,  4.31427146]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10276780138848136
{'scaleFactor': 20, 'currentTarget': array([22.2033139 , 14.12369559]), 'dynamicTrap': False, 'previousTarget': array([22.03697279, 13.45014876]), 'currentState': array([ 2.25004336, 12.75731535,  0.51821549]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:179
target Thresh 65.84882168424393
target distance 27.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.99429767, 16.76050575]), 'dynamicTrap': False, 'previousTarget': array([27.5237412 , 16.66139084]), 'currentState': array([ 6.36583717, 20.59763209,  2.37301302]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10219686915854535
{'scaleFactor': 20, 'currentTarget': array([25.98811416, 16.69402044]), 'dynamicTrap': False, 'previousTarget': array([27.27550324, 16.7135019 ]), 'currentState': array([ 6.33236806, 20.38883375,  2.56491405]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:180
target Thresh 65.94982759612753
target distance 8.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.71854311, 21.72793342,  4.29294685]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.687174449237715}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10163224557203406
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.87382367, 22.43048974,  0.74154726]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.663119007203389}
episode index:181
target Thresh 66.0498284823956
target distance 27.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.59555565, 13.71039184]), 'dynamicTrap': False, 'previousTarget': array([27.87767469, 14.20863052]), 'currentState': array([ 7.89216507, 10.27871592,  3.71180367]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10107382664031958
{'scaleFactor': 20, 'currentTarget': array([27.92622511, 14.33551586]), 'dynamicTrap': False, 'previousTarget': array([26.20507857, 14.1244817 ]), 'currentState': array([ 8.01388565, 12.46502465,  5.55952013]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:182
target Thresh 66.14883434322005
target distance 9.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.7885091 , 19.73776527,  3.05471325]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.257040740420699}
done in step count: 99
reward sum = -0.15357918921569516
running average episode reward sum: 0.09968228010558726
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.9065566 , 20.25617895,  4.00348245]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.380027102425707}
episode index:183
target Thresh 66.2468550792695
target distance 30.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.09386293, 14.50129283]), 'dynamicTrap': False, 'previousTarget': array([25., 15.]), 'currentState': array([ 5.11915929, 13.49570125,  3.78143644]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09914052858327428
{'scaleFactor': 20, 'currentTarget': array([23.6114428 , 14.51479839]), 'dynamicTrap': False, 'previousTarget': array([25.17389186, 14.41557151]), 'currentState': array([ 3.6295694 , 13.66348449,  1.86265826]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:184
target Thresh 66.34390049269922
target distance 28.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.89919292, 16.67284464]), 'dynamicTrap': False, 'previousTarget': array([26.79898987, 16.17157288]), 'currentState': array([ 6.22873666, 20.28852586,  1.10015821]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0986046338341755
{'scaleFactor': 20, 'currentTarget': array([25.65566325, 16.91382677]), 'dynamicTrap': False, 'previousTarget': array([27.06082022, 16.41469422]), 'currentState': array([ 6.0623883 , 20.92675301,  1.47739154]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:185
target Thresh 66.43998028813144
target distance 2.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.0046499 , 16.21013534,  1.2491138 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.2305649294375893}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.10217311813800845
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.05788991, 15.8553117 ,  4.46390698]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.2724502014810635}
episode index:186
target Thresh 66.53510407362576
target distance 7.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.41440475, 10.84187341,  5.81007821]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.963396511153413}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10162673782711001
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.45322698, 10.67104367,  0.53327233]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.848573113078341}
episode index:187
target Thresh 66.62928136164001
target distance 13.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.51658531, 22.79856686,  5.81098289]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.881154782662868}
done in step count: 99
reward sum = -0.15390939334343007
running average episode reward sum: 0.10026750308684118
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.95470066, 22.72698473,  6.09898707]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.479797124210723}
episode index:188
target Thresh 66.72252156998145
target distance 8.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.71096845,  5.5745301 ,  3.60591841]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.429900432368006}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09973698719749281
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.53121762,  6.41293889,  4.95241135]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.599847410015142}
episode index:189
target Thresh 66.81483402274864
target distance 6.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.56545773, 20.46163888,  2.49061251]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.646893903906871}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09921205568592706
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.40796635, 21.88612698,  5.88249528]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.067765975298024}
episode index:190
target Thresh 66.90622795126379
target distance 5.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.52322679, 17.55468549,  0.85835981]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.043298996251345}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09869262083940389
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.731906  , 17.93188752,  5.53381235]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.39049004289098}
episode index:191
target Thresh 66.99671249499588
target distance 31.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.52414122, 14.79172153]), 'dynamicTrap': False, 'previousTarget': array([23.98960229, 14.64482588]), 'currentState': array([ 2.52692771, 14.45787766,  1.88276029]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09817859677253199
{'scaleFactor': 20, 'currentTarget': array([22.32407955, 14.55267911]), 'dynamicTrap': False, 'previousTarget': array([24.00013868, 14.65416236]), 'currentState': array([ 2.33652109, 13.84733763,  2.23775441]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:192
target Thresh 67.08629670247471
target distance 6.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.52904626,  9.8177305 ,  1.62421679]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.741213249269034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09766989937992819
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.42940912,  9.12574811,  3.46733624]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.080591334935356}
episode index:193
target Thresh 67.17498953219567
target distance 9.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.76528243, 16.59044919,  0.25856149]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.386900624542323}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09716644629034094
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.74163887, 16.72105231,  0.38947129]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.435789800339249}
episode index:194
target Thresh 67.26279985351566
target distance 21.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([33.90990945, 14.89618185]), 'currentState': array([15.65694601, 12.70518141,  5.09709919]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.478704524677376}
done in step count: 99
reward sum = -0.2795288757634505
running average episode reward sum: 0.09523467540801381
{'scaleFactor': 20, 'currentTarget': array([34.83128118, 14.98464588]), 'dynamicTrap': False, 'previousTarget': array([33.52879815, 14.79099515]), 'currentState': array([14.91358792, 13.17205241,  6.06199534]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:195
target Thresh 67.34973644753997
target distance 2.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.32402995, 13.15333775,  2.04035473]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.8748751659204352}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.09964978425797291
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.88384802, 14.55963791,  0.32354004]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9874746011968593}
episode index:196
target Thresh 67.43580800800046
target distance 25.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.33100204, 14.94456537]), 'dynamicTrap': False, 'previousTarget': array([30., 15.]), 'currentState': array([ 8.33169294, 14.77832536,  2.26369286]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09914394778965833
{'scaleFactor': 20, 'currentTarget': array([28.00458764, 15.32589812]), 'dynamicTrap': False, 'previousTarget': array([28.59061516, 14.92971391]), 'currentState': array([ 8.02625624, 16.25663678,  0.91144824]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:197
target Thresh 67.5210231421249
target distance 27.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.89027898, 13.22716877]), 'dynamicTrap': False, 'previousTarget': array([27.6656401, 13.6417852]), 'currentState': array([8.48447895, 8.38827166, 3.9943924 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09864322078061966
{'scaleFactor': 20, 'currentTarget': array([28.41620599, 13.7118925 ]), 'dynamicTrap': False, 'previousTarget': array([26.82558472, 13.57047799]), 'currentState': array([8.78833822, 9.87173535, 5.12199766]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:198
target Thresh 67.60539037149772
target distance 21.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.44819767, 15.6513813 ]), 'dynamicTrap': True, 'previousTarget': array([33.79898987, 15.17157288]), 'currentState': array([14.       , 18.       ,  0.5925778], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.58949724381704}
done in step count: 99
reward sum = -0.21803271172332322
running average episode reward sum: 0.09705188443637873
{'scaleFactor': 20, 'currentTarget': array([33.26286997, 14.4074717 ]), 'dynamicTrap': True, 'previousTarget': array([33.32916309, 14.30394407]), 'currentState': array([13.41941332, 16.20180183,  1.34349197]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.92441699917798}
episode index:199
target Thresh 67.68891813291215
target distance 13.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.78755502,  3.26683303,  1.44888175]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.735090119726534}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09656662501419683
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.77070032,  1.41902275,  3.25570399]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.602827719841256}
episode index:200
target Thresh 67.77161477921395
target distance 9.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.2308885 ,  5.90999007,  2.34097004]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.172969364481458}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09608619404397695
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.25847655,  5.8566789 ,  2.60071671]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.229522410522797}
episode index:201
target Thresh 67.85348858013666
target distance 11.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.3225893 , 16.0516311 ,  5.95632857]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.734382657811478}
done in step count: 99
reward sum = -0.2259947703432988
running average episode reward sum: 0.09449173382423796
{'scaleFactor': 20, 'currentTarget': array([36.29360489, 16.1948462 ]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([26.06196967, 14.68932616,  4.741439  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.341805923696725}
episode index:202
target Thresh 67.9345477231286
target distance 28.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.47170909, 13.91433954]), 'dynamicTrap': False, 'previousTarget': array([26.79898987, 13.82842712]), 'currentState': array([ 8.74266157, 10.63338176,  5.24271107]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0940262573029363
{'scaleFactor': 20, 'currentTarget': array([26.01896271, 13.12770335]), 'dynamicTrap': False, 'previousTarget': array([27.46277843, 13.35890834]), 'currentState': array([6.43989738, 9.04601244, 2.18528784]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:203
target Thresh 68.01480031417161
target distance 30.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.51763963, 15.13504462]), 'dynamicTrap': False, 'previousTarget': array([24.98889814, 14.6662966 ]), 'currentState': array([ 5.51966758, 15.41984909,  0.20993757]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0935653442769415
{'scaleFactor': 20, 'currentTarget': array([25.12453674, 15.06108825]), 'dynamicTrap': False, 'previousTarget': array([25.63355591, 14.58689099]), 'currentState': array([ 5.12491938, 15.18480311,  0.9036941 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:204
target Thresh 68.0942543785917
target distance 21.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.08538575, 15.83076942]), 'dynamicTrap': False, 'previousTarget': array([33.45612429, 15.36758945]), 'currentState': array([12.85146345, 21.31312592,  1.38738883]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11835503338673178
running average episode reward sum: 0.09253158633711872
{'scaleFactor': 20, 'currentTarget': array([33.94425439, 15.35440153]), 'dynamicTrap': False, 'previousTarget': array([32.5640204 , 15.91629616]), 'currentState': array([14.98402664, 21.71912962,  4.4461694 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:205
target Thresh 68.1729178618615
target distance 4.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.11559197, 17.56859468,  3.14894056]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.800397126250011}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09208240387917153
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.72788421, 18.34738947,  0.85319799]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.358431664768013}
episode index:206
target Thresh 68.2507986303949
target distance 3.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.57055855, 19.63288797,  0.30563188]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.849148593797034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09163756134835428
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.63636137, 19.56588535,  0.12172097]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.272353395748793}
episode index:207
target Thresh 68.32790447233364
target distance 12.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.61448352,  1.36178082,  3.4712677 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.509573696614828}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09119699614956413
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.12367482,  2.84401524,  1.90412045]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.051814312871649}
episode index:208
target Thresh 68.40424309832619
target distance 9.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.60321189, 17.60718139,  5.71283502]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.842822776296669}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0907606468856906
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.38424099, 17.33560289,  5.36844194]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.01593244494517}
episode index:209
target Thresh 68.47982214229874
target distance 19.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.18603402, 14.64630461]), 'dynamicTrap': False, 'previousTarget': array([33.69836445, 14.31492866]), 'currentState': array([15.84296165,  6.67562736,  0.65424252]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2942064588106196
running average episode reward sum: 0.08892747019189866
{'scaleFactor': 20, 'currentTarget': array([33.87585071, 14.50998365]), 'dynamicTrap': False, 'previousTarget': array([34.00319243, 14.46919304]), 'currentState': array([15.54194673,  6.51824012,  1.14176654]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:210
target Thresh 68.5546491622187
target distance 30.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.95228597, 16.99063126]), 'dynamicTrap': False, 'previousTarget': array([24.82455801, 16.3567256 ]), 'currentState': array([ 4.26925434, 20.53721637,  1.09246397]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08850601298719771
{'scaleFactor': 20, 'currentTarget': array([26.04000345, 16.45423347]), 'dynamicTrap': False, 'previousTarget': array([24.47065576, 16.82954691]), 'currentState': array([ 6.2983331 , 19.65836391,  4.92750615]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:211
target Thresh 68.62873164085039
target distance 28.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.4438203 , 11.99728574]), 'dynamicTrap': False, 'previousTarget': array([26.23047895, 12.49442256]), 'currentState': array([7.57219688, 5.37445935, 4.11042762]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08808853179386188
{'scaleFactor': 20, 'currentTarget': array([24.91506057, 11.50463033]), 'dynamicTrap': False, 'previousTarget': array([26.4657121 , 12.08413922]), 'currentState': array([6.01790598, 4.95500833, 2.56320458]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:212
target Thresh 68.70207698650344
target distance 13.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.51381898,  1.15237908,  4.03166103]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.279708097552287}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08767497061173106
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.7069735 ,  2.72670669,  1.7239126 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.36599634098521}
episode index:213
target Thresh 68.7746925337735
target distance 8.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.47025089,  6.2983906 ,  2.56161118]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.714306741545027}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08726527448737718
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.84579868,  5.78083037,  3.16049379]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.257886583953658}
episode index:214
target Thresh 68.84658554427583
target distance 6.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.47978444, 19.72203374,  1.68818116]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.879822339997407}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08685938948976148
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.07060341, 19.35478762,  4.51941258]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.3567600965586575}
episode index:215
target Thresh 68.91776320737138
target distance 14.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.01063186, 15.1993836 ]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.      , 10.      ,  4.758116], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.944276332333168}
done in step count: 99
reward sum = -0.4098174253356125
running average episode reward sum: 0.08455995979149585
{'scaleFactor': 20, 'currentTarget': array([33.40010288, 15.12779777]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([19.8067293 , 10.53579035,  0.75371355]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.34804297656194}
episode index:216
target Thresh 68.9882326408858
target distance 20.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([34.9007438 , 15.00992562]), 'currentState': array([15.36486621, 18.64269489,  0.55188131]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.970170378005}
done in step count: 99
reward sum = -0.27515394023793494
running average episode reward sum: 0.08290229204942473
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([34.93278183, 17.40345868]), 'currentState': array([16.02036579, 17.29700921,  3.03434541]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.118126639403727}
episode index:217
target Thresh 69.05800089182114
target distance 29.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.40485652, 14.20059356]), 'dynamicTrap': False, 'previousTarget': array([25.95260657, 14.37604183]), 'currentState': array([ 7.51472501, 12.10711116,  4.87411324]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08252200630607875
{'scaleFactor': 20, 'currentTarget': array([26.64472792, 13.86933411]), 'dynamicTrap': False, 'previousTarget': array([25.70547215, 14.14569593]), 'currentState': array([ 6.82537549, 11.18730706,  4.44535933]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:218
target Thresh 69.12707493706064
target distance 13.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.66101683, 20.28817991,  2.62020922]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.283039129024806}
done in step count: 99
reward sum = -0.185605987989481
running average episode reward sum: 0.08129767756500314
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.251383  , 21.35198476,  5.05629384]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.243417635043956}
episode index:219
target Thresh 69.1954616840664
target distance 20.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.73669365, 15.57307508]), 'dynamicTrap': False, 'previousTarget': array([33.56953382, 15.57218647]), 'currentState': array([15.52309587, 23.83532996,  0.37642121]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11294808885354322
running average episode reward sum: 0.08041474226310066
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([16.16572054, 21.39003902,  2.77522049]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.88875766513396}
episode index:220
target Thresh 69.2631679715701
target distance 4.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.84513907, 12.13004914,  1.35705304]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.049701700491271}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.08005087465105043
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.94951563, 12.78535019,  0.07613301]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.7696324346608354}
episode index:221
target Thresh 69.3302005702569
target distance 4.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.58338653, 18.36818254,  5.50165385]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.145440180930524}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07969028512559526
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.60624069, 16.98102864,  3.03292394]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.1071784843111527}
episode index:222
target Thresh 69.39656618344256
target distance 26.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.36445451, 12.97703661]), 'dynamicTrap': False, 'previousTarget': array([28.48782391, 13.49719013]), 'currentState': array([8.0314713 , 7.85495107, 3.00031471]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07933292958691546
{'scaleFactor': 20, 'currentTarget': array([26.97372675, 13.0795723 ]), 'dynamicTrap': False, 'previousTarget': array([27.47218896, 12.72721381]), 'currentState': array([7.52275288, 8.42558311, 1.05821955]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:223
target Thresh 69.46227144774366
target distance 24.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.86555024, 14.92408021]), 'dynamicTrap': False, 'previousTarget': array([30.93091516, 14.6609096 ]), 'currentState': array([11.87141428, 14.43980042,  0.01653814]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07897876472268815
{'scaleFactor': 20, 'currentTarget': array([31.00786799, 14.58985281]), 'dynamicTrap': False, 'previousTarget': array([32.31938213, 14.6555311 ]), 'currentState': array([11.11259246, 12.54583437,  1.8682974 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:224
target Thresh 69.52732293374143
target distance 23.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.09422604, 13.3003178 ]), 'dynamicTrap': False, 'previousTarget': array([31.13347761, 13.82323232]), 'currentState': array([11.19632679,  6.75284473,  3.13634944]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07862774799058732
{'scaleFactor': 20, 'currentTarget': array([29.63784357, 13.16416137]), 'dynamicTrap': False, 'previousTarget': array([31.22647744, 13.71719287]), 'currentState': array([10.71609545,  6.6859326 ,  2.49235189]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:225
target Thresh 69.59172714663865
target distance 13.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.70437705,  4.34319731,  3.52581739]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.03939650245729}
done in step count: 99
reward sum = -0.2602093202843912
running average episode reward sum: 0.07712846892742369
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([19.76201409,  4.95544031,  2.03314579]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.250736804387905}
episode index:226
target Thresh 69.65549052691028
target distance 23.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.45665762, 15.15100018]), 'dynamicTrap': False, 'previousTarget': array([31.83200822, 15.41321632]), 'currentState': array([12.49181358, 16.3363282 ,  4.11108696]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07678869593655398
{'scaleFactor': 20, 'currentTarget': array([31.22878213, 15.15764422]), 'dynamicTrap': False, 'previousTarget': array([32.87588956, 15.06875689]), 'currentState': array([11.24623329, 15.99295354,  1.97940939]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:227
target Thresh 69.7186194509475
target distance 5.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.47337567, 16.4648264 ,  1.83603096]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.688986587207395}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07645190341051646
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.10359616, 15.96043038,  4.17736465]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.989709119774664}
episode index:228
target Thresh 69.78112023169533
target distance 17.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([16.46529691, 14.36993212,  1.89506006]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.5454092505543}
done in step count: 99
reward sum = -0.2799308157076708
running average episode reward sum: 0.07489564699515319
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([17.7635733 , 14.35352089,  4.11244273]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.248546040686207}
episode index:229
target Thresh 69.84299911928389
target distance 13.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.62455753, 16.64055815]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([22.       , 20.       ,  5.9273653], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.063893075122627}
done in step count: 99
reward sum = -0.29371909785922284
running average episode reward sum: 0.0732929741914385
{'scaleFactor': 20, 'currentTarget': array([34.70997297, 16.84908182]), 'dynamicTrap': True, 'previousTarget': array([34.5378637 , 16.74786296]), 'currentState': array([21.32826896, 19.8006442 ,  6.16707427]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.703347129890549}
episode index:230
target Thresh 69.90426230165353
target distance 13.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.16191479, 12.88316483,  5.55711966]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.011434312450803}
done in step count: 99
reward sum = -0.4295003255421649
running average episode reward sum: 0.07111637982029737
{'scaleFactor': 20, 'currentTarget': array([33.42232486, 16.04436332]), 'dynamicTrap': True, 'previousTarget': array([33.40433176, 15.84550884]), 'currentState': array([21.23713375, 12.2922789 ,  0.56826121]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.749785083796157}
episode index:231
target Thresh 69.96491590517354
target distance 29.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.03344565, 15.40824606]), 'dynamicTrap': False, 'previousTarget': array([25.89383588, 15.94201698]), 'currentState': array([ 6.05414317, 16.31790113,  3.73456573]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07080984370038229
{'scaleFactor': 20, 'currentTarget': array([24.97777159, 15.1938849 ]), 'dynamicTrap': False, 'previousTarget': array([26.47922363, 15.42842444]), 'currentState': array([ 4.98151302, 15.58072227,  2.75062126]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:232
target Thresh 70.02496599525482
target distance 33.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.12543634, 16.67096395]), 'dynamicTrap': False, 'previousTarget': array([21.6773982 , 17.42229124]), 'currentState': array([ 2.29178691, 19.24513377,  4.04040325]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07050593879179697
{'scaleFactor': 20, 'currentTarget': array([22.76554942, 17.5888565 ]), 'dynamicTrap': False, 'previousTarget': array([21.22291762, 17.59131029]), 'currentState': array([ 3.19881434, 21.72925242,  5.54872769]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:233
target Thresh 70.08441857695641
target distance 26.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.29718104, 14.08952092]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 13.77694787]), 'currentState': array([10.6617835 , 10.28805231,  5.45818097]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.07020463136106278
{'scaleFactor': 20, 'currentTarget': array([30.38849134, 14.12189585]), 'dynamicTrap': False, 'previousTarget': array([28.98762619, 13.88636703]), 'currentState': array([10.74150199, 10.38079848,  5.36936999]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:234
target Thresh 70.14327959558602
target distance 26.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.8042068 , 17.44052022]), 'dynamicTrap': False, 'previousTarget': array([28.11558017, 17.11828302]), 'currentState': array([ 7.63599417, 23.14837661,  2.02323866]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06990588824888805
{'scaleFactor': 20, 'currentTarget': array([26.7061767 , 17.29057138]), 'dynamicTrap': False, 'previousTarget': array([28.04621743, 16.68932563]), 'currentState': array([ 7.42788643, 22.61481068,  1.34678835]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:235
target Thresh 70.20155493729459
target distance 12.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.3853913 ,  1.34648519,  3.95655143]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.69907683642431}
done in step count: 99
reward sum = -0.13723381604650778
running average episode reward sum: 0.06902817763746688
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.46311565,  1.76828856,  4.03902816]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.310436867428116}
episode index:236
target Thresh 70.25925042966482
target distance 24.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.06295583, 12.00335817]), 'dynamicTrap': False, 'previousTarget': array([28.88854382, 11.94427191]), 'currentState': array([9.70276854, 4.07218405, 1.44089055]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06873691950397547
{'scaleFactor': 20, 'currentTarget': array([28.17552952, 11.68019762]), 'dynamicTrap': False, 'previousTarget': array([29.47789586, 12.44199741]), 'currentState': array([10.19060974,  2.93133137,  2.90113432]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:237
target Thresh 70.31637184229405
target distance 22.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.21400546, 13.26135613]), 'dynamicTrap': True, 'previousTarget': array([31.20732955, 13.27605889]), 'currentState': array([13.       ,  5.       ,  1.4284567], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.237945453320151
running average episode reward sum: 0.06744833810555477
{'scaleFactor': 20, 'currentTarget': array([31.80677008, 13.26085981]), 'dynamicTrap': False, 'previousTarget': array([31.69131327, 15.44251429]), 'currentState': array([14.24280035,  3.69493241,  3.54339273]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:238
target Thresh 70.37292488737113
target distance 8.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.5598846 ,  6.28309884,  4.90035308]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.855371574635367}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06716612748586626
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.45688054,  6.51782257,  5.32535017]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.606383381407767}
episode index:239
target Thresh 70.4289152202477
target distance 31.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.23544837, 11.08336555]), 'dynamicTrap': False, 'previousTarget': array([22.65136197, 10.21988205]), 'currentState': array([4.25942099, 4.7658989 , 0.59980369]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06688626862134181
{'scaleFactor': 20, 'currentTarget': array([23.68901364, 11.13042429]), 'dynamicTrap': False, 'previousTarget': array([22.46205535, 10.30392676]), 'currentState': array([4.76574816, 4.65662914, 6.25035733]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:240
target Thresh 70.48434844000371
target distance 6.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.98283488, 18.43202407,  1.30858684]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.811491250681206}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06660873223702089
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.31177355, 16.76515548,  2.4881844 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.917235494994334}
episode index:241
target Thresh 70.53923009000735
target distance 16.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.0670162, 16.3014855]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([19.      , 21.      ,  5.015664], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.701951320430467}
done in step count: 99
reward sum = -0.23651846899340345
running average episode reward sum: 0.0653561404963993
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([18.80189758, 20.43539173,  4.08105266]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.085725193780764}
episode index:242
target Thresh 70.5935656584693
target distance 27.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.2362304 , 15.86649585]), 'dynamicTrap': False, 'previousTarget': array([27.87767469, 15.79136948]), 'currentState': array([ 9.45847614, 18.83978297,  5.79561669]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06508718518571453
{'scaleFactor': 20, 'currentTarget': array([30.02537483, 15.61259447]), 'dynamicTrap': False, 'previousTarget': array([29.03569224, 16.00918863]), 'currentState': array([10.17531571, 18.05700706,  4.31340659]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:243
target Thresh 70.64736057899175
target distance 26.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.58205653, 14.114289  ]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 13.77694787]), 'currentState': array([ 8.76983349, 11.38009477,  0.80314577]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06482043442675668
{'scaleFactor': 20, 'currentTarget': array([29.85988317, 14.12256983]), 'dynamicTrap': False, 'previousTarget': array([30.08216782, 14.44339054]), 'currentState': array([10.14505875, 10.75720239,  3.68060505]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:244
target Thresh 70.70062023111153
target distance 8.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.7653339 , 21.06799401,  2.8428731 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.049869198666068}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06455586122501482
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.82072742, 23.29506826,  0.15155601]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.372033080783856}
episode index:245
target Thresh 70.75334994083828
target distance 8.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.67354947,  7.17782086,  5.37904185]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.266459533900385}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06429343902491313
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.04723992,  7.12912637,  4.86545986]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.440161302877934}
episode index:246
target Thresh 70.80555498118687
target distance 7.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.54548112, 14.66624035,  5.68020231]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.4647206485254145}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06403314170092564
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.97521593, 15.17421581,  5.63012625]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.027803305841329}
episode index:247
target Thresh 70.85724057270487
target distance 7.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.5301258 , 18.29921249,  4.84370399]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.387826458872357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06377494354890578
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.62180674, 17.80159867,  1.49081892]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.8921917669936335}
episode index:248
target Thresh 70.90841188399449
target distance 23.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.60219508, 15.32327029]), 'dynamicTrap': False, 'previousTarget': array([31.98112317, 15.13125551]), 'currentState': array([10.65601017, 16.78945833,  1.60049009]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06351881927762504
{'scaleFactor': 20, 'currentTarget': array([31.65908454, 15.32452644]), 'dynamicTrap': False, 'previousTarget': array([30.079932  , 15.58661662]), 'currentState': array([11.75277795, 17.25816494,  4.84665537]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:249
target Thresh 70.9590740322295
target distance 25.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.39259673, 11.60093882]), 'dynamicTrap': False, 'previousTarget': array([27.74433602, 11.22705473]), 'currentState': array([9.13243026, 3.44212026, 1.10238647]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06326474400051454
{'scaleFactor': 20, 'currentTarget': array([26.88447577, 11.58239856]), 'dynamicTrap': False, 'previousTarget': array([27.19997153, 11.16464827]), 'currentState': array([8.4522058 , 3.82021949, 1.05622799]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:250
target Thresh 71.00923208366696
target distance 33.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.97639537, 11.80185386]), 'dynamicTrap': False, 'previousTarget': array([21.43700211, 11.71200051]), 'currentState': array([3.64843938, 6.66083087, 5.07026548]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06301269322760412
{'scaleFactor': 20, 'currentTarget': array([20.72062488, 11.23366477]), 'dynamicTrap': False, 'previousTarget': array([22.20390202, 11.27194053]), 'currentState': array([1.38200535, 6.13290066, 1.85768217]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:251
target Thresh 71.05889105415379
target distance 13.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.63305994, 14.59317901,  5.02903903]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.374217760410763}
done in step count: 99
reward sum = -0.3047343357618051
running average episode reward sum: 0.06155337962050329
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.04797349, 15.57691648,  0.81830232]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.964868812570694}
episode index:252
target Thresh 71.10805590962842
target distance 28.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.49840997, 15.83816594]), 'dynamicTrap': False, 'previousTarget': array([26.88618308, 15.86933753]), 'currentState': array([ 8.66256277, 18.39534525,  5.5509407 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.06131008562990842
{'scaleFactor': 20, 'currentTarget': array([29.10313323, 15.46562694]), 'dynamicTrap': False, 'previousTarget': array([28.41767333, 15.87565305]), 'currentState': array([ 9.16519276, 17.03996168,  4.07327992]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:253
target Thresh 71.1567315666174
target distance 16.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([19.01091384, 12.17898988,  1.40889709]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.236039364768057}
done in step count: 99
reward sum = -0.38852108511545297
running average episode reward sum: 0.0595390967687062
{'scaleFactor': 20, 'currentTarget': array([35.90224952, 13.56737996]), 'dynamicTrap': True, 'previousTarget': array([35.98750991, 13.7479278 ]), 'currentState': array([20.55438256, 10.2848775 ,  3.22860922]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.694962333658506}
episode index:254
target Thresh 71.20492289272697
target distance 14.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.92518673, 16.18058616,  1.86479325]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.124239879637848}
done in step count: 99
reward sum = -0.30812469117486124
running average episode reward sum: 0.058097277992456925
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([36.22490201, 16.3729781 ]), 'currentState': array([21.60852665, 15.17923187,  2.78127695]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.392672716533204}
episode index:255
target Thresh 71.25263470712989
target distance 3.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.68967585, 18.35144579,  5.55234081]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.598491109169333}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05787033550029889
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.66543825, 18.23293814,  1.17663461]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.98773965399116}
episode index:256
target Thresh 71.29987178104737
target distance 31.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.39174098, 11.72732396]), 'dynamicTrap': False, 'previousTarget': array([23.03417236, 11.1400556 ]), 'currentState': array([5.28051905, 5.83146132, 5.84907943]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05764515909757399
{'scaleFactor': 20, 'currentTarget': array([22.69892159, 11.48260537]), 'dynamicTrap': False, 'previousTarget': array([22.49935682, 10.75979659]), 'currentState': array([3.46959959, 5.98413509, 0.61513573]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:257
target Thresh 71.34663883822618
target distance 9.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.58854539,  6.08878324,  5.32901687]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.94337548263893}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05742172824835859
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.00631104,  5.00842605,  1.81439102]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.651431556802995}
episode index:258
target Thresh 71.39294055541099
target distance 27.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.90661618, 11.99182594]), 'dynamicTrap': False, 'previousTarget': array([27.17596225, 12.68176659]), 'currentState': array([6.91862018, 5.71042419, 3.00457716]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.057200022733886156
{'scaleFactor': 20, 'currentTarget': array([26.13950799, 11.95607684]), 'dynamicTrap': False, 'previousTarget': array([26.70613866, 12.61506881]), 'currentState': array([7.22454847, 5.45805357, 3.54391754]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:259
target Thresh 71.43878156281212
target distance 15.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.55056392, 20.58706583,  5.01291383]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.563743864140564}
done in step count: 99
reward sum = -0.15192377415490443
running average episode reward sum: 0.05639570043816004
{'scaleFactor': 20, 'currentTarget': array([35.6781741 , 16.86178447]), 'dynamicTrap': True, 'previousTarget': array([35.47853951, 16.86908665]), 'currentState': array([20.44667805, 20.89247279,  5.7312643 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.755790049322597}
episode index:260
target Thresh 71.4841664445685
target distance 12.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.50542638, 23.85311529,  1.72152674]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.13942897005471}
done in step count: 99
reward sum = -0.14718874323794298
running average episode reward sum: 0.05561568341258111
{'scaleFactor': 20, 'currentTarget': array([33.39038886, 14.50853274]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.96545869, 22.54579972,  1.28634217]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.968775532437249}
episode index:261
target Thresh 71.52909973920613
target distance 22.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.50941315, 15.63453066]), 'dynamicTrap': False, 'previousTarget': array([32.81660336, 15.29773591]), 'currentState': array([11.83189442, 19.21157643,  1.32792902]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.045153783456446335
running average episode reward sum: 0.055231067126821465
{'scaleFactor': 20, 'currentTarget': array([31.95380864, 15.66102645]), 'dynamicTrap': False, 'previousTarget': array([32.11175739, 15.40215061]), 'currentState': array([12.40869901, 19.90233397,  0.6093857 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:262
target Thresh 71.57358594009192
target distance 23.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.60178027, 15.27265481]), 'dynamicTrap': False, 'previousTarget': array([31.7042351, 15.5731765]), 'currentState': array([12.72979591, 17.53191074,  4.18107152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.055021063069305036
{'scaleFactor': 20, 'currentTarget': array([32.68572883, 15.18913875]), 'dynamicTrap': False, 'previousTarget': array([31.57482108, 15.46909683]), 'currentState': array([12.75218932, 16.81824977,  4.33847553]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:263
target Thresh 71.61762949588302
target distance 18.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.97476978, 15.19808465]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([17.      ,  9.      ,  4.935346], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.013379554104215}
done in step count: 99
reward sum = -0.2999153966122947
running average episode reward sum: 0.05367660678263231
{'scaleFactor': 20, 'currentTarget': array([35.76159649, 14.41673019]), 'dynamicTrap': True, 'previousTarget': array([35.7003904 , 14.58585311]), 'currentState': array([16.92486752,  7.69529867,  3.94159503]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:264
target Thresh 71.66123481097173
target distance 27.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.41210968, 15.20837195]), 'dynamicTrap': False, 'previousTarget': array([28., 15.]), 'currentState': array([ 6.41799423, 15.69349842,  1.9188906 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0534740535494903
{'scaleFactor': 20, 'currentTarget': array([26.52795954, 15.22242168]), 'dynamicTrap': False, 'previousTarget': array([27.92743777, 14.94855014]), 'currentState': array([ 6.53484851, 15.74731312,  1.53592443]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:265
target Thresh 71.70440624592588
target distance 13.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.46111019,  2.68132682,  1.71479321]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.330454621261847}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05327302327298846
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.02119558,  3.45370351,  0.16369593]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.546315943802485}
episode index:266
target Thresh 71.74714811792495
target distance 13.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.94240967, 21.72016152,  3.70041454]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.685408950524668}
done in step count: 99
reward sum = -0.18889752923100758
running average episode reward sum: 0.05236601745836675
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.58781037, 21.27773638,  4.31568448]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.808673296775854}
episode index:267
target Thresh 71.78946470119176
target distance 27.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.76956691, 12.22661235]), 'dynamicTrap': False, 'previousTarget': array([27.17596225, 12.68176659]), 'currentState': array([6.61547463, 6.47154945, 2.55824935]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05217062187083553
{'scaleFactor': 20, 'currentTarget': array([25.85982163, 12.82489137]), 'dynamicTrap': False, 'previousTarget': array([26.0893283 , 12.33230612]), 'currentState': array([6.4031582 , 8.19474578, 0.87890774]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:268
target Thresh 71.8313602274199
target distance 23.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.30734488, 13.38371068]), 'dynamicTrap': False, 'previousTarget': array([30.04268443, 12.62910995]), 'currentState': array([12.98556988,  5.36419944,  6.21834499]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05197667903860194
{'scaleFactor': 20, 'currentTarget': array([30.2880571 , 12.81354666]), 'dynamicTrap': False, 'previousTarget': array([30.17391142, 12.52509022]), 'currentState': array([12.1460628 ,  4.39523074,  0.78905994]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:269
target Thresh 71.8728388861969
target distance 19.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.58047111, 16.03263011]), 'dynamicTrap': True, 'previousTarget': array([34.4327075, 15.23886  ]), 'currentState': array([16.       , 23.       ,  6.0882907], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.843844131358168}
done in step count: 99
reward sum = -0.15463664435858565
running average episode reward sum: 0.051211444507501244
{'scaleFactor': 20, 'currentTarget': array([33.14335039, 15.64819004]), 'dynamicTrap': False, 'previousTarget': array([34.68863249, 15.1153781 ]), 'currentState': array([14.260997  , 22.24036192,  2.02701009]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:270
target Thresh 71.91390482542322
target distance 3.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.22128593, 10.84204976,  4.51440104]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.163834489554148}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05102247238754736
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.46569069, 10.91825937,  4.79525962]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.108220340440383}
episode index:271
target Thresh 71.95456215172698
target distance 5.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.08056631, 10.69594517,  4.41224291]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.821241140167034}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05083488976847549
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.03469066, 11.67027109,  5.72593119]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.17791200633053}
episode index:272
target Thresh 71.9948149308747
target distance 29.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.59444436, 13.61255363]), 'dynamicTrap': False, 'previousTarget': array([25.70920231, 13.39813833]), 'currentState': array([ 4.76989606, 10.96920632,  1.50159323]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05064868138104518
{'scaleFactor': 20, 'currentTarget': array([27.11532412, 13.71011592]), 'dynamicTrap': False, 'previousTarget': array([26.37579395, 14.09992587]), 'currentState': array([ 7.37769859, 10.48116279,  4.40866715]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:273
target Thresh 72.03466718817785
target distance 2.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.53568426, 17.75681533,  1.07043719]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.6976861431356105}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05046383217892458
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.21114738, 18.3083846 ,  6.09242326]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.4011317403081662}
episode index:274
target Thresh 72.07412290889535
target distance 30.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.90645504, 17.87621571]), 'dynamicTrap': False, 'previousTarget': array([24.47682419, 17.45540769]), 'currentState': array([ 3.44917138, 22.5037542 ,  1.81751537]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05028032733463758
{'scaleFactor': 20, 'currentTarget': array([23.75247866, 17.42666981]), 'dynamicTrap': False, 'previousTarget': array([23.99498177, 17.99729633]), 'currentState': array([ 4.20232071, 21.64464607,  3.40349495]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:275
target Thresh 72.11318603863218
target distance 7.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.82892995,  6.53532829,  4.21739483]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.039156622344336}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05009815223559903
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.31619978,  5.65353237,  2.78096014]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.454398195226496}
episode index:276
target Thresh 72.15186048373384
target distance 26.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.32724185, 15.44236837]), 'dynamicTrap': False, 'previousTarget': array([28.86817872, 15.70751784]), 'currentState': array([10.41626731, 17.32733333,  4.88237995]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04991729248023586
{'scaleFactor': 20, 'currentTarget': array([29.10649205, 15.67974176]), 'dynamicTrap': False, 'previousTarget': array([28.75957671, 15.36110559]), 'currentState': array([ 9.23820649, 17.97129772,  0.2665295 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:277
target Thresh 72.19015011167708
target distance 26.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.94424309, 14.87139944]), 'dynamicTrap': False, 'previousTarget': array([28.98522349, 15.23133756]), 'currentState': array([ 8.94875128, 14.4467735 ,  3.66940594]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04973773387419184
{'scaleFactor': 20, 'currentTarget': array([28.42082087, 14.99106536]), 'dynamicTrap': False, 'previousTarget': array([29.81313611, 15.00234872]), 'currentState': array([ 8.42083931, 14.96390503,  2.16569573]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:278
target Thresh 72.22805875145662
target distance 32.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.79854981, 15.26731164]), 'dynamicTrap': False, 'previousTarget': array([22.96105157, 15.75243428]), 'currentState': array([ 2.80334777, 15.70537024,  3.55164289]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0495594624266141
{'scaleFactor': 20, 'currentTarget': array([22.62868053, 15.112313  ]), 'dynamicTrap': False, 'previousTarget': array([23.82285278, 15.42135354]), 'currentState': array([ 2.62950467, 15.29387549,  2.76184136]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:279
target Thresh 72.265590193968
target distance 25.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.02126311, 13.21617346]), 'dynamicTrap': False, 'previousTarget': array([29.44774604, 13.66745905]), 'currentState': array([8.6442549 , 8.26323971, 2.62938213]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049382464346519044
{'scaleFactor': 20, 'currentTarget': array([28.09860355, 13.22037018]), 'dynamicTrap': False, 'previousTarget': array([29.35112337, 13.72748929]), 'currentState': array([8.73212336, 8.22642935, 2.9272722 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:280
target Thresh 72.30274819238674
target distance 8.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.52705159,  9.48051001,  4.07523668]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.290302885202522}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04920672603923606
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.57088934, 11.3841304 ,  0.4799726 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.171936517044028}
episode index:281
target Thresh 72.33953646254369
target distance 4.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.10229831, 15.42355884,  1.12341523]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.915982494530315}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04903223410292671
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.59562092, 15.49920443,  4.50695402]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.440785079500037}
episode index:282
target Thresh 72.37595868329649
target distance 32.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.2583017 , 11.25411712]), 'dynamicTrap': False, 'previousTarget': array([21.91373199, 10.50159537]), 'currentState': array([4.20442777, 5.17547515, 6.04641861]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048858975325177856
{'scaleFactor': 20, 'currentTarget': array([21.79394901,  9.93374534]), 'dynamicTrap': False, 'previousTarget': array([22.95646769, 10.7848979 ]), 'currentState': array([3.12088696, 2.77017405, 3.13899624]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:283
target Thresh 72.41201849689757
target distance 2.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.32954925, 15.34961054,  3.89947653]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.3556373327253297}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.052172852172624413
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.63815901, 15.64725331,  2.01649547]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.9089465153990423}
episode index:284
target Thresh 72.44771950935835
target distance 31.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.4427359 , 12.13223831]), 'dynamicTrap': False, 'previousTarget': array([23.63559701, 12.80043813]), 'currentState': array([2.94473298, 7.67938732, 3.04081166]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05198978953342222
{'scaleFactor': 20, 'currentTarget': array([23.12227768, 12.11189011]), 'dynamicTrap': False, 'previousTarget': array([23.51606621, 12.71515098]), 'currentState': array([3.68852521, 7.38650497, 3.55478442]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:285
target Thresh 72.48306529080982
target distance 21.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.80009279, 14.10868614]), 'dynamicTrap': False, 'previousTarget': array([33.23047895, 14.49442256]), 'currentState': array([12.53354471,  8.74211133,  2.30567122]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14127565423797003
running average episode reward sum: 0.051314036233522244
{'scaleFactor': 20, 'currentTarget': array([34.00472955, 14.76453358]), 'dynamicTrap': False, 'previousTarget': array([32.40657427, 14.43838743]), 'currentState': array([14.54200376, 10.15993751,  5.24613674]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:286
target Thresh 72.5180593758596
target distance 1.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.63164324, 14.92422788,  1.5375483 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.3704530771364563}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.05458471903410231
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.06647384, 15.81046936,  5.83236474]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8131908447968156}
episode index:287
target Thresh 72.55270526394531
target distance 32.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.08442731, 14.18968739]), 'dynamicTrap': False, 'previousTarget': array([22.99024152, 14.62469505]), 'currentState': array([ 4.13930827, 12.70907048,  4.492344  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05439518875967834
{'scaleFactor': 20, 'currentTarget': array([23.5307041 , 14.08187284]), 'dynamicTrap': False, 'previousTarget': array([22.82894702, 14.56082199]), 'currentState': array([ 3.59447918, 12.48596061,  4.27503209]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:288
target Thresh 72.58700641968467
target distance 6.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.57717263, 22.525728  ,  0.83114314]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.729145651857888}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05420697011345108
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.08968587, 22.28194243,  4.56846744]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.841977678323209}
episode index:289
target Thresh 72.62096627322182
target distance 14.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([19.29102059, 16.34123517,  2.03910732]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.766132879856837}
done in step count: 99
reward sum = -0.27076989437560406
running average episode reward sum: 0.053086360235902616
{'scaleFactor': 20, 'currentTarget': array([34.51008427, 16.71221539]), 'dynamicTrap': True, 'previousTarget': array([34.37696576, 16.56339899]), 'currentState': array([20.24400964, 17.73614565,  6.08161248]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.30277310390241}
episode index:290
target Thresh 72.65458822057043
target distance 2.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.17115765, 15.79137357,  4.47204383]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.1459719765224843}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.05634035899797855
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.17115765, 15.79137357,  4.47204383]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.1459719765224843}
episode index:291
target Thresh 72.68787562395326
target distance 30.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.68571129, 14.53105823]), 'dynamicTrap': False, 'previousTarget': array([24.9007438 , 13.99007438]), 'currentState': array([ 5.7110109 , 13.52540211,  0.1246233 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.056147412563053965
{'scaleFactor': 20, 'currentTarget': array([26.51845864, 14.06617003]), 'dynamicTrap': False, 'previousTarget': array([27.04946042, 14.51372527]), 'currentState': array([ 6.63859081, 11.87736783,  3.39573396]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:292
target Thresh 72.72083181213836
target distance 29.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.78812989, 11.31257534]), 'dynamicTrap': False, 'previousTarget': array([25.10128274, 11.9279843 ]), 'currentState': array([5.97693709, 4.52000399, 3.68680716]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05595578316864081
{'scaleFactor': 20, 'currentTarget': array([26.21747721, 12.08697813]), 'dynamicTrap': False, 'previousTarget': array([24.85439288, 12.0236183 ]), 'currentState': array([7.23443909, 5.79060904, 4.9067753 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:293
target Thresh 72.75346008077202
target distance 33.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.47919541, 11.04365703]), 'dynamicTrap': False, 'previousTarget': array([21.14048809, 10.80014791]), 'currentState': array([1.18261192, 5.78610384, 1.64559221]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0557654573755502
{'scaleFactor': 20, 'currentTarget': array([21.04603443, 10.64369463]), 'dynamicTrap': False, 'previousTarget': array([22.4793664, 11.334121 ]), 'currentState': array([1.95475727, 4.68356575, 2.79075247]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:294
target Thresh 72.7857636927083
target distance 9.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.96227264, 18.38073149,  6.23531932]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.719771018414475}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05557642192681952
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.58806212, 18.59869712,  0.65344495]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.149389041519065}
episode index:295
target Thresh 72.81774587833532
target distance 27.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.94818748, 15.55232227]), 'dynamicTrap': False, 'previousTarget': array([27.98629668, 15.25976679]), 'currentState': array([ 9.03096481, 17.37007945,  6.25318164]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05538866374463432
{'scaleFactor': 20, 'currentTarget': array([28.56966301, 15.09132407]), 'dynamicTrap': False, 'previousTarget': array([30.02559788, 15.25191535]), 'currentState': array([ 8.57167969, 15.37533674,  2.74590147]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:296
target Thresh 72.84940983589827
target distance 32.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.50222955, 14.93734161]), 'dynamicTrap': False, 'previousTarget': array([22.99024152, 14.62469505]), 'currentState': array([ 4.5025858 , 14.81796906,  5.81926817]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05520216992731231
{'scaleFactor': 20, 'currentTarget': array([24.48583605, 14.84801715]), 'dynamicTrap': False, 'previousTarget': array([23.05186217, 14.81982385]), 'currentState': array([ 4.4879252 , 14.55894618,  5.32909507]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:297
target Thresh 72.88075873181931
target distance 11.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.16249706, 15.03662797]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([24.       ,  5.       ,  3.4583635], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.011170559821377}
done in step count: 99
reward sum = -0.2716361437154239
running average episode reward sum: 0.05410539706273938
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35.35544879, 12.8649457 ]), 'currentState': array([22.62510547,  7.28208151,  0.40900719]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.58438481787501}
episode index:298
target Thresh 72.91179570101417
target distance 7.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.45574198, 21.67806767,  2.47910511]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.6936005633884}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.053924442557512824
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.60215228, 22.86831138,  0.56321209]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.029770605461854}
episode index:299
target Thresh 72.94252384720562
target distance 27.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.0450192 , 16.71386903]), 'dynamicTrap': False, 'previousTarget': array([27.5237412 , 16.66139084]), 'currentState': array([ 6.4015449 , 20.47337929,  2.44984961]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05374469441565445
{'scaleFactor': 20, 'currentTarget': array([26.9932696 , 17.19956206]), 'dynamicTrap': False, 'previousTarget': array([25.33302   , 17.55618614]), 'currentState': array([ 7.70775295, 22.49756617,  5.23481196]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:300
target Thresh 72.97294624323388
target distance 7.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.88078385, 20.56591218,  4.25316763]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.924400335621362}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.053566140613609085
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.36094492, 20.3506376 ,  2.37726349]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.773561916613531}
episode index:301
target Thresh 73.00306593136389
target distance 11.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.53309496, 18.58312323,  4.06225204]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.063221649398717}
done in step count: 99
reward sum = -0.2273604274251536
running average episode reward sum: 0.052635920189639675
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.90541387, 19.61223383,  5.72514855]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.098349876198347}
episode index:302
target Thresh 73.03288592358962
target distance 21.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.35831571, 15.75402811]), 'dynamicTrap': False, 'previousTarget': array([33.45612429, 15.36758945]), 'currentState': array([13.12641658, 21.24347763,  1.17321992]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2212913536852078
running average episode reward sum: 0.05173186978081179
{'scaleFactor': 20, 'currentTarget': array([33.1134764 , 15.44040585]), 'dynamicTrap': False, 'previousTarget': array([34.34235149, 15.17814357]), 'currentState': array([13.63714809, 19.98712286,  2.61116002]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:303
target Thresh 73.06240920193507
target distance 22.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.51952411, 16.42445554]), 'dynamicTrap': False, 'previousTarget': array([32.0585156 , 15.93592685]), 'currentState': array([11.45959503, 22.48408453,  2.01703262]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0515616991565328
{'scaleFactor': 20, 'currentTarget': array([30.44823995, 16.46171498]), 'dynamicTrap': False, 'previousTarget': array([30.80230289, 16.03240632]), 'currentState': array([11.40602316, 22.57677667,  0.6986295 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:304
target Thresh 73.09163871875273
target distance 4.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.36848378, 12.674849  ,  1.11641383]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.511581525042959}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0513926444051999
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.36316862, 12.73352688,  1.49657464]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.4770361143459967}
episode index:305
target Thresh 73.12057739701862
target distance 9.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.9720643 ,  7.220217  ,  1.26087356]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.841800261848435}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05122469458688225
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.66225334,  8.09220734,  0.67938304]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.3746803764995}
episode index:306
target Thresh 73.1492281306247
target distance 24.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.78693965, 16.14031573]), 'dynamicTrap': False, 'previousTarget': array([29.97366596, 16.67544468]), 'currentState': array([11.4815783 , 21.36554521,  4.02456617]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05105783890418882
{'scaleFactor': 20, 'currentTarget': array([29.34213904, 16.41864601]), 'dynamicTrap': False, 'previousTarget': array([31.01548757, 16.08222667]), 'currentState': array([ 9.94266664, 21.28284899,  2.31586385]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:307
target Thresh 73.17759378466818
target distance 29.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.24400136, 13.88342549]), 'dynamicTrap': False, 'previousTarget': array([25.70920231, 13.39813833]), 'currentState': array([ 7.44808707, 11.03355221,  6.06659013]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.05089206669995444
{'scaleFactor': 20, 'currentTarget': array([27.67237647, 13.7241435 ]), 'dynamicTrap': False, 'previousTarget': array([26.65429427, 13.97570737]), 'currentState': array([ 7.96881681, 10.29343849,  4.49390597]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:308
target Thresh 73.2056771957381
target distance 12.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.81679211,  9.9205839 ,  3.4516116 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.199659940412136}
done in step count: 99
reward sum = -0.31545161449232645
running average episode reward sum: 0.04970648844366874
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([36.62889717, 14.97697977]), 'currentState': array([18.66830143,  9.04297745,  1.8251398 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.384202478889826}
episode index:309
target Thresh 73.233481172199
target distance 7.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.39482998, 21.49421615,  2.43684149]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.642318429030894}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049546144932560134
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.24573059, 22.4839739 ,  4.54548228]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.813652877823302}
episode index:310
target Thresh 73.26100849447168
target distance 26.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.59643717, 13.28489454]), 'dynamicTrap': False, 'previousTarget': array([28.31231517, 13.19946947]), 'currentState': array([10.53363512,  7.2343096 ,  4.86355311]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04938683256943293
{'scaleFactor': 20, 'currentTarget': array([28.12684483, 12.71603739]), 'dynamicTrap': False, 'previousTarget': array([29.37596491, 13.29320034]), 'currentState': array([9.14731428, 6.4091031 , 2.95273036]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:311
target Thresh 73.28826191531131
target distance 31.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.80709554, 12.5336196 ]), 'dynamicTrap': False, 'previousTarget': array([23.50882004, 12.40521743]), 'currentState': array([5.36807487, 7.82995346, 5.14952332]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.049228541439402695
{'scaleFactor': 20, 'currentTarget': array([23.20949558, 12.35019773]), 'dynamicTrap': False, 'previousTarget': array([23.10234614, 11.67345106]), 'currentState': array([3.69621606, 7.96477616, 0.64696259]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:312
target Thresh 73.31524416008267
target distance 10.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.6757964 , 16.84477598,  5.18082184]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.526169357525687}
done in step count: 99
reward sum = -0.04127507328233381
running average episode reward sum: 0.04893939251057926
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.33032797, 17.13604253,  0.82102722]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.902789244774556}
episode index:313
target Thresh 73.34195792703274
target distance 13.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.843268  ,  2.14538004,  5.36385984]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.416853696912707}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048783534572647475
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.51297524,  3.47209392,  1.40881747]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.626767077629927}
episode index:314
target Thresh 73.36840588756047
target distance 5.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.08596728, 11.59567294,  0.5242424 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.594848939316308}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04862866620892479
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.91835525, 10.35953592,  1.12849111]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.730463325771716}
episode index:315
target Thresh 73.39459068648395
target distance 4.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.95324171, 20.73082872,  0.67195964]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.015600509923374}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048474778024719334
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.13670261, 20.8224247 ,  0.178649  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.9875386270439686}
episode index:316
target Thresh 73.4205149423049
target distance 8.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.56055063,  7.94980055,  1.6201539 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.548393608932626}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.048321860743884254
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.37962453,  7.79717833,  4.94567126]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.557365809249594}
episode index:317
target Thresh 73.4461812474705
target distance 29.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.27592941, 11.68922852]), 'dynamicTrap': False, 'previousTarget': array([24.69995053, 11.09308468]), 'currentState': array([7.57714564, 4.59306862, 5.66081602]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0481699052069538
{'scaleFactor': 20, 'currentTarget': array([24.52976888, 10.86188301]), 'dynamicTrap': False, 'previousTarget': array([25.52789183, 10.73438175]), 'currentState': array([5.92977913, 3.51066671, 1.5036943 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:318
target Thresh 73.47159216863268
target distance 10.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.91910518,  6.18851223,  1.29880905]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.389053630856608}
done in step count: 99
reward sum = -0.06157877399138489
running average episode reward sum: 0.04782586546025054
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([36.04731731, 13.03794359]), 'currentState': array([25.14800884,  7.43888961,  0.38217525]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.41902251659492}
episode index:319
target Thresh 73.49675024690468
target distance 33.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.77377695, 18.59065616]), 'dynamicTrap': False, 'previousTarget': array([21.43700211, 18.28799949]), 'currentState': array([ 2.47240284, 23.83059484,  0.27311897]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04767640963068726
{'scaleFactor': 20, 'currentTarget': array([22.00426941, 18.50425536]), 'dynamicTrap': False, 'previousTarget': array([21.77871523, 17.87253921]), 'currentState': array([ 2.69397175, 23.71121264,  0.27999992]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:320
target Thresh 73.52165799811533
target distance 21.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([33.97736275, 15.04869701]), 'currentState': array([15.59854109, 16.04696908,  5.30255932]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.42968739005226}
done in step count: 99
reward sum = -0.24155978063672023
running average episode reward sum: 0.04677536230898194
{'scaleFactor': 20, 'currentTarget': array([34.53919114, 14.98865532]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([14.54524936, 14.49642342,  3.15666095]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:321
target Thresh 73.5463179130605
target distance 8.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.59441137,  6.46126029,  4.94733709]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.871129152087867}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04663009720864348
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.82914963,  7.2894727 ,  0.64345664]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.766311890407701}
episode index:322
target Thresh 73.57073245775224
target distance 31.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.12548666, 12.984323  ]), 'dynamicTrap': False, 'previousTarget': array([23.83555733, 13.55942675]), 'currentState': array([4.46045497, 9.33925395, 4.07905245]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046485731582610525
{'scaleFactor': 20, 'currentTarget': array([23.86120314, 13.12229769]), 'dynamicTrap': False, 'previousTarget': array([23.98847688, 13.73335944]), 'currentState': array([4.13945582, 9.79774019, 3.71448559]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:323
target Thresh 73.59490407366536
target distance 4.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.12783652,  9.61988597,  3.1388073 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.628670830038184}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04634225710241728
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.07706218, 11.64960988,  6.16873163]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.158929644623948}
episode index:324
target Thresh 73.61883517798158
target distance 26.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.59600927, 14.60095978]), 'dynamicTrap': False, 'previousTarget': array([29., 15.]), 'currentState': array([ 8.63472341, 13.35714849,  3.48360491]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.046199665542102156
{'scaleFactor': 20, 'currentTarget': array([27.8829195 , 14.62379588]), 'dynamicTrap': False, 'previousTarget': array([28.22175767, 15.02057938]), 'currentState': array([ 7.91080218, 13.56808312,  3.57600152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:325
target Thresh 73.6425281638313
target distance 33.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.86061867, 14.5084068 ]), 'dynamicTrap': False, 'previousTarget': array([22., 15.]), 'currentState': array([ 2.87699758, 13.69915532,  4.30013424]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04605794877663559
{'scaleFactor': 20, 'currentTarget': array([23.17573514, 14.69621938]), 'dynamicTrap': False, 'previousTarget': array([21.57129601, 14.45340256]), 'currentState': array([ 3.1823323 , 14.18256307,  5.60335022]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:326
target Thresh 73.66598540053283
target distance 30.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.92043969,  9.87407178]), 'dynamicTrap': False, 'previousTarget': array([23.56953382, 10.42781353]), 'currentState': array([5.76891742, 1.47631974, 4.21019375]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04591709878037677
{'scaleFactor': 20, 'currentTarget': array([24.40735695, 10.41332629]), 'dynamicTrap': False, 'previousTarget': array([23.49735738, 10.65171358]), 'currentState': array([6.05405068, 2.46624197, 4.55453569]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:327
target Thresh 73.6892092338294
target distance 25.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.11565605, 13.45103593]), 'dynamicTrap': False, 'previousTarget': array([29.61161351, 13.9223227 ]), 'currentState': array([8.60345285, 9.06082795, 2.72361994]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04577710762555855
{'scaleFactor': 20, 'currentTarget': array([29.50160573, 13.59519973]), 'dynamicTrap': False, 'previousTarget': array([29.44545529, 13.97057446]), 'currentState': array([10.12405844,  8.64437548,  3.96142745]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:328
target Thresh 73.71220198612369
target distance 7.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.30719568,  9.56284009,  1.02339661]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.182278213348605}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04563796748080001
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.34931259,  9.1691641 ,  4.53478152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.4050597998571535}
episode index:329
target Thresh 73.73496595671007
target distance 5.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.78982802, 20.03445031,  1.42688334]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.994493476243744}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04549967060964607
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.75650728, 20.70830749,  0.97465152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.459667589780285}
episode index:330
target Thresh 73.7575034220046
target distance 22.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.00895464, 13.35768916]), 'dynamicTrap': False, 'previousTarget': array([30.88854382, 12.94427191]), 'currentState': array([14.47779975,  3.73175501,  5.09362388]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14181310967770056
running average episode reward sum: 0.04493377097131572
{'scaleFactor': 20, 'currentTarget': array([30.49397035, 12.68948516]), 'dynamicTrap': False, 'previousTarget': array([31.97541289, 13.32557524]), 'currentState': array([12.69718878,  3.56399528,  2.35316215]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:331
target Thresh 73.77981663577259
target distance 23.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.1641835, 11.3925946]), 'dynamicTrap': False, 'previousTarget': array([29.73169692, 12.25132013]), 'currentState': array([10.47607547,  2.05820473,  2.90383625]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044798428287667176
{'scaleFactor': 20, 'currentTarget': array([26.72113081, 11.03884769]), 'dynamicTrap': False, 'previousTarget': array([28.11383879, 11.61971855]), 'currentState': array([8.67988276, 2.40673504, 2.44141769]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:332
target Thresh 73.80190782935398
target distance 8.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.47556205, 23.829501  ,  0.27872252]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.183293251949804}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044663898472989495
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.24253175, 21.11821435,  2.95973355]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.365629697405228}
episode index:333
target Thresh 73.82377921188656
target distance 25.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.35584515, 15.65902553]), 'dynamicTrap': False, 'previousTarget': array([29.74881264, 15.84018998]), 'currentState': array([11.67508319, 19.21818397,  5.41325504]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04453017422606438
{'scaleFactor': 20, 'currentTarget': array([29.87846644, 15.78095394]), 'dynamicTrap': False, 'previousTarget': array([30.5793562 , 15.44953692]), 'currentState': array([10.1070036 , 18.79579344,  0.97079641]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:334
target Thresh 73.8454329705268
target distance 30.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.05985376, 11.15860624]), 'dynamicTrap': False, 'previousTarget': array([24.1565257 , 11.74695771]), 'currentState': array([4.02090424, 5.03337959, 2.9105792 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.044397248332852246
{'scaleFactor': 20, 'currentTarget': array([24.3427368 , 11.93369719]), 'dynamicTrap': False, 'previousTarget': array([23.05278808, 11.97174022]), 'currentState': array([5.12247162, 6.40365142, 4.85112173]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:335
target Thresh 73.86687127066863
target distance 10.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.54822085,  4.60403258,  2.39786243]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.398086036043006}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04426511366519495
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.66135874,  6.75936506,  0.58803248]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.723523458722113}
episode index:336
target Thresh 73.8880962561599
target distance 14.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.1297853 , 14.86270614]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.       , 18.       ,  2.4270883], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.473888394667185}
done in step count: 99
reward sum = -0.3091749334054552
running average episode reward sum: 0.04321633014273011
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.44368908, 18.21381884,  1.95375211]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.906871538529183}
episode index:337
target Thresh 73.90911004951687
target distance 9.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.65609723, 18.87430382,  2.95261955]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.045657728195309}
done in step count: 99
reward sum = -0.1341299933562377
running average episode reward sum: 0.04269163687794027
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.57012009, 19.53117624,  4.65247463]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.462035804793256}
episode index:338
target Thresh 73.92991475213636
target distance 10.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.36425554,  5.57436719,  5.67167277]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.432668547972584}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04256570284585195
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.66066995,  5.08792362,  0.23970747]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.91788298929591}
episode index:339
target Thresh 73.95051244450599
target distance 8.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.66998785, 13.64055532,  1.68276   ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.428532048270418}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042440509602187676
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.6598456 , 12.96274377,  2.27518589]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.55975403295753}
episode index:340
target Thresh 73.97090518641218
target distance 27.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.48159469, 10.02617146]), 'dynamicTrap': False, 'previousTarget': array([26.0200334 , 10.67631238]), 'currentState': array([6.40114008, 1.47648369, 2.44907999]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04231605062974724
{'scaleFactor': 20, 'currentTarget': array([25.73591923, 10.76621682]), 'dynamicTrap': False, 'previousTarget': array([24.7693545 , 10.82701224]), 'currentState': array([7.54552019, 2.4530115 , 4.70845527]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:341
target Thresh 73.99109501714608
target distance 11.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.18031734,  2.38302786,  3.81344557]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.18248692966005}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.042192319487555
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.41544233,  3.99689181,  0.11539548]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.920006666197114}
episode index:342
target Thresh 74.01108395570759
target distance 29.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.98342653, 15.54671896]), 'dynamicTrap': False, 'previousTarget': array([25.89383588, 15.94201698]), 'currentState': array([ 5.01315147, 16.63672525,  3.07581472]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04206930980974872
{'scaleFactor': 20, 'currentTarget': array([24.6777137 , 15.36773268]), 'dynamicTrap': False, 'previousTarget': array([26.41143851, 15.22522812]), 'currentState': array([ 4.69039312, 16.07978335,  2.07799077]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:343
target Thresh 74.03087400100725
target distance 9.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.45042231,  5.31767986,  4.06649947]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.916756633680256}
done in step count: 99
reward sum = -0.08675220724394057
running average episode reward sum: 0.041694828655522875
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([24.22981229,  6.77953022,  0.58491581]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.548913854335453}
episode index:344
target Thresh 74.05046713206606
target distance 5.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.57300531, 20.58241998,  0.21337676]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.124724192267413}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.041573974079709766
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.85991102, 19.45127565,  5.14800096]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.4473859530982445}
episode index:345
target Thresh 74.06986530821347
target distance 24.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.46727161, 11.39601121]), 'dynamicTrap': False, 'previousTarget': array([28.88854382, 11.94427191]), 'currentState': array([9.42587346, 2.76421229, 2.28027678]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.04145381808525974
{'scaleFactor': 20, 'currentTarget': array([28.4457178 , 12.35651063]), 'dynamicTrap': False, 'previousTarget': array([27.38765178, 11.63790657]), 'currentState': array([9.89751036, 4.87560154, 0.01251733]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:346
target Thresh 74.08907046928324
target distance 16.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.46749323, 17.93690071,  5.23021377]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.826298892980876}
done in step count: 99
reward sum = -0.5560148866133829
running average episode reward sum: 0.039732006256157026
{'scaleFactor': 20, 'currentTarget': array([33.56329089, 16.70485504]), 'dynamicTrap': True, 'previousTarget': array([33.42484624, 16.56098032]), 'currentState': array([18.69703289, 18.64679005,  0.62464937]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.99255610685241}
episode index:347
target Thresh 74.1080845358075
target distance 23.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.37300863, 15.08314377]), 'dynamicTrap': False, 'previousTarget': array([32., 15.]), 'currentState': array([10.37623681, 15.44247167,  1.86555433]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03961783382438646
{'scaleFactor': 20, 'currentTarget': array([31.13834705, 15.02006932]), 'dynamicTrap': False, 'previousTarget': array([32.65333093, 15.00479808]), 'currentState': array([11.13861714, 15.12400954,  2.07993472]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:348
target Thresh 74.12690940920874
target distance 13.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.9649706 , 21.08688918,  6.11793226]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.486739878412036}
done in step count: 99
reward sum = -0.19021659614155564
running average episode reward sum: 0.03895928244912589
{'scaleFactor': 20, 'currentTarget': array([36.43997721, 15.65333359]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([23.77037575, 19.89493008,  4.55259931]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.360761273878797}
episode index:349
target Thresh 74.14554697198997
target distance 12.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.56656586,  2.08100453,  2.79807484]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.658542463850642}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03884797021355696
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.06839857,  3.48425248,  0.58641136]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.527295543183318}
episode index:350
target Thresh 74.16399908792303
target distance 21.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.2780135 , 14.83937537]), 'dynamicTrap': False, 'previousTarget': array([33.79898987, 14.82842712]), 'currentState': array([14.75532179, 10.49604619,  4.16781425]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1925803048057929
running average episode reward sum: 0.038188630398687015
{'scaleFactor': 20, 'currentTarget': array([34.68766636, 14.9295274 ]), 'dynamicTrap': False, 'previousTarget': array([33.31895017, 13.71712628]), 'currentState': array([15.17811712, 10.52754052,  5.84808039]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:351
target Thresh 74.18226760223489
target distance 30.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.96285973, 13.37698016]), 'dynamicTrap': False, 'previousTarget': array([24.61161351, 12.9223227 ]), 'currentState': array([ 4.17565405, 10.46725657,  1.07267523]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03808013997141802
{'scaleFactor': 20, 'currentTarget': array([25.28230528, 13.17444405]), 'dynamicTrap': False, 'previousTarget': array([25.09489984, 13.70932336]), 'currentState': array([5.62614072, 9.48185756, 3.92014105]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:352
target Thresh 74.20035434179216
target distance 5.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.53554872, 12.68882379,  5.69486171]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.1646078024535225}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03797226422079077
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.39207307, 12.79988722,  1.04685635]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.106220402807693}
episode index:353
target Thresh 74.21826111528392
target distance 2.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.50586969, 17.50720002,  5.59806639]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.9246701510291926}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.040062233416209854
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.2948209 , 15.98913572,  2.67135516]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0321379966490805}
episode index:354
target Thresh 74.23598971340242
target distance 13.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.81990061,  1.14888011,  4.22209644]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.1352524482624}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.039949382054474056
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.11686497,  1.83242811,  2.32930231]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.214852891131267}
episode index:355
target Thresh 74.25354190902225
target distance 32.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.63820571, 16.9189222 ]), 'dynamicTrap': False, 'previousTarget': array([22.65744374, 17.3142293 ]), 'currentState': array([ 1.8413151 , 19.76200541,  3.17252076]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0398371646891525
{'scaleFactor': 20, 'currentTarget': array([23.0481503 , 17.18182176]), 'dynamicTrap': False, 'previousTarget': array([22.01081939, 16.93538291]), 'currentState': array([ 3.37329453, 20.77348577,  5.93466222]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:356
target Thresh 74.27091945737759
target distance 14.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.12038589, 15.12401618]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([21.       , 14.       ,  4.0408435], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.16505241729676}
done in step count: 99
reward sum = -0.37756604805224003
running average episode reward sum: 0.03866796801480687
{'scaleFactor': 20, 'currentTarget': array([34.53657333, 16.98106806]), 'dynamicTrap': True, 'previousTarget': array([34.38150713, 16.8552852 ]), 'currentState': array([19.94381767, 15.77530353,  6.10512093]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.642485644798803}
episode index:357
target Thresh 74.28812409623775
target distance 24.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.48912359, 13.91843814]), 'dynamicTrap': False, 'previousTarget': array([30.40285  , 13.8507125]), 'currentState': array([12.37552186,  8.03029505,  4.65912995]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03855995693096662
{'scaleFactor': 20, 'currentTarget': array([29.38484652, 13.21582698]), 'dynamicTrap': False, 'previousTarget': array([30.92212386, 13.5998995 ]), 'currentState': array([10.3239131 ,  7.15935795,  2.09942505]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:358
target Thresh 74.30515754608098
target distance 4.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.54695116, 17.62084386,  0.83335447]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.167055889861479}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03845254758018399
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.22231558, 17.66657663,  0.56726021]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.471462272732212}
episode index:359
target Thresh 74.32202151026645
target distance 20.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.58189676, 13.20495765]), 'dynamicTrap': False, 'previousTarget': array([31.76887233, 12.89976701]), 'currentState': array([16.52300581,  1.28388296,  4.83365411]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.19247910897399373
running average episode reward sum: 0.037811070756422385
{'scaleFactor': 20, 'currentTarget': array([31.16112109, 13.68310028]), 'dynamicTrap': True, 'previousTarget': array([31.25311229, 13.53468015]), 'currentState': array([14.20772304,  3.07261987,  1.23025173]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:360
target Thresh 74.3387176752046
target distance 20.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.5196502 , 15.01363486]), 'dynamicTrap': False, 'previousTarget': array([34.97504678, 15.00124766]), 'currentState': array([13.52049849, 15.19783832,  2.62841511]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1701516576157053
running average episode reward sum: 0.037234996716610395
{'scaleFactor': 20, 'currentTarget': array([33.14631987, 16.1381638 ]), 'dynamicTrap': True, 'previousTarget': array([33.0691555 , 15.20531042]), 'currentState': array([13.18127046, 17.32002756,  0.18278604]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:361
target Thresh 74.3552477105259
target distance 9.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.12315632, 13.54128158,  3.19343758]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.98398219795191}
done in step count: 99
reward sum = -0.25688328426081464
running average episode reward sum: 0.036422515277446235
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([36.41534594, 13.85035422]), 'currentState': array([24.2885412 , 15.64271136,  1.07038342]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.730723531629556}
episode index:362
target Thresh 74.3716132692476
target distance 4.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.31209132,  9.91734716,  4.61407233]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.749627196252851}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03632217776979487
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.90496781,  9.89009331,  2.03918088]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.548315431642158}
episode index:363
target Thresh 74.38781598793923
target distance 3.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.72718192, 18.09918406,  5.41212714]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.5479711446320477}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.0375682439654938
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.82689587, 15.9252537 ,  2.82100394]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.240907411985714}
episode index:364
target Thresh 74.40385748688617
target distance 13.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.92944622, 19.08523981,  0.03112555]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.743133558036606}
done in step count: 99
reward sum = -0.367606224218446
running average episode reward sum: 0.03645817692937341
{'scaleFactor': 20, 'currentTarget': array([35.52704317, 16.62602865]), 'dynamicTrap': True, 'previousTarget': array([35.34966317, 16.5818221 ]), 'currentState': array([23.36757339, 18.54590111,  5.48483653]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.310102180490581}
episode index:365
target Thresh 74.41973937025168
target distance 29.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.49511065, 11.65595634]), 'dynamicTrap': False, 'previousTarget': array([25.27985236, 12.31857996]), 'currentState': array([5.43742282, 5.58928224, 3.32292795]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0363585644241019
{'scaleFactor': 20, 'currentTarget': array([25.12447261, 11.75642508]), 'dynamicTrap': False, 'previousTarget': array([25.56880058, 12.43703391]), 'currentState': array([6.12312952, 5.51551484, 3.61574656]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:366
target Thresh 74.43546322623732
target distance 33.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.57208011, 13.66807151]), 'dynamicTrap': False, 'previousTarget': array([21.77431008, 12.99610759]), 'currentState': array([ 2.68595902, 11.53683061,  0.14098477]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.036259494766270564
{'scaleFactor': 20, 'currentTarget': array([21.80619701, 13.46965077]), 'dynamicTrap': False, 'previousTarget': array([22.66511239, 13.03621692]), 'currentState': array([ 1.93939135, 11.16529994,  1.26046557]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:367
target Thresh 74.45103062724179
target distance 23.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.62705999, 14.83344358]), 'dynamicTrap': False, 'previousTarget': array([32., 15.]), 'currentState': array([10.64155116, 14.07223607,  2.80344939]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.036160963530492654
{'scaleFactor': 20, 'currentTarget': array([29.82809004, 14.78203381]), 'dynamicTrap': False, 'previousTarget': array([31.64011522, 14.87426318]), 'currentState': array([ 9.84582778, 13.93989666,  2.48713219]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:368
target Thresh 74.46644313001818
target distance 11.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.29510114,  3.44951314,  2.64219809]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.675633896531025}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0360629663393531
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.01250881,  5.58997455,  0.3763901 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.617624458284627}
episode index:369
target Thresh 74.4817022758296
target distance 3.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.55470131, 12.43821882,  2.502316  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.130633809497114}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03596549886276026
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.60905344, 13.42848286,  4.70830161]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 2.8611696170682417}
episode index:370
target Thresh 74.49680959060336
target distance 10.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.1715395 ,  9.32579503,  3.80449295]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.34879895958161}
done in step count: 99
reward sum = -0.32543386291728205
running average episode reward sum: 0.03499137659381136
{'scaleFactor': 20, 'currentTarget': array([36.31516355, 15.02086197]), 'dynamicTrap': True, 'previousTarget': array([36.43081531, 15.17045681]), 'currentState': array([24.88825858, 10.57851305,  5.473642  ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.260041648093626}
episode index:371
target Thresh 74.51176658508352
target distance 11.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.67193752,  4.00009049,  5.42350138]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.079790578221695}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03489731375350542
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.1078264 ,  2.7289065 ,  2.26493531]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.873567923352384}
episode index:372
target Thresh 74.52657475498198
target distance 17.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.17722656, 14.9381103 ]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([18.       , 17.       ,  2.9046197], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.300534714697736}
done in step count: 99
reward sum = -0.32625021246670277
running average episode reward sum: 0.03392908982262014
{'scaleFactor': 20, 'currentTarget': array([34.00332675, 16.5749042 ]), 'dynamicTrap': True, 'previousTarget': array([33.87295454, 16.42360058]), 'currentState': array([17.79806482, 19.00268739,  0.39500521]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.386111363200556}
episode index:373
target Thresh 74.54123558112808
target distance 25.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.18055333, 11.20046698]), 'dynamicTrap': False, 'previousTarget': array([28.30630065, 12.05477229]), 'currentState': array([9.19176105, 2.45956586, 3.31808078]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03383837033111581
{'scaleFactor': 20, 'currentTarget': array([26.20636243, 11.61365845]), 'dynamicTrap': False, 'previousTarget': array([26.35926726, 11.08774803]), 'currentState': array([7.54241961, 4.42636144, 0.94347262]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:374
target Thresh 74.55575052961666
target distance 6.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.5401486 , 15.46104835,  4.03994083]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.479283067003422}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.033748134676899494
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.77580923, 15.47524526,  2.88597781]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.2423079746909345}
episode index:375
target Thresh 74.57012105195464
target distance 16.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.52600569,  8.25638287,  4.84443462]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.96787034653134}
done in step count: 99
reward sum = -0.31531434907396305
running average episode reward sum: 0.032819777007349324
{'scaleFactor': 20, 'currentTarget': array([36.11431081, 13.59102739]), 'dynamicTrap': True, 'previousTarget': array([36.17374461, 13.77434638]), 'currentState': array([19.24239745,  7.20641901,  3.30123717]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.039531172075563}
episode index:376
target Thresh 74.58434858520626
target distance 23.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.52050708, 13.12394633]), 'dynamicTrap': False, 'previousTarget': array([30.62485559, 13.28798697]), 'currentState': array([10.59880405,  6.64558583,  1.70056462]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.032732721895923994
{'scaleFactor': 20, 'currentTarget': array([30.90151396, 13.34147408]), 'dynamicTrap': False, 'previousTarget': array([30.81816049, 13.63935878]), 'currentState': array([12.36197192,  5.83911572,  4.07981605]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:377
target Thresh 74.5984345521367
target distance 5.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.15061199, 18.28806682,  3.85129023]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.9289331084080037}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.032646127393553825
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.00049602, 20.85108815,  6.06117767]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.936010848985744}
episode index:378
target Thresh 74.61238036135434
target distance 32.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.25055659, 14.29886451]), 'dynamicTrap': False, 'previousTarget': array([22.96105157, 14.24756572]), 'currentState': array([ 1.2765096 , 13.28031323,  2.08133411]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03255998985425685
{'scaleFactor': 20, 'currentTarget': array([21.39714714, 15.0064833 ]), 'dynamicTrap': False, 'previousTarget': array([22.41822089, 14.53843509]), 'currentState': array([ 1.39714941, 15.01601557,  1.26785159]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:379
target Thresh 74.6261874074518
target distance 29.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.39168968, 15.69940995]), 'dynamicTrap': False, 'previousTarget': array([25.95260657, 15.62395817]), 'currentState': array([ 4.43501663, 17.01516104,  2.1874615 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03247430567042986
{'scaleFactor': 20, 'currentTarget': array([25.99040571, 15.52436348]), 'dynamicTrap': False, 'previousTarget': array([26.44212059, 15.9404493 ]), 'currentState': array([ 6.02419295, 16.68640833,  3.34478897]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:380
target Thresh 74.63985707114513
target distance 9.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.89687263, 19.66643461,  4.29631586]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.350737147209184}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03238907127234474
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([25.58587772, 19.61906392,  2.02984269]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.486250507217438}
episode index:381
target Thresh 74.65339071941213
target distance 7.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.78258991, 23.01106853,  1.43852425]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.560260820360622}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03230428312765274
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.75442008, 23.80546796,  6.24570126]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.775541669113824}
episode index:382
target Thresh 74.66678970562889
target distance 7.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.71070979, 18.76957713,  1.59345579]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.106154178109719}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.032219937740896466
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.68397707, 18.28832188,  2.43555129]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.942555460136742}
episode index:383
target Thresh 74.68005536970522
target distance 30.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.83038314, 17.20955482]), 'dynamicTrap': False, 'previousTarget': array([24.61161351, 17.0776773 ]), 'currentState': array([ 6.38690213, 21.89474918,  5.92790169]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03213603165302955
{'scaleFactor': 20, 'currentTarget': array([26.67668756, 16.82891691]), 'dynamicTrap': False, 'previousTarget': array([25.11173345, 17.09366363]), 'currentState': array([ 7.14270957, 21.12120114,  5.20114583]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:384
target Thresh 74.69318903821853
target distance 1.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.33004676, 16.54387524,  0.97021699]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.5787595169003446}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.0344979644264009
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.16774201, 15.99847359,  1.54944516]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0124657459620892}
episode index:385
target Thresh 74.70619202454667
target distance 20.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([34.40285  , 15.1492875]), 'currentState': array([16.52374771, 19.28545049,  4.83469123]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.966733631065}
done in step count: 99
reward sum = -0.21275556369503712
running average episode reward sum: 0.03385741124473914
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([16.66884545, 17.84223197,  3.32723678]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.550188940385592}
episode index:386
target Thresh 74.71906562899908
target distance 6.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.90860398,  9.78548624,  3.62727737]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.018494824546694}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.033769924393977545
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.6888101 , 10.12142146,  1.22587574]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.789426966277679}
episode index:387
target Thresh 74.73181113894695
target distance 9.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.96704224,  4.62260486,  4.31450988]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.811529156972503}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0336828885063642
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.12546782,  5.65372903,  5.13857008]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.778328911448158}
episode index:388
target Thresh 74.74442982895188
target distance 30.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.15055338, 16.14334284]), 'dynamicTrap': False, 'previousTarget': array([24.95570316, 15.66961979]), 'currentState': array([ 4.26069178, 18.23938807,  1.15120375]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.033596300104034214
{'scaleFactor': 20, 'currentTarget': array([24.39088265, 16.39378611]), 'dynamicTrap': False, 'previousTarget': array([24.5758702, 15.8218914]), 'currentState': array([ 4.56127728, 18.99892542,  0.63460333]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:389
target Thresh 74.75692296089339
target distance 15.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([18.60762694,  6.94534932,  1.53513551]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.26437220702668}
done in step count: 99
reward sum = -0.2680662902637199
running average episode reward sum: 0.032822806282578436
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([18.52016555,  6.97582684,  1.02368063]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.32954714738378}
episode index:390
target Thresh 74.76929178409509
target distance 15.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.53172394, 14.58642284]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([19.54120055,  1.34692857,  3.48755682]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1939588669346124
running average episode reward sum: 0.03224280200325058
{'scaleFactor': 20, 'currentTarget': array([33.56314167, 15.83779401]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([18.84560349,  3.55430064,  0.33980666]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.170032329737865}
episode index:391
target Thresh 74.78153753544962
target distance 7.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.52945383, 21.63769673,  1.84113729]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.761466918250628}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.032160549957323925
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.38891669, 21.27517531,  3.23585165]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.1150561017751}
episode index:392
target Thresh 74.79366143954229
target distance 32.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.47600904, 11.61447364]), 'dynamicTrap': False, 'previousTarget': array([22.08959956, 10.96549986]), 'currentState': array([4.28694977, 5.977098  , 5.92257101]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03207871649687272
{'scaleFactor': 20, 'currentTarget': array([23.64195178, 11.18949594]), 'dynamicTrap': False, 'previousTarget': array([22.42834029, 11.34916443]), 'currentState': array([4.68058572, 4.82815982, 4.79371828]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:393
target Thresh 74.80566470877363
target distance 4.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.27200581, 18.97669068,  2.22329724]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.450872357432071}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03199729843469792
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.20360088, 21.00800595,  6.17401141]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.626913576206941}
episode index:394
target Thresh 74.81754854348057
target distance 9.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.61776429,  7.00765349,  1.5487783 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.635666242745604}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0319162926158759
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.5431951 ,  7.58810635,  5.55788416]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.648657545908755}
episode index:395
target Thresh 74.82931413205648
target distance 12.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([23.44163454, 10.70061153,  4.03001881]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.332094439552538}
done in step count: 99
reward sum = -0.2545448628542034
running average episode reward sum: 0.031192905859638326
{'scaleFactor': 20, 'currentTarget': array([35.33983416, 16.53260254]), 'dynamicTrap': True, 'previousTarget': array([35.18389544, 16.47779306]), 'currentState': array([23.79319387, 12.4387043 ,  5.57858616]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.250914442611279}
episode index:396
target Thresh 74.84096265107001
target distance 33.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.15124749, 15.17231987]), 'dynamicTrap': False, 'previousTarget': array([21.99082358, 15.39421747]), 'currentState': array([ 1.15279559, 15.42116039,  2.53446102]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.031114334308354603
{'scaleFactor': 20, 'currentTarget': array([23.08542363, 15.83332775]), 'dynamicTrap': False, 'previousTarget': array([21.42720578, 15.88972726]), 'currentState': array([ 3.13416353, 17.22875618,  5.3014822 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:397
target Thresh 74.8524952653828
target distance 5.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.86205517, 11.44404556,  0.07434356]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.149171004324423}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.031036157588986875
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([32.05044118,  9.74738806,  2.35550172]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.02410403487246}
episode index:398
target Thresh 74.86391312826585
target distance 25.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.89686038, 10.78031441]), 'dynamicTrap': False, 'previousTarget': array([28.03046115, 11.65462135]), 'currentState': array([9.15795125, 1.54283083, 3.17840338]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030958372732874127
{'scaleFactor': 20, 'currentTarget': array([27.36054307, 11.69941496]), 'dynamicTrap': False, 'previousTarget': array([26.11331178, 10.99626477]), 'currentState': array([9.00080833, 3.76719329, 5.99416894]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:399
target Thresh 74.87521738151499
target distance 24.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.85721258, 15.43978783]), 'dynamicTrap': False, 'previousTarget': array([30.98266146, 15.16738911]), 'currentState': array([ 9.92994286, 17.14387744,  1.31286335]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030880976801041942
{'scaleFactor': 20, 'currentTarget': array([30.01270853, 15.40826222]), 'dynamicTrap': False, 'previousTarget': array([31.22241013, 15.11133428]), 'currentState': array([10.07938504, 17.04001421,  1.42015666]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:400
target Thresh 74.88640915556496
target distance 30.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.06579083, 11.6573434 ]), 'dynamicTrap': False, 'previousTarget': array([23.97366596, 11.32455532]), 'currentState': array([3.80696221, 6.2631317 , 1.41270769]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03080396688383236
{'scaleFactor': 20, 'currentTarget': array([23.7823574 , 11.34179445]), 'dynamicTrap': False, 'previousTarget': array([25.18845078, 11.953165  ]), 'currentState': array([4.76789982, 5.14095558, 2.71996338]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:401
target Thresh 74.8974895696025
target distance 33.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.00159612, 17.12044371]), 'dynamicTrap': False, 'previousTarget': array([21.5646825 , 17.84991583]), 'currentState': array([ 2.26251735, 20.34050103,  3.85927963]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030727340100539246
{'scaleFactor': 20, 'currentTarget': array([21.74576958, 17.71360998]), 'dynamicTrap': False, 'previousTarget': array([21.55918219, 17.07381909]), 'currentState': array([ 2.1522019 , 21.72510662,  0.34020543]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:402
target Thresh 74.90845973167822
target distance 26.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.4805167 , 12.45412116]), 'dynamicTrap': False, 'previousTarget': array([28.11558017, 12.88171698]), 'currentState': array([7.31783231, 6.72773248, 2.32793164]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030651093599049075
{'scaleFactor': 20, 'currentTarget': array([26.86700854, 12.93358424]), 'dynamicTrap': False, 'previousTarget': array([27.86353453, 12.7986146 ]), 'currentState': array([7.48290168, 8.00850543, 1.41517615]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:403
target Thresh 74.9193207388175
target distance 8.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.51278318,  9.60294775,  0.25118399]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.229658102470582}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.03057522455548707
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.73005339,  8.78720383,  1.45672733]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.343638296021126}
episode index:404
target Thresh 74.9300736771301
target distance 12.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.60191109,  4.57165235,  0.19504809]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.550666034162107}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030499730173868585
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.13927481,  4.23984325,  0.91796071]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.820301301747232}
episode index:405
target Thresh 74.9407196219188
target distance 28.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.60800443, 14.60695703]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([ 6.62990402, 13.67127351,  3.43073988]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.030424607685755608
{'scaleFactor': 20, 'currentTarget': array([26.40230244, 14.51358873]), 'dynamicTrap': False, 'previousTarget': array([28.0078528 , 14.76477822]), 'currentState': array([ 6.43423266, 13.38390324,  2.64438731]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:406
target Thresh 74.95125963778695
target distance 20.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.81733291, 13.52025146]), 'dynamicTrap': False, 'previousTarget': array([32.52431817, 13.638375  ]), 'currentState': array([13.68168686,  5.08826815,  1.4867419 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22079022811160867
running average episode reward sum: 0.029807372216966013
{'scaleFactor': 20, 'currentTarget': array([32.77277003, 13.96172371]), 'dynamicTrap': False, 'previousTarget': array([32.92502886, 13.86540028]), 'currentState': array([14.64568512,  5.51135138,  1.09327228]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:407
target Thresh 74.96169477874491
target distance 14.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.56323595,  2.51156875,  5.58945078]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.62481248483687}
done in step count: 99
reward sum = -0.19897405916684027
running average episode reward sum: 0.029246633414554725
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.95012796,  1.64170579,  2.62492371]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.674666921545477}
episode index:408
target Thresh 74.97202608831552
target distance 5.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.75397965, 19.1312919 ,  1.39441681]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.488680993359298}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.029175125753394444
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.95282452, 18.41909266,  3.39556832]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.096242692933438}
episode index:409
target Thresh 74.9822545996383
target distance 17.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.19331105, 14.95002305]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([18.       , 16.       ,  2.9876008], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.225341692795862}
done in step count: 99
reward sum = -0.30447031399187796
running average episode reward sum: 0.028361356388162073
{'scaleFactor': 20, 'currentTarget': array([36.67802149, 14.56686825]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([18.62928969, 16.05857606,  3.79374207]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 18.11027088315145}
episode index:410
target Thresh 74.99238133557292
target distance 3.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.85324964, 16.20626851,  6.14323932]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 4.0376498720570675}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02829235065485754
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.50189526, 14.94677339,  4.44862718]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.5022997443716823}
episode index:411
target Thresh 75.00240730880141
target distance 2.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.6993825 , 18.66360128,  0.77066815]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.72976007969307}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028223679900840896
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.3286442 , 18.12138825,  5.34931189]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.8943097488599245}
episode index:412
target Thresh 75.01233352192945
target distance 11.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.18857337,  2.7313658 ,  4.55198593]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.401639064107588}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028155341692848546
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.77786321,  1.75109023,  3.28452082]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.635093531256656}
episode index:413
target Thresh 75.02216096758661
target distance 21.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.1090941 , 12.94055829]), 'dynamicTrap': False, 'previousTarget': array([31.00530293, 12.52709229]), 'currentState': array([13.43249113,  3.5843995 ,  0.90474463]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14238766943299028
running average episode reward sum: 0.02774340205244797
{'scaleFactor': 20, 'currentTarget': array([31.96572859, 13.15115347]), 'dynamicTrap': False, 'previousTarget': array([32.04263029, 13.44032853]), 'currentState': array([14.88650596,  2.74441747,  4.17365122]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:414
target Thresh 75.03189062852569
target distance 11.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.62351321,  5.48506538,  0.90981674]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.613984433634474}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027676550481237253
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.84043619,  4.20436993,  3.86902386]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.796809217474614}
episode index:415
target Thresh 75.04152347772084
target distance 25.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.84584092, 11.49837608]), 'dynamicTrap': False, 'previousTarget': array([28.30630065, 12.05477229]), 'currentState': array([8.46864435, 3.60669398, 2.42132771]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0276100203118112
{'scaleFactor': 20, 'currentTarget': array([27.49828565, 11.49528669]), 'dynamicTrap': False, 'previousTarget': array([28.65803481, 12.29994957]), 'currentState': array([9.37824676, 3.0298162 , 3.15348184]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:416
target Thresh 75.05106047846505
target distance 12.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.33310599,  4.02723732,  5.9297151 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.923985094460354}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02754380923192676
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.81806949,  3.34223937,  1.2561276 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.195440415028825}
episode index:417
target Thresh 75.0605025844663
target distance 28.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.55426812, 15.66228096]), 'dynamicTrap': False, 'previousTarget': array([26.88618308, 15.86933753]), 'currentState': array([ 5.60324777, 17.06113297,  2.72340155]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027477914951467605
{'scaleFactor': 20, 'currentTarget': array([26.09053389, 16.28987979]), 'dynamicTrap': False, 'previousTarget': array([25.0098899 , 16.06531185]), 'currentState': array([ 6.29689729, 19.1555298 ,  6.07112009]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:418
target Thresh 75.06985073994309
target distance 8.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.62335133,  8.21427123,  0.11129975]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.692515179276269}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027412335202180095
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.30862733,  7.06792557,  1.85320002]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.03929786800062}
episode index:419
target Thresh 75.07910587971874
target distance 4.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.65864297, 11.26043761,  5.42893213]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 3.7971222551038473}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027347067737412998
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.12863211,  9.48791658,  2.80608684]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.58053274128582}
episode index:420
target Thresh 75.08826892931496
target distance 9.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.73877341, 14.81622714]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([26.       , 10.       ,  3.9133961], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.769337206653386}
done in step count: 99
reward sum = -0.28999623507019767
running average episode reward sum: 0.026593283170174016
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([36.39628503, 13.62721186]), 'currentState': array([24.24158544, 11.25206087,  0.82167813]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.3925647500264}
episode index:421
target Thresh 75.09734080504433
target distance 8.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.30128885, 21.50050275,  3.2663393 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.718791231309766}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026530265911476923
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([33.23527915, 23.06036605,  5.53046364]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.251287209587973}
episode index:422
target Thresh 75.10632241410197
target distance 8.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.09339615, 23.85172848,  1.14664376]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.55709716317946}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026467546606721657
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.25075573, 23.16364665,  0.90359909]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.966386334855658}
episode index:423
target Thresh 75.1152146546563
target distance 19.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.32735994, 15.10641182]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([14.57303124, 18.23155139,  2.62557793]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026405123147743542
{'scaleFactor': 20, 'currentTarget': array([34.05797014, 15.1616633 ]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([14.34612637, 18.54444548,  1.97638285]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:424
target Thresh 75.12401841593876
target distance 23.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.63616345, 16.96643779]), 'dynamicTrap': False, 'previousTarget': array([30.88993593, 16.4295875 ]), 'currentState': array([10.85829512, 23.85059795,  1.44965315]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02634299344621944
{'scaleFactor': 20, 'currentTarget': array([30.8682797 , 16.11070514]), 'dynamicTrap': False, 'previousTarget': array([31.61348205, 16.11983139]), 'currentState': array([11.55399316, 21.30284683,  2.92868454]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:425
target Thresh 75.13273457833284
target distance 25.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.12055106, 11.2951905 ]), 'dynamicTrap': False, 'previousTarget': array([28.30630065, 12.05477229]), 'currentState': array([9.02137577, 2.78520497, 3.02425003]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026281155433434886
{'scaleFactor': 20, 'currentTarget': array([28.44582717, 12.09334699]), 'dynamicTrap': False, 'previousTarget': array([26.92754374, 11.61521649]), 'currentState': array([10.16306675,  3.98528486,  5.33486945]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:426
target Thresh 75.14136401346202
target distance 33.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.96314122, 12.99500397]), 'dynamicTrap': False, 'previousTarget': array([21.77431008, 12.99610759]), 'currentState': array([ 1.16409813, 10.16694939,  2.01874638]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026219607060054474
{'scaleFactor': 20, 'currentTarget': array([22.53723327, 13.23249012]), 'dynamicTrap': False, 'previousTarget': array([23.76173033, 13.79465851]), 'currentState': array([ 2.7353873 , 10.42412828,  2.95788479]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:427
target Thresh 75.14990758427703
target distance 13.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.26205917,  3.6624419 ,  0.40444779]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.34058634544432}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02615834629589547
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([3.54150435e+01, 4.21283501e+00, 5.26756429e-03]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.795146581091835}
episode index:428
target Thresh 75.15836614514204
target distance 8.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.06287356, 10.75864403,  0.66138732]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.99928200756635}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02609737112970457
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.9326881 , 10.87577196,  0.48890812]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.060396141193161}
episode index:429
target Thresh 75.16674054192022
target distance 8.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.68339649, 23.81800791,  0.09790707]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.84696932002607}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026036679568937817
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.63300354, 22.08865531,  1.87728936]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.22358405716524}
episode index:430
target Thresh 75.17503161205819
target distance 8.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.48792739, 22.25211424,  2.59755874]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.26850975457126}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02597626963954353
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.67753651, 23.12253484,  0.19191456]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.15074401163906}
episode index:431
target Thresh 75.18324018466988
target distance 1.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.35101226, 15.52607552,  0.96829998]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8354283547695973}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.028230954200563103
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.35101226, 15.52607552,  0.96829998]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 0.8354283547695973}
episode index:432
target Thresh 75.19136708061941
target distance 33.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.75862062, 12.76448074]), 'dynamicTrap': False, 'previousTarget': array([21.85467564, 13.40662735]), 'currentState': array([2.03769854, 9.43503145, 3.72647333]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02816575569201677
{'scaleFactor': 20, 'currentTarget': array([21.05706948, 12.91950491]), 'dynamicTrap': False, 'previousTarget': array([22.58377529, 12.8534672 ]), 'currentState': array([1.27607044, 9.96788214, 1.81653756]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:433
target Thresh 75.19941311260315
target distance 16.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.70463837, 14.99795694]), 'dynamicTrap': True, 'previousTarget': array([35., 15.]), 'currentState': array([19.      ,  4.      ,  4.480473], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.280785781229248
running average episode reward sum: 0.027453885791276528
{'scaleFactor': 20, 'currentTarget': array([33.53013673, 13.90057945]), 'dynamicTrap': True, 'previousTarget': array([33.67614481, 13.79237012]), 'currentState': array([17.19309535,  3.29528148,  2.0534958 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.47745532555507}
episode index:434
target Thresh 75.20737908523098
target distance 7.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.55107794,  6.60492244,  4.13405836]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.008400412541219}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027390773410147156
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.13969805,  6.25222869,  2.80850732]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.11698006500142}
episode index:435
target Thresh 75.21526579510683
target distance 6.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.69619051,  8.34467892,  3.52087402]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.166859483568986}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02732795053535324
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.19162774,  8.59640236,  3.15841478]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.346656921561614}
episode index:436
target Thresh 75.22307403090822
target distance 15.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.66913039, 22.21538761,  5.40151793]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 15.158294853809021}
done in step count: 99
reward sum = -0.17415726512392074
running average episode reward sum: 0.026866885968627213
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.9238377, 21.4530015,  4.7446186]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 14.581743684383365}
episode index:437
target Thresh 75.23080457346528
target distance 10.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.46826352, 12.51640804,  5.65308613]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.885873977816725}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026805546046324413
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.38372662, 13.0709024 ,  6.21566404]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.829585746832041}
episode index:438
target Thresh 75.23845819583867
target distance 9.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.55553784,  5.36296913,  4.89287997]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.272011157425576}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026744485576970598
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.2918507,  5.5253446,  0.2839433]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.747906161877074}
episode index:439
target Thresh 75.24603566339704
target distance 2.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.07751272, 14.30536418,  0.50148582]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.2820111472966715}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.028888927655204757
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.7220634 , 14.25618356,  0.82498008]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 1.0366476938557445}
episode index:440
target Thresh 75.25353773389344
target distance 6.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.17751248, 10.459685  ,  0.4725337 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.035476237015267}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028823419882743975
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.12466505,  8.76291585,  2.72523522]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.589037883138516}
episode index:441
target Thresh 75.26096515754116
target distance 27.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.99208841, 16.29674285]), 'dynamicTrap': False, 'previousTarget': array([27.5237412 , 16.66139084]), 'currentState': array([ 7.24926359, 19.49375204,  3.24001026]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028758208525543196
{'scaleFactor': 20, 'currentTarget': array([27.58905939, 16.48519548]), 'dynamicTrap': False, 'previousTarget': array([26.03250121, 16.82202779]), 'currentState': array([ 7.97897771, 20.41516959,  5.02661598]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:442
target Thresh 75.2683186770888
target distance 17.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.09973595, 13.63249652]), 'dynamicTrap': False, 'previousTarget': array([34.33935727, 14.53366395]), 'currentState': array([16.86626815,  1.95026544,  2.90639639]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2235818571600499
running average episode reward sum: 0.02818859212444705
{'scaleFactor': 20, 'currentTarget': array([33.36846892, 13.89701698]), 'dynamicTrap': False, 'previousTarget': array([34.53196269, 14.66078397]), 'currentState': array([16.79950327,  2.69570557,  2.27888096]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:443
target Thresh 75.27559902789439
target distance 6.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.82660557, 15.20645497,  1.33230352]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.1763647727064015}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02812510430434694
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.3380016 , 15.67966872,  1.27539432]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.692084827775946}
episode index:444
target Thresh 75.28280693799911
target distance 6.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([38.85327478, 22.46619424,  0.09308004]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.401891627014061}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.028061901822764142
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.63675245, 23.50266809,  0.12674659]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.902124924869312}
episode index:445
target Thresh 75.28994312819995
target distance 8.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.32623752, 19.17510587,  4.81914115]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.872141684197357}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02799898276038126
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.4952657 , 17.31417869,  3.24628783]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.853436182125977}
episode index:446
target Thresh 75.29700831212192
target distance 27.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.48995038, 17.46587094]), 'dynamicTrap': False, 'previousTarget': array([27.35993796, 16.98075682]), 'currentState': array([ 8.48802762, 23.70501601,  0.44463825]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027936345215056023
{'scaleFactor': 20, 'currentTarget': array([27.32330041, 17.02121545]), 'dynamicTrap': False, 'previousTarget': array([28.525428, 16.568152]), 'currentState': array([ 7.98244932, 22.11351152,  1.48306811]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:447
target Thresh 75.30400319628924
target distance 24.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.89637034, 15.43936552]), 'dynamicTrap': False, 'previousTarget': array([30.72787848, 15.71202025]), 'currentState': array([12.09381424, 18.2427156 ,  4.74772126]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02787398730162956
{'scaleFactor': 20, 'currentTarget': array([31.27799268, 15.68369104]), 'dynamicTrap': False, 'previousTarget': array([29.81441885, 15.76974497]), 'currentState': array([11.60710292, 19.29701309,  5.63178152]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:448
target Thresh 75.31092848019622
target distance 7.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.30753404, 20.94040441,  4.59214884]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.082602228411834}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02781190715173729
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.39361235, 20.0404688 ,  2.70744008]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 5.076813147549551}
episode index:449
target Thresh 75.31778485637699
target distance 15.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([20.69478536,  5.56194759,  0.18757164]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.138144566749517}
done in step count: 99
reward sum = -0.03064546514879591
running average episode reward sum: 0.027682001879958326
{'scaleFactor': 20, 'currentTarget': array([33.14880069, 15.70762914]), 'dynamicTrap': True, 'previousTarget': array([33.07302545, 15.52289969]), 'currentState': array([20.69710985,  4.56402189,  1.32700216]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.710014584899877}
episode index:450
target Thresh 75.32457301047486
target distance 25.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.25796533, 14.68213013]), 'dynamicTrap': False, 'previousTarget': array([30., 15.]), 'currentState': array([10.30274787, 13.34448443,  3.88326263]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027620622718361968
{'scaleFactor': 20, 'currentTarget': array([30.30609463, 14.98589149]), 'dynamicTrap': False, 'previousTarget': array([28.70454684, 14.85456851]), 'currentState': array([10.30618498, 14.9257776 ,  5.59550256]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:451
target Thresh 75.33129362131095
target distance 5.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.43420036, 19.38302386,  2.50694418]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.084421204960233}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02755951514597621
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([30.10025962, 19.44151943,  3.92745376]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.61321030926711}
episode index:452
target Thresh 75.33794736095189
target distance 24.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.5246394 , 15.03683927]), 'dynamicTrap': False, 'previousTarget': array([30.98266146, 15.16738911]), 'currentState': array([12.52685388, 15.33445403,  4.86212391]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027498677364197012
{'scaleFactor': 20, 'currentTarget': array([30.52329486, 15.08903622]), 'dynamicTrap': False, 'previousTarget': array([31.53777911, 14.87715007]), 'currentState': array([10.52724932, 15.48673327,  1.22059936]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:453
target Thresh 75.34453489477723
target distance 8.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.625692  , 21.49030039,  3.4593544 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.097021577189556}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027438107590267064
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([27.59865659, 21.85376613,  2.40084058]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.087318500417428}
episode index:454
target Thresh 75.35105688154582
target distance 28.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.66842121, 12.68214374]), 'dynamicTrap': False, 'previousTarget': array([26.04057123, 12.12018361]), 'currentState': array([8.59872642, 6.65331792, 5.7373969 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02737780405710164
{'scaleFactor': 20, 'currentTarget': array([27.56971152, 12.18671713]), 'dynamicTrap': False, 'previousTarget': array([26.94202115, 12.43733672]), 'currentState': array([8.86550519, 5.10486245, 4.3606034 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:455
target Thresh 75.35751397346175
target distance 24.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.65597792, 11.69084702]), 'dynamicTrap': False, 'previousTarget': array([29.18129646, 12.33309421]), 'currentState': array([9.42158535, 3.47458708, 2.4663626 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02731776501311677
{'scaleFactor': 20, 'currentTarget': array([29.10622047, 12.51747554]), 'dynamicTrap': False, 'previousTarget': array([27.62323551, 12.15659392]), 'currentState': array([10.67455676,  4.75385702,  5.22540045]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:456
target Thresh 75.36390681623965
target distance 27.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.43417772, 14.21357845]), 'dynamicTrap': False, 'previousTarget': array([27.78406925, 13.93097322]), 'currentState': array([ 9.63087942, 11.4154761 ,  5.52263505]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02725798872205962
{'scaleFactor': 20, 'currentTarget': array([27.94730314, 13.94606321]), 'dynamicTrap': False, 'previousTarget': array([28.29407877, 13.65499448]), 'currentState': array([ 8.16694659, 10.99013785,  0.94834298]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:457
target Thresh 75.37023604916908
target distance 12.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.38301221,  4.35408045,  0.28513956]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.036866489934344}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02719847346284115
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([29.78515538,  4.71704465,  5.93570108]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.52969102988925}
episode index:458
target Thresh 75.37650230517863
target distance 10.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.50270709,  6.60613625,  0.25746369]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.093294577889116}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02713921752937091
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([31.1668174 ,  5.34414185,  1.68015272]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 10.388882782255045}
episode index:459
target Thresh 75.38270621089912
target distance 32.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.51663673, 14.73698089]), 'dynamicTrap': False, 'previousTarget': array([22.99024152, 15.37530495]), 'currentState': array([ 2.52107452, 14.31568296,  3.5851351 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027080219230394015
{'scaleFactor': 20, 'currentTarget': array([23.43209223, 15.46237057]), 'dynamicTrap': False, 'previousTarget': array([21.8895209 , 15.64287444]), 'currentState': array([ 3.44804921, 16.261135  ,  5.07391686]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:460
target Thresh 75.38884838672628
target distance 8.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.37732797,  8.51942019,  0.32030571]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 6.902869187790364}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.027021476889330254
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.37929521,  8.01781393,  1.10430813]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.117118630012801}
episode index:461
target Thresh 75.39492944688281
target distance 24.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.35925015, 14.89289444]), 'dynamicTrap': False, 'previousTarget': array([30.98266146, 14.83261089]), 'currentState': array([ 9.36285455, 14.51320641,  1.88396001]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026962988844115254
{'scaleFactor': 20, 'currentTarget': array([29.6001638 , 14.85313973]), 'dynamicTrap': False, 'previousTarget': array([31.19800657, 14.95513065]), 'currentState': array([ 9.60755656, 14.30939736,  2.39560127]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:462
target Thresh 75.4009499994798
target distance 33.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.57142237, 16.15935927]), 'dynamicTrap': False, 'previousTarget': array([21.91786413, 16.18928508]), 'currentState': array([ 3.67354347, 18.17787732,  5.37907559]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02690475344704373
{'scaleFactor': 20, 'currentTarget': array([22.02839459, 15.45406388]), 'dynamicTrap': False, 'previousTarget': array([23.6851449 , 15.53799986]), 'currentState': array([ 2.04063645, 16.15372431,  2.35872054]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:463
target Thresh 75.40691064657754
target distance 26.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.04207463, 13.68003258]), 'dynamicTrap': False, 'previousTarget': array([28.64012894, 13.77694787]), 'currentState': array([ 7.31164765, 10.40738055,  1.98864508]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026846769064614757
{'scaleFactor': 20, 'currentTarget': array([26.85842076, 13.60997149]), 'dynamicTrap': False, 'previousTarget': array([28.14676969, 13.5764903 ]), 'currentState': array([ 7.14369337, 10.24403568,  1.68426007]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:464
target Thresh 75.4128119842457
target distance 11.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.49996477,  3.29060867,  2.63763559]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 13.39254655703537}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026789034077379027
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.64944948,  4.12809222,  1.80767155]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 12.590785147093902}
episode index:465
target Thresh 75.41865460262295
target distance 19.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.61532569, 14.47102427]), 'dynamicTrap': True, 'previousTarget': array([33.69836445, 14.31492866]), 'currentState': array([16.       ,  5.       ,  4.9891853], dtype=float32), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2403437131398
running average episode reward sum: 0.02621578783871555
{'scaleFactor': 20, 'currentTarget': array([32.32391132, 12.33372439]), 'dynamicTrap': True, 'previousTarget': array([31.82371485, 13.4177873 ]), 'currentState': array([13.92182244,  4.50026252,  2.42290898]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:466
target Thresh 75.42443908597602
target distance 30.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.150865  , 12.79301142]), 'dynamicTrap': False, 'previousTarget': array([24.61161351, 12.9223227 ]), 'currentState': array([3.4890099 , 9.13084623, 2.04521179]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02615965124805449
{'scaleFactor': 20, 'currentTarget': array([23.27702871, 13.15927361]), 'dynamicTrap': False, 'previousTarget': array([23.90677248, 12.67878673]), 'currentState': array([ 3.51911032, 10.05690981,  1.06402862]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
episode index:467
target Thresh 75.43016601275802
target distance 10.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.70499296,  6.52819272,  0.12856483]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.64167342114124}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026103754557353517
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([37.26399203,  4.17697441,  3.5485105 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.05728460593068}
episode index:468
target Thresh 75.43583595566646
target distance 8.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([28.24347523, 17.86591118,  4.53376249]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 7.339214802741935}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.026048096232071313
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([26.75359557, 18.36867375,  6.01203147]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.907926177759713}
episode index:469
target Thresh 75.44144948170033
target distance 14.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([22.68207426,  3.88664845,  5.21080381]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 16.590294664848862}
done in step count: 99
reward sum = -0.25391108397235884
running average episode reward sum: 0.025452438401849124
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([21.80298811,  2.91640661,  3.5398325 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 17.89341644105712}
episode index:470
target Thresh 75.4470071522169
target distance 8.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.67944019,  6.67159801,  5.12842805]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.334568868430829}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.025398399254499125
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([34.17554568,  5.51715295,  3.73558605]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.518619285908338}
episode index:471
target Thresh 75.45250952298785
target distance 9.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([35.50747557,  5.22233753,  2.61192894]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 9.790822987747088}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.02534458908658705
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.09250123,  6.65903448,  6.22633666]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 8.412209268721925}
episode index:472
target Thresh 75.45795714425488
target distance 27.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.3186323 , 17.64483189]), 'dynamicTrap': False, 'previousTarget': array([27.35993796, 16.98075682]), 'currentState': array([ 7.18679843, 23.47346235,  1.0650959 ]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.025291006445812024
{'scaleFactor': 20, 'currentTarget': array([25.76970627, 17.20290451]), 'dynamicTrap': False, 'previousTarget': array([27.20660658, 16.6941324 ]), 'currentState': array([ 6.31606153, 21.84571687,  1.54091763]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:473
target Thresh 75.46335056078462
target distance 12.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 15.]), 'dynamicTrap': False, 'previousTarget': array([35., 15.]), 'currentState': array([36.47329096,  3.58232557,  1.76720071]), 'targetState': array([35, 15], dtype=int32), 'currentDistance': 11.512335800707817}
Traceback (most recent call last):
  File "/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40/DDPGHER_CNN.py", line 212, in <module>
    agent.train()
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 238, in train
    self.update_net(state, action, nextState, reward, info)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 202, in update_net
    for target_param, param in zip(self.actorNet_target.parameters(), self.actorNet.parameters()):
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 814, in parameters
    for name, param in self.named_parameters(recurse=recurse):
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 840, in named_parameters
    for elem in gen:
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 786, in _named_members
    for k, v in members:
KeyboardInterrupt

Process finished with exit code 1
