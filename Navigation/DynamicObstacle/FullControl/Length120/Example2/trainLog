/home/yangyutu/anaconda3/bin/python /home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length120/DDPGHER_CNN.py
episode index:0
target Thresh 7.599999999999998
target distance 3.0
model initialize at round 0
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52817138,  10.96287361,   2.96193421]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.064604749935107}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59703482,   7.63237363,   3.89890237]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.391777111178848}
episode index:1
target Thresh 7.668365811397145
target distance 5.0
model initialize at round 1
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.33631106,  10.74597799,   4.30310845]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.905786083046686}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.40445145,  13.59329475,   5.82973857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.860931113097836}
episode index:2
target Thresh 7.736663291154414
target distance 6.0
model initialize at round 2
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.52939077,  12.8728818 ,   1.27381819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.811271220474611}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.0
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.72569617,  12.42696489,   0.73287165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.164321704533992}
episode index:3
target Thresh 7.804892507569287
target distance 2.0
model initialize at round 3
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.66686033,  14.43720607,   2.14636306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.725597373511761}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.2150145886603221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05888452,  14.91522793,   5.06477854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9449257419577359}
episode index:4
target Thresh 7.8730535288709795
target distance 7.0
model initialize at round 4
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.85323399,  21.29388972,   0.13846105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.379733058697848}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1720116709282577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.91677767,  21.24680123,   3.79590108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.534260653778965}
episode index:5
target Thresh 7.941146423220525
target distance 5.0
model initialize at round 5
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.58314741,   8.42128939,   6.07622528]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.729554553766623}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14334305910688142
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24086796,   7.75177743,   4.30970534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.28786744515072}
episode index:6
target Thresh 8.009171258710824
target distance 7.0
model initialize at round 6
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.38062131,  22.74962841,   3.53394401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.572573184997996}
done in step count: 99
reward sum = -0.028906476979372174
running average episode reward sum: 0.1187359825231309
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.97215836,  21.30987983,   0.84031224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.21081900831807}
episode index:7
target Thresh 8.077128103366714
target distance 4.0
model initialize at round 7
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.40648259,  15.9242266 ,   1.5913636 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.75327575179571}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10389398470773954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.96329502,  14.20285644,   6.10331345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.099395545114373}
episode index:8
target Thresh 8.14501702514505
target distance 6.0
model initialize at round 8
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.67886243,  21.11751175,   1.07988089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.678342151977531}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.09235020862910182
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11857991,  23.84107029,   0.71993467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.841865473716423}
episode index:9
target Thresh 8.21283809193476
target distance 8.0
model initialize at round 9
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.51433122,  23.37629703,   2.92003524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.923461186072538}
done in step count: 99
reward sum = -1.46203541351602
running average episode reward sum: -0.06308835358541037
{'dynamicTrap': 42, 'scaleFactor': 20, 'currentTarget': array([116.24331348,  16.39677939]), 'previousTarget': array([116.48125718,  15.61431768]), 'currentState': array([105.06230925,  16.69998039,   2.3007849 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.185114496080645}
episode index:10
target Thresh 8.280591371556902
target distance 3.0
model initialize at round 10
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.41264976,  15.84930419,   0.65985697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7231780814590985}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: -0.003986068537874482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05427854,  14.30838338,   1.19429475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.171632468872481}
episode index:11
target Thresh 8.34827693176478
target distance 5.0
model initialize at round 11
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.0950908 ,  16.86561543,   3.7415247 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.152502498298001}
done in step count: 99
reward sum = -1.4885155111255612
running average episode reward sum: -0.12769685542018172
{'dynamicTrap': 27, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.45558957,  15.63205704,   3.70087977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.588154562191538}
episode index:12
target Thresh 8.415894840243947
target distance 6.0
model initialize at round 12
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.92819601,  10.62222638,   2.56920969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.783601331072559}
done in step count: 99
reward sum = 0.0
running average episode reward sum: -0.11787402038786005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.19574048,   7.39629215,   3.63567016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.914395072957151}
episode index:13
target Thresh 8.483445164612329
target distance 7.0
model initialize at round 13
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02170471,   8.83544013,   2.49084824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.164598077752705}
done in step count: 89
reward sum = 0.40882017442254925
running average episode reward sum: -0.08025300647283082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81268148,  15.75788788,   1.68596252]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7806934540999767}
episode index:14
target Thresh 8.550927972420245
target distance 7.0
model initialize at round 14
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.13149097,   7.41186579,   0.78075218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.592662771610923}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: -0.0435303632689321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14154138e+02, 1.55231289e+01, 7.70152112e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9945579951957076}
episode index:15
target Thresh 8.618343331150516
target distance 3.0
model initialize at round 15
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.89470534,  19.80891343,   2.39863586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.612928596440807}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.0078041193978228215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62878361,  15.93925344,   3.26598562]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1302945834202696}
episode index:16
target Thresh 8.685691308218493
target distance 3.0
model initialize at round 16
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.17200281,  17.16836881,   2.33167636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4648354675081836}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.05356141477484611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25491571,  14.34562835,   5.91409478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7022707977704454}
episode index:17
target Thresh 8.75297197097218
target distance 6.0
model initialize at round 17
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.78254281,   8.6040919 ,   5.97997069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.254059600237746}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.09557066218853569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69905759,  14.56605223,   0.86885416]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5280880620138153}
episode index:18
target Thresh 8.820185386692227
target distance 3.0
model initialize at round 18
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62622441,  17.82790629,   3.55912167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.896413467679368}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.13910613756955592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99333302,  15.37650684,   2.61780769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0622936944978654}
episode index:19
target Thresh 8.887331622592066
target distance 8.0
model initialize at round 19
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.00785042,  23.03079962,   0.5530647 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.570105115239006}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.17429799036027446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42386983,  15.78099934,   4.75812179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9705080852912265}
episode index:20
target Thresh 8.954410745817937
target distance 8.0
model initialize at round 20
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.9267314 ,   7.23252106,   0.86605287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.770703855920198}
done in step count: 91
reward sum = 0.40068465295154054
running average episode reward sum: 0.18507830762652522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07902839,  14.92590828,   6.165859  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9239471280590684}
episode index:21
target Thresh 9.021422823448965
target distance 2.0
model initialize at round 21
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.62556618,  14.64762144,   1.2714166 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.6426502384054893}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.20769086614397347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41009557,  14.23473113,   3.7006587 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9662420370110101}
episode index:22
target Thresh 9.08836792249723
target distance 7.0
model initialize at round 22
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.01698784,   7.34042883,   1.84752452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.633273791988211}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.23386638797342063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23292876,  15.99766329,   0.71349736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2584634821666827}
episode index:23
target Thresh 9.155246109907846
target distance 8.0
model initialize at round 23
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.38664213,   8.66499922,   2.30766201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.537213360992488}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.25889336186828177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54644058,  14.65883851,   4.99364746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5675449859409227}
episode index:24
target Thresh 9.222057452559003
target distance 7.0
model initialize at round 24
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.21676357,  21.00195848,   5.52846176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.057361763615138}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.2850783172928961
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12186403,  14.5412712 ,   0.72942692]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9907345276222058}
episode index:25
target Thresh 9.28880201726205
target distance 9.0
model initialize at round 25
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.00356446,   5.94766715,   4.95384789]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.339685589327253}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.3014427369074883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58800523,  14.34304722,   0.91824951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7754525440134086}
episode index:26
target Thresh 9.355479870761538
target distance 8.0
model initialize at round 26
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.00426933,  11.10214966,   0.5190258 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.89522041723745}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.3150549228949769
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14877842e+02, 1.42619738e+01, 1.62875652e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7480676803701729}
episode index:27
target Thresh 9.422091079735361
target distance 1.0
model initialize at round 27
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.22885189,  14.98726231,   5.67047355]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2289179062112388}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.3388065327915849
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7170673 ,  14.90651985,   2.19835446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7231348785875683}
episode index:28
target Thresh 9.488635710794714
target distance 9.0
model initialize at round 28
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.56047234,  22.27555381,   4.71319813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.917984964950136}
done in step count: 99
reward sum = -0.4344163347213568
running average episode reward sum: 0.3121436752911386
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0303813 ,  10.04551354,   0.85201703]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.04847467291115}
episode index:29
target Thresh 9.555113830484231
target distance 6.0
model initialize at round 29
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.13471483,  12.00229365,   5.2902096 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.586942656532035}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.32927784024262025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24677953,  15.11839392,   5.85206423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7624684917294894}
episode index:30
target Thresh 9.621525505282044
target distance 9.0
model initialize at round 30
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.60198584,   5.23488206,   6.06603932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.674421335161327}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.34324767489115193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74427642,  14.26512699,   3.83323046]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0459377223058168}
episode index:31
target Thresh 9.687870801599828
target distance 5.0
model initialize at round 31
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.38259209,  19.96736289,   5.57742185]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.614758970376775}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.35517682054948635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66323653,  15.86116761,   4.27180316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9246725339668007}
episode index:32
target Thresh 9.75414978578288
target distance 6.0
model initialize at round 32
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98012096,  21.47415469,   1.57909822]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.54792456156972}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.368222315102751
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11751751,  15.73848941,   6.26426745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.747781365005088}
episode index:33
target Thresh 9.820362524110209
target distance 9.0
model initialize at round 33
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.68367244,   7.02385146,   0.84730548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.17422917993468}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.3829435944458753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35707296,  15.72717775,   3.96602827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9706403346306037}
episode index:34
target Thresh 9.886509082794547
target distance 7.0
model initialize at round 34
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.20240776,  19.23518439,   0.39837146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.008997880257118}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.39632971377870385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15391792e+02, 1.49698981e+01, 1.00979817e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3929467095848014}
episode index:35
target Thresh 9.952589527982456
target distance 9.0
model initialize at round 35
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03374172,   6.50811193,   1.64747548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.54668462381744}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.41044227936842886
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36275848,  14.09238437,   1.32901646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9774251066187931}
episode index:36
target Thresh 10.018603925754384
target distance 7.0
model initialize at round 36
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.65021856,  21.28929383,   0.39455157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.125745755095847}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.42454020013974136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56147635,  14.981296  ,   4.34058477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.561787800520001}
episode index:37
target Thresh 10.084552342124738
target distance 7.0
model initialize at round 37
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09129782e+02, 1.07082487e+01, 9.06077027e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.27176636850619}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.4351093691843689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08283246,  15.41682794,   2.3022673 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4249785284453262}
episode index:38
target Thresh 10.150434843041952
target distance 8.0
model initialize at round 38
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28510064,  21.21253837,   5.64429092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.219076725803452}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.44785183017725655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5570178 ,  15.10733332,   4.50803226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5672647290864331}
episode index:39
target Thresh 10.216251494388512
target distance 9.0
model initialize at round 39
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.68109086,  19.03369467,   5.16862226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.15444533918912}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.4533798283870671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34876219,  15.58004279,   4.19510642]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6768195458999915}
episode index:40
target Thresh 10.282002361981094
target distance 6.0
model initialize at round 40
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.56907542,  22.57148048,   2.18505061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.995465293274139}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4648277519490391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04445948,  15.89500954,   5.48176402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3092363259924555}
episode index:41
target Thresh 10.347687511570555
target distance 7.0
model initialize at round 41
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.45763719,  21.15176981,   5.60083693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.656416490330444}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.4757305362937744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75644802,  14.99042573,   3.56519341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7565086048255446}
episode index:42
target Thresh 10.41330700884206
target distance 10.0
model initialize at round 42
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88426194,   4.96857337,   2.39914441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.032094271144736}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.480700735121107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.944802  ,  14.84659235,   2.24203458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9571753913677511}
episode index:43
target Thresh 10.4788609194151
target distance 2.0
model initialize at round 43
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.9720854 ,  11.28526278,   4.1239104 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.232222849453614}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.48836451245010987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01224899,  15.65113489,   3.96779183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1830590400146521}
episode index:44
target Thresh 10.544349308843607
target distance 9.0
model initialize at round 44
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.58876363,   5.91655235,   2.33179331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.221344320715483}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.4976093471736364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85314802,  14.4493224 ,   3.87784499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0154345633187714}
episode index:45
target Thresh 10.60977224261596
target distance 8.0
model initialize at round 45
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.04701358,  10.61919596,   2.18871629]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.967317082801113}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5064522325613574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54929556,  15.13252447,   5.81476558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4697842308599685}
episode index:46
target Thresh 10.675129786155114
target distance 3.0
model initialize at round 46
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.84835745,  14.81545822,   3.24660957]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.857547021294324}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5167404829323923
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33707736,  14.35390156,   3.63326353]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7287416148562584}
episode index:47
target Thresh 10.740422004818615
target distance 9.0
model initialize at round 47
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.51500476,   5.7534828 ,   3.13943946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.896353817275596}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.523536372733466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07924864,  14.5645731 ,   0.74411554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4425798553289779}
episode index:48
target Thresh 10.805648963898676
target distance 9.0
model initialize at round 48
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.44611171,  13.6507903 ,   5.79366803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.673434378985371}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5224557659753474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16379501,  14.55480461,   5.70565746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9473318990828459}
episode index:49
target Thresh 10.870810728622267
target distance 8.0
model initialize at round 49
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.1838807 ,   9.45516486,   0.47203481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.583158017371884}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5272535049427824
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41875997,  14.14192171,   5.59413755]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.036406449518867}
episode index:50
target Thresh 10.935907364151157
target distance 8.0
model initialize at round 50
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.619093  ,  21.27838904,   5.36172807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.428458066136966}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5357504168066495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90053756,  14.72477447,   5.52728367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29264632321021034}
episode index:51
target Thresh 11.000938935581997
target distance 6.0
model initialize at round 51
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.69295741,   8.78166418,   2.29711008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.632506760571994}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5424933870933702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61174378,  14.96720222,   2.25174741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38963904142877753}
episode index:52
target Thresh 11.065905507946352
target distance 4.0
model initialize at round 52
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.47802459,  14.87271684,   0.24278879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5242746489201378}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5489819056711581
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27152373,  15.63552645,   5.23097795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6910998478180184}
episode index:53
target Thresh 11.130807146210804
target distance 8.0
model initialize at round 53
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.81968079,  23.58052477,   6.27954239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.771353576277146}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5555633902885219
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3027092 ,  15.63038223,   3.72221408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6992958026853577}
episode index:54
target Thresh 11.195643915276989
target distance 11.0
model initialize at round 54
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.34443903,   4.04974526,   2.27502489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.814233128718024}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5600373393658041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05381711,  14.77507351,   0.22107323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.972550247855013}
episode index:55
target Thresh 11.260415879981709
target distance 5.0
model initialize at round 55
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.99132598,  12.9275041 ,   0.44171405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.4205216212201455}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5671901727699862
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51105741,  15.21114193,   0.23113057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5325840501680383}
episode index:56
target Thresh 11.325123105096894
target distance 4.0
model initialize at round 56
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.64844912,  12.72131001,   2.6514442 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.493810361318567}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5704801572337345
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36423283,  14.81978962,   0.88707461]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6608144011614213}
episode index:57
target Thresh 11.389765655329803
target distance 5.0
model initialize at round 57
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72775841,   9.547528  ,   3.55445433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.500825680186987}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5737881323563788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46364446,  14.48316136,   3.40022942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6943258358157082}
episode index:58
target Thresh 11.454343595322982
target distance 10.0
model initialize at round 58
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.25356074,   5.82545198,   6.14997477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.388010100805689}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5742147064938327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01001005,  14.29784945,   5.17634276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2137114493081858}
episode index:59
target Thresh 11.518856989654381
target distance 6.0
model initialize at round 59
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86376801,  21.68496539,   1.86092606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.74053836515954}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5803357972089522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79660422,  15.9340784 ,   4.95367688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9559666815917269}
episode index:60
target Thresh 11.583305902837385
target distance 4.0
model initialize at round 60
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.07819231,  16.34854123,   4.78486049]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.103210206278846}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5867286365989693
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89574255,  15.88118162,   0.4495348 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.887327824873355}
episode index:61
target Thresh 11.647690399320938
target distance 9.0
model initialize at round 61
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.23863966,  19.4257098 ,   1.14139712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.717791886505038}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5919994206454963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89023932,  15.59107831,   4.80727238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.601182979439832}
episode index:62
target Thresh 11.712010543489521
target distance 10.0
model initialize at round 62
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.08933026,  14.82546092]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.      ,  18.      ,   2.553349], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.576969484303062}
done in step count: 62
reward sum = -0.4433232923037954
running average episode reward sum: 0.5755657267891583
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09889047,  15.14777773,   5.69903363]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1778133354105342}
episode index:63
target Thresh 11.776266399663289
target distance 3.0
model initialize at round 63
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26406158,  17.17809215,   4.27985835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.194040546373991}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5818865748080777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25532225,  15.55551071,   3.65309644]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6113768082304378}
episode index:64
target Thresh 11.840458032098104
target distance 10.0
model initialize at round 64
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.5016498 ,   6.63810639,   2.09554288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.231496805602017}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5853918254759728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34972014,  14.34599193,   1.24261338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9222746035030179}
episode index:65
target Thresh 11.904585504985604
target distance 5.0
model initialize at round 65
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.90713526,  19.35684489,   4.83474297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.755971229499218}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5899523261765812
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24185908,  15.50237261,   1.60098595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9094811151738132}
episode index:66
target Thresh 11.968648882453271
target distance 6.0
model initialize at round 66
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.13788231,  20.79805099,   3.43396115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.179638825204034}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5947816533602687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40554788,  14.40933309,   6.23659259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8380099739900051}
episode index:67
target Thresh 12.032648228564476
target distance 8.0
model initialize at round 67
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.14774574,  21.32352773,   3.79029083]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.970657079644869}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5984310877723813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54436087,  15.03208265,   6.26159031]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4567672421855628}
episode index:68
target Thresh 12.09658360731859
target distance 9.0
model initialize at round 68
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.01106717,  21.02767339,   0.18035793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.82283511096709}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.598794773382799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1892461 ,  15.76289509,   2.90750605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1132523558915979}
episode index:69
target Thresh 12.160455082650982
target distance 1.0
model initialize at round 69
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14304353,  15.02005598,   2.17925082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8571911307858237}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6045262766201875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14304353,  15.02005598,   2.17925082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8571911307858237}
episode index:70
target Thresh 12.224262718433135
target distance 2.0
model initialize at round 70
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.74135706,  15.61859459,   4.61350155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.316837808594068}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6073023937035518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38696012,  14.81310513,   1.47207379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4297299501273993}
episode index:71
target Thresh 12.288006578472697
target distance 2.0
model initialize at round 71
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.90060162,  13.51833802,   1.06447738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8449929637190214}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6108128931610204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98288697,  15.13778812,   3.82437017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9924980415632855}
episode index:72
target Thresh 12.351686726513528
target distance 2.0
model initialize at round 72
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83634279,  15.01579306,   4.46350494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16441746102698482}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6161442233916913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83634279,  15.01579306,   4.46350494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16441746102698482}
episode index:73
target Thresh 12.415303226235778
target distance 5.0
model initialize at round 73
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.91881701,  18.35577346,   4.3119846 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.8656272750468768}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6206691669931549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59102562,  14.09414325,   2.86486068]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0816134860084514}
episode index:74
target Thresh 12.478856141255957
target distance 5.0
model initialize at round 74
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.89889575,  20.01857511,   5.51174897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.36580853366769}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6252015248999129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77349509,  15.88995978,   5.10715998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.179119612904957}
episode index:75
target Thresh 12.542345535126984
target distance 9.0
model initialize at round 75
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.39837353,   4.77934472,   3.75022888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.315873347531408}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6252623420881145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18015498,  14.68578077,   3.59827611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8779974801331337}
episode index:76
target Thresh 12.605771471338254
target distance 7.0
model initialize at round 76
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.77770638,   8.9146132 ,   0.4253533 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.885862955972957}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6283116409524415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6103344 ,  14.22006172,   0.92740396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8718618040449808}
episode index:77
target Thresh 12.669134013315718
target distance 4.0
model initialize at round 77
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.08360299,  12.34267973,   2.51861084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8697641799320968}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6328217481197179
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23772277,  14.60998088,   1.76941181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45675707683589417}
episode index:78
target Thresh 12.732433224421909
target distance 10.0
model initialize at round 78
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80833396,   5.94536903,   0.76082706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.056659308067113}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6334512778271948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67245767,  14.97919573,   6.20988479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32820236508749173}
episode index:79
target Thresh 12.795669167956067
target distance 12.0
model initialize at round 79
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.5666769 ,   3.45500825,   5.94971711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.297124643711948}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6366129477508065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74646408,  14.1093808 ,   3.39023569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9260037945674198}
episode index:80
target Thresh 12.858841907154112
target distance 8.0
model initialize at round 80
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.17858501,  23.12799015,   5.82003391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.997654542771228}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6362981859209553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16858686,  15.3294319 ,   5.87415797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8943003900241063}
episode index:81
target Thresh 12.921951505188812
target distance 10.0
model initialize at round 81
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.68196438,  11.05817133,   1.04457157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.204766688045083}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6377423456926953
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85076319,  14.75781228,   3.03682664]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8845636715232019}
episode index:82
target Thresh 12.984998025169745
target distance 5.0
model initialize at round 82
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.85287725,  10.69571789,   0.12814772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.3320939700259355}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.640631245419277
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15264774e+02, 1.46166459e+01, 2.80162692e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46590300999793766}
episode index:83
target Thresh 13.047981530143469
target distance 9.0
model initialize at round 83
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.92945037,   8.80448564,   5.74930418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.400907945154952}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6408101498854064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27994616,  15.84412985,   3.58726536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1095191508026492}
episode index:84
target Thresh 13.110902083093473
target distance 9.0
model initialize at round 84
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.83492573,   6.15656405,   1.37264729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.392766073819356}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6418865407062181
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46582464,  14.16781118,   6.07149522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9536932625436034}
episode index:85
target Thresh 13.173759746940313
target distance 2.0
model initialize at round 85
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.68060271,  16.38197694,   4.99138117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.595588777030847}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6344227437212621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.84215424,  13.97275147,   2.64876526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.109211178035948}
episode index:86
target Thresh 13.236554584541679
target distance 6.0
model initialize at round 86
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.13083695e+02, 9.06165126e+00, 9.97127295e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.239888529494278}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.637116112331006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97101801,  15.09624399,   2.01365876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9757760359884232}
episode index:87
target Thresh 13.299286658692381
target distance 11.0
model initialize at round 87
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41092535,   5.92783548,   1.65861285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.081466216632068}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.6357891169937253
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53070339,  15.50205733,   4.84967282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6872414945259094}
episode index:88
target Thresh 13.361956032124523
target distance 11.0
model initialize at round 88
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.44891292,   4.63364781,   0.45637542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.824982404303471}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6377434849850458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66780677,  15.41447249,   6.09074047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5311683225816122}
episode index:89
target Thresh 13.424562767507487
target distance 5.0
model initialize at round 89
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.84469723,  14.22278923,   5.47107482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.2273629711946406}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.638473531762985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4034782 ,  14.72885642,   0.850779  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6552534647192336}
episode index:90
target Thresh 13.487106927447993
target distance 5.0
model initialize at round 90
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.82575448,  19.59570171,   5.05482003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.979704977180369}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6419077792150402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64977934,  14.97507742,   3.67343894]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35110631430585365}
episode index:91
target Thresh 13.549588574490219
target distance 6.0
model initialize at round 91
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.71003836,  15.80544679,   1.04294538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.341321788431959}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6434705005366943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72981974,  14.00203483,   1.96372974]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2363540483702329}
episode index:92
target Thresh 13.61200777111582
target distance 12.0
model initialize at round 92
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.28028373,  14.85271428]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.        ,  22.        ,   0.26892945], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.208767065628319}
done in step count: 36
reward sum = 0.15562607904501413
running average episode reward sum: 0.6382248615959236
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41035995,  14.93304114,   6.13431474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4157869379029767}
episode index:93
target Thresh 13.674364579743983
target distance 3.0
model initialize at round 93
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.33840606,  17.67871447,   4.53088737]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5557916278670065}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6418618311534138
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84411096,  15.43895607,   4.81679189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9514230137118627}
episode index:94
target Thresh 13.73665906273154
target distance 10.0
model initialize at round 94
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.28515571,  13.3919638 ,   5.86723614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.84702899765107}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6445300040282064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29847213,  14.15597101,   6.02161056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8952488812285978}
episode index:95
target Thresh 13.798891282372969
target distance 10.0
model initialize at round 95
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.23906432,   3.74671759,   6.01450086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.39022648476639}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6467750910137593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63390281,  15.75995272,   4.90872463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8435373655974174}
episode index:96
target Thresh 13.861061300900497
target distance 6.0
model initialize at round 96
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.874455  ,  10.2200173 ,   0.70916098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.314139382938127}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.6453648915832005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55363252,  15.91654403,   4.86512103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0707763212139498}
episode index:97
target Thresh 13.923169180484143
target distance 3.0
model initialize at round 97
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.62627818,  15.73415074,   2.83613789]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7269606495397674}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6488815763629638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67731312,  15.60435305,   3.58756179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9077420723669456}
episode index:98
target Thresh 13.985214983231804
target distance 8.0
model initialize at round 98
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.63917837,  22.78505564,   5.3055259 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.220240491637549}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6520302070057622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99226891,  15.61128233,   4.9060694 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.165445700138809}
episode index:99
target Thresh 14.04719877118928
target distance 13.0
model initialize at round 99
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.14192823,  18.71962706,   0.36607075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.385276805649866}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6536889743116768
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77630579,  14.24901837,   1.58336687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0801037391533344}
episode index:100
target Thresh 14.10912060634037
target distance 13.0
model initialize at round 100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76552453,   3.00003495,   6.19063551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.02435815520775}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6537740541797195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.60401   ,  15.69598635,   3.06343177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9215340898356945}
episode index:101
target Thresh 14.170980550606894
target distance 12.0
model initialize at round 101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.90820284,   3.18325628,   1.02518153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.867083215282062}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6554641970194828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40532765,  14.30592791,   5.23864967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9139864685417092}
episode index:102
target Thresh 14.23277866584883
target distance 5.0
model initialize at round 102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.74170896,  15.9691023 ,   0.65245238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.399355792385115}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6566521306348193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50008514,  15.286493  ,   4.79139031]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5763361762023}
episode index:103
target Thresh 14.29451501386427
target distance 5.0
model initialize at round 103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.26300146,  19.97823305,   5.41671426]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.272567519421643}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6580461542781292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71942562,  14.20862474,   5.89036036]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.069508304371617}
episode index:104
target Thresh 14.356189656389585
target distance 7.0
model initialize at round 104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38513912,   8.26321867,   1.72710371]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.764782081369962}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6603922106660403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49904899,  14.54657872,   0.15615183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6742705331727437}
episode index:105
target Thresh 14.417802655099432
target distance 6.0
model initialize at round 105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.14183341,  17.84259654,   0.17261618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.5114108261622174}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6604731497972067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26828031,  14.70926183,   1.42404687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7873642017544924}
episode index:106
target Thresh 14.479354071606794
target distance 1.0
model initialize at round 106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15466656,  14.07790246,   0.98113721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9349789359757391}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6636462979299431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15466656,  14.07790246,   0.98113721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9349789359757391}
episode index:107
target Thresh 14.540843967463106
target distance 7.0
model initialize at round 107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82242381,   8.98734986,   0.52095199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.015271818502357}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.66562661945836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72572059,  14.43941878,   1.98173867]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9170178185676707}
episode index:108
target Thresh 14.602272404158253
target distance 6.0
model initialize at round 108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.1442025 ,  13.83265686,   1.29194164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.9710178692524565}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6668008182123535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34802724,  14.16176852,   1.1634976 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.061932437295234}
episode index:109
target Thresh 14.66363944312069
target distance 7.0
model initialize at round 109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4604535 ,  21.14012106,   4.61118913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.157361772972001}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6669440343650629
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65440052,  15.9722874 ,   5.40817508]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.171999504423836}
episode index:110
target Thresh 14.72494514571747
target distance 3.0
model initialize at round 110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.79773592,  12.69868514,   1.23647094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1852813317343127}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6681574267540177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87945922,  14.15713965,   0.43483913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8514362247700845}
episode index:111
target Thresh 14.786189573254275
target distance 6.0
model initialize at round 111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.92576202,   9.27248657,   6.01154262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.091541126403041}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6697940369713467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93864019,  15.95670006,   5.45770401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9586657593517649}
episode index:112
target Thresh 14.847372786975566
target distance 10.0
model initialize at round 112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.54129159,   5.53908036,   6.05653674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.585644488833044}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6693845799617879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61781428,  14.80978247,   4.12485933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4269058853076573}
episode index:113
target Thresh 14.908494848064539
target distance 3.0
model initialize at round 113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.95917627,  16.61923285,   6.20417455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.924897154963973}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6715260945891726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3510516 ,  15.00175572,   6.24466452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6489507713042557}
episode index:114
target Thresh 14.969555817643272
target distance 6.0
model initialize at round 114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91450449,  20.61400946,   3.84992278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.68800673519235}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6740397460275276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86703052,  15.62270364,   4.55312937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0674744677210748}
episode index:115
target Thresh 15.030555756772735
target distance 13.0
model initialize at round 115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.02529125,  22.1624713 ,   5.56796241]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.820393458654218}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6743545174175686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54812333,  14.06269763,   6.26126234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0405422841903875}
episode index:116
target Thresh 15.091494726452868
target distance 13.0
model initialize at round 116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.02106957,   2.59406828,   0.52534461]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.95438044146225}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6732673560925532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51033648,  14.29906351,   2.05275283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8670382238345231}
episode index:117
target Thresh 15.152372787622664
target distance 5.0
model initialize at round 117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.50962976,  16.35019788,   6.11144489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.688972084507398}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6734635074650702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26145057,  14.07929969,   2.12727383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9571026402580874}
episode index:118
target Thresh 15.21319000116017
target distance 14.0
model initialize at round 118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.6476533 ,   5.65703252,   0.80477541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.48778584238861}
done in step count: 99
reward sum = -2.5516507310890892
running average episode reward sum: 0.6463617071410858
{'dynamicTrap': 47, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.45293381,  14.28381196,   1.70040417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6198586243029667}
episode index:119
target Thresh 15.273946427882608
target distance 8.0
model initialize at round 119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.66270931,   8.04457393,   5.3770004 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.110083406526266}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6470778876620301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37552495,  14.67096269,   6.02271846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7058573743433177}
episode index:120
target Thresh 15.334642128546424
target distance 12.0
model initialize at round 120
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.80855085,  15.14845353]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,   6.       ,   5.0127225], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.937739964827944}
done in step count: 99
reward sum = -4.376961404193564
running average episode reward sum: 0.605556901778926
{'dynamicTrap': 99, 'scaleFactor': 20, 'currentTarget': array([115.04173575,  11.57204505]), 'previousTarget': array([114.1977989 ,  13.02812152]), 'currentState': array([99.53959511,  3.77677517,  5.61832021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.351731813911464}
episode index:121
target Thresh 15.395277163847313
target distance 2.0
model initialize at round 121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.04326097,  13.06000854,   1.94808405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8175312326635176}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6005933206168037
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.23876366,  10.76673916,   6.06777611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.585024622294032}
episode index:122
target Thresh 15.455851594420306
target distance 3.0
model initialize at round 122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.78547975,  13.42791679,   0.5579021 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9865812499496456}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6037592285792687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03636888,  14.82604492,   2.24545264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9792064667354515}
episode index:123
target Thresh 15.516365480839845
target distance 12.0
model initialize at round 123
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.47427175,  14.37257532]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  17.       ,   3.2409432], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.728050105864012}
done in step count: 99
reward sum = -4.300552681087392
running average episode reward sum: 0.5642083260819568
{'dynamicTrap': 98, 'scaleFactor': 20, 'currentTarget': array([115.22799464,  19.37704559]), 'previousTarget': array([116.45410017,  17.95770535]), 'currentState': array([99.86664524, 20.7888456 ,  1.47240674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.426089421951565}
episode index:124
target Thresh 15.576818883619838
target distance 9.0
model initialize at round 124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05975846e+02, 6.03401936e+00, 5.66121896e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.721012621216534}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5663040084639859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12664367,  14.56371332,   3.08930826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9762670451211045}
episode index:125
target Thresh 15.637211863213668
target distance 6.0
model initialize at round 125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12846013,   9.19417356,   1.0923466 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.807247422689668}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5690596690911259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09517692,  15.58450836,   3.6952967 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5922066064556658}
episode index:126
target Thresh 15.697544480014342
target distance 7.0
model initialize at round 126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.3296041 ,  22.1181497 ,   5.61737639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.100628809326768}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5710190964021976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34159029,  14.81388155,   0.11475717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6842100733073363}
episode index:127
target Thresh 15.757816794354465
target distance 12.0
model initialize at round 127
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.81043347,  14.89748289]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  17.       ,   1.4643282], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.99612090892068}
done in step count: 99
reward sum = -4.437773611087392
running average episode reward sum: 0.5318879033749352
{'dynamicTrap': 100, 'scaleFactor': 20, 'currentTarget': array([119.61207414,  18.95773457]), 'previousTarget': array([120.59197236,  20.20459388]), 'currentState': array([103.       ,  17.       ,   1.4643282], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.727035961517203}
episode index:128
target Thresh 15.818028866506365
target distance 3.0
model initialize at round 128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06278128,  12.13075339,   1.58463407]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.0184358597061682}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5350630370650598
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63343821,  14.18902338,   1.83420461]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.029041803703827}
episode index:129
target Thresh 15.878180756682111
target distance 10.0
model initialize at round 129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.15441983,  19.96048531,   5.02266962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.024602637314235}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5366371704213921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17522934,  14.93116735,   1.23044851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8276379529181253}
episode index:130
target Thresh 15.938272525033607
target distance 7.0
model initialize at round 130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.75809337,  20.16386041,   4.90033925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.682755883035733}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5391060344230707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79665008,  14.4759232 ,   4.11288059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9535763384799659}
episode index:131
target Thresh 15.998304231652623
target distance 11.0
model initialize at round 131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.62544334,  22.8906038 ,   5.44623539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.034302985139966}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5396515738557206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46347647,  14.49619421,   0.59533495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7359876166142812}
episode index:132
target Thresh 16.05827593657087
target distance 12.0
model initialize at round 132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.47707806,   4.52549009,   1.51672006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.326020080029828}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.540674712767033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0057936 ,  14.17079393,   4.46213576]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2946154124847276}
episode index:133
target Thresh 16.11818769976006
target distance 3.0
model initialize at round 133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.02311245,  11.83121591,   1.20283425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7348730862997264}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5422157380025896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39464814,  14.19615044,   3.46845122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0062926950964186}
episode index:134
target Thresh 16.178039581131948
target distance 2.0
model initialize at round 134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.92455655,  15.02493479,   1.81238556]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.92471807641308}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5453148511284963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88821804,  15.84083346,   4.8516121 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2230830687365448}
episode index:135
target Thresh 16.237831640538445
target distance 10.0
model initialize at round 135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.37040149,   5.46601705,   6.17240483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.859502374119172}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5471405822499313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14858608e+02, 1.55728719e+01, 5.25450667e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5900625862269308}
episode index:136
target Thresh 16.297563937771603
target distance 8.0
model initialize at round 136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.61758318,  15.53544349,   0.73033493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.4048375660346775}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5491772832834032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30891765,  15.7357735 ,   6.21689677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.797992958212814}
episode index:137
target Thresh 16.357236532563718
target distance 9.0
model initialize at round 137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.03981959,  22.06254824,   0.32828319]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.408962296646811}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5518842935090881
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02967498,  14.45945795,   4.26558072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1107278509867207}
episode index:138
target Thresh 16.416849484587395
target distance 6.0
model initialize at round 138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.13413811,   9.67517723,   5.87366998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.922251779293929}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5543551853130423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35717409,  15.17323631,   4.03934254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3969687021970381}
episode index:139
target Thresh 16.47640285345559
target distance 9.0
model initialize at round 139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.68552564,  16.19038927,   5.71758742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.399256548858537}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5559513722708002
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20802151,  14.56581287,   1.25088591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9031879063838576}
episode index:140
target Thresh 16.535896698721682
target distance 5.0
model initialize at round 140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.98119328,  16.01241447,   6.04944453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1840504149171527}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5586856189171137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39576299,  14.41986733,   5.66171274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8376492606898449}
episode index:141
target Thresh 16.595331079879518
target distance 5.0
model initialize at round 141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.95723778,  11.4050858 ,   5.91429639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.409929242384046}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5606874328218096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16958296,  15.25456642,   0.97182108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8685600244678154}
episode index:142
target Thresh 16.65470605636348
target distance 9.0
model initialize at round 142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.23332433,  22.2491842 ,   5.12149867]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.624119849190025}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5606705006940197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14569872e+02, 1.40506177e+01, 3.31150256e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.042274987586776}
episode index:143
target Thresh 16.714021687548552
target distance 8.0
model initialize at round 143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.85374311,  22.94595647,   4.35964894]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.546177890461614}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5632496315774431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6133829 ,  15.93193893,   4.37661023]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0089514130756114}
episode index:144
target Thresh 16.773278032750383
target distance 9.0
model initialize at round 144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.44461565,  10.60214129,   5.40010128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.742138975856871}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5653565017925573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35412792,  15.45480233,   1.03059165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7899341107610047}
episode index:145
target Thresh 16.832475151225292
target distance 14.0
model initialize at round 145
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.51832478,  14.20186053]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  15.       ,   1.4841058], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.5437427348169}
done in step count: 19
reward sum = 0.6182616238355867
running average episode reward sum: 0.5657188656421671
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05366273,  15.51481577,   1.40157342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0773065964353303}
episode index:146
target Thresh 16.89161310217043
target distance 12.0
model initialize at round 146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34230659,   3.48001694,   1.96132344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.53874214237546}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5674344307575077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35665376,  14.30810342,   1.50227379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.778410417879409}
episode index:147
target Thresh 16.950691944723754
target distance 8.0
model initialize at round 147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.32230852,  21.73875534,   5.16285264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.127688295666749}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5698351757823078
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02373362,  15.72777322,   5.76863253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.217682183720984}
episode index:148
target Thresh 17.009711737964082
target distance 7.0
model initialize at round 148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.02967722,  16.99664878,   5.16073937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.250655553687914}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5711788534334679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18018145,  15.00649359,   5.52666986]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8198442662320209}
episode index:149
target Thresh 17.06867254091125
target distance 12.0
model initialize at round 149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.34642572,   4.08552104,   0.25187993]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.03902889125093}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5728237073278929
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82283879,  14.12303816,   4.07887125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2025496833566023}
episode index:150
target Thresh 17.127574412526048
target distance 3.0
model initialize at round 150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.89507449,  12.05516655,   2.08784235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1452987809698576}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5734604493890968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37469032,  14.97681688,   1.10802361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6257392830727975}
episode index:151
target Thresh 17.18641741171034
target distance 9.0
model initialize at round 151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.61558739,  21.36152051,   3.47194147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.337378148279667}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5756375653471211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97911029,  15.04034913,   1.15209955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.04543602234988556}
episode index:152
target Thresh 17.245201597307155
target distance 6.0
model initialize at round 152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09924047e+02, 1.55933996e+01, 2.04397996e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.110520417124419}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5782170518481203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17018568,  15.83530253,   0.34339982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8524631839964248}
episode index:153
target Thresh 17.303927028100663
target distance 6.0
model initialize at round 153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.5573347 ,  14.63804131,   1.39884108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.4573858972460965}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5805759031309313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36110554,  14.31215247,   0.67078922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9387866375640441}
episode index:154
target Thresh 17.362593762816303
target distance 7.0
model initialize at round 154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47817844,   9.56425489,   2.28412676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.4607346261024245}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5826649752075627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17385072,  14.28096378,   1.47931022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.739754793258989}
episode index:155
target Thresh 17.42120186012084
target distance 10.0
model initialize at round 155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.74827891,   5.13605479,   0.13537329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.11769059817393}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5826929996096776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53388207,  14.42096547,   0.53963091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7875982771068543}
episode index:156
target Thresh 17.479751378622332
target distance 7.0
model initialize at round 156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.72534299,   7.57055024,   3.59730101]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.5380020091431845}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5847419746122198
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06825338,  15.5565065 ,   5.84714434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5606763860841143}
episode index:157
target Thresh 17.538242376870336
target distance 5.0
model initialize at round 157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6652946 ,  10.84108621,   1.03192824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.211790693282314}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5871207976210032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55444889,  14.82251991,   1.68494838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5821621369841978}
episode index:158
target Thresh 17.596674913355855
target distance 6.0
model initialize at round 158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.3826877 ,  19.43433286,   3.32682276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.9656845249071955}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5888373860299357
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20421034,  15.85340705,   4.01282107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1668696439935848}
episode index:159
target Thresh 17.655049046511404
target distance 3.0
model initialize at round 159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.2304821 ,  14.72068067,   5.27758729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.2425350977805945}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5904787634365916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18450374,  14.50280161,   1.4917365 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9551127640526672}
episode index:160
target Thresh 17.71336483471114
target distance 5.0
model initialize at round 160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.00604029,  18.59439308,   6.26468599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.6779756748996055}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5920997510617982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34711154,  14.23131659,   0.59738651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.008532360406584}
episode index:161
target Thresh 17.771622336270866
target distance 11.0
model initialize at round 161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.41553354,   3.34334884,   5.54503161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.14674383171994}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5935961338419729
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4099177 ,  15.92082787,   5.79667904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0936732096559023}
episode index:162
target Thresh 17.829821609448068
target distance 13.0
model initialize at round 162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.71211038,   3.65604674,   1.95380646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.572368577471979}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5951781070766532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59006717,  15.14903279,   4.65024351]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4361830963289161}
episode index:163
target Thresh 17.887962712442025
target distance 11.0
model initialize at round 163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.16547467e+02, 5.08587647e+00, 2.79593468e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.034166615473666}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5968462028430698
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41615485,  14.86438741,   0.90006387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5993879679460735}
episode index:164
target Thresh 17.946045703393857
target distance 9.0
model initialize at round 164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.91607612,  15.55952784,   5.9228471 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.103264567365434}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5976671553691991
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30960982,  15.59424417,   4.21708796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.67006295783073}
episode index:165
target Thresh 18.00407064038655
target distance 13.0
model initialize at round 165
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.51813712,  14.20220899]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,  19.      ,   5.787056], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.477430880902862}
done in step count: 16
reward sum = 0.7121577710948755
running average episode reward sum: 0.5983568578735706
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04452388,  15.79243868,   0.97763493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2413273874506667}
episode index:166
target Thresh 18.06203758144506
target distance 11.0
model initialize at round 166
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.39345196,  16.56986865]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  12.       ,   4.0910892], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.353745777777235}
done in step count: 71
reward sum = 0.4198902730042049
running average episode reward sum: 0.5972881956887242
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72907112,  14.37190179,   0.12441144]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9623159879593105}
episode index:167
target Thresh 18.119946584536326
target distance 6.0
model initialize at round 167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.55381037,  20.32059916,   1.20817256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.901755914062329}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5985527175490368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43503489,  15.68627639,   3.85570702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8889099294990781}
episode index:168
target Thresh 18.177797707569344
target distance 7.0
model initialize at round 168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.59736442,  21.13325303,   0.90555274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.587066905381346}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5998995572312058
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39528533,  15.09203243,   4.5876602 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4058576839284456}
episode index:169
target Thresh 18.23559100839525
target distance 8.0
model initialize at round 169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.48792824,   8.61068747,   0.26665211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.565795888046111}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.601637431919603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90497603,  15.54879921,   0.8473071 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5569651039414906}
episode index:170
target Thresh 18.293326544807353
target distance 13.0
model initialize at round 170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.84640538,   3.46731468,   2.03905825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.123878216409599}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6029021658709341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01383302,  15.61624433,   0.2024884 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1628767721743911}
episode index:171
target Thresh 18.351004374541194
target distance 10.0
model initialize at round 171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.93575899,   5.69893615,   6.10256321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.987311256826658}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6042002266730542
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6264408 ,  15.3398113 ,   2.21003041]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7126708885852121}
episode index:172
target Thresh 18.408624555274606
target distance 11.0
model initialize at round 172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.44593897,  18.48500213,   4.31092489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.068204756201192}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6057293919106028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09575004,  14.22732546,   0.52374857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7785846283162666}
episode index:173
target Thresh 18.466187144627767
target distance 18.0
model initialize at round 173
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.98712766,  14.74432607]), 'previousTarget': array([114.88854382,  14.94427191]), 'currentState': array([97.       ,  6.       ,  2.8609903], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -3.081545074568956
running average episode reward sum: 0.5845381593446284
{'dynamicTrap': 72, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([113.87010892,  14.81176424]), 'currentState': array([95.71184639, 10.70990997,  0.30651948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.75949751808278}
episode index:174
target Thresh 18.52369220016329
target distance 9.0
model initialize at round 174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.42426199,   8.65276412,   6.02747059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.66914643566754}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5859189048560053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34488621,  14.27792285,   0.39422175]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.974971534971395}
episode index:175
target Thresh 18.58113977938622
target distance 6.0
model initialize at round 175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0354627 ,   8.84051973,   4.95677695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.23454324354348}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5877283546864189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36876936,  14.99367719,   5.75950643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3688235590740453}
episode index:176
target Thresh 18.63852993974414
target distance 10.0
model initialize at round 176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.77057784,   5.03883025,   2.64971048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.471583948823072}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.5873476324715255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30582406,  14.35519004,   0.91999365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7136583516146063}
episode index:177
target Thresh 18.695862738627223
target distance 7.0
model initialize at round 177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.83193013,  20.69955709,   5.03366226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.867942890544899}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5874811695898476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0597174 ,  14.3186446 ,   0.65800451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1611961657599146}
episode index:178
target Thresh 18.753138233368258
target distance 3.0
model initialize at round 178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.93190448,  15.74339362,   3.6607455 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.0246814647691878}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5894062208653623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89198179,  15.78008391,   2.6596086 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1849735917529467}
episode index:179
target Thresh 18.810356481242767
target distance 9.0
model initialize at round 179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06164322,   6.00989055,   0.59620648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.990320784677207}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5903246267894638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76366249,  14.18904037,   3.63345724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8446957645120651}
episode index:180
target Thresh 18.867517539468995
target distance 6.0
model initialize at round 180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22416504,  21.17311309,   4.80918694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.177181814957133}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5920097849522774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35262932,  15.79597414,   4.41362911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8705873086394971}
episode index:181
target Thresh 18.924621465207995
target distance 7.0
model initialize at round 181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.81195906,  21.08061377,   4.68106258]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.195587550650269}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.593342224383584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07677517,  14.10062428,   2.6595105 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2888835406910615}
episode index:182
target Thresh 18.981668315563702
target distance 14.0
model initialize at round 182
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.40370611,  14.46686408]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  16.      ,   5.615521], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.498097098153446}
done in step count: 16
reward sum = 0.7121577710948755
running average episode reward sum: 0.5939914896661593
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30816847,  15.26355358,   3.91602002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7403319218201477}
episode index:183
target Thresh 19.038658147582975
target distance 6.0
model initialize at round 183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10131302,  10.56883047,   0.45090902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.432327575522329}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5960366391788432
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92399002,  14.76023554,   1.11165261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9545913047442879}
episode index:184
target Thresh 19.095591018255647
target distance 4.0
model initialize at round 184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98463497,  11.1533621 ,   1.12527466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.560555677240009}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5973712692015735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65021211,  14.28985734,   1.27536404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7916149120673345}
episode index:185
target Thresh 19.152466984514604
target distance 12.0
model initialize at round 185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.09098971,   3.99029336,   1.52375513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.683064743133158}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5976144967977194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9417041 ,  14.35776987,   1.66081801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6448704920789886}
episode index:186
target Thresh 19.209286103235808
target distance 10.0
model initialize at round 186
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.01529061,  14.72254929]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.       ,   7.       ,   3.7774444], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.646889478931664}
done in step count: 15
reward sum = 0.7900583546412885
running average episode reward sum: 0.5986436083369898
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.96376569,  15.21139321,   4.75914433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9866769399682965}
episode index:187
target Thresh 19.266048431238367
target distance 12.0
model initialize at round 187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.06785360e+02, 4.48848842e+00, 7.52956867e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.340621658921254}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5983697414968503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41348503,  14.21312102,   2.89393783]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9814164935159223}
episode index:188
target Thresh 19.32275402528464
target distance 2.0
model initialize at round 188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.45886539,  15.86561109,   3.7822926 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.684520010492297}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5996191807558621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17754174,  15.88475343,   1.53846182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.207984364117689}
episode index:189
target Thresh 19.379402942080215
target distance 11.0
model initialize at round 189
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.13264656,  16.07417789]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.      ,  18.      ,   4.401661], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.314034955371561}
done in step count: 47
reward sum = 0.5535253948912
running average episode reward sum: 0.5993765818828901
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29032933,  15.27433302,   1.13822112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.76084890849937}
episode index:190
target Thresh 19.43599523827402
target distance 6.0
model initialize at round 190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.65478196,  12.20000088,   2.21440327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.935545183026266}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6006963786850471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32146784,  15.10945466,   6.12468818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6873035826351868}
episode index:191
target Thresh 19.49253097045834
target distance 11.0
model initialize at round 191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.64792704,   4.01191091,   2.70048559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.802000746269043}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6018707132952061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42723426,  15.7062129 ,   3.65595484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8253882616877383}
episode index:192
target Thresh 19.549010195168925
target distance 6.0
model initialize at round 192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.57819383,   8.8731215 ,   6.27413625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.647234284136855}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.603630347679174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70603004,  14.36163288,   1.37011617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7028021875511083}
episode index:193
target Thresh 19.605432968885005
target distance 13.0
model initialize at round 193
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.71291167,  15.72778681]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,   2.      ,   5.053979], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.045620810421195}
done in step count: 44
reward sum = 0.4347046020847181
running average episode reward sum: 0.6027595964132232
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06484541,  15.22935762,   1.12543312]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9628702019099825}
episode index:194
target Thresh 19.66179934802934
target distance 9.0
model initialize at round 194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.40444038,  18.68694545,   5.07991815]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.443109184740502}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6025027958016804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51045442,  14.68834208,   4.93207949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5803322621412915}
episode index:195
target Thresh 19.718109388968344
target distance 5.0
model initialize at round 195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.61939533,  16.09587813,   4.09114563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.49107052591514}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6029463993224324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00569889,  14.21443703,   3.11258491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2671795022060444}
episode index:196
target Thresh 19.774363148012043
target distance 6.0
model initialize at round 196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.09601356,   8.97591326,   2.22983479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.7731027602054805}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6042515361514622
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7658365 ,  14.6171275 ,   3.61594364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.856210782994673}
episode index:197
target Thresh 19.83056068141419
target distance 3.0
model initialize at round 197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.48596796,  15.42056078,   4.20073777]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.539109004744125}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6050888675133496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93717721,  14.18608935,   0.26297291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8163315780145225}
episode index:198
target Thresh 19.886702045372356
target distance 12.0
model initialize at round 198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.01505151,   2.99605475,   0.71611589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.36950358978603}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6056549060313643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52367559,  14.68879512,   5.19529971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6091671318962637}
episode index:199
target Thresh 19.94278729602789
target distance 4.0
model initialize at round 199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27844615,  17.14970852,   4.54773557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.267572858333546}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6075766315012074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02690217,  15.48395721,   4.57758707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0867998772124048}
episode index:200
target Thresh 19.998816489466044
target distance 8.0
model initialize at round 200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.31102784,  21.02901592,   4.74986866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.60148500753441}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6091446318142756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07409916,  14.98871835,   0.32927239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9259695675018816}
episode index:201
target Thresh 20.054789681716013
target distance 4.0
model initialize at round 201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.77931771,  13.83353709,   4.45661283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.378903367371383}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6106971073717689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14100315e+02, 1.49754395e+01, 1.03061940e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9000205140157038}
episode index:202
target Thresh 20.110706928751018
target distance 20.0
model initialize at round 202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.3991848 ,  14.68091937]), 'previousTarget': array([112.88854382,  13.94427191]), 'currentState': array([96.73562265,  5.30016394,  5.92871076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6118411767609914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02787554,  14.10868632,   1.64321052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3188881790127678}
episode index:203
target Thresh 20.166568286488282
target distance 11.0
model initialize at round 203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41165451,   4.14805556,   1.35525435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.859749419923693}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6128918015015531
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99667398,  14.9294444 ,   6.25944644]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07063395051446554}
episode index:204
target Thresh 20.222373810789176
target distance 19.0
model initialize at round 204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.8563128 ,  15.02995981]), 'previousTarget': array([115.,  15.]), 'currentState': array([95.27738411, 19.11230608,  3.22442579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6126806278477755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76581092,  14.19501992,   1.40635216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8383540155419404}
episode index:205
target Thresh 20.278123557459246
target distance 8.0
model initialize at round 205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.87092587,  22.9961389 ,   4.18149757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.07545947020612}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6127638463106662
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.371487  ,  15.49511674,   0.49094182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8001057341287517}
episode index:206
target Thresh 20.333817582248223
target distance 2.0
model initialize at round 206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.19627416,  15.11913206,   3.20947337]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2021914677222583}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6145862431883924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88977391,  15.19990588,   2.55178058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9119539345866149}
episode index:207
target Thresh 20.38945594085014
target distance 2.0
model initialize at round 207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.30221207,  15.44052566,   3.27817786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7335183504563156}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6147209804907786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48583058,  14.54042965,   5.60900954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6896195293911548}
episode index:208
target Thresh 20.44503868890337
target distance 7.0
model initialize at round 208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.55035865,  11.6609782 ,   4.15740931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.163713789963625}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6157326917029549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29583387,  15.44321087,   4.18534488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5328729268439096}
episode index:209
target Thresh 20.50056588199066
target distance 8.0
model initialize at round 209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.0224965 ,  15.91534441,   0.82053471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.029845421646753}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6162529185803591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53875234,  15.40203817,   2.12868627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6722267314972835}
episode index:210
target Thresh 20.556037575639206
target distance 12.0
model initialize at round 210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.39692601,  22.01117184,   4.17973113]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.556838004318118}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6164396783054481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15818281,  14.67976801,   2.10319764]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3571698888497818}
episode index:211
target Thresh 20.611453825320698
target distance 6.0
model initialize at round 211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.63974991,  19.6562824 ,   4.14673805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.379086660693178}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6169862051514338
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16672606,  14.94940897,   1.47486572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.17423269626948884}
episode index:212
target Thresh 20.666814686451414
target distance 4.0
model initialize at round 212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.80085111,  11.52543545,   5.85383422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.723044827275234}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6171065121792896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03925815,  14.68072674,   0.28064581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0124032338319602}
episode index:213
target Thresh 20.72212021439219
target distance 5.0
model initialize at round 213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.70170058,  19.79868639,   3.04921955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.442264279167468}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6182418011627568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1784136 ,  14.77341671,   6.18931619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8522582912790614}
episode index:214
target Thresh 20.777370464448587
target distance 13.0
model initialize at round 214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.37586841,   2.38047451,   5.82990349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.870612227762203}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6190574871277843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3623928 ,  14.62806352,   2.53219593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5192930663606157}
episode index:215
target Thresh 20.832565491870838
target distance 20.0
model initialize at round 215
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.97605462,  14.97838726]), 'previousTarget': array([114.97504678,  14.99875234]), 'currentState': array([95.      , 14.      ,  3.505849], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = -0.3384168317137895
running average episode reward sum: 0.6146247356516659
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32848897,  15.2259963 ,   5.3683344 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7085205639500511}
episode index:216
target Thresh 20.887705351853988
target distance 13.0
model initialize at round 216
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.96314915,  16.32564264]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  19.       ,   4.3861504], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.258430769381107}
done in step count: 42
reward sum = 0.4477522205741436
running average episode reward sum: 0.6138557378863315
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34255055,  14.80276663,   5.92127365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6863969572613787}
episode index:217
target Thresh 20.9427900995379
target distance 8.0
model initialize at round 217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.75240807,  12.36865347,   5.34865522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.657179514032793}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.613575131185763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42310788,  15.67011213,   5.72782293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7925090213210456}
episode index:218
target Thresh 20.997819790007323
target distance 8.0
model initialize at round 218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.59644776,   9.15549625,   0.32660216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.66970042564404}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6149447298903196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2963266 ,  14.03047534,   1.13876284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1979709163899228}
episode index:219
target Thresh 21.052794478291947
target distance 7.0
model initialize at round 219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.30132353,  13.06721374,   5.96006197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.017522435557328}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6161382584953589
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48443556,  14.75957169,   0.92243854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5688694659787814}
episode index:220
target Thresh 21.107714219366468
target distance 6.0
model initialize at round 220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.31964356,  11.98829167,   0.93118757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.565619778328361}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6177407957872351
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51177724,  14.38630793,   5.87467766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7842062353324666}
episode index:221
target Thresh 21.162579068150645
target distance 4.0
model initialize at round 221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.12967539,  11.64446286,   6.26438362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.841580879115993}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6192851886440492
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21629884,  14.1897771 ,   1.61451334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1272305244391303}
episode index:222
target Thresh 21.217389079509314
target distance 19.0
model initialize at round 222
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.33953019,  14.93083928]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 17.       ,  3.9306464], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.449906272189583}
done in step count: 33
reward sum = 0.4584306390036747
running average episode reward sum: 0.618563867793644
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3317272 ,  15.65236315,   0.90566383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9338983948725817}
episode index:223
target Thresh 21.2721443082525
target distance 21.0
model initialize at round 223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.50758363,  14.88471613]), 'previousTarget': array([113.90990945,  14.89618185]), 'currentState': array([92.52894355, 13.96062701,  3.44211388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.619138011662117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22485493,  15.38527648,   2.10343915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8656141395941114}
episode index:224
target Thresh 21.326844809135427
target distance 7.0
model initialize at round 224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.59172327,  22.28586303,   6.22914571]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.043640371714883}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6199850776912686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28772209,  15.48536043,   0.77637342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8619249193879729}
episode index:225
target Thresh 21.381490636858604
target distance 6.0
model initialize at round 225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49686376,   9.8584214 ,   2.23650116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.16613749130382}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6215351392944047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3150747 ,  15.08855974,   2.26153603]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6906268819478988}
episode index:226
target Thresh 21.436081846067882
target distance 6.0
model initialize at round 226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.7165045 ,  20.03385024,   3.42538428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.051208853617984}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6229864825129316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17023823,  15.26820861,   4.70322406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8720323705344354}
episode index:227
target Thresh 21.490618491354446
target distance 2.0
model initialize at round 227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.45652336,  14.76589597,   2.06188309]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4752169366095758}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6245961909229627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73812814,  15.31900232,   3.61402011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41272187920859665}
episode index:228
target Thresh 21.54510062725496
target distance 5.0
model initialize at round 228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.13106844,  13.75994894,   5.86479253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.062801743190128}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.625857854925411
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15630049,  14.64869487,   1.64047671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9139169349126116}
episode index:229
target Thresh 21.59952830825158
target distance 4.0
model initialize at round 229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98718182,  14.94918525,   5.46938193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.013139905399418}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6265872307024468
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23496921,  14.93535885,   1.34764117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7677568521068193}
episode index:230
target Thresh 21.653901588771944
target distance 7.0
model initialize at round 230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.9967065 ,  17.99486731,   0.40701175]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.616780822273159}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6268593599455925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05466254,  15.24286978,   1.75386151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9760372135289465}
episode index:231
target Thresh 21.708220523189397
target distance 7.0
model initialize at round 231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.89931412,  12.54725964,   1.55666798]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.575279721567043}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6276475862743669
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49940026,  15.70444349,   0.88874147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.864199478753626}
episode index:232
target Thresh 21.762485165822838
target distance 15.0
model initialize at round 232
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.99576901,  15.20960848]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   3.       ,   3.8682623], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.337725497128112}
done in step count: 47
reward sum = -0.8211746196032116
running average episode reward sum: 0.6214294652190984
{'dynamicTrap': 23, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51199411,  15.37116364,   0.7140218 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6131167890206715}
episode index:233
target Thresh 21.816695570936915
target distance 8.0
model initialize at round 233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.08872091,  11.85408224,   1.57280195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.513820259656491}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6219990798429639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06525096,  15.90524844,   2.87979327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3012419048089119}
episode index:234
target Thresh 21.870851792742062
target distance 11.0
model initialize at round 234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.81587543,   3.95605635,   1.24428004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.107242785608175}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.623239582683988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63990277,  14.15609894,   2.83263823]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9175178575742922}
episode index:235
target Thresh 21.924953885394487
target distance 4.0
model initialize at round 235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.34414156,  12.78643963,   5.97650546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4573737938379407}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6245880596616025
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95175828,  14.37824456,   2.74194223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6236241563074666}
episode index:236
target Thresh 21.979001902996295
target distance 8.0
model initialize at round 236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.62451239,  22.23004285,   5.04137808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.97918769265049}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.62558160521004
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22342212,  14.2879829 ,   1.02030492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.053585092759462}
episode index:237
target Thresh 22.03299589959549
target distance 10.0
model initialize at round 237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.42967539,   5.2932951 ,   0.3779878 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.832905946532795}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6264244078093071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27069061,  14.60580185,   1.90334108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8290261566966456}
episode index:238
target Thresh 22.086935929186104
target distance 5.0
model initialize at round 238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.98236409,  19.69991026,   1.45182842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.58527375366215}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6271913678947127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68630451,  15.43459637,   0.89810711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5359840125443901}
episode index:239
target Thresh 22.14082204570814
target distance 10.0
model initialize at round 239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.75252945,   6.01052396,   1.77503534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.40896856720331}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6275988219283092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14963097,  14.624206  ,   0.11478291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9297035083148144}
episode index:240
target Thresh 22.19465430304774
target distance 8.0
model initialize at round 240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.51169642,  21.83918827,   5.27314905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.141458804626081}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6282221519593085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76073668,  15.98339139,   5.67054823]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0120798233860047}
episode index:241
target Thresh 22.24843275503716
target distance 6.0
model initialize at round 241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.24702337,  19.83809415,   5.85844441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.782178253959422}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6293259375060003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89946166,  15.426735  ,   3.58568402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43841842363768657}
episode index:242
target Thresh 22.302157455454857
target distance 4.0
model initialize at round 242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.68847749,  17.26556459,   4.75092784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.236652470699924}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.630135989713118
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17606009,  15.218341  ,   5.83556941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.852378886194025}
episode index:243
target Thresh 22.355828458025535
target distance 1.0
model initialize at round 243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.86167331,  15.3172672 ,   4.22544265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1817132146208573}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6308388364337161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43353287,  15.84952871,   0.88286924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9537556169282524}
episode index:244
target Thresh 22.409445816420206
target distance 3.0
model initialize at round 244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.23556177,  17.86443488,   2.05865622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.321324701441368}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6310780619416155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85898493,  14.43127399,   2.93941303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0301962838963208}
episode index:245
target Thresh 22.46300958425622
target distance 7.0
model initialize at round 245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.074283  ,   9.4306445 ,   3.11929047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.943094363960851}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6316745794109552
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9824427 ,  15.11376065,   4.54574266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11510753419529283}
episode index:246
target Thresh 22.516519815097357
target distance 9.0
model initialize at round 246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.76994937,   6.80490541,   4.65034497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.343152349679878}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6322980756109401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01423247,  15.50516927,   5.02308383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5053697218928699}
episode index:247
target Thresh 22.569976562453842
target distance 8.0
model initialize at round 247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.7118085 ,  23.39516362,   3.01650965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.797156039327733}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6334692313319763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48225734,  15.19036055,   5.93031693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5516290404969254}
episode index:248
target Thresh 22.623379879782455
target distance 13.0
model initialize at round 248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.81859455,   3.67663899,   2.68276745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.452528538156631}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6345572347202367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04201887,  14.95975522,   1.34701025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9588260956562574}
episode index:249
target Thresh 22.676729820486486
target distance 9.0
model initialize at round 249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.136161  ,   5.89847918,   5.93888044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.34884297775204}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6356730747712903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25355851,  14.35998387,   2.0804522 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9832576222771118}
episode index:250
target Thresh 22.730026437915907
target distance 6.0
model initialize at round 250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.97048918,   9.14542593,   4.34043956]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.565047108746005}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6363023226154033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14201288e+02, 1.51805361e+01, 5.43144494e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8188619456601982}
episode index:251
target Thresh 22.783269785367324
target distance 9.0
model initialize at round 251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.67717046,  12.08294657,   1.05941599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.882450958605023}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6375510834379613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72059659,  15.74192412,   0.39471578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7927910607559588}
episode index:252
target Thresh 22.836459916084088
target distance 12.0
model initialize at round 252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.76253148,  21.27291059,   5.60906554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.006463633885476}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6382316240892786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47103575,  14.70707097,   4.52809517]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5546909846458321}
episode index:253
target Thresh 22.889596883256335
target distance 5.0
model initialize at round 253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.08254345,  14.71140085,   0.13791817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9280727191436777}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6395775625771161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52972009,  15.12786229,   0.91114285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48735198706863153}
episode index:254
target Thresh 22.942680740021036
target distance 2.0
model initialize at round 254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.47269929,  12.72200615,   3.67658535]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.198941090644864}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6406518358512593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15166283,  14.87541972,   5.82728563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8574358291245838}
episode index:255
target Thresh 22.995711539462064
target distance 10.0
model initialize at round 255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.44012276,   5.99153574,   5.99132914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.586059855492314}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6414752965358047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68497298,  15.21564087,   0.37716221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3817630246113282}
episode index:256
target Thresh 23.048689334610206
target distance 4.0
model initialize at round 256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.571117  ,  19.3348662 ,   0.64473678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.616399345046757}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6426796340975331
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03480769,  15.54549757,   3.78220493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5466069677364195}
episode index:257
target Thresh 23.10161417844328
target distance 9.0
model initialize at round 257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.69115712,  14.79102703,   5.1860472 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.311829724679551}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6432034392343612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7705083 ,  15.05623086,   3.2368698 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7725574084024613}
episode index:258
target Thresh 23.154486123886116
target distance 12.0
model initialize at round 258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98343195,   3.2790375 ,   0.62217748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.390067827870697}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6438151270733754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23216968,  15.92472596,   0.68383169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2019490393335444}
episode index:259
target Thresh 23.207305223810682
target distance 4.0
model initialize at round 259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.36885122,  18.70512737,   4.33043718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9499015816827225}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6451085304307855
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00978918,  15.33308045,   4.7064362 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.33322427491655615}
episode index:260
target Thresh 23.26007153103607
target distance 8.0
model initialize at round 260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.47984757,  22.27783334,   4.59509158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.084449961530817}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.645677517990988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01848376,  14.59906241,   5.01245651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0602476519778963}
episode index:261
target Thresh 23.31278509832859
target distance 15.0
model initialize at round 261
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.49345685,  16.07954867]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.0000000e+02, 1.1000000e+01, 4.2236812e-02], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.417877528323528}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.646098742175216
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69286449,  14.50135172,   5.793876  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5856469286669161}
episode index:262
target Thresh 23.36544597840181
target distance 7.0
model initialize at round 262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.28436543,   8.84116522,   0.19461745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.402364209524059}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6472218653966069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00411631,  15.52872199,   6.09843105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5287380161525864}
episode index:263
target Thresh 23.418054223916638
target distance 12.0
model initialize at round 263
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.51291299,  14.21181133]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,   7.       ,   1.4772384], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.748786699669685}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.6479308055845319
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35857135,  15.32699831,   6.12760502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7199712487531643}
episode index:264
target Thresh 23.4706098874813
target distance 4.0
model initialize at round 264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.68816177,  11.16450258,   3.99885464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.770874228674758}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6486348922104397
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56248212,  15.60734118,   4.60671397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8277979475741085}
episode index:265
target Thresh 23.52311302165147
target distance 6.0
model initialize at round 265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.89982202,  18.42338323,   5.55037189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9951214528682035}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6495963477848696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86576505,  14.79234105,   5.7858136 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24726759053244962}
episode index:266
target Thresh 23.575563678930283
target distance 14.0
model initialize at round 266
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.4476617 ,  15.65010299]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.        ,  15.        ,   0.59340143], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.464626581275914}
done in step count: 20
reward sum = 0.6786069375972308
running average episode reward sum: 0.649705001679298
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73037138,  14.93297053,   0.6274237 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27783545562401535}
episode index:267
target Thresh 23.627961911768423
target distance 19.0
model initialize at round 267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.22815291,  15.14504218]), 'previousTarget': array([115.,  15.]), 'currentState': array([94.57219075, 18.83870588,  3.42999268]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.44845018843359596
running average episode reward sum: 0.648954050883605
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2747947 ,  14.43984917,   4.54498751]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6239239401690815}
episode index:268
target Thresh 23.680307772564106
target distance 11.0
model initialize at round 268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.65333139,   5.55098233,   2.18211715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.400839532544357}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6498037422297588
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81249316,  15.51613951,   5.19304167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5491437009460868}
episode index:269
target Thresh 23.73260131366319
target distance 8.0
model initialize at round 269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.88294415,   8.58847569,   0.58014512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.508107082673962}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.650919247072982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11690055,  14.79425521,   0.72049063]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9067499935584583}
episode index:270
target Thresh 23.784842587359247
target distance 15.0
model initialize at round 270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.4125824 ,  21.32642129,   4.03353214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.900199952372425}
done in step count: 16
reward sum = 0.7842160503948755
running average episode reward sum: 0.651411117195941
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63142794,  15.53229625,   4.74126115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8258574531671906}
episode index:271
target Thresh 23.83703164589353
target distance 16.0
model initialize at round 271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.61731013, 20.22431382,  0.8637495 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.195522158308663}
done in step count: 11
reward sum = 0.7588103242587163
running average episode reward sum: 0.6518059672219071
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32200412,  15.66902897,   5.9386538 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7424866427005871}
episode index:272
target Thresh 23.889168541455106
target distance 10.0
model initialize at round 272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.59299385,   5.80832921,   1.19464111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.298735301052302}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6528670448123067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57320422,  14.51246639,   1.35446155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7524972416663501}
episode index:273
target Thresh 23.941253326180895
target distance 5.0
model initialize at round 273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.00480531,  11.55477323,   2.83487141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.588763184840414}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6540255555976633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68777625,  15.06008308,   2.37493396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3179522712418708}
episode index:274
target Thresh 23.993286052155668
target distance 20.0
model initialize at round 274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.77626518,  13.3504542 ]), 'previousTarget': array([111.76887233,  12.89976701]), 'currentState': array([96.71319179,  1.43501576,  0.46769708]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = -0.965868440583268
running average episode reward sum: 0.6481350319751872
{'dynamicTrap': 27, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76987126,  14.61097091,   1.94377194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8625806512815751}
episode index:275
target Thresh 24.04526677141216
target distance 6.0
model initialize at round 275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.82659972,  20.27611342,   4.57127213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.405019987997426}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6492671369680306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14073317,  15.7383495 ,   4.99072042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7516420769833064}
episode index:276
target Thresh 24.097195535931093
target distance 22.0
model initialize at round 276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.10700436,  14.99637279]), 'previousTarget': array([113.,  15.]), 'currentState': array([93.10704107, 14.95805044,  3.60992539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.07288876905549457
running average episode reward sum: 0.6471863486362164
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49582148,  14.79107917,   0.26865942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5380398258290972}
episode index:277
target Thresh 24.149072397641234
target distance 12.0
model initialize at round 277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.32254539,   4.57539504,   1.0216161 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.558704585878974}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6482110932379098
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2039423 ,  14.97554678,   0.95032963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7964331891866369}
episode index:278
target Thresh 24.200897408419447
target distance 22.0
model initialize at round 278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.42398359,  15.74879264]), 'previousTarget': array([111.79586847,  16.16513874]), 'currentState': array([93.21889043, 21.33130074,  3.83285952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.648261526742376
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07692614,  15.63568616,   6.15574418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1207864401405223}
episode index:279
target Thresh 24.25267062009076
target distance 9.0
model initialize at round 279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.47614754,  12.79585511,   6.03706264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.804221453122365}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6492751118179639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0785551 ,  15.75029482,   0.77919226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1882773281018497}
episode index:280
target Thresh 24.30439208442838
target distance 5.0
model initialize at round 280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.28727173,  20.1570539 ,   3.93470287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.6415261154800715}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6500873748470778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23791336,  15.39130195,   5.79236734]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.856675701807388}
episode index:281
target Thresh 24.356061853153772
target distance 3.0
model initialize at round 281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.78574321,  14.07981903,   5.98810375]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.933785655546258}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6511884692979747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49080152,  15.41439337,   2.85591328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6423457015407508}
episode index:282
target Thresh 24.407679977936713
target distance 20.0
model initialize at round 282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.40285  ,  15.1492875]), 'currentState': array([96.65965805, 19.29054966,  0.30611199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.835523860409054}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6518067737309698
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33930452,  14.60485152,   2.5759789 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7698446833462785}
episode index:283
target Thresh 24.45924651039533
target distance 14.0
model initialize at round 283
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.66621163,  15.23691111]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   7.       ,   2.2928314], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.69962970193308}
done in step count: 44
reward sum = 0.09408123489333065
running average episode reward sum: 0.649842951411119
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29390509,  15.51131064,   1.66853946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8717847139389749}
episode index:284
target Thresh 24.510761502096177
target distance 23.0
model initialize at round 284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.97187008,  15.10211107]), 'previousTarget': array([111.98112317,  15.13125551]), 'currentState': array([91.98323134, 15.7761447 ,  0.48602748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = -0.0843894824637722
running average episode reward sum: 0.647266697257172
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19400403,  15.42629981,   5.62253187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9117900175942997}
episode index:285
target Thresh 24.56222500455422
target distance 15.0
model initialize at round 285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.73137919, 20.59857675,  0.72727847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.26268253042573}
done in step count: 12
reward sum = 0.8184639417161292
running average episode reward sum: 0.6478652890210145
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05864865,  14.81908586,   5.50885807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9585782688204436}
episode index:286
target Thresh 24.613637069232993
target distance 14.0
model initialize at round 286
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.51703914,  15.39438773]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  17.       ,   0.7300704], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.619598233054989}
done in step count: 10
reward sum = 0.7650820750088044
running average episode reward sum: 0.6482737098781147
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89856844,  15.72567171,   6.19930856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.732726269688212}
episode index:287
target Thresh 24.66499774754455
target distance 18.0
model initialize at round 287
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.53591659,  15.29057973]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 13.       ,  2.3385725], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.66974307263898}
done in step count: 41
reward sum = 0.23857309293293244
running average episode reward sum: 0.6468511382914995
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15069344,  15.58383212,   4.91083486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0306219341640994}
episode index:288
target Thresh 24.716307090849575
target distance 23.0
model initialize at round 288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.88777047,  15.72540512]), 'previousTarget': array([111.54352728,  15.75140711]), 'currentState': array([92.40986414, 20.26535723,  5.96560079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = -0.48084059692665726
running average episode reward sum: 0.6429490907647931
{'dynamicTrap': 20, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68854179,  15.49724026,   6.05004092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5867317039134258}
episode index:289
target Thresh 24.76756515045742
target distance 22.0
model initialize at round 289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.80043452,  15.36595384]), 'previousTarget': array([112.29527642,  15.73765188]), 'currentState': array([94.67081989, 21.20186373,  1.13023442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6437885244922115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88735237,  15.15503104,   5.26165271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19163535773975152}
episode index:290
target Thresh 24.818771977626135
target distance 11.0
model initialize at round 290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36476579,   4.88521528,   3.30837321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.12135978912887}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.644591728954434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27921488,  15.8197001 ,   1.50021891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.091530779261711}
episode index:291
target Thresh 24.869927623562575
target distance 12.0
model initialize at round 291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.16016611,   4.36470631,   0.53149796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.48902353335237}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.645185274189512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03301177,  15.54928025,   4.62386915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1121038756318544}
episode index:292
target Thresh 24.92103213942237
target distance 24.0
model initialize at round 292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.67703019,  13.88341233]), 'previousTarget': array([109.97366596,  13.32455532]), 'currentState': array([92.7187058 ,  7.51301702,  1.09760981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6454083730054943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32463857,  14.90351422,   4.79957997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3386734516802727}
episode index:293
target Thresh 24.972085576310047
target distance 9.0
model initialize at round 293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.40385206,   7.69555975,   6.02262783]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.280452420716689}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6461680241611524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18307246,  15.98702366,   5.03883353]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0038581715360675}
episode index:294
target Thresh 25.023087985279055
target distance 6.0
model initialize at round 294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.66588339,   9.02814223,   2.55715108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.705407422984345}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6471690822128129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50519777,  15.27068551,   0.34824854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5640034529643796}
episode index:295
target Thresh 25.074039417331797
target distance 22.0
model initialize at round 295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.01897984,  14.58585321]), 'previousTarget': array([112.91786413,  14.81071492]), 'currentState': array([91.12633234, 12.51641479,  2.94618237]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.1779772138262813
running average episode reward sum: 0.6455839745493448
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36938157,  14.36570483,   4.68077558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8944327640160689}
episode index:296
target Thresh 25.124939923419713
target distance 22.0
model initialize at round 296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.49058291,  14.36606008]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([91.80911521, 10.81080637,  2.91628027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.46189558004149683
running average episode reward sum: 0.6449654951065574
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64287105,  15.18496829,   6.10684326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40218697042939916}
episode index:297
target Thresh 25.175789554443313
target distance 13.0
model initialize at round 297
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.32642226,  14.18462573]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  10.       ,   1.9542722], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.925061720939256}
done in step count: 42
reward sum = 0.3798312905741436
running average episode reward sum: 0.6440757830108111
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48251099,  14.84709766,   6.16238395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.506158054401417}
episode index:298
target Thresh 25.226588361252233
target distance 12.0
model initialize at round 298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.47000749,   3.72005123,   1.46547716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.03372726757646}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6449161257240148
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15041559e+02, 1.54767939e+01, 9.40209289e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47860169787790363}
episode index:299
target Thresh 25.277336394645282
target distance 25.0
model initialize at round 299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.089282 ,  15.2007672]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([89.10080935, 15.87970835,  3.02537888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.15445883958100537
running average episode reward sum: 0.643281268103538
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15209497,  15.95912509,   4.88957274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2801811919744979}
episode index:300
target Thresh 25.3280337053705
target distance 22.0
model initialize at round 300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.75435214,  15.35916388]), 'previousTarget': array([112.50265712,  15.56757793]), 'currentState': array([94.53723291, 20.90013206,  1.36171919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.5509304090507414
running average episode reward sum: 0.6429744546183128
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02111013,  14.63325721,   3.900958  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.045335182858947}
episode index:301
target Thresh 25.3786803441252
target distance 13.0
model initialize at round 301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.41972785,   3.50800981,   2.92927629]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.60013355903821}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6437510988844739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14092333e+02, 1.55763837e+01, 1.13314062e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0752101784561003}
episode index:302
target Thresh 25.429276361556017
target distance 10.0
model initialize at round 302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.2616346 ,   6.29523141,   0.39297462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.334181210137615}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6445814195292734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49323119,  15.29786546,   6.1060458 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5878251984090145}
episode index:303
target Thresh 25.47982180825899
target distance 7.0
model initialize at round 303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.58532607,   8.33538097,   1.45151949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.488467456485439}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6455580600880619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80419301,  14.89960887,   1.58580637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22004262775223915}
episode index:304
target Thresh 25.530316734779568
target distance 11.0
model initialize at round 304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.11813528,  12.25784004,   6.11731476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.255276277688223}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6463770115443591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03220242,  15.67586865,   1.03593832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1804366088952623}
episode index:305
target Thresh 25.580761191612655
target distance 7.0
model initialize at round 305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.91406299,  14.24285821,   6.20985622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.132853574590327}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6474038710164363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14155998e+02, 1.47856883e+01, 5.79999387e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8707859979587783}
episode index:306
target Thresh 25.631155229202754
target distance 11.0
model initialize at round 306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51603597,   5.69839589,   2.58991802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.315907474938697}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6483617742033567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80304174,  14.54932424,   1.51641339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49183452691718843}
episode index:307
target Thresh 25.681498897943868
target distance 11.0
model initialize at round 307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.65938515,   4.31946872,   1.15119487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.764339110804269}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6492526278404495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61668541,  15.40619859,   5.95458362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7384430891313756}
episode index:308
target Thresh 25.731792248179698
target distance 18.0
model initialize at round 308
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.41021251,  14.4477647 ]), 'previousTarget': array([114.48314552,  14.71285862]), 'currentState': array([97.     ,  5.     ,  5.62751], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.935557359250268}
done in step count: 20
reward sum = 0.47818605975074147
running average episode reward sum: 0.648699014351486
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28934728,  15.11928921,   0.79567888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7205950339120045}
episode index:309
target Thresh 25.782035330203588
target distance 2.0
model initialize at round 309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.21621783,  13.87388042,   2.33824486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4859136657592282}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6493256084773971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99285453,  15.36966676,   4.80755958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0594402400460612}
episode index:310
target Thresh 25.83222819425862
target distance 4.0
model initialize at round 310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.56985686,  15.2348855 ,   0.57639331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.441468180772602}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.650421024527309
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13263772,  15.55233067,   6.09999985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.028293000676672}
episode index:311
target Thresh 25.882370890537658
target distance 8.0
model initialize at round 311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.67998131,   6.93345948,   2.92898667]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.588433329048291}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6510929390469051
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12381324,  15.19790468,   4.8866518 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8982591497530825}
episode index:312
target Thresh 25.932463469183414
target distance 6.0
model initialize at round 312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.10315701,  19.54053226,   4.62765932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.6726211741250845}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6514729716563563
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67794771,  15.21876531,   2.30094366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7123702367941845}
episode index:313
target Thresh 25.98250598028847
target distance 13.0
model initialize at round 313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.51423287,   3.2302028 ,   0.93687034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.158744326426586}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6521928699090398
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02508942,  15.09741405,   0.82081893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9797653491439723}
episode index:314
target Thresh 26.03249847389533
target distance 12.0
model initialize at round 314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.27375917,   3.64101537,   1.47624176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.201092664885756}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6529934705601502
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93945855,  14.22355616,   0.22878429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.77880055132034}
episode index:315
target Thresh 26.08244099999651
target distance 3.0
model initialize at round 315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.4783385 ,  16.28101853,   4.59299731]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9561424234506375}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.654059946919137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63104838,  14.55241209,   3.91349971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7736646571129535}
episode index:316
target Thresh 26.132333608534537
target distance 26.0
model initialize at round 316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.76206076,  16.173592  ]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([89.10689134, 19.8714718 ,  1.57980955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = -0.12724393055963262
running average episode reward sum: 0.6515952659176267
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41309316,  14.1490784 ,   4.28083538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0336958977146955}
episode index:317
target Thresh 26.182176349402003
target distance 5.0
model initialize at round 317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.5510669 ,  10.75392512,   1.27218788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.486486263661291}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.652566966370716
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14112805e+02, 1.54265255e+01, 3.01672032e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9843981243429549}
episode index:318
target Thresh 26.231969272441667
target distance 6.0
model initialize at round 318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.05990429,  19.37305813,   5.5134725 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.886254465101376}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6532174095941348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00073323,  15.60132685,   1.71633318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.166245283672527}
episode index:319
target Thresh 26.281712427446465
target distance 4.0
model initialize at round 319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.0041862 ,  16.77731312,   6.02662942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4833521562923226}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6540596823592404
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48171339,  14.83326404,   0.97961375]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5444464062821236}
episode index:320
target Thresh 26.331405864159542
target distance 18.0
model initialize at round 320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.54455919,  14.72930903]), 'previousTarget': array([114.88854382,  14.94427191]), 'currentState': array([97.35198988,  4.51091578,  4.06384683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = -0.04825980722685741
running average episode reward sum: 0.6518717711767292
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07241621,  15.56862897,   4.35639643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.573221612757972}
episode index:321
target Thresh 26.381049632274348
target distance 4.0
model initialize at round 321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.02692759,  18.0883649 ,   4.90208888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.6941079088243023}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6526843347677445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89714031,  14.57873922,   5.87624413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43363666761618985}
episode index:322
target Thresh 26.430643781434643
target distance 7.0
model initialize at round 322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.83029682,   9.58185211,   0.84140718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.542971409132906}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6536676619046864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82050562,  14.34519015,   2.05049026]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6789655112967875}
episode index:323
target Thresh 26.480188361234596
target distance 7.0
model initialize at round 323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96340586,  20.46252921,   4.79451632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.462651787525371}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6545559720512799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45604474,  15.43600871,   5.64521808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.697130486455145}
episode index:324
target Thresh 26.529683421218785
target distance 7.0
model initialize at round 324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.62602977,  16.28161437,   0.42329079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.524680210790787}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6554976337065067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79323437,  14.62353109,   3.85433215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4295123555299585}
episode index:325
target Thresh 26.57912901088226
target distance 2.0
model initialize at round 325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08037621,  14.45535337,   1.7007373 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0688067494980922}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6565543894313334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08037621,  14.45535337,   1.7007373 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0688067494980922}
episode index:326
target Thresh 26.628525179670632
target distance 9.0
model initialize at round 326
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.61995271,  14.34113644]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.       ,  21.       ,   3.0206172], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.53490563803769}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.6570982049835581
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28187869,  15.10461106,   4.11012814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7257008246205152}
episode index:327
target Thresh 26.67787197698008
target distance 20.0
model initialize at round 327
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.82450764,  12.07121421]), 'previousTarget': array([112.14985851,  13.28991511]), 'currentState': array([95.        ,  3.        ,  0.91529167], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.12525675954369636
running average episode reward sum: 0.6554767371620951
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03240097,  15.99070337,   5.60507663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.384825277519283}
episode index:328
target Thresh 26.727169452157387
target distance 23.0
model initialize at round 328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.00374454,  14.00268965]), 'previousTarget': array([111.35234545,  14.04843794]), 'currentState': array([93.02734156,  7.68635133,  0.10290449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = -0.10999817557759689
running average episode reward sum: 0.6531500656948013
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46650107,  15.44424552,   5.1526603 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6441873427467859}
episode index:329
target Thresh 26.776417654500044
target distance 7.0
model initialize at round 329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.08737452e+02, 1.73636395e+01, 6.50463104e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.693750779408762}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6537251963847683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78376094,  14.87813775,   6.12717952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2482130910823399}
episode index:330
target Thresh 26.825616633256253
target distance 7.0
model initialize at round 330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.06064321,  20.71869309,   4.81468582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.944204963670573}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6546232775132131
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73368566,  15.51997883,   4.94206733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5842099866086156}
episode index:331
target Thresh 26.874766437624988
target distance 24.0
model initialize at round 331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.49412291,  13.12028815]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([92.03587747,  5.42008196,  1.81806111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = -0.09296231072294503
running average episode reward sum: 0.6523715136932248
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10064532,  15.60526196,   6.02980594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0840575959386394}
episode index:332
target Thresh 26.923867116756078
target distance 13.0
model initialize at round 332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.78224326,   3.58212166,   0.2477628 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.553602315684765}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6531834451669026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13346078,  14.8075518 ,   1.31270447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8876522553977758}
episode index:333
target Thresh 26.97291871975019
target distance 7.0
model initialize at round 333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.79725878,  12.34798598,   5.7450195 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.8396656234422295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6541038420676005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55536697,  14.85262421,   5.90613917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5745886339846258}
episode index:334
target Thresh 27.021921295658938
target distance 7.0
model initialize at round 334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.9136021 ,   9.4646959 ,   0.61971498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.915458365308039}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6549900695536675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92470529,  15.77959002,   6.02304572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7832176500083748}
episode index:335
target Thresh 27.0708748934849
target distance 25.0
model initialize at round 335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.49260887,  13.93993053]), 'previousTarget': array([109.61161351,  13.9223227 ]), 'currentState': array([89.85311229, 10.15969589,  1.31828344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6554026416194114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00957474,  14.51252451,   5.59049851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1038906434278386}
episode index:336
target Thresh 27.11977956218168
target distance 4.0
model initialize at round 336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38758491,  11.34207826,   3.1050342 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.678398225995629}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6561146167311007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4407495 ,  14.05409702,   2.68897127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.043548068660308}
episode index:337
target Thresh 27.168635350653943
target distance 2.0
model initialize at round 337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.2231467 ,  15.33263131,   4.62947339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8077199028887931}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6570154492555649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43797204,  15.23967416,   5.48044116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6109984678583744}
episode index:338
target Thresh 27.21744230775749
target distance 7.0
model initialize at round 338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.34028192,   9.34895351,   0.85461991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.2456726332470165}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6578826309683804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38386706,  15.73440882,   0.88531922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9586324187527363}
episode index:339
target Thresh 27.26620048229928
target distance 3.0
model initialize at round 339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03668649,  10.63468756,   3.9086287 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.365466595195234}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6583292346073594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3113924 ,  15.54158505,   3.57585702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6247236095085678}
episode index:340
target Thresh 27.314909923037497
target distance 2.0
model initialize at round 340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.60616639,  16.64717091,   2.97676307]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.157763688678791}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6589208156045263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19262772,  14.44798317,   0.39790178]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9780452854926704}
episode index:341
target Thresh 27.36357067868158
target distance 5.0
model initialize at round 341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05137127,  10.05025547,   2.93151689]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.039828109965943}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.659802906816209
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57720537,  14.22161516,   1.99456112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8857980906016446}
episode index:342
target Thresh 27.412182797892275
target distance 2.0
model initialize at round 342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.91383823,  17.65508271,   0.97744465]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.868660244835268}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6606798546389022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06290755,  14.68430931,   3.21075076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32189745942620546}
episode index:343
target Thresh 27.460746329281726
target distance 6.0
model initialize at round 343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5810232 ,  19.52622117,   4.88925862]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.545571432586678}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6615517039277425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02804127,  14.39120034,   2.92754197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.609445106761425}
episode index:344
target Thresh 27.509261321413458
target distance 6.0
model initialize at round 344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.61526903,  19.93173446,   5.25612462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.599081032933624}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6621777019540359
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86930431,  14.6101551 ,   6.02864595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41116955801537247}
episode index:345
target Thresh 27.55772782280248
target distance 6.0
model initialize at round 345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.79533891,  15.76417344,   0.27627912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.273539042395317}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6630401826131282
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0172162 ,  14.50672105,   5.16975676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0996309001131226}
episode index:346
target Thresh 27.606145881915285
target distance 13.0
model initialize at round 346
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.58342256,  15.90867841]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.        ,  11.        ,   0.41965142], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.580572401379552}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.6633081885832884
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55607078,  14.87463049,   1.26305722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4612923827236559}
episode index:347
target Thresh 27.654515547169936
target distance 12.0
model initialize at round 347
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.28560944,  15.01536709]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,   3.       ,   3.9724872], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.184447780979767}
done in step count: 16
reward sum = 0.6435507710948756
running average episode reward sum: 0.6632514143951034
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51890305,  15.1324864 ,   1.79989842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.535549272592409}
episode index:348
target Thresh 27.702836866936103
target distance 18.0
model initialize at round 348
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.87575919,  13.55124375]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       ,  9.       ,  1.1409116], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.416696544603127}
done in step count: 81
reward sum = 0.05301147754797947
running average episode reward sum: 0.6615028758941088
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4819487 ,  15.43920914,   5.48377293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6791773084736115}
episode index:349
target Thresh 27.75110988953512
target distance 14.0
model initialize at round 349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.28434461,   3.61437751,   1.21487904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.605991315513887}
done in step count: 19
reward sum = 0.6882616238355866
running average episode reward sum: 0.6615793294596559
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58755493,  14.23537846,   0.80921127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8687675354840153}
episode index:350
target Thresh 27.799334663240003
target distance 16.0
model initialize at round 350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.87433229, 20.70448037,  1.47236457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.104919043247584}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.662194547959768
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31435094,  14.6876848 ,   5.88131151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7534291046268907}
episode index:351
target Thresh 27.847511236275523
target distance 13.0
model initialize at round 351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.88073199,  23.803868  ,   3.12088859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.639165272602852}
done in step count: 15
reward sum = 0.7921374246412884
running average episode reward sum: 0.6625637038594313
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56435685,  15.23676097,   5.31854067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4958232631268882}
episode index:352
target Thresh 27.89563965681828
target distance 6.0
model initialize at round 352
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.72106771,   9.02598046,   5.69169128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.348344781190921}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.663380775661246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3091946 ,  14.84804281,   0.50980636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34451747376315645}
episode index:353
target Thresh 27.94371997299668
target distance 8.0
model initialize at round 353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.25053975,   6.74840952,   2.4554677 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.697601801552862}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6641397716280418
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20678868,  14.02847542,   2.10737763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2542105895390985}
episode index:354
target Thresh 27.99175223289105
target distance 22.0
model initialize at round 354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.33182656,  14.9237749 ]), 'previousTarget': array([113.,  15.]), 'currentState': array([91.3361433 , 14.50826209,  2.50949717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6641724174799636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23883653,  15.47968776,   5.11132065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5358574780347501}
episode index:355
target Thresh 28.039736484533645
target distance 13.0
model initialize at round 355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.67200985,   1.96691271,   0.98868233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.46663454610536}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6642827412932208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04297693,  14.94214712,   4.38876076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07206921492457653}
episode index:356
target Thresh 28.087672775908736
target distance 18.0
model initialize at round 356
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.60603154,  15.28324085]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 15.       ,  2.3261583], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.608077374462756}
done in step count: 48
reward sum = -0.01473106434298277
running average episode reward sum: 0.6623807418376573
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56472233,  15.32479337,   4.79120981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5430997941712249}
episode index:357
target Thresh 28.135561154952605
target distance 4.0
model initialize at round 357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.94715296,  19.14125902,   1.17820168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.79444521463348}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6631080154482446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34054181,  14.31548981,   5.57528592]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9504942392269337}
episode index:358
target Thresh 28.18340166955364
target distance 3.0
model initialize at round 358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.01826564,  19.28976007,   2.85505426]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.74082668485301}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6637052661656561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55158208,  15.55973013,   5.32144243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7858375244094107}
episode index:359
target Thresh 28.231194367552373
target distance 24.0
model initialize at round 359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.04298242,  13.60077744]), 'previousTarget': array([110.2,  13.6]), 'currentState': array([90.79509448,  8.16765589,  1.6045543 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6640440797063271
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59040388,  15.27657267,   5.13786389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4942281090916149}
episode index:360
target Thresh 28.278939296741473
target distance 16.0
model initialize at round 360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.75290704, 20.47588598,  6.14303958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.200591683684706}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6647098359260015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71255742,  15.82236728,   6.1195279 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8711550807535481}
episode index:361
target Thresh 28.326636504865906
target distance 28.0
model initialize at round 361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.42484152,  16.37403042]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([88.8477371 , 20.46511616,  0.20459675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.48648628978455316
running average episode reward sum: 0.661529736131221
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([111.15349085,  16.48211765]), 'previousTarget': array([112.23057259,  15.91556847]), 'currentState': array([92.49095678, 23.67307183,  2.97740746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:362
target Thresh 28.37428603962286
target distance 8.0
model initialize at round 362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.10411719,  22.47928201,   1.62631829]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.875873515095876}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6620294977214488
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33598358,  15.16025261,   0.49711516]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6830803043569053}
episode index:363
target Thresh 28.42188794866189
target distance 3.0
model initialize at round 363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.51553508,  13.30437716,   0.74729055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.253613303538949}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6629305155848514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74820328,  14.34076756,   0.6499583 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7056833582163271}
episode index:364
target Thresh 28.46944227958491
target distance 3.0
model initialize at round 364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.29253911,  14.06116804,   0.60909522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.865615102639164}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6637994730763996
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15132553,  14.94279703,   0.36249849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16177637186222205}
episode index:365
target Thresh 28.51694907994624
target distance 27.0
model initialize at round 365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.73384057,  12.0127521 ]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([87.92439567,  5.21534211,  0.68056953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6637415827184988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14600374,  14.70782169,   5.65368215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.902595018049467}
episode index:366
target Thresh 28.5644083972527
target distance 24.0
model initialize at round 366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.05551918,  14.7613589 ]), 'previousTarget': array([110.84555753,  14.48069469]), 'currentState': array([92.12088338, 13.14571811,  6.06953985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6677268503260472
running average episode reward sum: 0.6637524417583014
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16705614,  15.05742192,   5.77558155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1766494568951507}
episode index:367
target Thresh 28.611820278963602
target distance 27.0
model initialize at round 367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.85278162,  14.21731351]), 'previousTarget': array([107.98629668,  14.74023321]), 'currentState': array([87.97163643, 12.0401443 ,  4.84651858]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 33
reward sum = 0.43936494082923405
running average episode reward sum: 0.6631426931144724
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96233582,  15.99192594,   6.07084043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9926407556798705}
episode index:368
target Thresh 28.659184772490843
target distance 15.0
model initialize at round 368
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.78972031,  16.85107042]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.      ,  23.      ,   5.381629], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.017027244483497}
done in step count: 16
reward sum = 0.5083881203948755
running average episode reward sum: 0.6627233040285113
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47960658,  14.85270032,   0.44444151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.540838706648004}
episode index:369
target Thresh 28.706501925198896
target distance 11.0
model initialize at round 369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.32518685,   5.66577661,   2.75585961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.709909976399247}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6634767009078965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90519622,  14.15999299,   1.56534889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8453398901637389}
episode index:370
target Thresh 28.75377178440494
target distance 17.0
model initialize at round 370
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.87770148,  15.41528088]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 12.       ,  3.8376455], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.184153790022876}
done in step count: 38
reward sum = 0.543254595010387
running average episode reward sum: 0.6631526521049382
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11404148,  15.02808987,   2.12816516]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11744998761528998}
episode index:371
target Thresh 28.80099439737883
target distance 26.0
model initialize at round 371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.77981579,  11.77847822]), 'previousTarget': array([106.88854382,  10.94427191]), 'currentState': array([89.51538737,  3.62920497,  0.29846883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 65
reward sum = 0.10102048514045836
running average episode reward sum: 0.6616415441292273
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27357957,  15.20950004,   3.4085252 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34458097377380587}
episode index:372
target Thresh 28.84816981134319
target distance 10.0
model initialize at round 372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.47047079,   9.79124882,   0.78539729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.860157262093828}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6623415525750682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75248043,  15.78063714,   6.04795394]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8189385090077786}
episode index:373
target Thresh 28.895298073473427
target distance 25.0
model initialize at round 373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.47803463,  16.65910637]), 'previousTarget': array([109.25928039,  16.60740149]), 'currentState': array([90.32391333, 22.41407267,  0.2053324 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.9548508653989354
running average episode reward sum: 0.658017508676742
{'dynamicTrap': 21, 'scaleFactor': 20, 'currentTarget': array([112.74410672,  15.09577288]), 'previousTarget': array([114.52881612,  16.78162191]), 'currentState': array([92.76210625, 15.94409919,  4.20740095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:374
target Thresh 28.942379230897803
target distance 17.0
model initialize at round 374
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.60701976,  15.68545675]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 13.       ,  2.6652036], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.799810163029196}
done in step count: 24
reward sum = 0.6463781408072188
running average episode reward sum: 0.6579864703624233
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11733645,  14.99288059,   0.35351007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8826922588687887}
episode index:375
target Thresh 28.9894133306975
target distance 5.0
model initialize at round 375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40729359,   9.60223468,   5.13863891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.413109873287512}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6586906145753635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09821178,  14.88208025,   1.06664303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9094652578558088}
episode index:376
target Thresh 29.036400419906613
target distance 26.0
model initialize at round 376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.89364509,  11.6568284 ]), 'previousTarget': array([107.41934502,  11.79279982]), 'currentState': array([88.40431045,  4.03157401,  2.07863808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.24366362084938298
running average episode reward sum: 0.6562971020145552
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([112.80569812,  14.87137881]), 'previousTarget': array([111.23459458,  14.68405324]), 'currentState': array([92.83996819, 13.70106763,  1.43198211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:377
target Thresh 29.083340545512222
target distance 8.0
model initialize at round 377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.07818732,  19.44862007,   4.7325983 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.228104961491562}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6568591356408896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20201461,  14.36717477,   5.67904601]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6642873463454521}
episode index:378
target Thresh 29.130233754454466
target distance 7.0
model initialize at round 378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.77688174,  20.84398538,   4.90879321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.970609966396838}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6576605521959268
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21975047,  14.69763744,   5.07577981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8367869808727504}
episode index:379
target Thresh 29.17708009362657
target distance 26.0
model initialize at round 379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.84977101,  16.92561943]), 'previousTarget': array([108.31231517,  16.80053053]), 'currentState': array([87.38565205, 21.52432282,  4.43813944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6731478271244108
running average episode reward sum: 0.6541584248819259
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([109.60092013,  16.46300234]), 'previousTarget': array([110.48253772,  15.95721661]), 'currentState': array([90.29707086, 21.69381523,  0.97384668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:380
target Thresh 29.223879609874857
target distance 27.0
model initialize at round 380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.64422561,  17.13485843]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([89.68516705, 23.50306841,  5.69763261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.456292357135972
running average episode reward sum: 0.6512438559002516
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([112.49584657,  15.95965865]), 'previousTarget': array([113.91053243,  15.68422993]), 'currentState': array([93.82025919, 23.11664378,  4.45729941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:381
target Thresh 29.27063234999886
target distance 5.0
model initialize at round 381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68878208,  10.84290206,   1.1789192 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.168731211823484}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6520285318007746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14835345,  14.02817798,   1.1154726 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.29218415260188}
episode index:382
target Thresh 29.317338360751318
target distance 9.0
model initialize at round 382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64055349,   7.51229442,   2.08136597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.515054468456294}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6527842801496002
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83228602,  14.58858265,   0.51705669]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44428843950917474}
episode index:383
target Thresh 29.363997688838246
target distance 3.0
model initialize at round 383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.13198917,  17.26392675,   0.76434028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.65388155566099}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6533926150234713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46413712,  15.03104483,   4.48942984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4651742112448061}
episode index:384
target Thresh 29.410610380918975
target distance 29.0
model initialize at round 384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.60908883,  14.40895631]), 'previousTarget': array([105.95260657,  14.37604183]), 'currentState': array([87.67273413, 12.81466618,  0.89965313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6137177396967534
running average episode reward sum: 0.6501014192969252
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([109.47705814,  16.52631022]), 'previousTarget': array([110.77121015,  16.2297122 ]), 'currentState': array([90.19965957, 21.85377722,  2.21438163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:385
target Thresh 29.457176483606197
target distance 6.0
model initialize at round 385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.82770924,  19.66679319,   4.48957169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.147601950089375}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6509058094282804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89876105,  15.54258697,   4.4672181 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5519510311975381}
episode index:386
target Thresh 29.503696043466043
target distance 26.0
model initialize at round 386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.53299043,  14.43989443]), 'previousTarget': array([108.76743395,  14.04114369]), 'currentState': array([89.63713522, 12.40152606,  2.10797267]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4028499579487679
running average episode reward sum: 0.648182926308443
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([112.63590383,  14.85355404]), 'previousTarget': array([113.26824209,  14.94167621]), 'currentState': array([92.67416664, 13.61700714,  3.14222255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:387
target Thresh 29.550169107018046
target distance 8.0
model initialize at round 387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92070597,   6.34088803,   2.73441505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.707922810516921}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6488199245763561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17713239,  14.65235726,   5.51751271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8932897522434099}
episode index:388
target Thresh 29.596595720735284
target distance 9.0
model initialize at round 388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.15851449,  12.96045398,   0.23667496]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.102385020453312}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6494306313813426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15172035,  14.83014303,   5.76577497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22775085799443126}
episode index:389
target Thresh 29.642975931044376
target distance 22.0
model initialize at round 389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.15373973,  14.31380283]), 'previousTarget': array([112.50265712,  14.43242207]), 'currentState': array([91.46462593, 10.80114034,  3.58586466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3420122066438358
running average episode reward sum: 0.6468884702582012
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([110.54182867,  14.88780105]), 'previousTarget': array([110.85080203,  14.76584752]), 'currentState': array([90.54815946, 14.38461969,  0.98239419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:390
target Thresh 29.689309784325534
target distance 3.0
model initialize at round 390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.7847664 ,  13.7414745 ,   2.07210207]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.7494796737286062}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6477156071629117
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85793711,  15.60843167,   5.83337975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6247967342417042}
episode index:391
target Thresh 29.73559732691262
target distance 29.0
model initialize at round 391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.74657229,  11.05235545]), 'previousTarget': array([104.69995053,  11.09308468]), 'currentState': array([86.08211213,  3.86640198,  6.20495915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = -1.505862721473187
running average episode reward sum: 0.6422217848959828
{'dynamicTrap': 41, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20175828,  14.6539261 ,   1.73311311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8700327520540623}
episode index:392
target Thresh 29.781838605093178
target distance 22.0
model initialize at round 392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.24339233,  15.05220277]), 'previousTarget': array([112.9793708 ,  14.90815322]), 'currentState': array([93.25221805, 15.64629933,  2.42877531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.4083061323071938
running average episode reward sum: 0.6395486858700206
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([110.16344958,  13.90961427]), 'previousTarget': array([110.14978052,  14.19710167]), 'currentState': array([90.65312387,  9.5110701 ,  3.86630447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:393
target Thresh 29.828033665108485
target distance 4.0
model initialize at round 393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73237062,  18.17087009,   3.70757952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.254348455868671}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6403881536723809
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13477243,  15.36856642,   4.14839727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9404573092571346}
episode index:394
target Thresh 29.874182553153613
target distance 26.0
model initialize at round 394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.06193412,  12.95960458]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([88.87446525,  7.3168182 ,  0.93805361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.1674616919744764
running average episode reward sum: 0.6391908714908672
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05567656,  15.77608   ,   3.77993199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2223121226144376}
episode index:395
target Thresh 29.92028531537745
target distance 23.0
model initialize at round 395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.6210487 ,  11.86245744]), 'previousTarget': array([109.41125677,  11.84114513]), 'currentState': array([92.34522043,  1.78546594,  0.41915804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.20308386056130906
running average episode reward sum: 0.638089591160237
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44118604,  15.81942689,   4.76265683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9306479156365173}
episode index:396
target Thresh 29.96634199788277
target distance 17.0
model initialize at round 396
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.52660693,  15.8843866 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.        ,  6.        ,  0.37496105], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.40588550926728}
done in step count: 20
reward sum = 0.2864884626588622
running average episode reward sum: 0.637203946000284
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46308169,  15.49051163,   0.13351582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7272433814617253}
episode index:397
target Thresh 30.012352646726246
target distance 30.0
model initialize at round 397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.79139661,  10.7692853 ]), 'previousTarget': array([103.77752632,  10.88509298]), 'currentState': array([83.89390578,  4.22063352,  1.93470597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5798026283684021
running average episode reward sum: 0.6370597215841235
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01779618,  14.79849814,   5.29705283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.002660135756222}
episode index:398
target Thresh 30.058317307918532
target distance 12.0
model initialize at round 398
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.73021448,  16.56490566]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  17.       ,   3.9862404], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.73828091944514}
done in step count: 9
reward sum = 0.7742172474836408
running average episode reward sum: 0.6374034747818667
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39009781,  15.47970338,   4.97846732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6182973666491064}
episode index:399
target Thresh 30.1042360274243
target distance 21.0
model initialize at round 399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.96662487,  13.65154672]), 'previousTarget': array([112.05721038,  13.59867161]), 'currentState': array([93.69103469,  5.52733577,  1.09078264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6704691574360915
running average episode reward sum: 0.6374861389885023
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25198684,  15.8148523 ,   1.03629537]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.106122945322407}
episode index:400
target Thresh 30.150108851162273
target distance 4.0
model initialize at round 400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.80186044,  10.94433797,   2.8015312 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.43791567175241}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6382207504820645
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16070765,  15.53099946,   1.21547621]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9931626597998823}
episode index:401
target Thresh 30.19593582500527
target distance 12.0
model initialize at round 401
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.40160927,  14.47318393]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,   8.       ,   1.3083792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.251350361655954}
done in step count: 13
reward sum = 0.8075210229989678
running average episode reward sum: 0.6386418954385742
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2869011 ,  15.09049573,   5.4425821 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7188181361470901}
episode index:402
target Thresh 30.241716994780283
target distance 7.0
model initialize at round 402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.41338393,   9.79956439,   1.79690135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.934088080143297}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6392128729009325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28382585,  15.42890655,   4.41606975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5143130770879714}
episode index:403
target Thresh 30.28745240626848
target distance 24.0
model initialize at round 403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.39844695,  15.40553611]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([92.63709929, 18.48598069,  0.64927429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.535386034319436
running average episode reward sum: 0.6363054498632583
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([109.90762847,  14.74758221]), 'previousTarget': array([110.33832165,  15.08030436]), 'currentState': array([89.93215298, 13.75744128,  3.51984572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:404
target Thresh 30.333142105205262
target distance 5.0
model initialize at round 404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.67867124,  13.87978793,   0.93851059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5051533203333345}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6371543252956947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54309006,  15.71866245,   0.59264762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8516116588485018}
episode index:405
target Thresh 30.37878613728035
target distance 7.0
model initialize at round 405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.26849813,   9.8939708 ,   0.29291695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.676043767530162}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6378350221483744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97165438,  15.91540301,   5.13264951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9158417668624502}
episode index:406
target Thresh 30.424384548137773
target distance 24.0
model initialize at round 406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.98452724,  15.33563923]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([92.10727739, 17.54808997,  4.62421572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.4279628198521136
running average episode reward sum: 0.6373193656316759
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.60564981,  15.24869911,   4.7366595 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6547235613470123}
episode index:407
target Thresh 30.469937383375946
target distance 9.0
model initialize at round 407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.50655767,   7.16672339,   1.90738595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.976837620967385}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6379298203034515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65275179,  15.12340765,   5.1613155 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36852512370595136}
episode index:408
target Thresh 30.51544468854771
target distance 23.0
model initialize at round 408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.57224773,  15.1404599 ]), 'previousTarget': array([112.,  15.]), 'currentState': array([90.58230338, 15.77459327,  1.6315372 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4670945276547367
running average episode reward sum: 0.6352280492815489
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([106.03163748,  17.80973243]), 'previousTarget': array([107.22198984,  17.01987747]), 'currentState': array([86.94635608, 23.78903288,  2.36052766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:409
target Thresh 30.560906509160368
target distance 4.0
model initialize at round 409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.48464654,  19.75238652,   3.67574954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.978890741413839}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6360452955028133
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18767228,  15.87354481,   3.98604478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1928775531019056}
episode index:410
target Thresh 30.606322890675756
target distance 28.0
model initialize at round 410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.02461806,  14.55740215]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([87.05534461, 13.44919724,  0.5263449 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.04150765124810497
running average episode reward sum: 0.6343967481871178
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.42247293,  15.08369107]), 'previousTarget': array([112.89905673,  15.21688576]), 'currentState': array([94.62921955, 17.95198687,  1.2674596 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:411
target Thresh 30.651693878510244
target distance 20.0
model initialize at round 411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.61161351,  15.0776773 ]), 'currentState': array([96.11880146, 20.38256083,  1.7280359 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.633431168818067}
done in step count: 37
reward sum = 0.42527065281377396
running average episode reward sum: 0.6338891605769883
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13752962,  14.33687258,   0.53252176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.087930670790316}
episode index:412
target Thresh 30.697019518034832
target distance 30.0
model initialize at round 412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.62222515,  10.68275469]), 'previousTarget': array([103.35111251,   9.95214875]), 'currentState': array([86.15637645,  3.00079975,  5.98658759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1524800839826221
running average episode reward sum: 0.6319851188226067
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.67544005,  15.79106898]), 'previousTarget': array([112.74090755,  15.67934241]), 'currentState': array([92.21866596, 20.42074949,  4.68701999]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:413
target Thresh 30.742299854575176
target distance 8.0
model initialize at round 413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.79173225,  21.45706167,   3.81214952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.701040954424582}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6327788649365618
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99567655,  15.38766165,   4.0218915 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0684817978691679}
episode index:414
target Thresh 30.78753493341158
target distance 10.0
model initialize at round 414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.16465609,  12.32510405,   5.82038236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.1925982013577}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6334775777787097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20042365,  15.3588299 ,   6.20395179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.876402443169198}
episode index:415
target Thresh 30.832724799779175
target distance 15.0
model initialize at round 415
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.87393636,  14.16627078]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   2.       ,   3.8639545], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3874200910928332
running average episode reward sum: 0.6310234968439222
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([109.91312071,  14.84261764]), 'previousTarget': array([109.9713426 ,  15.18684795]), 'currentState': array([89.92268599, 14.22413592,  5.64573259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:416
target Thresh 30.877869498867785
target distance 28.0
model initialize at round 416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.94263111,  11.44018762]), 'previousTarget': array([105.3829006 ,  10.87838597]), 'currentState': array([88.64854619,  3.35770923,  5.49347955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.18061833569535996
running average episode reward sum: 0.6290771135524611
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([111.21876287,  14.97801625]), 'previousTarget': array([110.11876777,  15.23313933]), 'currentState': array([9.12191009e+01, 1.48617401e+01, 7.85720000e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:417
target Thresh 30.922969075822145
target distance 26.0
model initialize at round 417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.94639639,  13.73447443]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([89.36960673,  9.64188275,  0.33937573]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6291094927116292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43038819,  15.41012595,   0.76281385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7018980776684187}
episode index:418
target Thresh 30.96802357574182
target distance 2.0
model initialize at round 418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.34408455,  13.44685675,   1.92461079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8119364039002486}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6299237874784273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58751647,  14.39322715,   1.61755806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7337001815002493}
episode index:419
target Thresh 31.013033043681315
target distance 14.0
model initialize at round 419
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.64638666,  14.6510028 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  19.      ,   3.922707], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.239556404282318}
done in step count: 99
reward sum = -0.25288981214593026
running average episode reward sum: 0.6278218503364644
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([113.11350381,  15.0567606 ]), 'previousTarget': array([111.49908665,  15.15253007]), 'currentState': array([93.12255046, 15.65824524,  5.04960738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:420
target Thresh 31.05799752465012
target distance 6.0
model initialize at round 420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.71811815,  12.85157622,   5.33163095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.639108700586714}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6285894707629811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38023332,  14.8153547 ,   5.36345828]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.646687422021606}
episode index:421
target Thresh 31.10291706361269
target distance 7.0
model initialize at round 421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.2297173 ,   7.85017275,   2.32370329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.2548076706469775}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6293086079126115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36906294,  14.48547332,   2.3455205 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8141371389658737}
episode index:422
target Thresh 31.147791705488583
target distance 5.0
model initialize at round 422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.30996696,   9.85287682,   0.15839511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.807680672046682}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6299588997969997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57628751,  15.50570463,   5.90837608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6597495296284734}
episode index:423
target Thresh 31.19262149515245
target distance 2.0
model initialize at round 423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.37743643,  16.89544946,   3.56046718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.2358257545087654}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6306494323315066
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52872279,  14.28377561,   2.05712711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8573678245378727}
episode index:424
target Thresh 31.237406477434075
target distance 21.0
model initialize at round 424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.22499392,  15.76824938]), 'previousTarget': array([113.45612429,  15.36758945]), 'currentState': array([92.95001151, 21.10445149,  1.32230711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1567854707665828
running average episode reward sum: 0.628796644324217
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.15131954,  15.05243295]), 'previousTarget': array([109.56156958,  15.23945296]), 'currentState': array([91.15317531, 15.32488002,  4.94499304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:425
target Thresh 31.282146697118446
target distance 27.0
model initialize at round 425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.804029  ,  13.17171853]), 'previousTarget': array([107.35993796,  13.01924318]), 'currentState': array([87.28380176,  8.81732665,  1.3488133 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3395495727156922
running average episode reward sum: 0.6281176605880466
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40070572,  15.69531627,   5.06626936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9179424593809985}
episode index:426
target Thresh 31.326842198945787
target distance 6.0
model initialize at round 426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.15902446,  19.38271347,   4.27322483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.533377887002199}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6289190220386601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09185193,  15.93847313,   4.25158502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9429573645709385}
episode index:427
target Thresh 31.371493027611606
target distance 6.0
model initialize at round 427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.85135076,  20.00753588,   0.14821738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.502822928756665}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6296715244402054
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35248825,  15.47073963,   5.43158977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.800541854655484}
episode index:428
target Thresh 31.416099227766736
target distance 28.0
model initialize at round 428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.06687433,  11.68551265]), 'previousTarget': array([105.61502987,  11.31304745]), 'currentState': array([88.6127925 ,  3.97533341,  5.86691535]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.05024113728187925
running average episode reward sum: 0.6280866464408531
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([106.64224818,  14.34487956]), 'previousTarget': array([107.68116186,  14.78889312]), 'currentState': array([86.70340821, 12.7819782 ,  5.02419002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:429
target Thresh 31.460660844017376
target distance 9.0
model initialize at round 429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.70763092,   7.23856719,   1.07761651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.6498115538919}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6287718977152417
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30677377,  14.40934511,   5.79462683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9107336642232705}
episode index:430
target Thresh 31.505177920925146
target distance 10.0
model initialize at round 430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.5212448 ,  17.09351395,   1.25155768]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.733389398765162}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.629519503636784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49291644,  15.09810497,   5.55047385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5164865132561083}
episode index:431
target Thresh 31.549650503007133
target distance 24.0
model initialize at round 431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.59012934,  14.26136725]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([89.77397893, 11.55578359,  2.48093033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07315039623404943
running average episode reward sum: 0.6278929529426386
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([109.10770633,  15.7875729 ]), 'previousTarget': array([110.80046731,  15.57562943]), 'currentState': array([89.28400163, 18.43723932,  2.08886206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:432
target Thresh 31.59407863473591
target distance 23.0
model initialize at round 432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.25435881,  15.77351442]), 'previousTarget': array([111.54352728,  15.75140711]), 'currentState': array([90.51485246, 18.9909496 ,  2.93462574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.20759408614717642
running average episode reward sum: 0.6259634216745328
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.0365208,  15.1115438]), 'previousTarget': array([109.53273433,  15.34062712]), 'currentState': array([91.04443633, 15.67417902,  4.80391994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:433
target Thresh 31.638462360539634
target distance 13.0
model initialize at round 433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.00396135,  18.64991038,   0.55839539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.498846858235465}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6266259880934477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47565371,  15.62140711,   5.71554929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8130718424062336}
episode index:434
target Thresh 31.682801724802015
target distance 19.0
model initialize at round 434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.94963746,  14.19969441]), 'previousTarget': array([114.4327075,  14.76114  ]), 'currentState': array([94.31859012,  6.92755063,  2.17465448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6266927311566217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15982839,  15.18932326,   4.37198964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.247766847472588}
episode index:435
target Thresh 31.72709677186243
target distance 9.0
model initialize at round 435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.28980607,  11.08110517,   1.70759886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.648978386792034}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.627414720648008
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25405612,  15.34197877,   5.44805485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8205984089730223}
episode index:436
target Thresh 31.77134754601593
target distance 25.0
model initialize at round 436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.50420784,  12.78815329]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([88.32189441,  7.12786604,  2.05554295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.627122780852244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15472677,  14.88240943,   5.475162  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8534133691574879}
episode index:437
target Thresh 31.815554091513285
target distance 11.0
model initialize at round 437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.87956306,   5.21996073,   1.31415987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.265782718253684}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6278404917393416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51120681,  14.1545766 ,   0.78159082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.976554918171695}
episode index:438
target Thresh 31.85971645256105
target distance 11.0
model initialize at round 438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.54493598,   5.57322912,   5.41364169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.835177813123407}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6284704270087482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90993173,  14.51420506,   0.71976574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.494073899174105}
episode index:439
target Thresh 31.903834673321594
target distance 25.0
model initialize at round 439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.33619175,  12.56221119]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([88.27717637,  6.49970898,  2.61739063]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6117039591664646
running average episode reward sum: 0.6284323214000157
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.070472  ,  15.5253772 ,   6.25998177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0677282005063178}
episode index:440
target Thresh 31.94790879791313
target distance 22.0
model initialize at round 440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.47476598,  15.51950484]), 'previousTarget': array([112.81660336,  15.29773591]), 'currentState': array([92.88501815, 19.54961404,  2.65486091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5777695227623648
running average episode reward sum: 0.6283174397704518
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63697848,  15.53141703,   6.01127981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6435749199537246}
episode index:441
target Thresh 31.991938870409793
target distance 7.0
model initialize at round 441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21391332,   8.20294826,   1.97030497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.842356660799738}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6290474683001567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51754626,  14.47396094,   2.62549424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.713777770685083}
episode index:442
target Thresh 32.03592493484166
target distance 11.0
model initialize at round 442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.85207602,   5.05635164,   1.38771367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.19719893879208}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6297104417225671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16048356,  14.42686042,   1.8339414 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0165022566300435}
episode index:443
target Thresh 32.07986703519481
target distance 4.0
model initialize at round 443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.44582343,  12.39599507,   2.25424677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.0325412534389242}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6304775330700387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77132202,  14.57587533,   1.39063743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4818457741398783}
episode index:444
target Thresh 32.123765215411325
target distance 27.0
model initialize at round 444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.56236516,  13.65200618]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([88.98690752,  9.55304801,  6.20019937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17931784863117708
running average episode reward sum: 0.6286577681673394
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([109.41329518,  15.06763436]), 'previousTarget': array([109.5996958 ,  15.45536162]), 'currentState': array([89.41476064, 15.30974277,  4.20052803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:445
target Thresh 32.167619519389405
target distance 10.0
model initialize at round 445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.95949051,   6.68248241,   2.60486889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.545215146367774}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6292557064769614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39412621,  14.47228584,   4.91839278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8034707735993178}
episode index:446
target Thresh 32.21142999098333
target distance 5.0
model initialize at round 446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.70700613,  10.98601748,   0.95320296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.191903727292682}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6299969599524045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24045332,  15.0646232 ,   0.30917901]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7622908320231218}
episode index:447
target Thresh 32.25519667400362
target distance 11.0
model initialize at round 447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.69254134,   5.24105799,   5.46017331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.485760484442974}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6305494690217047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2508497 ,  15.16026494,   5.97341502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7661011854412756}
episode index:448
target Thresh 32.29891961221693
target distance 13.0
model initialize at round 448
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.12279097,  13.19813737]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  22.       ,   1.8077672], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.801279332810875}
done in step count: 26
reward sum = -0.3883605240442316
running average episode reward sum: 0.6282801817320256
{'dynamicTrap': 18, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5326163 ,  15.02115666,   6.07007765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46786229362044673}
episode index:449
target Thresh 32.34259884934621
target distance 24.0
model initialize at round 449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.1163388 ,  16.09788086]), 'previousTarget': array([110.2,  16.4]), 'currentState': array([90.60333717, 20.48453876,  5.44184673]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6282017067646244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40395626,  14.84070682,   6.07588004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6169622855028222}
episode index:450
target Thresh 32.386234429070704
target distance 24.0
model initialize at round 450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.59948187,  14.87143777]), 'previousTarget': array([110.98266146,  15.16738911]), 'currentState': array([90.60801172, 14.28738202,  3.85956979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.46308815242247886
running average episode reward sum: 0.6257819953251852
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([108.50036194,  14.98309699]), 'previousTarget': array([108.13059497,  14.63414957]), 'currentState': array([88.50042957, 14.93108499,  0.31486055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:451
target Thresh 32.429826395026
target distance 19.0
model initialize at round 451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.59678972,  14.06008158]), 'previousTarget': array([113.30852571,  14.02072541]), 'currentState': array([96.98011731,  2.92966389,  6.26587415]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 15
reward sum = 0.5924227899902185
running average episode reward sum: 0.6257081917735592
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02818501,  14.40830088,   0.27634385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1377751217320669}
episode index:452
target Thresh 32.473374790804044
target distance 21.0
model initialize at round 452
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.69762269,  16.46462992]), 'previousTarget': array([113.90990945,  14.89618185]), 'currentState': array([94.      , 13.      ,  5.743464], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5415334174669337
running average episode reward sum: 0.6255223754947367
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5562682 ,  15.38371219,   0.47583298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5866284580733778}
episode index:453
target Thresh 32.51687965995326
target distance 29.0
model initialize at round 453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.46031804,  12.02444076]), 'previousTarget': array([104.90745963,  11.51981367]), 'currentState': array([87.57397739,  5.44370092,  5.59317094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6199433686572566
running average episode reward sum: 0.6227790588776618
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([112.29668339,  15.32677361]), 'previousTarget': array([110.69393638,  15.73250423]), 'currentState': array([92.44121818, 17.726878  ,  5.45668501]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:454
target Thresh 32.56034104597851
target distance 6.0
model initialize at round 454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.44393377,  17.34898923,   4.84961713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.125962338833512}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6235215137152933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08843081,  14.93039156,   0.35008132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9142230151466019}
episode index:455
target Thresh 32.60375899234119
target distance 8.0
model initialize at round 455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.399903  ,  18.39826695,   5.96705604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.325244304694332}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6240979684477513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21716246,  14.88674647,   0.28992918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7909873389153304}
episode index:456
target Thresh 32.64713354245925
target distance 12.0
model initialize at round 456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.42521751,   4.62414631,   2.32473376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.283570514020703}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6247514623776861
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55390732,  15.05321784,   5.9840723 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44925584839974086}
episode index:457
target Thresh 32.69046473970723
target distance 25.0
model initialize at round 457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.5541012 ,  16.00502486]), 'previousTarget': array([109.25928039,  16.60740149]), 'currentState': array([91.04633024, 20.41488565,  4.7364347 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6247904583159111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27518676,  14.62568098,   5.98731441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.815762811493704}
episode index:458
target Thresh 32.733752627416344
target distance 3.0
model initialize at round 458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.55525286,  18.03275796,   4.1289227 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.408288765043872}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6255645531779679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57179386,  15.71573925,   3.99416283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9160954611399436}
episode index:459
target Thresh 32.77699724887448
target distance 7.0
model initialize at round 459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.39032675,   9.07211851,   3.0055998 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.088742681489417}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6262513262132354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31938918,  14.37127053,   0.9053945 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9265699313344302}
episode index:460
target Thresh 32.820198647326265
target distance 32.0
model initialize at round 460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.35996128,  12.25256611]), 'previousTarget': array([102.65744374,  12.6857707 ]), 'currentState': array([83.89482949,  7.6581516 ,  6.25731218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6248928634665689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.90418145,  16.26609922]), 'previousTarget': array([112.51552439,  15.91046711]), 'currentState': array([93.39246288, 23.83685056,  2.72992106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:461
target Thresh 32.86335686597309
target distance 8.0
model initialize at round 461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.04197263,   8.96152193,   1.89154434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.724071933751175}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6255375643993857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67047087,  15.94819394,   5.84288533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0038232855820175}
episode index:462
target Thresh 32.9064719479732
target distance 23.0
model initialize at round 462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.31297989,  15.3154323 ]), 'previousTarget': array([111.98112317,  15.13125551]), 'currentState': array([90.35811833, 16.65837682,  1.9040848 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.43583263606333206
running average episode reward sum: 0.6251278345325691
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46698608,  15.32319742,   0.46334923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6233461371939887}
episode index:463
target Thresh 32.94954393644165
target distance 11.0
model initialize at round 463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.00038266,   5.35783854,   6.21505458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.097969190216622}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6258096283146132
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05471597,  14.07992213,   1.12643492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3191304678676206}
episode index:464
target Thresh 32.99257287445046
target distance 7.0
model initialize at round 464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.57207308,   9.68767084,   2.72752291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.5008923279965245}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6265295990279152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56993451,  14.48991565,   0.86661378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6671899065529887}
episode index:465
target Thresh 33.03555880502857
target distance 9.0
model initialize at round 465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.14677817,   7.77869481,   0.16368168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.668661666155117}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.627165253739074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09674638,  14.63956618,   0.49902066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9725120218763811}
episode index:466
target Thresh 33.078501771161896
target distance 13.0
model initialize at round 466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.66314581,   2.22409828,   0.96866911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.050517629385666}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6277588657760541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20873232,  14.21794816,   0.7520782 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1125239879984978}
episode index:467
target Thresh 33.12140181579341
target distance 7.0
model initialize at round 467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47491445,  20.31575202,   4.09512269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.34162282629571}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6284907891397805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41203984,  15.81373744,   5.0504179 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0039251822613402}
episode index:468
target Thresh 33.164258981823174
target distance 3.0
model initialize at round 468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.47326912,  12.65630604,   2.15780711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7682888711617024}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6292195912951328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16148052,  14.75482385,   2.24074569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.873628219176657}
episode index:469
target Thresh 33.20707331210835
target distance 9.0
model initialize at round 469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.38955097,   4.99664176,   0.33156281]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.56925257736163}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6298050433881407
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72423513,  14.30309862,   0.89810735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7494783554082043}
episode index:470
target Thresh 33.249844849463265
target distance 12.0
model initialize at round 470
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.85545608,  13.87125526]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  11.       ,   1.7684484], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.228759209989194}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.6302783759805819
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74842874,  14.55385836,   6.23245585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5121820636170807}
episode index:471
target Thresh 33.29257363665947
target distance 11.0
model initialize at round 471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.4683417 ,   4.82241154,   5.78374631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.944024413961943}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6308784583354612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10993155,  14.77212317,   0.29931617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9187761964858149}
episode index:472
target Thresh 33.33525971642575
target distance 2.0
model initialize at round 472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.61073829,  13.15251616,   1.96455574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3115459395607934}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6316167702628703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14606511e+02, 1.44606428e+01, 4.66507077e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6676374065707072}
episode index:473
target Thresh 33.3779031314482
target distance 7.0
model initialize at round 473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.0744547 ,   9.72048577,   2.47012001]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.6196970744437715}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.632290553553244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2491023 ,  14.37339198,   0.6448673 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9780004941716628}
episode index:474
target Thresh 33.42050392437022
target distance 17.0
model initialize at round 474
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.81897234,  13.24413527]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.        ,  9.        ,  0.98114014], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.34619597516568}
done in step count: 99
reward sum = -0.31490655969835046
running average episode reward sum: 0.6302964543674513
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([110.39799683,  15.65408743]), 'previousTarget': array([112.02248542,  15.38682232]), 'currentState': array([90.59699907, 18.46842353,  3.79037855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:475
target Thresh 33.463062137792626
target distance 19.0
model initialize at round 475
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.1329263 ,  13.36568502]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.        , 13.        ,  0.71351904], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.136420622524145}
done in step count: 50
reward sum = 0.4657060671375364
running average episode reward sum: 0.6299506762430187
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33725937,  14.34158926,   0.46685257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9342001113151486}
episode index:476
target Thresh 33.505577814273614
target distance 15.0
model initialize at round 476
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.68432923,  16.00953021]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.        ,   7.        ,   0.33551085], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.38390982101376}
done in step count: 12
reward sum = 0.6784778717161293
running average episode reward sum: 0.6300524104054361
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29252497,  15.16314611,   0.32563496]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7260424052991503}
episode index:477
target Thresh 33.54805099632888
target distance 29.0
model initialize at round 477
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([105.81671596,  15.29856176]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([86.      , 18.      ,  3.292125], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.275957825859211
running average episode reward sum: 0.6281569915011168
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([112.24840164,  15.2406584 ]), 'previousTarget': array([113.6564934 ,  15.06335828]), 'currentState': array([92.32446037, 16.98323244,  1.5062608 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:478
target Thresh 33.59048172643161
target distance 27.0
model initialize at round 478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.36243803,  11.52428891]), 'previousTarget': array([106.0200334 ,  10.67631238]), 'currentState': array([89.15878437,  3.24014782,  1.80507856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.28522163709412246
running average episode reward sum: 0.6262501467650098
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([109.74595797,  15.7070976 ]), 'previousTarget': array([109.29378476,  16.10656715]), 'currentState': array([89.92465632, 18.37468092,  3.87292106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:479
target Thresh 33.63287004701253
target distance 4.0
model initialize at round 479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.20855784,  17.81226124,   6.02068126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.96244402046347}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6269669152092494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28683508,  15.03345692,   5.60951843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7139492740068271}
episode index:480
target Thresh 33.67521600045996
target distance 25.0
model initialize at round 480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.0792573 ,  13.68026436]), 'previousTarget': array([109.61161351,  13.9223227 ]), 'currentState': array([90.76194512,  8.49939088,  4.17222309]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6273638799127587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26085324,  14.56233748,   0.34980689]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.859003150133298}
episode index:481
target Thresh 33.71751962911988
target distance 16.0
model initialize at round 481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.98147838,  14.98243058]), 'previousTarget': array([114.52228  ,  14.6118525]), 'currentState': array([100.47127625,   1.21820034,   0.14938134]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6278828781349293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39474401,  15.08082241,   0.59156459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6106284266265103}
episode index:482
target Thresh 33.75978097529588
target distance 7.0
model initialize at round 482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.42742901,   9.58257718,   2.92794144]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.936403082133377}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6285717251988321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15546847,  14.16784055,   2.21307242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.18563183797206}
episode index:483
target Thresh 33.80200008124936
target distance 29.0
model initialize at round 483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.45875996,  15.62219001]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([87.52648515, 17.26670233,  0.54441589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.34788582127227485
running average episode reward sum: 0.6265542509292638
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([109.84282076,  14.95024913]), 'previousTarget': array([110.69169002,  14.7707987 ]), 'currentState': array([89.84375133, 14.75731978,  1.25920409]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:484
target Thresh 33.84417698919939
target distance 30.0
model initialize at round 484
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([104.85415774,  12.41089622]), 'previousTarget': array([104.72787848,  13.28797975]), 'currentState': array([85.       , 10.       ,  2.3294344], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5592450010441875
running average episode reward sum: 0.6264154689707379
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69184052,  14.7066306 ,   0.43059754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4254736973976874}
episode index:485
target Thresh 33.8863117413229
target distance 13.0
model initialize at round 485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.02817715,   3.33238127,   6.18674666]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.107097126655534}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6270251998873164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79826872,  14.41421023,   1.97259428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6195523934922136}
episode index:486
target Thresh 33.92840437975464
target distance 4.0
model initialize at round 486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.39242376,  17.84849625,   3.4999485 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.17061111078344}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6277501994768702
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16916587,  15.76855113,   4.66726249]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7869484904003667}
episode index:487
target Thresh 33.97045494658725
target distance 33.0
model initialize at round 487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.28602299,  9.71297635]), 'previousTarget': array([100.79586847,   9.83486126]), 'currentState': array([80.33017571,  3.33521389,  1.96776545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = -0.0554815373082681
running average episode reward sum: 0.6263501344424744
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06147073,  14.92994167,   5.49464206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9411404567302861}
episode index:488
target Thresh 34.01246348387131
target distance 11.0
model initialize at round 488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.30880889,  20.91709037,   4.60866419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.354785047227129}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6269562582870254
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73657169,  15.77236292,   4.79729209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8160508296452623}
episode index:489
target Thresh 34.05443003361536
target distance 8.0
model initialize at round 489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.76128265,  14.67541136,   1.04146004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.245108933613938}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6275410766323246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2687682 ,  15.13203607,   0.58424929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7430568398104307}
episode index:490
target Thresh 34.096354637785936
target distance 33.0
model initialize at round 490
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.99928558,  15.16904518]), 'previousTarget': array([102.,  15.]), 'currentState': array([82.      , 15.      ,  5.132088], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5607755688732073
running average episode reward sum: 0.6274050980014507
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29693674,  15.50436595,   6.209033  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8652646771737486}
episode index:491
target Thresh 34.13823733830767
target distance 5.0
model initialize at round 491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.31567581,  18.87514124,   4.64911558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.347051931026192}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6281020368266511
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88083027,  15.87648221,   5.30189209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8845464888300928}
episode index:492
target Thresh 34.180078177063244
target distance 7.0
model initialize at round 492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.71727353,   7.56223653,   2.48884308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.129982687623485}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6287185952669763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24408837,  14.76461393,   0.64342376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7917126963469818}
episode index:493
target Thresh 34.22187719589352
target distance 6.0
model initialize at round 493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.67123847,   9.8841598 ,   3.48846626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.381901051917042}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6293709666326301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0755522 ,  14.51601008,   1.71593805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0434797438548509}
episode index:494
target Thresh 34.2636344365975
target distance 21.0
model initialize at round 494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.96479631,  15.34078421]), 'previousTarget': array([112.6897547 ,  15.88009345]), 'currentState': array([94.96767917, 21.59454632,  4.34524012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.7428675212907804
running average episode reward sum: 0.6265987676671283
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([111.61593922,  14.94420741]), 'previousTarget': array([110.07131066,  14.99996481]), 'currentState': array([91.61865683, 14.61451472,  5.14097842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:495
target Thresh 34.30534994093245
target distance 10.0
model initialize at round 495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.5303157 ,  19.06475574,   5.41237301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.39456180336405}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6271046593109424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49065793,  15.04927901,   6.22651228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5117203981422995}
episode index:496
target Thresh 34.34702375061386
target distance 26.0
model initialize at round 496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.12370481,  15.01542665]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([90.12380489, 15.07869834,  6.06132883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17216420219181033
running average episode reward sum: 0.6254964724668725
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([109.7352767 ,  15.54314608]), 'previousTarget': array([108.22478266,  15.59529504]), 'currentState': array([89.84086899, 17.59559382,  5.43695846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:497
target Thresh 34.38865590731554
target distance 5.0
model initialize at round 497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.73031067,  14.4109656 ,   6.18100101]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3223229535504144}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6261888470201519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32081727,  14.70962156,   0.44721121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7386533781751449}
episode index:498
target Thresh 34.43024645266967
target distance 6.0
model initialize at round 498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.12219084e+02, 2.12370930e+01, 4.99629974e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.82896944334732}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6268018259798449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07552878,  15.57992335,   4.66587019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5848210772772888}
episode index:499
target Thresh 34.47179542826679
target distance 25.0
model initialize at round 499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.44624514,  12.69731417]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([90.97129627,  5.03727083,  1.82823294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3391667772885952
running average episode reward sum: 0.6248698887733081
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.80322424,  15.02331519]), 'currentState': array([96.40092778, 16.01503019,  5.65025679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.626748874003795}
episode index:500
target Thresh 34.51330287565587
target distance 29.0
model initialize at round 500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.09818491,  16.39774692]), 'previousTarget': array([105.81242258,  16.26725206]), 'currentState': array([87.40392678, 19.88145141,  1.57064408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6133131948350643
running average episode reward sum: 0.6248468215199383
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63298667,  15.89022549,   5.43100844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9629123582648307}
episode index:501
target Thresh 34.55476883634437
target distance 28.0
model initialize at round 501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.04015954,  16.24548843]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.35291138, 19.76859215,  5.78982038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6252314034244748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10650288,  15.94606935,   5.38826776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3013010111098517}
episode index:502
target Thresh 34.596193351798256
target distance 12.0
model initialize at round 502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.46899063,  18.89348722,   1.83854568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.080037543809286}
done in step count: 12
reward sum = 0.8177778717161293
running average episode reward sum: 0.6256141995841004
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68731216,  15.75982776,   4.96067863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8216519390221115}
episode index:503
target Thresh 34.637576463442045
target distance 11.0
model initialize at round 503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.19769773,   5.45001598,   1.07410085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.95264250560284}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.626203744216727
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14535635e+02, 1.49434559e+01, 9.18495655e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4677952550195637}
episode index:504
target Thresh 34.67891821265885
target distance 22.0
model initialize at round 504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.80528192,  14.01681047]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([94.55307944,  5.84019107,  5.91682774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6266329312447808
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46157284,  15.29466211,   0.64657939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6137829978954538}
episode index:505
target Thresh 34.72021864079043
target distance 29.0
model initialize at round 505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.1708321,  15.5364256]), 'previousTarget': array([105.98811999,  15.31075448]), 'currentState': array([87.21761229, 16.90354636,  1.62978786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.34942433191717137
running average episode reward sum: 0.6247039643215359
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([109.29830593,  17.03320602]), 'previousTarget': array([110.44543807,  16.6998481 ]), 'currentState': array([90.46021184, 23.7508106 ,  3.44173829]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:506
target Thresh 34.7614777891372
target distance 7.0
model initialize at round 506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.38402289,   9.38972775,   2.16601306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.778466411134811}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6253475266205072
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18602644,  14.40850535,   1.61103022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0061902842374337}
episode index:507
target Thresh 34.80269569895833
target distance 33.0
model initialize at round 507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.7636982,  16.6862807]), 'previousTarget': array([101.91786413,  16.18928508]), 'currentState': array([82.95095028, 19.41666894,  1.94997473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6256787603941748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12652179,  14.76862457,   6.09667153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9036032201893024}
episode index:508
target Thresh 34.84387241147172
target distance 16.0
model initialize at round 508
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.58649451,  16.25044852]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.      , 23.      ,  6.139766], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.07240702067868}
done in step count: 11
reward sum = 0.6874312542587164
running average episode reward sum: 0.6258000816001955
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96892838,  15.72752015,   4.94505636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7281833668613512}
episode index:509
target Thresh 34.885007967854094
target distance 10.0
model initialize at round 509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.80158846,   3.36852225,   4.20989913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.063213127350101}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6263642329058493
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31882905,  14.75386899,   1.03600776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7242750440906193}
episode index:510
target Thresh 34.92610240924101
target distance 10.0
model initialize at round 510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.48602958,   4.6291286 ,   2.46839893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.950021086438062}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6269624738354015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07825957,  14.62701793,   0.64258593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9943445323395095}
episode index:511
target Thresh 34.967155776726905
target distance 9.0
model initialize at round 511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.17441508,   7.79437644,   1.36545706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.18454471210388}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6275401734849962
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90146131,  14.40788044,   1.69894341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6002628146856526}
episode index:512
target Thresh 35.00816811136517
target distance 13.0
model initialize at round 512
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.97340429,  16.74776393]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,   7.       ,   3.8496091], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.227387976583692}
done in step count: 16
reward sum = 0.5117845099368055
running average episode reward sum: 0.6273145289166763
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2339864 ,  14.38889619,   1.4797948 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.979910555845426}
episode index:513
target Thresh 35.04913945416813
target distance 28.0
model initialize at round 513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.12936069,  12.80901799]), 'previousTarget': array([106.04057123,  12.12018361]), 'currentState': array([8.78619683e+01, 7.44547526e+00, 2.30858326e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20856738027948624
running average episode reward sum: 0.625688299521353
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([111.39750261,  14.89575577]), 'previousTarget': array([111.44464398,  15.15981392]), 'currentState': array([91.40587066, 14.3172648 ,  3.77117095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:514
target Thresh 35.09006984610713
target distance 3.0
model initialize at round 514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.53713171,  17.92024282,   5.52967561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.868443542782073}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6263386057552921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54775266,  15.39826108,   3.74850228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6772332438914347}
episode index:515
target Thresh 35.13095932811256
target distance 8.0
model initialize at round 515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.15833424,   8.52187891,   2.98795915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.5808655274810555}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6269677752206888
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26657308,  14.71921142,   1.24932104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7853389533781353}
episode index:516
target Thresh 35.171807941073915
target distance 30.0
model initialize at round 516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.35911121,  17.03581269]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([86.89210595, 21.62228258,  0.27016872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14298233720703285
running average episode reward sum: 0.6254785100128982
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([111.22126637,  14.75037362]), 'previousTarget': array([109.76169962,  14.47388595]), 'currentState': array([91.26476442, 13.43202999,  1.58280872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:517
target Thresh 35.212615725839804
target distance 23.0
model initialize at round 517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.55635087,  16.31346799]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([9.28694891e+01, 2.34409638e+01, 1.78747177e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5655406949995692
running average episode reward sum: 0.625362799945305
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46689822,  15.82388697,   5.88426165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.981319130643476}
episode index:518
target Thresh 35.25338272321803
target distance 9.0
model initialize at round 518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.85371802,   7.78727413,   1.36855435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.860453294835466}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6259537489779864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45921339,  15.09236261,   0.55722559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.548617360932243}
episode index:519
target Thresh 35.29410897397557
target distance 9.0
model initialize at round 519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.75156993,  11.72619536,   0.79439443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.953460587831739}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6265788187874518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63667727,  14.31713421,   0.36116897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.773504424871966}
episode index:520
target Thresh 35.334794518838706
target distance 11.0
model initialize at round 520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.99332581,   3.02025811,   3.02581871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.021963590380809}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6271472753625775
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45152054,  14.6460964 ,   1.0338264 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6527461062539537}
episode index:521
target Thresh 35.37543939849297
target distance 32.0
model initialize at round 521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.03426248,  17.58018318]), 'previousTarget': array([102.53800035,  17.72606242]), 'currentState': array([84.56592293, 22.16098608,  5.37563128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3634714089640607
running average episode reward sum: 0.625249538419423
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([110.84763946,  14.73683312]), 'previousTarget': array([109.05953341,  14.73874854]), 'currentState': array([90.88768613, 13.47181807,  5.38643157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:522
target Thresh 35.416043653583245
target distance 12.0
model initialize at round 522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.60526014,  22.60150782,   5.68530995]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.87763712310947}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6257832526385232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17823121,  14.60913915,   0.42962742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9099868955546374}
episode index:523
target Thresh 35.45660732471379
target distance 2.0
model initialize at round 523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.56262949,  12.69632774,   3.06181514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7153157937752326}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6263857276323447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31832466,  15.48383782,   5.73385898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8359308047988314}
episode index:524
target Thresh 35.49713045244828
target distance 26.0
model initialize at round 524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.14083259,  15.25862085]), 'previousTarget': array([108.86817872,  15.70751784]), 'currentState': array([89.16028714, 16.1405526 ,  4.61986563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17729068701497377
running average episode reward sum: 0.6248549154139688
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.28141194,  15.56340483]), 'previousTarget': array([109.77322762,  15.9229529 ]), 'currentState': array([91.50708799, 18.55942088,  4.80461239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:525
target Thresh 35.53761307730984
target distance 35.0
model initialize at round 525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.98127283, 18.44725178]), 'previousTarget': array([99.61161351, 18.0776773 ]), 'currentState': array([78.37935504, 22.4177453 ,  2.07519579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2857791268097644
running average episode reward sum: 0.6231236719876879
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.41238573, 10.70361769,  5.22955329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.077691296888784}
episode index:526
target Thresh 35.57805523978112
target distance 24.0
model initialize at round 526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.22771281,  15.60318228]), 'previousTarget': array([110.57960839,  15.92091492]), 'currentState': array([92.68493381, 19.85521581,  5.20124746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7077805426839268
running average episode reward sum: 0.6232843112110205
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45594779,  14.89102806,   6.27097917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5548582592917999}
episode index:527
target Thresh 35.61845698030427
target distance 4.0
model initialize at round 527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.94137019,  15.69215979,   6.16261971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.13596898592058}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6239600985003935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17248613,  15.04744266,   6.18117931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8288727397129644}
episode index:528
target Thresh 35.65881833928103
target distance 7.0
model initialize at round 528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.88899375,  23.34888433,   1.45806277]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.422482085474101}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6244394197187273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50021086,  14.72488495,   0.31015249]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5705063291584377}
episode index:529
target Thresh 35.69913935707277
target distance 8.0
model initialize at round 529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.87242011,  12.56080883,   6.26734591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.533395559361249}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6250555529832202
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43347414,  14.58339665,   0.60403861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7032139838118022}
episode index:530
target Thresh 35.7394200740005
target distance 7.0
model initialize at round 530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80124617,   9.60303699,   0.6841526 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.456116309732287}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6256874559154553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9312911 ,  14.18257277,   2.04746605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8203098158601618}
episode index:531
target Thresh 35.779660530344955
target distance 23.0
model initialize at round 531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.18673811,  15.930207  ]), 'previousTarget': array([110.88993593,  16.4295875 ]), 'currentState': array([93.19784543, 22.20889754,  4.76025534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6259443267019809
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2954196 ,  14.24126451,   2.07295967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.035428938042421}
episode index:532
target Thresh 35.8198607663466
target distance 6.0
model initialize at round 532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.65530683,   8.00774817,   3.60741949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.4794545269949815}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6265363263693337
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12057188,  14.28628812,   1.75664377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.13259810828128}
episode index:533
target Thresh 35.86002082220565
target distance 22.0
model initialize at round 533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.22801135,  14.42112167]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([92.65035222, 10.33269127,  3.53071856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.626805065731573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14222804e+02, 1.42012942e+01, 9.95415846e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1144345048071609}
episode index:534
target Thresh 35.90014073808217
target distance 12.0
model initialize at round 534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.61441756,   3.64624417,   6.15321088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.544541317058934}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6273409763516703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2551791 ,  14.56605731,   1.25794598]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8620118527771982}
episode index:535
target Thresh 35.94022055409611
target distance 34.0
model initialize at round 535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.87777729,  17.79020013]), 'previousTarget': array([100.46834337,  18.41921333]), 'currentState': array([82.31512414, 21.94983973,  4.59934405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6273574773026259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01696264,  14.95131533,   5.94251154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9842421714101524}
episode index:536
target Thresh 35.980260310327246
target distance 25.0
model initialize at round 536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.11662061,  13.51897008]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([89.72169651,  8.63666334,  0.78397632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4191786576239133
running average episode reward sum: 0.6254086204405654
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([110.57763577,  16.51086946]), 'previousTarget': array([111.39979894,  15.97765794]), 'currentState': array([91.65167785, 22.97678911,  1.05401818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:537
target Thresh 36.020260046815366
target distance 9.0
model initialize at round 537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.99711262,   5.55291607,   6.19395459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.381179317791926}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6259612897230697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51893723,  15.1643043 ,   0.84953994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5083476134998742}
episode index:538
target Thresh 36.0602198035602
target distance 27.0
model initialize at round 538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.33583909,  15.34413832]), 'previousTarget': array([107.87767469,  15.79136948]), 'currentState': array([88.36245294, 16.37556714,  5.94191885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.31998221277212124
running average episode reward sum: 0.6253936105450532
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04571959,  14.94575048,   0.53440968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9558211680146477}
episode index:539
target Thresh 36.1001396205215
target distance 10.0
model initialize at round 539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.7647956 ,  20.39098818,   0.1293813 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.693537950657369}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.62587692769537
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14590184e+02, 1.47480136e+01, 3.55383644e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48108830293440674}
episode index:540
target Thresh 36.140019537619104
target distance 6.0
model initialize at round 540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.10299574,  20.4260816 ,   4.7934087 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.53705346836237}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.626309795397673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34872008,  14.84268956,   0.15329788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6700090419671804}
episode index:541
target Thresh 36.179859594732925
target distance 6.0
model initialize at round 541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.58404928,  13.59530413,   1.23469329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.567921600992557}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6269088364576403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45976371,  14.70693063,   0.39441724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6146095533102917}
episode index:542
target Thresh 36.21965983170302
target distance 25.0
model initialize at round 542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.67972148,  13.97241256]), 'previousTarget': array([109.85753677,  14.38290441]), 'currentState': array([88.93893496, 10.76284128,  2.99346304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5578066926189814
running average episode reward sum: 0.6267815765242357
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06203822,  15.19276627,   5.6362077 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9575652080016099}
episode index:543
target Thresh 36.25942028832963
target distance 11.0
model initialize at round 543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.55977285,   4.63205756,   5.6581871 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.369056246699303}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6272918715582146
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88907696,  14.33600482,   1.39345717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6731964970731857}
episode index:544
target Thresh 36.29914100437322
target distance 13.0
model initialize at round 544
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.47761225,  14.25508847]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.      ,   2.      ,   3.664645], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.790556161448436}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.627528103453078
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25885314,  14.61185432,   0.96692978]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8366335768636487}
episode index:545
target Thresh 36.338822019554506
target distance 4.0
model initialize at round 545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.98308073,  19.76695781,   1.23102218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.874219069281557}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6281205246004167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84980186,  15.80140135,   4.6149478 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8153548906936909}
episode index:546
target Thresh 36.3784633735545
target distance 8.0
model initialize at round 546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.27656484,  21.41672683,   3.9029758 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.644141171905494}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6287107796740905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69581537,  15.63257428,   5.04787019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7019106089958599}
episode index:547
target Thresh 36.41806510601458
target distance 33.0
model initialize at round 547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.40108503,  15.84655339]), 'previousTarget': array([101.96336993,  15.79009879]), 'currentState': array([83.45414208, 17.30239235,  5.54865522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6093211421596108
running average episode reward sum: 0.6264515973349779
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([109.04133408,  15.54194013]), 'previousTarget': array([109.68076604,  15.16154814]), 'currentState': array([89.12354327, 17.35346142,  0.90407015]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:548
target Thresh 36.457627256536455
target distance 35.0
model initialize at round 548
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.10386804,  8.91964744]), 'previousTarget': array([98.91891892,  9.48648649]), 'currentState': array([80.        ,  3.        ,  0.28677478], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.207907
running average episode reward sum: 0.6249318184691582
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([111.49933156,  14.7700199 ]), 'previousTarget': array([110.2628984 ,  14.50232081]), 'currentState': array([91.54235207, 13.45892514,  0.26840184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:549
target Thresh 36.49714986468229
target distance 27.0
model initialize at round 549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.73528956,  14.27991662]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([8.88661131e+01, 1.19960978e+01, 8.09881051e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6252240845097728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65214269,  15.28921756,   5.97744671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4523842465034942}
episode index:550
target Thresh 36.53663296997471
target distance 24.0
model initialize at round 550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.3118347 ,  15.44072704]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([90.39962906, 17.31264217,  3.55766356]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6251655407019538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19983344,  15.15831654,   5.7146028 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.815678034482534}
episode index:551
target Thresh 36.5760766118968
target distance 7.0
model initialize at round 551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.37877621,  22.53100416,   3.53811765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.703531027826682}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6257558024939792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08927139,  15.59790099,   5.70156732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0894549946787047}
episode index:552
target Thresh 36.61548082989223
target distance 27.0
model initialize at round 552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.9765325 ,  12.27013602]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([89.33510842,  5.02463339,  5.92776329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6258835735890166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0867337 ,  14.09670484,   0.35010168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2845222756270191}
episode index:553
target Thresh 36.654845663365194
target distance 28.0
model initialize at round 553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.53778825,  14.47550292]), 'previousTarget': array([107.,  15.]), 'currentState': array([86.57609447, 13.23825544,  5.24584949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.054704646250242225
running average episode reward sum: 0.6246550749972489
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.12759905e+02, 1.76196934e+01, 5.54693778e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.446856727033004}
episode index:554
target Thresh 36.69417115168054
target distance 34.0
model initialize at round 554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.29928086,  9.54188424]), 'previousTarget': array([100.02889037,  10.15640571]), 'currentState': array([79.28879122,  3.32889112,  2.80260181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1365312716371383
running average episode reward sum: 0.6232835680663762
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([107.10923944,  16.36886724]), 'previousTarget': array([106.16024002,  16.97737769]), 'currentState': array([87.40355637, 19.78735443,  4.24250101]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:555
target Thresh 36.73345733416378
target distance 10.0
model initialize at round 555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.69306058,  15.07825208,   5.33069241]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.307307985266824}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.623872968213559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49133821,  15.1980549 ,   6.26128893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5458594659478495}
episode index:556
target Thresh 36.77270425010107
target distance 31.0
model initialize at round 556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.01321234,  12.39827135]), 'previousTarget': array([103.50882004,  12.40521743]), 'currentState': array([85.65920163,  7.3562212 ,  5.07780216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.21505758705307204
running average episode reward sum: 0.6231390088218884
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31962524,  15.41509003,   5.18566132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7970003400268634}
episode index:557
target Thresh 36.81191193873934
target distance 11.0
model initialize at round 557
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.27997808,  16.5525012 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.      ,  14.      ,   4.204198], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.592129708472012}
done in step count: 6
reward sum = 0.871480149401
running average episode reward sum: 0.6235840646293779
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11935657,  15.59258216,   6.17479533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0614548803417445}
episode index:558
target Thresh 36.85108043928629
target distance 13.0
model initialize at round 558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.03395574,   3.68262744,   2.56061888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.498691128508966}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6241192357023628
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88546624,  14.35024763,   1.85469091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6597697493677227}
episode index:559
target Thresh 36.890209790910404
target distance 25.0
model initialize at round 559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.49738042,  15.91578785]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([89.7687377 , 19.19917881,  5.02877051]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.1716435317750422
running average episode reward sum: 0.6233112433739212
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16571093,  15.2123021 ,   6.14019683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8608777110368117}
episode index:560
target Thresh 36.92930003274106
target distance 1.0
model initialize at round 560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.91232657,  15.31044697,   1.51734847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1311104352252426}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6239472304623812
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4537509 ,  15.54643158,   5.89620656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7726419263916126}
episode index:561
target Thresh 36.968351203868494
target distance 8.0
model initialize at round 561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.3152891 ,  23.82712257,   0.50359905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.072644336017484}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6244624796029885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11267608,  15.83219375,   5.77825192]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.216507367816761}
episode index:562
target Thresh 37.00736334334388
target distance 27.0
model initialize at round 562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.56918817,  14.46378194]), 'previousTarget': array([107.98629668,  14.74023321]), 'currentState': array([88.63835436, 12.80189641,  6.21193409]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6248060754431204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66067007,  14.3509791 ,   0.6654447 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7323748616456836}
episode index:563
target Thresh 37.04633649017935
target distance 4.0
model initialize at round 563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29313703,  17.53028118,   4.80249667]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.5472047713513937}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6254360292100651
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10162599,  15.79409979,   4.1313338 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1990289202107376}
episode index:564
target Thresh 37.08527068334807
target distance 36.0
model initialize at round 564
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.93062447, 16.3356057 ]), 'previousTarget': array([98.93091516, 16.3390904 ]), 'currentState': array([79.        , 18.        ,  0.91043687], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6906098747815456
running average episode reward sum: 0.6231067444242393
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([113.68516052,  13.47128053]), 'previousTarget': array([113.84714678,  15.07015758]), 'currentState': array([93.88407827, 16.28502216,  0.5248662 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:565
target Thresh 37.12416596178423
target distance 7.0
model initialize at round 565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.97231377,   6.65353001,   3.15270817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.409500596562093}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6236860435505215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46572437,  14.24904432,   1.43612438]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9216207953214769}
episode index:566
target Thresh 37.16302236438312
target distance 26.0
model initialize at round 566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.83650335,  13.24449039]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([90.90095988,  6.80669624,  0.42482763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6239998787286318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34067723,  14.42397985,   0.95377647]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8755031265873978}
episode index:567
target Thresh 37.20183993000112
target distance 29.0
model initialize at round 567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.67457409,  17.06796547]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([87.26440039, 21.88928864,  4.55240518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3097761851041931
running average episode reward sum: 0.6223559067852642
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([110.87311475,  15.75495893]), 'previousTarget': array([111.19674877,  15.97442557]), 'currentState': array([91.19960112, 19.35396733,  3.36072053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:568
target Thresh 37.24061869745584
target distance 1.0
model initialize at round 568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61904655,  13.63592504,   3.4411906 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4162718775320036}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6229846310264149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05849793,  14.54895417,   1.38510048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0439676671688327}
episode index:569
target Thresh 37.27935870552601
target distance 9.0
model initialize at round 569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.69149822,  18.22124525,   5.44231326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.986902981666583}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6235433950937387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44917369,  14.66106683,   5.65946675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6467498104226548}
episode index:570
target Thresh 37.31805999295166
target distance 13.0
model initialize at round 570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.52741766,   2.89872241,   1.38174492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.362392944117948}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6240512302117596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15956703,  14.36964118,   1.27639579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.050561668101437}
episode index:571
target Thresh 37.35672259843408
target distance 6.0
model initialize at round 571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.41950723,  10.69466026,   1.46510523]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.286244051194269}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6246395952113893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38320577,  14.75583968,   0.84504328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6633621820160853}
episode index:572
target Thresh 37.395346560635886
target distance 37.0
model initialize at round 572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.87802957, 17.78838918]), 'previousTarget': array([97.81984861, 17.32164208]), 'currentState': array([79.17061865, 21.19690129,  6.07977182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6030681481116102
running average episode reward sum: 0.6246019487068523
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29285689,  14.72236752,   0.53018121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7596914991707473}
episode index:573
target Thresh 37.433931918181024
target distance 4.0
model initialize at round 573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.14121725,  19.28182486,   0.37131488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.148462190174159}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6251705690922061
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43307161,  15.91723304,   5.2281452 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0782969260992399}
episode index:574
target Thresh 37.47247870965488
target distance 28.0
model initialize at round 574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.54716982,  17.9708533 ]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([85.3090914 , 23.43860775,  2.68959516]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.528073725468937
running average episode reward sum: 0.6231649268407955
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([110.96230339,  18.28080664]), 'previousTarget': array([112.11090831,  16.9379302 ]), 'currentState': array([96.94962769, 21.01425396,  6.03438973]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.276792867692237}
episode index:575
target Thresh 37.51098697360423
target distance 16.0
model initialize at round 575
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.67375655,  13.96391776]), 'previousTarget': array([114.52228  ,  14.6118525]), 'currentState': array([99.      ,  2.      ,  5.956367], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.93289358738938}
done in step count: 99
reward sum = -0.419501325958141
running average episode reward sum: 0.6213547423741307
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([110.69325869,  16.18631719]), 'previousTarget': array([111.70837799,  15.72115948]), 'currentState': array([91.41139988, 21.49761844,  2.94726285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:576
target Thresh 37.549456748537345
target distance 7.0
model initialize at round 576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.40726344,  21.29308682,   4.80767399]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.419123702744695}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.621909552438302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83005248,  14.77140755,   6.08355572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2848449890806447}
episode index:577
target Thresh 37.587888072923995
target distance 5.0
model initialize at round 577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.25739611,  11.70485685,   0.48207545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.774968396985589}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6225123023475784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5227269 ,  14.57709066,   0.89775494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6376848139380992}
episode index:578
target Thresh 37.62628098519553
target distance 27.0
model initialize at round 578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.63326283,  17.54201116]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([86.33143474, 22.78027732,  4.28252292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = -0.18846484311537293
running average episode reward sum: 0.6211116509737219
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32226164,  15.1760138 ,   5.80990203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7002214938028252}
episode index:579
target Thresh 37.664635523744856
target distance 31.0
model initialize at round 579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.19117729,  16.92103032]), 'previousTarget': array([103.50882004,  17.59478257]), 'currentState': array([84.4997582 , 20.42074663,  4.03911495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.68875613103009
running average episode reward sum: 0.6212282793876123
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53701003,  14.2503888 ,   1.17479857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.881065636405032}
episode index:580
target Thresh 37.7029517269265
target distance 21.0
model initialize at round 580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.66705283,  15.02602458]), 'previousTarget': array([113.97736275,  14.95130299]), 'currentState': array([92.6682971 , 15.24911542,  2.25688916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = -0.343156410969784
running average episode reward sum: 0.6195684090083396
{'dynamicTrap': 19, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43999896,  14.68490003,   0.49013025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6425645119696177}
episode index:581
target Thresh 37.74122963305668
target distance 25.0
model initialize at round 581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.54062239,  14.06049952]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([89.83035015, 10.66857256,  0.66212177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.09578492454274434
running average episode reward sum: 0.6186684373855464
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52153665,  14.47537751,   0.59903398]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7100393874703574}
episode index:582
target Thresh 37.77946928041331
target distance 9.0
model initialize at round 582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.04299647,   7.67137027,   1.31604803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.444294830658478}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6192059964087393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24170747,  14.1418491 ,   1.0041489 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.145177073831461}
episode index:583
target Thresh 37.81767070723603
target distance 17.0
model initialize at round 583
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.46395206,  13.34953104]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 13.       ,  0.9076884], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.467449546335494}
done in step count: 77
reward sum = -0.27936112971388355
running average episode reward sum: 0.6176673540694881
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20178099,  15.42077876,   5.87787576]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9023349452278403}
episode index:584
target Thresh 37.855833951726275
target distance 9.0
model initialize at round 584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10090952,   7.36938806,   1.1580199 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.683397837106153}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6181888537965966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2526542 ,  14.94416153,   0.97665831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7494289036508666}
episode index:585
target Thresh 37.8939590520473
target distance 25.0
model initialize at round 585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.07246511,  13.57923088]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([88.48026432,  9.56106361,  1.81117201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5821820756584196
running average episode reward sum: 0.6181274087827089
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23725638,  14.56687435,   5.72674152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8771406157088386}
episode index:586
target Thresh 37.9320460463242
target distance 5.0
model initialize at round 586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.03029837,   9.96499264,   2.18404913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.845547616194039}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6187108305905747
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70037114,  14.11150677,   0.61484879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.937655410314004}
episode index:587
target Thresh 37.97009497264397
target distance 1.0
model initialize at round 587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20093662,  15.93300214,   3.44239318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9543943195188409}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6193592815589581
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20093662,  15.93300214,   3.44239318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9543943195188409}
episode index:588
target Thresh 38.00810586905555
target distance 34.0
model initialize at round 588
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.91762522,  14.81334104]), 'previousTarget': array([100.96548746,  14.17444044]), 'currentState': array([81.     , 13.     ,  3.00525], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 34
reward sum = 0.2109196004534366
running average episode reward sum: 0.6186658355808503
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38690064,  14.14530564,   1.41126231]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0518523104350705}
episode index:589
target Thresh 38.046078773569825
target distance 27.0
model initialize at round 589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.62078513,  14.90625093]), 'previousTarget': array([108.,  15.]), 'currentState': array([89.62382179, 14.55774344,  5.00727946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 21
reward sum = 0.6185028708190426
running average episode reward sum: 0.6186655593693896
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23701087,  14.7886568 ,   5.55543413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7917186061829303}
episode index:590
target Thresh 38.08401372415972
target distance 13.0
model initialize at round 590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.60483912,   4.54342901,   2.04266629]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.216654709495472}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6191490052503362
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26506262,  14.75461596,   0.97457284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7748201601435319}
episode index:591
target Thresh 38.12191075876017
target distance 38.0
model initialize at round 591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.10379123, 10.7335306 ]), 'previousTarget': array([96.46160575, 10.60932768]), 'currentState': array([78.7124497 ,  5.83701409,  5.24234444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6343662770294444
running average episode reward sum: 0.6191747101013145
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53154038,  14.89309049,   6.16092856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4805039628443808}
episode index:592
target Thresh 38.15976991526823
target distance 9.0
model initialize at round 592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.97916973,   7.71839039,   0.79168546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.556824298011925}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6197342637940609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78012014,  14.16432375,   1.46483335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8641191765402602}
episode index:593
target Thresh 38.19759123154304
target distance 24.0
model initialize at round 593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.02918673,  14.27111535]), 'previousTarget': array([110.40285  ,  13.8507125]), 'currentState': array([92.60526427,  9.50548462,  5.57824773]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.32563142947723167
running average episode reward sum: 0.618142739057914
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([108.28748336,  15.63761606]), 'previousTarget': array([107.9802322,  16.1104745]), 'currentState': array([88.37710665, 17.52888527,  3.8426485 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:594
target Thresh 38.23537474540595
target distance 9.0
model initialize at round 594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.91855546,   7.71465008,   2.57056612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.910222748695588}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.618686163276978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4825124 ,  14.98309828,   0.50148007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5177635424132381}
episode index:595
target Thresh 38.27312049464044
target distance 34.0
model initialize at round 595
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.74406219,  14.18935862]), 'previousTarget': array([100.86301209,  13.33682495]), 'currentState': array([81.      , 11.      ,  4.387289], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4066067984674681
running average episode reward sum: 0.6169658730727088
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([108.95888476,  16.96959246]), 'previousTarget': array([110.388992  ,  16.26782834]), 'currentState': array([89.94397588, 23.16904732,  1.92577928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:596
target Thresh 38.3108285169923
target distance 31.0
model initialize at round 596
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.782981 ,  14.9383095]), 'previousTarget': array([103.90700027,  13.9264839 ]), 'currentState': array([84.      , 12.      ,  4.319172], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 81
reward sum = 0.15658625422579858
running average episode reward sum: 0.6161947179322617
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25668957,  14.47798908,   6.05391234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9082982986205604}
episode index:597
target Thresh 38.34849885016952
target distance 5.0
model initialize at round 597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.66799646,  18.94902778,   4.62772191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.166920552681724}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6167868655611375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06532538,  15.85413499,   5.76418328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2661608166174352}
episode index:598
target Thresh 38.38613153184245
target distance 8.0
model initialize at round 598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.79768191,  21.82236708,   4.92661905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.927500372205214}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6173448007603677
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92878377,  15.77224058,   5.36821198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7755174213466756}
episode index:599
target Thresh 38.423726599643786
target distance 2.0
model initialize at round 599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.256144  ,  17.51927382,   1.22652073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3818524967473684}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6178850263414355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39451248,  15.92369358,   4.15101363]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0044152123558756}
episode index:600
target Thresh 38.461284091168594
target distance 29.0
model initialize at round 600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.50657491,  14.65430266]), 'previousTarget': array([105.98811999,  14.68924552]), 'currentState': array([87.52782392, 13.7326145 ,  5.21592349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.50522972193813
running average episode reward sum: 0.6176975799114799
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17362176,  15.58260129,   5.84440082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0111010114579757}
episode index:601
target Thresh 38.49880404397436
target distance 4.0
model initialize at round 601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20690014,  19.43379924,   0.74220419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.438624036729282}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6182354247112963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09042679,  15.76497729,   4.91900281]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7703033536249206}
episode index:602
target Thresh 38.53628649558104
target distance 12.0
model initialize at round 602
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.57587566,  14.35142688]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  14.       ,   5.7218604], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.581712850766746}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.6186243289728497
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11820001,  15.53100992,   5.96017824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0293409288523079}
episode index:603
target Thresh 38.5737314834711
target distance 27.0
model initialize at round 603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.22194898,  14.83887296]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([87.22623896, 14.42464971,  1.18910575]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6188125393051038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0958126 ,  14.2223332 ,   5.54001701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1926107932240897}
episode index:604
target Thresh 38.611139045089516
target distance 25.0
model initialize at round 604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.04368519,  14.93587931]), 'previousTarget': array([109.93630557,  14.59490445]), 'currentState': array([91.0463114 , 14.61177834,  1.48773843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6187124626096373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02865774,  14.70671986,   0.49617166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0146521699795634}
episode index:605
target Thresh 38.64850921784388
target distance 13.0
model initialize at round 605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.99408041,   3.35740734,   1.20955825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.719257848970164}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6191689408136787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.155287  ,  15.01914609,   0.90702206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8449299531784502}
episode index:606
target Thresh 38.68584203910434
target distance 29.0
model initialize at round 606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.82789187,  15.74929174]), 'previousTarget': array([105.81242258,  16.26725206]), 'currentState': array([86.91143406, 17.57541017,  6.2776686 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.16389195714249433
running average episode reward sum: 0.6178788899109502
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([114.06066518,  13.59531672]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.54036865,  16.90962276,   1.81887032]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.951542368244429}
episode index:607
target Thresh 38.723137546203716
target distance 9.0
model initialize at round 607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.51081402,   5.4527725 ,   2.75465512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.775595157686775}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.61833523754968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00522346,  14.65888571,   1.07212752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.051636501412433}
episode index:608
target Thresh 38.76039577643755
target distance 26.0
model initialize at round 608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.3098721 ,  14.85498638]), 'previousTarget': array([109.,  15.]), 'currentState': array([90.31942504, 14.23690367,  0.48564785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6183233853361877
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14252260e+02, 1.46211162e+01, 6.86291277e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8382525490864995}
episode index:609
target Thresh 38.79761676706405
target distance 30.0
model initialize at round 609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.89216936,  16.91803699]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([86.32143316, 21.03947902,  0.3814618 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6495446747932083
running average episode reward sum: 0.6183745677779205
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41276936,  14.87194343,   5.65134598]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6010310357415556}
episode index:610
target Thresh 38.83480055530422
target distance 23.0
model initialize at round 610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.64517001,  12.72005125]), 'previousTarget': array([109.41125677,  11.84114513]), 'currentState': array([9.29266120e+01, 3.44359172e+00, 4.57503796e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6187283144124085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09652826,  14.07064885,   1.89702159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2961306789752285}
episode index:611
target Thresh 38.87194717834184
target distance 7.0
model initialize at round 611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.59122013,   6.77698381,   3.57822078]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.761189316428107}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6192099956755968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5995647 ,  14.79084305,   0.57236606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45176880925798246}
episode index:612
target Thresh 38.90905667332355
target distance 24.0
model initialize at round 612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.89956893,  14.20109618]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([92.53219886,  9.21060757,  0.46417874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.29178997466158707
running average episode reward sum: 0.6186758683982492
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2858256 ,  15.22075667,   6.08668938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.747514936140547}
episode index:613
target Thresh 38.94612907735884
target distance 5.0
model initialize at round 613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.80911547,  14.66340713,   6.26673794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.204379605533801}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6192485445083498
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34922483,  15.30595284,   0.3730621 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7191074124463231}
episode index:614
target Thresh 38.98316442752012
target distance 4.0
model initialize at round 614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32835501,  19.51603175,   0.64316821]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.527953156434887}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6198035810376046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5684196 ,  15.93533566,   4.07365728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0945106871514168}
episode index:615
target Thresh 39.020162760842744
target distance 4.0
model initialize at round 615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33108053,  17.28186905,   4.26956344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3778939486828574}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6203884778216344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56901248,  15.20233953,   4.9736299 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4761213359199722}
episode index:616
target Thresh 39.057124114325056
target distance 23.0
model initialize at round 616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.81337286,  12.71668478]), 'previousTarget': array([109.41125677,  11.84114513]), 'currentState': array([93.25493326,  3.14061049,  1.72595375]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5832199221402049
running average episode reward sum: 0.6203282370506759
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14374071e+02, 1.45840533e+01, 4.75563447e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7515307116293016}
episode index:617
target Thresh 39.09404852492838
target distance 18.0
model initialize at round 617
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.71834945,  13.4419832 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 10.       ,  1.0677526], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.032179449863616}
done in step count: 99
reward sum = -0.5259407672367393
running average episode reward sum: 0.6184734328366185
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04293919,  16.35404708,   5.43414788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6581341618710228}
episode index:618
target Thresh 39.13093602957718
target distance 6.0
model initialize at round 618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29606168,  19.90145297,   5.01711011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.91038631638943}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6189952530572395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76617284,  15.84367587,   5.21229324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8754793641375506}
episode index:619
target Thresh 39.16778666515892
target distance 30.0
model initialize at round 619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.99926969,  11.03288368]), 'previousTarget': array([103.77752632,  10.88509298]), 'currentState': array([86.40854587,  3.65826587,  5.03516937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6191429272091992
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07578835,  15.14567438,   0.23333502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9356218242483647}
episode index:620
target Thresh 39.20460046852427
target distance 4.0
model initialize at round 620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45306755,  11.96834566,   2.98681164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.0805946110317617}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6196927711428398
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38739045,  14.16352089,   0.70783716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0368161645815093}
episode index:621
target Thresh 39.241377476487
target distance 28.0
model initialize at round 621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.32714259,  13.7195857 ]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([87.59993357, 10.42759172,  4.06687927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.4934236399560181
running average episode reward sum: 0.6194897661087774
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15658258,  15.25714063,   5.79796378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8817450027639593}
episode index:622
target Thresh 39.27811772582417
target distance 21.0
model initialize at round 622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.10290029,  14.16505748]), 'previousTarget': array([113.45612429,  14.63241055]), 'currentState': array([92.88508767,  8.62649465,  3.22862637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.315201102849999
running average episode reward sum: 0.6190013412881372
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29963031,  14.30184359,   0.46337025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9889085249326428}
episode index:623
target Thresh 39.314821253275994
target distance 12.0
model initialize at round 623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.40449407,  21.29037999,   5.82394242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.19183983724367}
done in step count: 10
reward sum = 0.8357750750088044
running average episode reward sum: 0.6193487350921768
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42391192,  15.14943309,   5.60948166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5951535283951833}
episode index:624
target Thresh 39.351488095546
target distance 10.0
model initialize at round 624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.57495528,  21.59321492,   5.63340623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.698217681151304}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6198194047120031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18367606,  14.54364177,   0.43366142]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9352259670782189}
episode index:625
target Thresh 39.38811828930106
target distance 28.0
model initialize at round 625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.68977787,  16.07868623]), 'previousTarget': array([106.949174,  15.575059]), 'currentState': array([86.85616472, 18.6531355 ,  0.72333717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.6989156712611155
running average episode reward sum: 0.6199457565755001
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04904217,  14.8460151 ,   5.68577364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9633442523216313}
episode index:626
target Thresh 39.42471187117137
target distance 18.0
model initialize at round 626
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.34099963,  15.28302976]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.        , 10.        ,  0.82102424], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.17377862280773}
done in step count: 61
reward sum = -0.027565835469357536
running average episode reward sum: 0.6189130427125896
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79670474,  14.21659897,   0.42571773]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8093492091940929}
episode index:627
target Thresh 39.461268877750484
target distance 13.0
model initialize at round 627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56601636,   2.32252618,   1.91035795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.690103107424047}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6193532102468987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1199273 ,  14.19232171,   1.28371415]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.194517549686094}
episode index:628
target Thresh 39.497789345595436
target distance 29.0
model initialize at round 628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.92156373,  10.99577821]), 'previousTarget': array([104.25018649,  10.18111808]), 'currentState': array([86.33481274,  3.61115312,  2.32508942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.619421459580344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17476553,  15.08429583,   5.75623759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8295286150462305}
episode index:629
target Thresh 39.5342733112267
target distance 6.0
model initialize at round 629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.57869679,  12.48815876,   0.21772134]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.9749372299802195}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6199784080572006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02318691,  14.24275017,   0.68363374]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2359575709259056}
episode index:630
target Thresh 39.57072081112823
target distance 7.0
model initialize at round 630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.03041321,  18.81361122,   0.52343321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.944732257518541}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6205029906908659
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4530242 ,  15.83378479,   5.820833  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.997185843532755}
episode index:631
target Thresh 39.607131881747534
target distance 25.0
model initialize at round 631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.39925429,  15.20787779]), 'previousTarget': array([110.,  15.]), 'currentState': array([91.43250089, 16.36059584,  0.48628914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.24184522098120081
running average episode reward sum: 0.6191385156723974
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([112.42321752,  13.43988012]), 'currentState': array([95.76709748, 19.98176767,  2.58715207]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.867625638596742}
episode index:632
target Thresh 39.64350655949569
target distance 19.0
model initialize at round 632
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.55669937,  14.56161813]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([96.      ,  6.      ,  5.883726], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.533023259357627}
done in step count: 60
reward sum = 0.012442212396544527
running average episode reward sum: 0.6181800696956583
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22403857,  15.30949033,   5.34843901]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8354043370879282}
episode index:633
target Thresh 39.679844880747375
target distance 27.0
model initialize at round 633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.44993762,  15.77394111]), 'previousTarget': array([107.87767469,  15.79136948]), 'currentState': array([89.64160228, 18.53615964,  1.12749129]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10107307916842076
running average episode reward sum: 0.6170456010065982
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([113.27005201,  14.99684309]), 'currentState': array([99.83311136, 18.65251582,  1.94189476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.600493033278987}
episode index:634
target Thresh 39.71614688184092
target distance 8.0
model initialize at round 634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.52142784,  22.28052421,   4.83145577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.429145874326204}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6175565215552508
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15356853,  15.51089878,   5.44309876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9886676861871887}
episode index:635
target Thresh 39.75241259907831
target distance 34.0
model initialize at round 635
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.97595569,  14.01959483]), 'previousTarget': array([101.,  15.]), 'currentState': array([81.       , 15.       ,  2.3725462], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5712532272722921
running average episode reward sum: 0.6174837176334222
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40041781,  14.21981638,   1.16991293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9839640659870461}
episode index:636
target Thresh 39.788642068725295
target distance 39.0
model initialize at round 636
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.79940431, 13.82552455]), 'previousTarget': array([95.89562877, 13.04057731]), 'currentState': array([76.       , 11.       ,  5.5070033], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07181028530306288
running average episode reward sum: 0.6164016234372895
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15052999,  14.13436441,   6.11707714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2128166713021196}
episode index:637
target Thresh 39.82483532701132
target distance 11.0
model initialize at round 637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.76678275,   5.40459793,   1.86846963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.64348081994141}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6168673219075816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95074461,  14.39076678,   1.08863154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6112210794950304}
episode index:638
target Thresh 39.86099241012966
target distance 11.0
model initialize at round 638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1982118 ,   5.18181071,   6.05169243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.850873321845661}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6173460032417294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1185997 ,  14.32591843,   1.92063229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6844354259147335}
episode index:639
target Thresh 39.89711335423739
target distance 17.0
model initialize at round 639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.9695584 ,  15.01448071]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.90882737, 23.60575505,  1.94130021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = -0.04956582287389644
running average episode reward sum: 0.6163039535134238
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44438685,  14.55972588,   5.55645178]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7089056880104461}
episode index:640
target Thresh 39.93319819545548
target distance 24.0
model initialize at round 640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.45790825,  15.1101941 ]), 'previousTarget': array([110.98266146,  14.83261089]), 'currentState': array([90.46379145, 15.59526439,  0.88505745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6165559307456948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12839505,  15.05201546,   5.98316589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8731556576822526}
episode index:641
target Thresh 39.969246969868756
target distance 6.0
model initialize at round 641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.29240349,  19.42461646,   3.35639071]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.609505138496994}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6171069324112001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62735466,  15.9025438 ,   4.29445821]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.976447574216349}
episode index:642
target Thresh 40.00525971352599
target distance 9.0
model initialize at round 642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.18666063,   9.52661371,   5.21799588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.23652913489344}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6175967588738686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30882987,  14.27699839,   0.76657343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.000223710473108}
episode index:643
target Thresh 40.04123646243995
target distance 8.0
model initialize at round 643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.70712042,  21.34271015,   3.52747297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.744425018747849}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6181144503195614
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03311532,  15.43614284,   5.61398149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0607009736153286}
episode index:644
target Thresh 40.07717725258737
target distance 2.0
model initialize at round 644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.13640841e+02, 1.45084585e+01, 7.26000071e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4453117628490457}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6186910170632519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66235218,  14.50198655,   6.19631941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6016838368321423}
episode index:645
target Thresh 40.113082119909045
target distance 4.0
model initialize at round 645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.66580065,  11.37787955,   0.75842517]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9868092922566647}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6192353018665596
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70126485,  14.48255106,   2.28848934]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5974914973803344}
episode index:646
target Thresh 40.14895110030986
target distance 27.0
model initialize at round 646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.49801962,  11.45536174]), 'previousTarget': array([106.0200334 ,  10.67631238]), 'currentState': array([89.41494247,  2.9112222 ,  5.84533066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.37715225459440826
running average episode reward sum: 0.6188611395060154
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49717541,  15.67496677,   5.63777216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8416725713426798}
episode index:647
target Thresh 40.184784229658774
target distance 13.0
model initialize at round 647
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.36770203,  15.40986758]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,   9.      ,   5.047173], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.050327656351714}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.6191937335422852
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52904016,  14.2136339 ,   0.82678054]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9166105018262517}
episode index:648
target Thresh 40.220581543788946
target distance 25.0
model initialize at round 648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.6817037 ,  11.99475747]), 'previousTarget': array([107.74433602,  11.22705473]), 'currentState': array([90.62064483,  3.40417235,  0.1446147 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.18387984943621027
running average episode reward sum: 0.6185229879581464
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16773369,  14.55452954,   0.81405205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.943986838380951}
episode index:649
target Thresh 40.25634307849768
target distance 35.0
model initialize at round 649
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.65128632,  9.22007747]), 'previousTarget': array([98.74850544,  8.96373059]), 'currentState': array([80.      ,  2.      ,  0.882147], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.013848426719094142
running average episode reward sum: 0.617592719402394
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14202251e+02, 1.50305720e+01, 9.05624817e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7983347744379621}
episode index:650
target Thresh 40.2920688695465
target distance 35.0
model initialize at round 650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.33487588,  16.9290377 ]), 'previousTarget': array([99.92693298, 16.29197717]), 'currentState': array([80.50568787, 19.53735189,  0.29815316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.44011847180841845
running average episode reward sum: 0.6159679710287983
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0975564 ,  13.79741649,   0.70877212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5035330222296692}
episode index:651
target Thresh 40.32775895266123
target distance 4.0
model initialize at round 651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.38258832,  14.4724375 ,   5.91123277]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.670049831657343}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6165264557358093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14079936e+02, 1.46220555e+01, 1.09867588e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9946653725241288}
episode index:652
target Thresh 40.36341336353192
target distance 7.0
model initialize at round 652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.54485659,   9.1488542 ,   1.08049255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.999656043964917}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6169953963769917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28692636,  14.42278029,   0.48271606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9174184454983387}
episode index:653
target Thresh 40.39903213781302
target distance 18.0
model initialize at round 653
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.40988077,  15.05236563]), 'previousTarget': array([114.48314552,  14.71285862]), 'currentState': array([97.     ,  5.     ,  0.95708], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.24407029812499}
done in step count: 24
reward sum = 0.32432289489550004
running average episode reward sum: 0.6165478849068365
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05913504,  14.02077141,   0.38759345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3579821464849526}
episode index:654
target Thresh 40.43461531112329
target distance 31.0
model initialize at round 654
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.78036131,   8.87735625]), 'previousTarget': array([102.44388764,   9.73453353]), 'currentState': array([84.       ,  2.       ,  2.9870582], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6771720943315962
running average episode reward sum: 0.6166404409517598
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.362708  ,  15.89001775,   5.88003904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0946564290613292}
episode index:655
target Thresh 40.470162919045904
target distance 1.0
model initialize at round 655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.93875971,  15.38542739,   4.98068851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1290638693580854}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6172095866210408
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02378169,  15.2187928 ,   5.39179683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0004361420391739}
episode index:656
target Thresh 40.50567499712847
target distance 8.0
model initialize at round 656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.80868015,  17.88380111,   0.88279736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.68412516112022}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6176888191344136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2214437 ,  15.25570974,   5.85620554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8194738473222617}
episode index:657
target Thresh 40.541151580883096
target distance 36.0
model initialize at round 657
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.62715808, 19.15621726]), 'previousTarget': array([98.5237412 , 18.66139084]), 'currentState': array([79.       , 23.       ,  2.5953948], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.7095759806061201
running average episode reward sum: 0.6156716993779691
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([89.8792745 , 14.24463997]), 'previousTarget': array([91.52337574, 14.47118889]), 'currentState': array([69.88830994, 13.64352772,  2.35768631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:658
target Thresh 40.57659270578633
target distance 24.0
model initialize at round 658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.4182674 ,  15.66012694]), 'previousTarget': array([110.93091516,  15.3390904 ]), 'currentState': array([89.55668459, 18.00906891,  3.40265226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.22934221999034815
running average episode reward sum: 0.6150854634456662
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07286963,  14.26983698,   0.91912884]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.180130822577035}
episode index:659
target Thresh 40.611998407279316
target distance 27.0
model initialize at round 659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.25057861,  11.12559657]), 'previousTarget': array([106.27623097,  11.12276932]), 'currentState': array([86.66441719,  3.73948774,  1.62592721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.05008663019445586
running average episode reward sum: 0.6142294046074067
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45086116,  14.51685944,   0.29349519]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7314220859981326}
episode index:660
target Thresh 40.64736872076777
target distance 38.0
model initialize at round 660
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.97070424, 15.91788535]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([77.       , 17.       ,  5.3248186], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.32498293415743085
running average episode reward sum: 0.6128085084821951
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([110.15875837,  14.20624914]), 'previousTarget': array([110.64678209,  13.98078199]), 'currentState': array([90.4222732 , 10.97033298,  2.96618924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:661
target Thresh 40.682703681622
target distance 35.0
model initialize at round 661
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.86506655, 14.31929537]), 'previousTarget': array([99.92693298, 13.70802283]), 'currentState': array([80.      , 12.      ,  2.963792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4285460183307411
running average episode reward sum: 0.6112354653903327
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([108.94213936,  13.26411493]), 'previousTarget': array([107.44978441,  12.71878567]), 'currentState': array([89.71591471,  7.75482406,  1.39857499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:662
target Thresh 40.71800332517696
target distance 2.0
model initialize at round 662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.56906026,  14.37948413,   0.41409567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5596885835986563}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6118067542811466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0395223 ,  14.56764354,   0.34614444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0533040962299107}
episode index:663
target Thresh 40.75326768673231
target distance 25.0
model initialize at round 663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.42017023,  13.51811747]), 'previousTarget': array([109.61161351,  13.9223227 ]), 'currentState': array([90.09024445,  8.38450576,  3.77107334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3364743615394107
running average episode reward sum: 0.6113920970631621
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07435897,  15.74781959,   6.0412906 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.189977080276182}
episode index:664
target Thresh 40.7884968015524
target distance 13.0
model initialize at round 664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.16956899,   6.76030046,   4.49503264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.417064411422475}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6118326834961632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07515953,  14.00061223,   0.63791606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.361655542324915}
episode index:665
target Thresh 40.82369070486638
target distance 28.0
model initialize at round 665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.13199125,  11.70674023]), 'previousTarget': array([105.61502987,  11.31304745]), 'currentState': array([88.68289981,  3.98462749,  5.26405102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.4080944113470034
running average episode reward sum: 0.6115267701746179
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13521334,  14.72696901,   6.03323709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9068637640105954}
episode index:666
target Thresh 40.85884943186811
target distance 2.0
model initialize at round 666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.48240614,  16.13291823,   6.1790517 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7287073824742616}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.612064659574656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99158625,  15.20913513,   3.42580653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0134006040576597}
episode index:667
target Thresh 40.893973017716355
target distance 29.0
model initialize at round 667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.71192996,  10.95535016]), 'previousTarget': array([104.25018649,  10.18111808]), 'currentState': array([87.37512208,  2.9702718 ,  5.88765282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1659825358041335
running average episode reward sum: 0.6108999182642086
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([111.97181722,  14.87635493]), 'previousTarget': array([110.81665136,  14.6957678 ]), 'currentState': array([91.98846847, 14.060406  ,  1.63960246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:668
target Thresh 40.92906149753468
target distance 30.0
model initialize at round 668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.45258719,  11.37678712]), 'previousTarget': array([103.77752632,  10.88509298]), 'currentState': array([84.36989116,  5.38924071,  1.05997133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.6107803452069455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1458626 ,  15.79418629,   5.6635126 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1663115207521393}
episode index:669
target Thresh 40.964114906411595
target distance 25.0
model initialize at round 669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.89279289,  16.05646736]), 'previousTarget': array([109.85753677,  15.61709559]), 'currentState': array([89.18548501, 19.46557519,  1.15617871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6407848507963383
running average episode reward sum: 0.6108251280511088
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45162768,  14.13042374,   0.39187181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0280442975049728}
episode index:670
target Thresh 40.999133279400496
target distance 26.0
model initialize at round 670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.25887367,  12.73576892]), 'previousTarget': array([107.89972147,  12.54221128]), 'currentState': array([90.65355323,  5.39805461,  0.52466338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.6223082482673468
running average episode reward sum: 0.610842241494054
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02829667,  14.5913352 ,   0.35662153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0541414932989512}
episode index:671
target Thresh 41.03411665151975
target distance 24.0
model initialize at round 671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.622661  ,  16.00722593]), 'previousTarget': array([110.2,  16.4]), 'currentState': array([91.13198335, 20.49203189,  5.80969048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.611175086017798
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04914258,  14.5331257 ,   0.39906503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0592929027385867}
episode index:672
target Thresh 41.06906505775275
target distance 21.0
model initialize at round 672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.88284245,  14.30390734]), 'previousTarget': array([113.45612429,  14.63241055]), 'currentState': array([92.36360832,  9.94506586,  4.09923625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5157527060002872
running average episode reward sum: 0.6110332994204466
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12287316,  14.12710658,   1.35858714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2374548146512032}
episode index:673
target Thresh 41.10397853304789
target distance 7.0
model initialize at round 673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.55112034,  19.37857753,   5.99945629]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.794869466087252}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6115376862905942
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09740076,  14.69855825,   0.35402967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9516052310430095}
episode index:674
target Thresh 41.13885711231866
target distance 36.0
model initialize at round 674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.09083733, 10.64571011]), 'previousTarget': array([98.5237412 , 11.33860916]), 'currentState': array([77.65698872,  5.92072033,  4.75961483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.30458502437792784
running average episode reward sum: 0.6110829416062791
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02110011,  15.67933757,   5.82335961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1915303331367901}
episode index:675
target Thresh 41.17370083044363
target distance 9.0
model initialize at round 675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25015969,   7.29952108,   1.08414197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.704541220086226}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6115439797021691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57782665,  14.17271499,   1.60032657]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9287792081641032}
episode index:676
target Thresh 41.20850972226655
target distance 9.0
model initialize at round 676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.49447852,  10.71909204,   5.34649563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.4250233412597}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6120036557948216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74846976,  14.22720127,   0.76804392]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.812702489397363}
episode index:677
target Thresh 41.24328382259627
target distance 22.0
model initialize at round 677
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.37149623,  14.02555191]), 'previousTarget': array([112.6773982 ,  15.42229124]), 'currentState': array([93.        , 19.        ,  0.32693925], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5989717585696803
running average episode reward sum: 0.611984434707469
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15635249,  14.76954832,   0.7741662 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8745565187522597}
episode index:678
target Thresh 41.27802316620692
target distance 5.0
model initialize at round 678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.26102937,  13.36237775,   1.36412445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.081875549101269}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6124697008557658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14173059e+02, 1.50734774e+01, 1.08428635e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8301992349304199}
episode index:679
target Thresh 41.312727787837844
target distance 16.0
model initialize at round 679
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.45495678,  15.34542928]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 10.       ,  0.7700446], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.411664078391356}
done in step count: 38
reward sum = 0.25118052146574127
running average episode reward sum: 0.6119383932390158
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25770947,  14.79812449,   0.42809017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.769252202096804}
episode index:680
target Thresh 41.347397722193676
target distance 4.0
model initialize at round 680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.32184168,  12.04102894,   5.97602874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9909950840203443}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6124503721182536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00747608,  14.03684573,   1.65088516]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3830292367020232}
episode index:681
target Thresh 41.38203300394433
target distance 26.0
model initialize at round 681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.65100949,  14.25723111]), 'previousTarget': array([108.94108971,  14.53392998]), 'currentState': array([89.84109012, 11.50639583,  4.30553782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.19290056662960195
running average episode reward sum: 0.6112695056391512
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([112.03536768,  15.52296351]), 'previousTarget': array([112.19715317,  15.28654675]), 'currentState': array([92.3394613 , 18.99733712,  2.30423085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:682
target Thresh 41.4166336677251
target distance 19.0
model initialize at round 682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.29463045,  14.59412916]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([96.95951431,  4.61947386,  4.30977511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5245199444951251
running average episode reward sum: 0.6096065635452503
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([111.1964252 ,  14.64076001]), 'previousTarget': array([112.53992026,  14.86783508]), 'currentState': array([91.28503702, 12.76016942,  2.8100227 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:683
target Thresh 41.451199748136666
target distance 1.0
model initialize at round 683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.2098994 ,  16.04930996,   6.03581357]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6015329966127563}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6101338916687222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70862398,  15.10414123,   3.83241284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7162355326128165}
episode index:684
target Thresh 41.485731279745096
target distance 12.0
model initialize at round 684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.32078517,   2.75331608,   2.30161011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.248904472123527}
done in step count: 13
reward sum = 0.8082210229989678
running average episode reward sum: 0.6104230699626351
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18279056,  14.88711883,   0.89599964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8249687456566991}
episode index:685
target Thresh 41.52022829708193
target distance 12.0
model initialize at round 685
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.75672332,  16.30081767]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  23.      ,   6.255219], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.672258697242967}
done in step count: 9
reward sum = 0.8435172474836408
running average episode reward sum: 0.6107628573934237
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17335509,  15.96440202,   5.75979549]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2702019790579941}
episode index:686
target Thresh 41.554690834644205
target distance 12.0
model initialize at round 686
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.58894778,  16.09896898]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  15.       ,   3.2970839], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.63331341292675}
done in step count: 16
reward sum = 0.5756298410948755
running average episode reward sum: 0.6107117176317083
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49325962,  14.20760265,   1.62928548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9405739601308423}
episode index:687
target Thresh 41.589118926894415
target distance 16.0
model initialize at round 687
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.11795782,  16.27927253]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 23.       ,  3.0307016], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.3900151633364}
done in step count: 50
reward sum = 0.21973317212309074
running average episode reward sum: 0.6101434348620737
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17697428,  14.23828899,   0.9090106 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1214165153966626}
episode index:688
target Thresh 41.6235126082607
target distance 19.0
model initialize at round 688
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.55038354,  14.21693057]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 10.       ,  1.8852818], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5910495818227868
running average episode reward sum: 0.6101157224483736
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29906364,  14.36282709,   5.89883115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9472597874832659}
episode index:689
target Thresh 41.65787191313672
target distance 18.0
model initialize at round 689
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.05751733,  14.19391513]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 23.       ,  1.9663966], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.196510834436662}
done in step count: 70
reward sum = -0.12383300778868583
running average episode reward sum: 0.6090520286364358
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0198648 ,  14.858772  ,   6.17012825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9902577255239793}
episode index:690
target Thresh 41.6921968758818
target distance 19.0
model initialize at round 690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.32467493,  15.71986317]), 'previousTarget': array([114.76686234,  15.08589282]), 'currentState': array([94.94919571, 23.61554321,  1.67289317]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.16483269556911573
running average episode reward sum: 0.6084091641891604
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01962738,  15.85285998,   5.57561801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2994231809985584}
episode index:691
target Thresh 41.72648753082089
target distance 7.0
model initialize at round 691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.73533286,   9.11100826,   1.95709818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.973731801792035}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6088368707076859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45719554,  14.14425508,   1.7987034 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9702201409846634}
episode index:692
target Thresh 41.76074391224466
target distance 7.0
model initialize at round 692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.35260141,  12.93083762,   5.87593645]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.014527726729125}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6093305982389879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20126232,  14.6714567 ,   0.96765359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8636681000762307}
episode index:693
target Thresh 41.79496605440949
target distance 31.0
model initialize at round 693
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.12318981,  10.85692849]), 'previousTarget': array([103.03417236,  11.1400556 ]), 'currentState': array([84.       ,  5.       ,  5.8816943], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.4828512864372827
running average episode reward sum: 0.6091483513920114
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14107669e+02, 1.54130744e+01, 9.92921700e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9833034280183328}
episode index:694
target Thresh 41.829153991537524
target distance 28.0
model initialize at round 694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.98739672,  13.33049064]), 'previousTarget': array([106.40285  ,  12.8507125]), 'currentState': array([88.5311736 ,  8.69849516,  1.43798369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6094369550133485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03823949,  14.51926411,   1.54035052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0752163830861137}
episode index:695
target Thresh 41.8633077578167
target distance 28.0
model initialize at round 695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.45135401,  16.23849385]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([87.71522282, 19.47656822,  6.16132498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.33774345267696015
running average episode reward sum: 0.6090465907858537
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01073654,  15.15465283,   0.18578612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0012790259923774}
episode index:696
target Thresh 41.89742738740079
target distance 7.0
model initialize at round 696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.86571683,  20.29195076,   5.59196329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.706128937934896}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6095509658492886
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19525296,  15.96814456,   5.77956395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2589367269443972}
episode index:697
target Thresh 41.93151291440942
target distance 13.0
model initialize at round 697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.74741015,   2.28693793,   5.59334881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.156753823665815}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6099604032252333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15169947,  14.36143394,   1.61048943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0617817156569764}
episode index:698
target Thresh 41.965564372928135
target distance 1.0
model initialize at round 698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28487558,  13.72363906,   5.57500511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4630448349672187}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6103686691065401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43169523,  14.19965338,   1.60798448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9815931096594848}
episode index:699
target Thresh 41.999581797008375
target distance 26.0
model initialize at round 699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.25998264,  14.13507009]), 'previousTarget': array([108.76743395,  14.04114369]), 'currentState': array([90.58485937, 10.54487173,  5.12694067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5763602549018354
running average episode reward sum: 0.6103200856576761
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08203698,  15.21830433,   5.53920521]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9435639306043894}
episode index:700
target Thresh 42.03356522066758
target distance 12.0
model initialize at round 700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.64140094,  23.37171929,   5.4959057 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.31864325774912}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6107139013296569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14031774e+02, 1.49590189e+01, 9.13307965e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9690931188223038}
episode index:701
target Thresh 42.06751467788919
target distance 37.0
model initialize at round 701
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.81454669, 12.71730368]), 'previousTarget': array([97.81984861, 12.67835792]), 'currentState': array([78.       , 10.       ,  3.6735888], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4767760610018795
running average episode reward sum: 0.6091647703291846
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.54993815, 14.16837203,  5.02564072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.46879495868856}
episode index:702
target Thresh 42.10143020262264
target distance 9.0
model initialize at round 702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.24464845,   7.3582679 ,   3.08832133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.74242979945623}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6095340179002227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00568787,  15.00377351,   0.56632172]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.994319287177388}
episode index:703
target Thresh 42.135311828783465
target distance 18.0
model initialize at round 703
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.10135186,  16.32621842]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 15.      ,  4.893963], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.147336576752725}
done in step count: 30
reward sum = 0.5317933733882803
running average episode reward sum: 0.6094235908483591
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27849681,  14.11648149,   0.51241225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1406891795569742}
episode index:704
target Thresh 42.1691595902533
target distance 7.0
model initialize at round 704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.57489975,   6.54483074,   3.59584403]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.574426970535804}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6098812387307118
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75105569,  14.01986975,   2.12505245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0112509932498994}
episode index:705
target Thresh 42.20297352087991
target distance 41.0
model initialize at round 705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.27337308, 14.05289339]), 'previousTarget': array([93.97624702, 13.97445107]), 'currentState': array([72.29071761, 13.22013871,  3.95376527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2576462740310286
running average episode reward sum: 0.6086524462197178
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.61572933,  14.49696401,   1.06108881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4728375675705834}
episode index:706
target Thresh 42.23675365447722
target distance 27.0
model initialize at round 706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.47350388,  14.65278549]), 'previousTarget': array([107.98629668,  14.74023321]), 'currentState': array([89.51286003, 13.39871327,  0.63169354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.38994022547589413
running average episode reward sum: 0.6083430937151296
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34102569,  15.33588567,   6.01665369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7396393202456227}
episode index:707
target Thresh 42.27050002482537
target distance 34.0
model initialize at round 707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.1489107,  16.9850958]), 'previousTarget': array([100.86301209,  16.66317505]), 'currentState': array([82.3833308 , 20.03826652,  5.9978984 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15981360979132914
running average episode reward sum: 0.6072581266197815
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([109.40491546,  20.44184683]), 'previousTarget': array([110.27761127,  19.00282282]), 'currentState': array([97.51436024, 21.44551447,  0.38291589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.932839229196125}
episode index:708
target Thresh 42.30421266567073
target distance 24.0
model initialize at round 708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.04890046,  13.0233374 ]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([91.47448428,  5.60774144,  0.30537844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5179147695719818
running average episode reward sum: 0.6071321134222528
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38740876,  15.23743198,   6.0063928 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6569946533178572}
episode index:709
target Thresh 42.33789161072596
target distance 40.0
model initialize at round 709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.12564371, 18.45653081]), 'previousTarget': array([94.70060934, 18.55239336]), 'currentState': array([73.37075811, 21.57814819,  4.40495372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.619908778047533
running average episode reward sum: 0.6054038868145488
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.1260287 ,  15.77397037,   5.8065317 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.0275104378740183}
episode index:710
target Thresh 42.37153689366999
target distance 4.0
model initialize at round 710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.70684678,  11.14748655,   1.71339619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.76827742517941}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6058502170643567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12971459,  15.15269432,   0.76336442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8835792248403608}
episode index:711
target Thresh 42.40514854814811
target distance 7.0
model initialize at round 711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77718747,   9.59228433,   0.2660954 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.41230396771889}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6063083843829559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90392453,  14.01062792,   1.98879266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9940259651051966}
episode index:712
target Thresh 42.438726607771976
target distance 13.0
model initialize at round 712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.94762641,  20.39082431,   6.24588818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.203056309333407}
done in step count: 15
reward sum = 0.7914513546412885
running average episode reward sum: 0.6065680519429255
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12395889,  14.84182585,   0.31061947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8902062089832401}
episode index:713
target Thresh 42.47227110611965
target distance 26.0
model initialize at round 713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.08214791,  13.36926078]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([87.49329696,  9.33479446,  1.91294169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 80
reward sum = 0.08435338120323749
running average episode reward sum: 0.6058366588466515
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12033637,  15.78612321,   0.11994056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.179744806483868}
episode index:714
target Thresh 42.50578207673564
target distance 9.0
model initialize at round 714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.54793764,   4.90997036,   3.58998668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.193977794378494}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6062669813482416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03074835,  14.4106611 ,   1.44586322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1343584517472085}
episode index:715
target Thresh 42.539259553130925
target distance 3.0
model initialize at round 715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.36801218,  18.08764296,   5.67141444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.05720333529441}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6067351561639578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13195509,  15.20065478,   5.65221292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8909345099721853}
episode index:716
target Thresh 42.57270356878296
target distance 33.0
model initialize at round 716
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.93024423,  17.33105878]), 'previousTarget': array([101.85467564,  16.59337265]), 'currentState': array([82.     , 19.     ,  3.72149], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6194490858690778
running average episode reward sum: 0.6067528882834907
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0284701 ,  15.65967501,   6.23346739]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.174325962037328}
episode index:717
target Thresh 42.60611415713579
target distance 13.0
model initialize at round 717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.38073284,   2.93894704,   3.24867439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.139827907856768}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6071300026772449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00336156,  14.00265384,   1.6991044 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4099601234455068}
episode index:718
target Thresh 42.639491351599986
target distance 34.0
model initialize at round 718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.05728086,  17.86437361]), 'previousTarget': array([100.46834337,  18.41921333]), 'currentState': array([80.41490628, 21.62962535,  5.25298655]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1160674380670707
running average episode reward sum: 0.606124164790257
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([109.50088969,  15.52875252]), 'previousTarget': array([108.12032531,  15.43349141]), 'currentState': array([89.59270632, 17.44297166,  5.75638485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:719
target Thresh 42.672835185552756
target distance 6.0
model initialize at round 719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.22413181,  21.10881166,   5.36184115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.754127735500004}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6065258510256298
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14077581e+02, 1.53041632e+01, 5.90467453e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9712732927368907}
episode index:720
target Thresh 42.70614569233794
target distance 32.0
model initialize at round 720
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.86946159,  15.28133648]), 'previousTarget': array([102.96105157,  14.24756572]), 'currentState': array([83.       , 13.       ,  3.9901636], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.35059027300420487
running average episode reward sum: 0.6061708779631869
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05830295,  15.38077781,   0.52173162]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0157682236704761}
episode index:721
target Thresh 42.73942290526604
target distance 13.0
model initialize at round 721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.71542964,   3.33583908,   1.98917729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.241612126196037}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6065839128621421
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90955723,  14.27180858,   2.15904289]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7337865071507667}
episode index:722
target Thresh 42.77266685761426
target distance 10.0
model initialize at round 722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24298272,   5.1995966 ,   1.87379766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.803415090116724}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6070212030164517
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04178431,  14.92732063,   0.72672051]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.960968050378959}
episode index:723
target Thresh 42.805877582626586
target distance 3.0
model initialize at round 723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.30496617,  17.37417146,   0.13258159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5916427269976983}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6074831628871487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09413498,  15.326626  ,   5.88804087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9629517001025333}
episode index:724
target Thresh 42.83905511351373
target distance 17.0
model initialize at round 724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.33987945, 20.59151826,  3.66808105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.479865895073484}
done in step count: 53
reward sum = 0.4616226406697512
running average episode reward sum: 0.6072819759599523
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10219506,  15.53596593,   6.00250571]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0456161761247007}
episode index:725
target Thresh 42.8721994834532
target distance 28.0
model initialize at round 725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.08614738,  11.96326328]), 'previousTarget': array([105.61502987,  11.31304745]), 'currentState': array([88.41366538,  4.79818017,  1.52399653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6071841608762708
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04985227,  15.62776582,   5.10527165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.138802280380571}
episode index:726
target Thresh 42.905310725589416
target distance 22.0
model initialize at round 726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.72945716,  13.39959881]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([92.00135675,  6.38117627,  1.22325945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6074740133889543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08934611,  14.49147463,   0.96877339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0430189631002782}
episode index:727
target Thresh 42.93838887303359
target distance 2.0
model initialize at round 727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.09527239,  16.93970907,   4.91194538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2275755581091548}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6079723993595739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50003683,  15.91518332,   4.50257002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0428793529322327}
episode index:728
target Thresh 42.971433958863884
target distance 30.0
model initialize at round 728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.50082707,  13.78774099]), 'previousTarget': array([104.82455801,  13.6432744 ]), 'currentState': array([86.70121521, 10.96367115,  5.28347683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.5305069360811481
running average episode reward sum: 0.6078661367213319
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03057328,  15.15751735,   0.85278085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9821404549957058}
episode index:729
target Thresh 43.004446016125385
target distance 43.0
model initialize at round 729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.31688118, 10.37931163]), 'previousTarget': array([91.66260099, 10.65815832]), 'currentState': array([7.37560965e+01, 6.21089422e+00, 5.87357839e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.027489771985046343
running average episode reward sum: 0.6069957861614601
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.12399774,  15.14811373]), 'previousTarget': array([112.24518423,  15.31702229]), 'currentState': array([92.15046746, 16.17674782,  3.54539821]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:730
target Thresh 43.03742507783015
target distance 41.0
model initialize at round 730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.46943286, 18.22416994]), 'previousTarget': array([93.71472851, 18.63407074]), 'currentState': array([72.67112233, 21.05734841,  4.46977365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -1.066338789598248
running average episode reward sum: 0.6047066827746479
{'dynamicTrap': 26, 'scaleFactor': 20, 'currentTarget': array([91.2519256 , 15.81499459]), 'previousTarget': array([91.35357689, 14.93376082]), 'currentState': array([71.26369269, 16.50095765,  0.61616199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:731
target Thresh 43.07037117695725
target distance 13.0
model initialize at round 731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.49668786,  21.68664876,   0.62912315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.305542518805886}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6050914890436937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15081895,  15.07850341,   0.32100683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8528019910200983}
episode index:732
target Thresh 43.10328434645278
target distance 2.0
model initialize at round 732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.91678124,  14.89262185,   4.16429949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.085984291868392}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6054997981650649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06565814,  14.91696846,   0.88171378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.938023962240957}
episode index:733
target Thresh 43.136164619229916
target distance 33.0
model initialize at round 733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.04373664,  17.5626911 ]), 'previousTarget': array([101.6773982 ,  17.42229124]), 'currentState': array([80.33101949, 20.94038001,  4.01505721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6598987289297839
running average episode reward sum: 0.605573911149758
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06341324,  14.60448269,   1.05556668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0166753197135838}
episode index:734
target Thresh 43.16901202816893
target distance 5.0
model initialize at round 734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.89057204,  10.00044157,   2.14517164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.775031703337173}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6060181171861624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05661212,  14.09738774,   1.91378027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9043858781346953}
episode index:735
target Thresh 43.201826606117244
target distance 29.0
model initialize at round 735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.05612854,  12.93791522]), 'previousTarget': array([105.44164417,  12.69281066]), 'currentState': array([87.69770848,  7.9128209 ,  0.88481122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6671485322893623
running average episode reward sum: 0.6061011748153787
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50631081,  14.04629068,   1.84859989]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0739136271504055}
episode index:736
target Thresh 43.23460838588942
target distance 6.0
model initialize at round 736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.17142576,  10.7032607 ,   2.43106073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.340403542311594}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6065308132409046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71144356,  14.13197156,   2.44595094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9147339448341033}
episode index:737
target Thresh 43.26735740026726
target distance 9.0
model initialize at round 737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.64613954,  10.62503874,   5.11688948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.556842272768028}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6069467840190113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09814376,  14.89888155,   0.33723969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9075073691697313}
episode index:738
target Thresh 43.30007368199977
target distance 42.0
model initialize at round 738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.24926727,  9.93157159]), 'previousTarget': array([92.55604828, 10.19058177]), 'currentState': array([74.82042943,  5.18601995,  5.72854075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4766169086164169
running average episode reward sum: 0.605480527330736
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([109.46390355,  14.88886445]), 'previousTarget': array([107.88190509,  14.836773  ]), 'currentState': array([89.46793227, 14.48745106,  5.34214529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:739
target Thresh 43.33275726380324
target distance 33.0
model initialize at round 739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.94419011,  11.73429245]), 'previousTarget': array([101.43700211,  11.71200051]), 'currentState': array([83.63990203,  6.50509922,  0.66557818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.04152649118219587
running average episode reward sum: 0.6047184272818866
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10048245,  14.56465297,   0.24757925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9993292040806532}
episode index:740
target Thresh 43.36540817836125
target distance 26.0
model initialize at round 740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.5039447 ,  16.79350723]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([88.05293767, 21.44735787,  1.22116053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1111651975189882
running average episode reward sum: 0.6037523225250704
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([112.74527297,  14.44335922]), 'previousTarget': array([111.83984235,  14.0222285 ]), 'currentState': array([93.32823989,  9.6497357 ,  2.2120501 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:741
target Thresh 43.398026458324715
target distance 4.0
model initialize at round 741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.38113132,  16.76627793,   5.98487711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.026903061684345}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6040409406046825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0571608 ,  14.92194959,   0.29700026]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9460642823108746}
episode index:742
target Thresh 43.430612136311915
target distance 42.0
model initialize at round 742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.13911677, 17.51601472]), 'previousTarget': array([92.79898987, 18.17157288]), 'currentState': array([74.28301457, 19.91084359,  4.56932778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5960822143317
running average episode reward sum: 0.6024257008268408
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([110.29392048,  15.16688281]), 'previousTarget': array([109.36757616,  15.53394199]), 'currentState': array([90.30648354, 15.87565952,  4.96277624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:743
target Thresh 43.46316524490855
target distance 6.0
model initialize at round 743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.14979447,   8.81645321,   5.01185155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9395573412952425}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6028073663791114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74055618,  14.10334035,   1.74384016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9334396739278257}
episode index:744
target Thresh 43.49568581666772
target distance 17.0
model initialize at round 744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.83516329,  14.24651513]), 'previousTarget': array([113.88715666,  14.14900215]), 'currentState': array([97.04222825,  3.38385742,  1.16620255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.7560210313049917
running average episode reward sum: 0.600983435643965
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([112.74868805,  14.50074812]), 'previousTarget': array([111.54879618,  14.43727128]), 'currentState': array([93.22303924, 10.17073211,  4.77137246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:745
target Thresh 43.52817388410999
target distance 13.0
model initialize at round 745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59521393,   1.28544907,   2.73118043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.727461051763457}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6011793989934121
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80730052,  14.06328403,   1.72052917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9563314822203639}
episode index:746
target Thresh 43.56062947972343
target distance 36.0
model initialize at round 746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.25260743, 17.82887115]), 'previousTarget': array([98.5237412 , 18.66139084]), 'currentState': array([79.56770868, 21.36507831,  5.9385854 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.4157847834323073
running average episode reward sum: 0.600931213430412
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03015915,  14.25796169,   1.57547285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2211519660143}
episode index:747
target Thresh 43.593052635963666
target distance 7.0
model initialize at round 747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.59556476,  17.33926955,   5.43861115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.765168588053855}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6009285727793903
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08203941,  14.39550048,   1.11617396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0991229718173512}
episode index:748
target Thresh 43.62544338525383
target distance 8.0
model initialize at round 748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.73976578,  10.93818397,   5.33887673]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.46249837324477}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6013959445779492
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10312891,  14.14379689,   1.79622554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2399441603701886}
episode index:749
target Thresh 43.65780175998468
target distance 9.0
model initialize at round 749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.66548788,  15.75805332,   5.12892372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.373582089317628}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.60184939018438
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24099654,  14.21695205,   0.96259785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.090527551831749}
episode index:750
target Thresh 43.690127792514595
target distance 12.0
model initialize at round 750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49827046,   4.60644189,   0.86352038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.405494875625072}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.602170420548161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17283573,  14.07690929,   2.10774463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2394745574449888}
episode index:751
target Thresh 43.72242151516961
target distance 42.0
model initialize at round 751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.58731063, 15.67896814]), 'previousTarget': array([92.99433347, 15.52394444]), 'currentState': array([74.5983651 , 16.34384169,  1.19360452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.09379448837776339
running average episode reward sum: 0.6014943887234662
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24193476,  15.23647566,   0.45498847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7940929713971167}
episode index:752
target Thresh 43.75468296024345
target distance 33.0
model initialize at round 752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.74453057,  10.06994188]), 'previousTarget': array([100.60816786,   9.33049037]), 'currentState': array([81.84295424,  3.53309155,  0.66287756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 47
reward sum = 0.4517591548154146
running average episode reward sum: 0.6012955371512112
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15336597,  14.90262607,   0.46193026]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.852215262947122}
episode index:753
target Thresh 43.786912159997556
target distance 9.0
model initialize at round 753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.51148778,  18.25986976,   0.55465048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.167286336421945}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6016048451409975
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06920712,  15.43430933,   0.47357885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.027131919710831}
episode index:754
target Thresh 43.81910914666115
target distance 33.0
model initialize at round 754
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.99495542,  14.55082533]), 'previousTarget': array([102.,  15.]), 'currentState': array([82.       , 15.       ,  1.3027111], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4826834771623851
running average episode reward sum: 0.6014473333953304
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03423325,  14.9127192 ,   2.3185455 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9697027140580244}
episode index:755
target Thresh 43.8512739524312
target distance 5.0
model initialize at round 755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.55552398,  12.21458045,   5.44296438]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.429782967945942}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6019352324252307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00576929,  14.22704756,   1.96954519]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2593451385336385}
episode index:756
target Thresh 43.88340660947252
target distance 22.0
model initialize at round 756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.47990637,  13.719838  ]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([94.64865596,  4.66188521,  5.07090503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6022314456239235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02687541,  15.10347766,   0.37561245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9786107957146277}
episode index:757
target Thresh 43.915507149917794
target distance 4.0
model initialize at round 757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.57336048,  11.13882024,   1.90337121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.873997493423996}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6026300612299722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14027877e+02, 1.49578448e+01, 9.48382989e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9730370526800256}
episode index:758
target Thresh 43.94757560586753
target distance 25.0
model initialize at round 758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.86751905,  14.51589921]), 'previousTarget': array([109.74881264,  14.15981002]), 'currentState': array([89.95589434, 12.63781467,  0.72048414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6026742712758667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16124156,  15.54092975,   5.79216327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9980584792984722}
episode index:759
target Thresh 43.979612009390195
target distance 18.0
model initialize at round 759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.48314552,  14.71285862]), 'currentState': array([98.62984698,  5.69393826,  5.83430106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.830419402296446}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6030016179861549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06049004,  15.02255672,   0.18603714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.939780698591613}
episode index:760
target Thresh 44.0116163925222
target distance 40.0
model initialize at round 760
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.94751275, 15.55198921]), 'previousTarget': array([94.97504678, 16.00124766]), 'currentState': array([75.      , 17.      ,  5.785849], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.23135893882194228
running average episode reward sum: 0.6019052177800996
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([110.21714359,  16.26560499]), 'previousTarget': array([111.19817339,  15.8015692 ]), 'currentState': array([90.88259386, 21.38177416,  3.05193189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:761
target Thresh 44.04358878726795
target distance 16.0
model initialize at round 761
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.62916814,  14.05808655]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 13.       ,  5.8952084], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.667382437544083}
done in step count: 32
reward sum = 0.3819106852578534
running average episode reward sum: 0.6016165110445062
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04637066,  15.2637016 ,   4.60791377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9894177341957285}
episode index:762
target Thresh 44.0755292255998
target distance 7.0
model initialize at round 762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.07653982,  11.29361899,   1.8867318 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.987463106209292}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6020496025738149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63535646,  14.16234835,   1.13230039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9135782330330227}
episode index:763
target Thresh 44.107437739458234
target distance 26.0
model initialize at round 763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.32649206,  15.70741573]), 'previousTarget': array([108.86817872,  15.70751784]), 'currentState': array([90.55174879, 18.70066289,  1.2693035 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.13596511977698728
running average episode reward sum: 0.6014395443502588
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2639208 ,  15.97044087,   4.69800884]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.218018088956361}
episode index:764
target Thresh 44.13931436075174
target distance 35.0
model initialize at round 764
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.8918805, 14.9232019]), 'previousTarget': array([99.96742669, 15.85900419]), 'currentState': array([80.       , 17.       ,  2.6690035], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 36
reward sum = 0.48988528804957343
running average episode reward sum: 0.601293721793003
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04088168,  15.25426841,   3.86271199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9922501563236191}
episode index:765
target Thresh 44.17115912135696
target distance 16.0
model initialize at round 765
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.44497014,  14.6718064 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 11.       ,  1.1980004], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.904339117998775}
done in step count: 34
reward sum = 0.24642451809224175
running average episode reward sum: 0.6008304460701561
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0156301 ,  14.43140216,   1.41488754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.136788282174985}
episode index:766
target Thresh 44.20297205311865
target distance 42.0
model initialize at round 766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.52677697, 13.65914001]), 'previousTarget': array([92.99433347, 14.47605556]), 'currentState': array([73.56565503, 12.41270068,  4.15563273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.23494475716786234
running average episode reward sum: 0.6003534112736734
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08239688,  14.56137078,   0.73930798]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0170501830479872}
episode index:767
target Thresh 44.23475318784973
target distance 13.0
model initialize at round 767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.88135026,   1.22549879,   2.91924334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.819850249478915}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6007258480711245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04149285,  14.32730613,   1.22486318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1710051272000295}
episode index:768
target Thresh 44.266502557331364
target distance 44.0
model initialize at round 768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.79565748, 18.03050426]), 'previousTarget': array([90.75160613, 18.85769903]), 'currentState': array([71.96407162, 20.62052402,  4.31235481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.600607850539497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00856811,  14.82171749,   1.24280871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0073340267815014}
episode index:769
target Thresh 44.2982201933129
target distance 36.0
model initialize at round 769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.54327035,  9.16181686]), 'previousTarget': array([97.97366596,  9.32455532]), 'currentState': array([77.47450952,  3.13003767,  2.04655504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5059869895324804
running average episode reward sum: 0.5991707143835594
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.06963308,  14.91370532,   4.98193176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9316372686795207}
episode index:770
target Thresh 44.329906127512004
target distance 27.0
model initialize at round 770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.19712308,  16.09610021]), 'previousTarget': array([107.87767469,  15.79136948]), 'currentState': array([86.35038588, 18.56733922,  1.90812171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4867341117682312
running average episode reward sum: 0.5990248822141491
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07959829,  14.98686898,   1.63130106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9204953679719958}
episode index:771
target Thresh 44.36156039161458
target distance 28.0
model initialize at round 771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.43864725,  12.2169951 ]), 'previousTarget': array([106.04057123,  12.12018361]), 'currentState': array([88.6695662 ,  5.30891315,  5.11049053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6005240427306539
running average episode reward sum: 0.5974710623631844
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([112.60721721,  15.32531205]), 'previousTarget': array([114.16215226,  15.54930329]), 'currentState': array([92.7895327 , 18.0196358 ,  3.58326507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:772
target Thresh 44.39318301727492
target distance 12.0
model initialize at round 772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.87762904,  21.35795173,   4.5461092 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.811349892010002}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5978799190062898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61161824,  14.11656081,   1.22798115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9650415519018977}
episode index:773
target Thresh 44.424774036115636
target distance 17.0
model initialize at round 773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.66209521, 19.64869901,  0.76294923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.950302091294397}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5981124014874175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08801476,  14.92881361,   0.9963394 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9147593035677499}
episode index:774
target Thresh 44.45633347972776
target distance 1.0
model initialize at round 774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.72083841,  15.82342681,   3.77784553]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5212777783323985}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5985193754822514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11195722,  14.99106083,   0.77092762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8880877732160197}
episode index:775
target Thresh 44.48786137967074
target distance 37.0
model initialize at round 775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.49903679,  9.42866211]), 'previousTarget': array([97.17072713,  9.69940536]), 'currentState': array([76.26846253,  3.93457313,  2.25588024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5003694185407963
running average episode reward sum: 0.5983928935789764
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23506798,  14.09445228,   1.28688473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1853850282527718}
episode index:776
target Thresh 44.519357767472464
target distance 29.0
model initialize at round 776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.69540346,  17.23669579]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([85.15052517, 21.47906926,  5.1095953 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5976227611548078
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.82747789,  15.98396519,   5.09448653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3849611703435083}
episode index:777
target Thresh 44.550822674629345
target distance 6.0
model initialize at round 777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.01318959,  16.03010977,   0.18952286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.074786004767585}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5979165219037549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01671558,  15.73638861,   0.92164911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.228461005312263}
episode index:778
target Thresh 44.58225613260627
target distance 31.0
model initialize at round 778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.43376255,  11.26893535]), 'previousTarget': array([103.20692454,  11.5762039 ]), 'currentState': array([82.14978166,  5.96534402,  2.58490109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5977177176158505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00230513,  14.54745007,   2.4532312 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0955347961753372}
episode index:779
target Thresh 44.613658172836715
target distance 12.0
model initialize at round 779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.79119166,   4.73439801,   0.93204224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.420698239134362}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5980651895327135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82152824,  15.88200766,   5.39226324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8998831530414587}
episode index:780
target Thresh 44.64502882672271
target distance 28.0
model initialize at round 780
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([106.12228624,  10.85987791]), 'previousTarget': array([105.83483823,  11.72672794]), 'currentState': array([87.       ,  5.       ,  3.4686642], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.1345921286099544
running average episode reward sum: 0.5971270879729917
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([114.94886481,  15.01039033]), 'previousTarget': array([113.57738279,  15.22160235]), 'currentState': array([95.34937941, 18.99287348,  5.69756756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:781
target Thresh 44.67636812563492
target distance 1.0
model initialize at round 781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92196369,  14.04816158,   2.40281522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9550319626246823}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5976422707249445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92196369,  14.04816158,   2.40281522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9550319626246823}
episode index:782
target Thresh 44.70767610091266
target distance 27.0
model initialize at round 782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.5793681 ,  12.85377562]), 'previousTarget': array([107.17596225,  12.68176659]), 'currentState': array([89.61103876,  6.5132329 ,  4.9797619 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5979235793671823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00594624,  14.43218997,   1.76941255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1447930446325336}
episode index:783
target Thresh 44.73895278386387
target distance 27.0
model initialize at round 783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.273645  ,  14.82974967]), 'previousTarget': array([107.98629668,  15.25976679]), 'currentState': array([88.28004836, 14.32369305,  5.85982585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.34830527532800604
running average episode reward sum: 0.5976051886732549
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36834124,  15.89209706,   4.49044589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0930827763780036}
episode index:784
target Thresh 44.77019820576527
target distance 10.0
model initialize at round 784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.59288596,   5.43495005,   1.08960992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.863284373286936}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5980193791264455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58619697,  14.20958563,   1.7853343 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8921815011658678}
episode index:785
target Thresh 44.80141239786228
target distance 8.0
model initialize at round 785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.75066122,  15.22209352,   1.4039228 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.252327907160932}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5984563521166167
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33209645,  14.93689219,   1.27455306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6708783449501811}
episode index:786
target Thresh 44.83259539136908
target distance 6.0
model initialize at round 786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.68713485,   9.79691616,   6.07715398]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.168237756972964}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5989165041596706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32137363,  14.46956667,   1.87726492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8613322607674555}
episode index:787
target Thresh 44.86374721746867
target distance 4.0
model initialize at round 787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11206963e+02, 1.34106421e+01, 9.39555168e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.11256490539104}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5993878017432244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26909011,  15.06572008,   6.10591352]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7338585651588954}
episode index:788
target Thresh 44.894867907312886
target distance 37.0
model initialize at round 788
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.86457524, 12.32349959]), 'previousTarget': array([97.81984861, 12.67835792]), 'currentState': array([78.       , 10.       ,  1.2838864], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.08230248306675769
running average episode reward sum: 0.5987324337854596
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43883857,  14.25007957,   1.55564306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9366337565816283}
episode index:789
target Thresh 44.92595749202242
target distance 11.0
model initialize at round 789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65625364,   5.47941591,   0.2400763 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.543175058443811}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5991543741830818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76368053,  14.12343763,   2.17539923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9078592879471367}
episode index:790
target Thresh 44.95701600268686
target distance 17.0
model initialize at round 790
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.37293534,  14.90863854]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       ,  5.       ,  4.4923334], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.4233763425732579
running average episode reward sum: 0.5989321516399594
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19722084,  14.50919196,   4.99092096]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9409287510291084}
episode index:791
target Thresh 44.988043470364715
target distance 17.0
model initialize at round 791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.78339447,  14.91055954]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.29735188,  7.27732764,  2.04287219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = -0.041759467380857285
running average episode reward sum: 0.5981231975755391
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14146071e+02, 1.46112646e+01, 3.04382831e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9382483782359249}
episode index:792
target Thresh 45.01903992608345
target distance 8.0
model initialize at round 792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62233114,   8.5836089 ,   2.81490946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.427496282238776}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5985802881334514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21441187,  14.262314  ,   1.64306426]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0776499187500594}
episode index:793
target Thresh 45.05000540083954
target distance 30.0
model initialize at round 793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.66451725,  14.9283563 ]), 'previousTarget': array([105.,  15.]), 'currentState': array([86.66525595, 14.75646212,  5.12796826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.5983260028920012
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08787062,  14.02078617,   1.96993536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3382226027865125}
episode index:794
target Thresh 45.080939925598436
target distance 27.0
model initialize at round 794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.5938989 ,  13.76765047]), 'previousTarget': array([107.87767469,  14.20863052]), 'currentState': array([87.86515687, 10.48485624,  3.68456924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6610495818227868
running average episode reward sum: 0.5984049004755619
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08116463,  14.1953359 ,   1.5250406 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.221369210336613}
episode index:795
target Thresh 45.111843531294696
target distance 21.0
model initialize at round 795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.39507644,  15.14445528]), 'previousTarget': array([113.97736275,  15.04869701]), 'currentState': array([92.42575794, 16.25184804,  3.9929564 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.09169637555224275
running average episode reward sum: 0.5975379390735169
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([109.89423236,  16.09017199]), 'previousTarget': array([110.39702766,  16.30865732]), 'currentState': array([90.33511192, 20.26639125,  5.15627137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:796
target Thresh 45.1427162488319
target distance 45.0
model initialize at round 796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.47341359, 10.25714517]), 'previousTarget': array([89.5237412 ,  9.33860916]), 'currentState': array([7.08371856e+01, 6.45996862e+00, 4.01449203e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3810879705264358
running average episode reward sum: 0.5972663581844992
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02904314,  15.38785643,   4.38355394]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0455571867126092}
episode index:797
target Thresh 45.1735581090828
target distance 6.0
model initialize at round 797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.75625886,  10.83007289,   2.39737332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.351457702221147}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5976859057906678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08361357,  15.81253686,   0.87522724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.224736802584913}
episode index:798
target Thresh 45.20436914288921
target distance 22.0
model initialize at round 798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.19208904,  15.20437099]), 'previousTarget': array([112.9793708 ,  15.09184678]), 'currentState': array([91.22083176, 16.2762305 ,  2.19396913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5978729973908442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08371151,  15.41842642,   0.89309072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.00730594192976}
episode index:799
target Thresh 45.23514938106222
target distance 8.0
model initialize at round 799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.58653833,  14.62632339,   5.68787522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.424338467379452}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5982790870121406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32291346,  15.10246263,   0.57392669]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6847954276201551}
episode index:800
target Thresh 45.265898854382044
target distance 18.0
model initialize at round 800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.17665078, 21.53218388,  5.21118927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.924555252834974}
done in step count: 27
reward sum = 0.6951009936471035
running average episode reward sum: 0.5983999633000744
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33578386,  15.92650558,   4.60718144]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1399981054310708}
episode index:801
target Thresh 45.29661759359815
target distance 34.0
model initialize at round 801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.27981287, 18.0356382 ]), 'previousTarget': array([100.69567118,  17.52429332]), 'currentState': array([79.64259131, 21.82767322,  1.71495271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4602168842125037
running average episode reward sum: 0.598227665196474
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02986187,  15.06135556,   0.80748678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9720763831613496}
episode index:802
target Thresh 45.327305629429304
target distance 2.0
model initialize at round 802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.96155051,  12.96616575,   3.0590539 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.283606596690255}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5985324167882391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0354796 ,  14.14145974,   1.11302751]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2912749459105692}
episode index:803
target Thresh 45.357962992563536
target distance 2.0
model initialize at round 803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.98047459,  17.1421702 ,   3.83887339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9173914329450654}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.598935665889781
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74291488,  15.93567907,   4.17601877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1947459275327412}
episode index:804
target Thresh 45.38858971365819
target distance 35.0
model initialize at round 804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.45192192,   9.78868933]), 'previousTarget': array([98.91891892,  9.48648649]), 'currentState': array([81.62347424,  3.04409462,  5.30033929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5991105288804623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05938519,  15.29875261,   0.53879352]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9869191172404366}
episode index:805
target Thresh 45.419185823340015
target distance 11.0
model initialize at round 805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.90161779,   3.39301492,   2.63644314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.013412299633144}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5994892776969988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05152029,  15.85380328,   1.00817126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2761637061086935}
episode index:806
target Thresh 45.449751352205126
target distance 36.0
model initialize at round 806
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.9413224 , 15.46909799]), 'previousTarget': array([98.96920706, 15.89059961]), 'currentState': array([79.       , 17.       ,  1.3304433], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.17410276113175782
running average episode reward sum: 0.598962156858628
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10740629,  15.55857491,   0.33612323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0529622328455899}
episode index:807
target Thresh 45.48028633081903
target distance 7.0
model initialize at round 807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.30598507,   9.62831373,   0.37504721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.582822919130834}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5993978349440752
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41305781,  14.38621621,   1.86709985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8492535926094649}
episode index:808
target Thresh 45.51079078971672
target distance 4.0
model initialize at round 808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.68795469,  12.77648727,   6.04784757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.207734770103528}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5998563036277043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21884167,  14.60169747,   2.08474559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8768427658072221}
episode index:809
target Thresh 45.541264759402665
target distance 15.0
model initialize at round 809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.67690438,   3.14275779,   5.35811228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.835332058392815}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6001990995775454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15818447,  14.62468659,   0.85527103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9216905883885976}
episode index:810
target Thresh 45.57170827035083
target distance 25.0
model initialize at round 810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.68377216,  14.80265303]), 'previousTarget': array([109.93630557,  14.59490445]), 'currentState': array([88.69352717, 14.1780691 ,  3.28261006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.12820463506412488
running average episode reward sum: 0.5993009445409958
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([110.31482981,  15.99306366]), 'previousTarget': array([111.08682312,  16.05332568]), 'currentState': array([90.74950304, 20.14010938,  2.95766052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:811
target Thresh 45.60212135300472
target distance 14.0
model initialize at round 811
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.55187795,  14.42185102]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   2.       ,   1.3698515], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.65933245552568}
done in step count: 12
reward sum = 0.7470848717161293
running average episode reward sum: 0.59948294445131
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14045872,  14.41806191,   1.48390338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0380092277687927}
episode index:812
target Thresh 45.63250403777745
target distance 26.0
model initialize at round 812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.77250476,  13.50110942]), 'previousTarget': array([108.76743395,  14.04114369]), 'currentState': array([88.18920522,  9.43978218,  3.35671568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1127950920432213
running average episode reward sum: 0.5986068337053143
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([110.41473028,  15.84719052]), 'previousTarget': array([110.09617029,  16.24436173]), 'currentState': array([90.74760697, 19.48095722,  3.72440363]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:813
target Thresh 45.662856355051666
target distance 4.0
model initialize at round 813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.23111937,  14.73202694,   5.65294814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.778395259398783}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5990634579882317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16279828,  15.11559119,   1.15908229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8451437979096023}
episode index:814
target Thresh 45.693178335179724
target distance 44.0
model initialize at round 814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.87873331, 18.00869831]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([70.02064967, 20.38704106,  1.23681939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07469098095636151
running average episode reward sum: 0.5982367654251095
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.91120135,  14.68123232,   6.19957687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.13450223866054}
episode index:815
target Thresh 45.72347000848359
target distance 33.0
model initialize at round 815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.13352445,  10.69042813]), 'previousTarget': array([100.79586847,   9.83486126]), 'currentState': array([83.16905229,  4.33835794,  1.6933158 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.34438183800678657
running average episode reward sum: 0.5979256687003321
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29395854,  15.44339166,   0.38551527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8337210011001117}
episode index:816
target Thresh 45.75373140525493
target distance 27.0
model initialize at round 816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.09068287,  14.65952822]), 'previousTarget': array([107.87767469,  14.20863052]), 'currentState': array([88.11492116, 13.67517868,  0.51905751]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.598081182368946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53360397,  14.11264182,   1.67705942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0024618644956804}
episode index:817
target Thresh 45.78396255575517
target distance 6.0
model initialize at round 817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.78711117,  13.01096985,   0.77384394]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.65882744873702}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5983702197516858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03903172,  14.09685067,   1.89003972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3187640991122616}
episode index:818
target Thresh 45.81416349021544
target distance 28.0
model initialize at round 818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.18331731,  16.92681537]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([87.76458264, 21.71354114,  4.23862982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3494312745079011
running average episode reward sum: 0.5980662649955883
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02318344,  15.55842671,   0.45342916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1251715362794676}
episode index:819
target Thresh 45.84433423883667
target distance 8.0
model initialize at round 819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.3134032 ,  23.05014205,   5.31134397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.4866123644069}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5984966598552278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20205501,  15.37905464,   5.00791878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.883401738172831}
episode index:820
target Thresh 45.87447483178965
target distance 44.0
model initialize at round 820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.19124582, 12.0634736 ]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([69.31946169,  9.80245711,  2.26402199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14576512690204857
running average episode reward sum: 0.5975901290552799
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([109.56152363,  14.14404751]), 'previousTarget': array([107.88273937,  13.93670817]), 'currentState': array([89.8047253 , 11.03455884,  0.37970171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:821
target Thresh 45.904585299214936
target distance 29.0
model initialize at round 821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.65559496,  12.26671661]), 'previousTarget': array([105.44164417,  12.69281066]), 'currentState': array([86.45992532,  6.65189098,  6.05116987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.0792466800591593
running average episode reward sum: 0.596959540917815
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67274327,  15.95988805,   6.08268889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.014141032626093}
episode index:822
target Thresh 45.93466567122303
target distance 33.0
model initialize at round 822
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.59959006,   9.35222753]), 'previousTarget': array([100.60816786,   9.33049037]), 'currentState': array([82.       ,  2.       ,  1.2161274], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.44153271255659987
running average episode reward sum: 0.5956977034287817
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23890086,  13.92453894,   1.07469769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3175311776749712}
episode index:823
target Thresh 45.96471597789427
target distance 4.0
model initialize at round 823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.81464309,  11.10923654,   1.59905505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.4624909053977975}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5961523166527759
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2588599 ,  14.35298448,   1.8953452 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9838280995571751}
episode index:824
target Thresh 45.994736249279
target distance 28.0
model initialize at round 824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.1784359 ,  15.78265408]), 'previousTarget': array([106.949174,  15.575059]), 'currentState': array([88.30878553, 18.06234574,  1.68729347]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.23057955847063388
running average episode reward sum: 0.5951502174102021
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([109.36959468,  15.10169402]), 'previousTarget': array([110.64287525,  14.98635706]), 'currentState': array([89.37285609, 15.46286674,  1.73016678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:825
target Thresh 46.02472651539747
target distance 6.0
model initialize at round 825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.58987179,  10.32855526,   2.11878686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.147858643429169}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5955695030421523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05242146,  14.76145359,   1.27613242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9771435301955709}
episode index:826
target Thresh 46.054686806239964
target distance 39.0
model initialize at round 826
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.86955274, 18.7194575 ]), 'previousTarget': array([95.76743395, 17.95885631]), 'currentState': array([76.       , 21.       ,  5.0076027], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.1557971689030373
running average episode reward sum: 0.5950377348025645
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52415853,  15.94964853,   4.81173706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.062194632432656}
episode index:827
target Thresh 46.08461715176676
target distance 27.0
model initialize at round 827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.7339601 ,  13.86821782]), 'previousTarget': array([107.5237412 ,  13.33860916]), 'currentState': array([89.05243022, 10.31330831,  1.90522784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5951859145100473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2377051 ,  15.66896882,   0.33096087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0142055007913973}
episode index:828
target Thresh 46.11451758190823
target distance 8.0
model initialize at round 828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.3239218 ,  21.03091288,   4.99214303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.259484665588525}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5956151112958011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11080815,  15.56612116,   4.7333768 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0541135177551313}
episode index:829
target Thresh 46.14438812656478
target distance 2.0
model initialize at round 829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02207106,  16.99002271,   5.12623233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.990145101592259}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5960432738724326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8909872 ,  15.78167436,   4.12799526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7892392561800831}
episode index:830
target Thresh 46.17422881560696
target distance 29.0
model initialize at round 830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.86565667,  17.29771226]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([87.61877281, 22.73438972,  5.11055321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6271800636471035
running average episode reward sum: 0.5960807429335333
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68482077,  15.68596079,   5.59383602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7549040695640777}
episode index:831
target Thresh 46.20403967887548
target distance 21.0
model initialize at round 831
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.52250656,  14.64166811]), 'previousTarget': array([112.05721038,  13.59867161]), 'currentState': array([94.      ,  5.      ,  4.852781], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.325158232828767
running average episode reward sum: 0.5957551149165805
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74460154,  14.01162188,   1.84065324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0208426309985799}
episode index:832
target Thresh 46.233820746181195
target distance 25.0
model initialize at round 832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.84451839,  14.45425693]), 'previousTarget': array([109.98401917,  14.79936077]), 'currentState': array([88.92266339, 12.68799152,  3.03975701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5960218037793424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53132526,  15.89873119,   4.78321724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.013594474881419}
episode index:833
target Thresh 46.26357204730518
target distance 27.0
model initialize at round 833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.00513609,  16.12189614]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([89.34642191, 19.80088468,  4.78873413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.4436059146135676
running average episode reward sum: 0.5958390509146352
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1012621 ,  15.76685707,   4.444133  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1814396196635293}
episode index:834
target Thresh 46.29329361199873
target distance 37.0
model initialize at round 834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.28650786,  9.56879033]), 'previousTarget': array([96.86920279,  8.6297199 ]), 'currentState': array([78.16514295,  3.70590676,  0.51724339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5432561351092131
running average episode reward sum: 0.5957760773627724
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21204074,  15.42939764,   5.67106341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8973639911014302}
episode index:835
target Thresh 46.32298546998342
target distance 27.0
model initialize at round 835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.48298155,  12.77596176]), 'previousTarget': array([107.17596225,  12.68176659]), 'currentState': array([89.55484328,  6.31642759,  0.56952458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 30
reward sum = 0.307478462598532
running average episode reward sum: 0.5954312237565951
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07152448,  14.4294691 ,   0.28199609]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0897579142480607}
episode index:836
target Thresh 46.352647650951106
target distance 4.0
model initialize at round 836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.6413062 ,  17.37161037,   4.61519951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.344842628520198}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5957788386287092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1933871 ,  14.80024989,   0.15075758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8309780286633986}
episode index:837
target Thresh 46.38228018456397
target distance 2.0
model initialize at round 837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.29095397,  15.29347651,   4.40096831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.7340607825647623}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5962027183557632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13801046,  14.854823  ,   0.33812061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8741294718245122}
episode index:838
target Thresh 46.41188310045455
target distance 26.0
model initialize at round 838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.24916399,  15.93938418]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([90.62903337, 19.81887826,  5.16245652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5964968071221853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97315457,  14.0044147 ,   1.70389794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9959471705959606}
episode index:839
target Thresh 46.44145642822576
target distance 4.0
model initialize at round 839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.85804621,  11.20012015,   1.17776513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9677632611186997}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.596953477589897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97887059,  14.17501301,   1.24008062]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8252575316380729}
episode index:840
target Thresh 46.47100019745094
target distance 7.0
model initialize at round 840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.23571118,  21.56005755,   4.14219928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.118683003645998}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5973190288353416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75182708,  14.83011119,   6.02522399]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3007523958375147}
episode index:841
target Thresh 46.500514437673864
target distance 38.0
model initialize at round 841
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.97526308, 14.99441686]), 'previousTarget': array([96.99307839, 14.52613364]), 'currentState': array([77.      , 14.      ,  4.810316], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3318826297457522
running average episode reward sum: 0.5962154639201622
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.93514709,  15.01262379,   0.82344453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.064927739077373}
episode index:842
target Thresh 46.52999917840876
target distance 10.0
model initialize at round 842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51976698,   6.4435047 ,   0.24262631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.569961232750709}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5966363115903637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67434664,  14.17256464,   2.04761374]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8892127929960992}
episode index:843
target Thresh 46.55945444914038
target distance 38.0
model initialize at round 843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.48870389, 16.5043708 ]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([75.54788819, 18.04185867,  3.33968568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.8013850561724357
running average episode reward sum: 0.5949798881688437
{'dynamicTrap': 17, 'scaleFactor': 20, 'currentTarget': array([106.86574107,  15.63608132]), 'previousTarget': array([106.35949443,  16.16360019]), 'currentState': array([86.92661104, 17.19527783,  5.95514361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:844
target Thresh 46.588880279323995
target distance 35.0
model initialize at round 844
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.92687961, 13.70864543]), 'previousTarget': array([99.92693298, 13.70802283]), 'currentState': array([80.       , 12.       ,  2.2378442], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4778112839063611
running average episode reward sum: 0.5937103128172755
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([113.88669733,  15.18345283]), 'previousTarget': array([112.56986362,  15.3003083 ]), 'currentState': array([94.15282262, 18.43525063,  5.78073593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:845
target Thresh 46.61827669838544
target distance 7.0
model initialize at round 845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.57855643,   9.61521863,   1.95247084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.380382331957946}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5940883351986778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41740641,  15.21055205,   6.24565193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6194735294039627}
episode index:846
target Thresh 46.64764373572113
target distance 17.0
model initialize at round 846
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.3703987 ,  16.40221892]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.      ,  9.      ,  6.124414], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.96615702978962}
done in step count: 39
reward sum = 0.2996666379713091
running average episode reward sum: 0.5937407298890823
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35507472,  15.95685281,   5.00347235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1539046418163477}
episode index:847
target Thresh 46.67698142069811
target distance 11.0
model initialize at round 847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.49359888,  18.77559685,   1.48895281]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.228723963280228}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5939954317031533
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21014049,  15.65560008,   0.44901308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0264937963680378}
episode index:848
target Thresh 46.706289782654075
target distance 40.0
model initialize at round 848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.792004  , 17.68406191]), 'previousTarget': array([94.70060934, 18.55239336]), 'currentState': array([74.96611983, 20.31737104,  5.70225644]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.16144399475287502
running average episode reward sum: 0.593105632614277
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.96948389,  14.83114555,   0.70589654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0442582452393498}
episode index:849
target Thresh 46.73556885089739
target distance 9.0
model initialize at round 849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.93601988,   7.22513164,   1.31768179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.278602951030186}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5935044087499154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44067827,  15.23455026,   0.63060769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6065102024170034}
episode index:850
target Thresh 46.76481865470709
target distance 12.0
model initialize at round 850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09369468,   1.6974764 ,   3.10592341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.302853560220363}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5938912951020635
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35491324,  14.76342386,   1.49790826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6870991144284424}
episode index:851
target Thresh 46.79403922333303
target distance 24.0
model initialize at round 851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.93236018,  12.22049635]), 'previousTarget': array([109.46153846,  12.69230769]), 'currentState': array([89.3199583 ,  4.90076298,  2.19059229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5792547856785236
running average episode reward sum: 0.5938741161003928
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05104294,  15.10692891,   1.00417318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.954962457791549}
episode index:852
target Thresh 46.823230585995745
target distance 35.0
model initialize at round 852
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.55931358, 12.17531464]), 'previousTarget': array([99.61161351, 11.9223227 ]), 'currentState': array([80.      ,  8.      ,  4.563616], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6359048470924052
running average episode reward sum: 0.5924324057097798
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([114.90378521,  15.01769146]), 'previousTarget': array([113.85783194,  15.13555599]), 'currentState': array([95.23354554, 18.63455076,  1.75570365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:853
target Thresh 46.852392771886635
target distance 10.0
model initialize at round 853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.45572301,   5.8445464 ,   1.5357005 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.817547051882695}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5928411267211279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02452209,  14.37240965,   2.0108287 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6280692505281834}
episode index:854
target Thresh 46.88152581016785
target distance 13.0
model initialize at round 854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57097337,   2.603681  ,   1.37687462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.403740910722686}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5932269788471007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91727902,  14.02112153,   1.89185899]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9823674540704131}
episode index:855
target Thresh 46.91062972997244
target distance 43.0
model initialize at round 855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.30564441, 16.41991002]), 'previousTarget': array([91.99459386, 15.53500945]), 'currentState': array([72.34467572, 17.66880115,  2.34130433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 57
reward sum = 0.3888595678340057
running average episode reward sum: 0.5929882318716182
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41211164,  15.64862125,   4.60371945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8753983345350382}
episode index:856
target Thresh 46.93970456040436
target distance 6.0
model initialize at round 856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.45275455,   9.84964245,   1.53920477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.745836947083192}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.593373011874601
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04869584,  14.07780519,   2.66326869]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9234795939142193}
episode index:857
target Thresh 46.9687503305384
target distance 31.0
model initialize at round 857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.43638813,  12.59597837]), 'previousTarget': array([103.74482241,  13.18464878]), 'currentState': array([83.85506922,  8.52511239,  5.62443781]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5935013040460754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36321025,  14.53743147,   5.67090143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7870646897008395}
episode index:858
target Thresh 46.99776706942036
target distance 3.0
model initialize at round 858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.52207989,  14.77677039,   5.7890147 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.487954886038606}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5939064016541719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32135182,  15.43994528,   0.57044492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8087738882977425}
episode index:859
target Thresh 47.026754806066975
target distance 8.0
model initialize at round 859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.21978017,   6.8404637 ,   0.24991625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.608930837751087}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.59429960973121
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32135287,  14.52291909,   2.1695919 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8295589943777194}
episode index:860
target Thresh 47.05571356946598
target distance 27.0
model initialize at round 860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.11995891,  16.30891146]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([89.59779082, 20.6545936 ,  5.06028396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5945127592662482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17200151,  15.85927086,   4.59914131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.193284505350544}
episode index:861
target Thresh 47.08464338857614
target distance 12.0
model initialize at round 861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39439974,   4.5715916 ,   0.9307282 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.445977856884836}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5949043515964579
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43516533,  14.32906231,   2.44662228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.877037961831335}
episode index:862
target Thresh 47.113544292327276
target distance 30.0
model initialize at round 862
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([104.96690387,  15.15010862]), 'previousTarget': array([104.98889814,  14.6662966 ]), 'currentState': array([85.      , 14.      ,  3.635138], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.46020335679674546
running average episode reward sum: 0.5936817470676129
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([106.37002519,  15.60329352]), 'previousTarget': array([107.05124201,  15.12158395]), 'currentState': array([86.41871634, 16.99802475,  1.06310742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:863
target Thresh 47.14241630962031
target distance 19.0
model initialize at round 863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.70213766, 20.52950672,  2.15043245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.115104259632997}
done in step count: 80
reward sum = 0.32745388354671134
running average episode reward sum: 0.5933736129663155
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56932612,  15.53717284,   4.22268716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6885017430117834}
episode index:864
target Thresh 47.17125946932723
target distance 37.0
model initialize at round 864
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.34677138, 10.0697571 ]), 'previousTarget': array([97.30726786, 10.2181805 ]), 'currentState': array([78.       ,  5.       ,  1.4220785], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5067299140771802
running average episode reward sum: 0.5921018169812943
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([112.2791405 ,  15.08302127]), 'previousTarget': array([113.7045956 ,  15.09254904]), 'currentState': array([92.28844436, 15.69299507,  2.64378356]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:865
target Thresh 47.200073800291236
target distance 21.0
model initialize at round 865
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.21503731,  14.45181642]), 'previousTarget': array([113.45612429,  15.36758945]), 'currentState': array([94.       , 20.       ,  1.3420072], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.3506337861395955
running average episode reward sum: 0.5918229855369043
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05494626,  15.94570527,   5.87630404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9473001410518992}
episode index:866
target Thresh 47.22885933132663
target distance 29.0
model initialize at round 866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.97107451,  17.4338664 ]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([86.66037196, 22.63932192,  0.37490964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17144121196324125
running average episode reward sum: 0.5909426346747357
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([109.58484819,  14.78721382]), 'previousTarget': array([110.07863363,  15.09374341]), 'currentState': array([89.60027098, 14.00192799,  3.38609563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:867
target Thresh 47.257616091218964
target distance 12.0
model initialize at round 867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.05044007,   4.14424492,   1.73083907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.889678095824353}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5913248951122394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12469804,  14.01806219,   0.2000032 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9898239611751162}
episode index:868
target Thresh 47.28634410872498
target distance 47.0
model initialize at round 868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.34425994, 10.14908927]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([68.66743527,  6.56822753,  0.19479561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22706717704975235
running average episode reward sum: 0.5903831320832842
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([91.42795949, 13.23096981]), 'previousTarget': array([92.8567391 , 13.77122945]), 'currentState': array([71.48404444, 11.73422253,  2.71626466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:869
target Thresh 47.31504341257273
target distance 23.0
model initialize at round 869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.847606  ,  15.06003151]), 'previousTarget': array([111.98112317,  15.13125551]), 'currentState': array([93.87468757, 16.1004782 ,  5.94402123]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5906832178752516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07953353,  15.70540698,   4.75580049]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1596799255448949}
episode index:870
target Thresh 47.343714031461495
target distance 6.0
model initialize at round 870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.00313426,  11.06910026,   0.51546931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.170381549146043}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5910859698058206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08488903,  14.32476278,   5.62369791]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6805523102642251}
episode index:871
target Thresh 47.372355994061905
target distance 25.0
model initialize at round 871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.56620053,  14.91219222]), 'previousTarget': array([110.,  15.]), 'currentState': array([91.5727364 , 14.40092713,  4.90924234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.3486192347139257
running average episode reward sum: 0.5908079116233759
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29150531,  15.81888996,   5.28901971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.082841397994755}
episode index:872
target Thresh 47.40096932901592
target distance 29.0
model initialize at round 872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.31342467,  13.65608763]), 'previousTarget': array([105.58520839,  13.05211208]), 'currentState': array([86.54856799, 10.59823867,  0.24160314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10037727892366351
running average episode reward sum: 0.5900161760099198
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([111.33163245,  15.27460458]), 'previousTarget': array([109.95658734,  15.6275935 ]), 'currentState': array([91.38743439, 16.76757606,  5.2285831 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:873
target Thresh 47.42955406493688
target distance 16.0
model initialize at round 873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.55359886,   6.99669339,   5.27105695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.515187634016016}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5903552706274326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64016213,  14.22448694,   1.28740478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.854929113654619}
episode index:874
target Thresh 47.458110230409524
target distance 23.0
model initialize at round 874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.4016044 ,  16.67665886]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([90.2423871 , 22.41463695,  3.69241571]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2038983976178054
running average episode reward sum: 0.5894475521494382
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.79740484,  15.7228626 ,   5.0047814 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.252573342901235}
episode index:875
target Thresh 47.486637853990025
target distance 43.0
model initialize at round 875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.8564587, 16.4387549]), 'previousTarget': array([91.99459386, 15.53500945]), 'currentState': array([71.89499388, 17.67969098,  0.62323022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3179697673369723
running average episode reward sum: 0.5884116876294765
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([110.67888945,  14.82237745]), 'previousTarget': array([111.50987349,  14.64050707]), 'currentState': array([90.6957649 , 14.00095583,  3.15870912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:876
target Thresh 47.51513696420601
target distance 40.0
model initialize at round 876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.73590993,  8.44433039]), 'previousTarget': array([94.02068137,  8.18172145]), 'currentState': array([76.80221675,  2.00109655,  5.4937622 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.1383840362767938
running average episode reward sum: 0.5875829581837453
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.34873103, 18.91282106,  4.87749438]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.104821745393842}
episode index:877
target Thresh 47.543607589556565
target distance 28.0
model initialize at round 877
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([106.30362949,  11.23162387]), 'previousTarget': array([106.04057123,  12.12018361]), 'currentState': array([87.       ,  6.       ,  3.3464475], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6923427143471035
running average episode reward sum: 0.5877022745347287
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15111184,  14.49095023,   0.69582624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9898195652901445}
episode index:878
target Thresh 47.57204975851235
target distance 10.0
model initialize at round 878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.93809295,   6.62705932,   0.59882641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.623085076829044}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.588104752208069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43445051,  14.20858917,   1.6179367 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9727164647374247}
episode index:879
target Thresh 47.60046349951552
target distance 24.0
model initialize at round 879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.20899615,  15.30010649]), 'previousTarget': array([110.84555753,  15.51930531]), 'currentState': array([90.24811824, 16.55044758,  3.22389412]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5882686142733489
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07505259,  14.5681403 ,   1.22797499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0207989549814676}
episode index:880
target Thresh 47.62884884097984
target distance 38.0
model initialize at round 880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.95008703, 16.687151  ]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([77.03688773, 18.54846602,  0.53697872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3854585998132051
running average episode reward sum: 0.5880384099436552
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66420959,  14.65820529,   5.90917632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4791438432473438}
episode index:881
target Thresh 47.657205811290616
target distance 11.0
model initialize at round 881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.31760101,   4.95616247,   2.15764332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.177821744695187}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5883970762305771
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15421019,  14.16147225,   0.84331602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1910034373332123}
episode index:882
target Thresh 47.68553443880485
target distance 13.0
model initialize at round 882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.68277975,   2.02530584,   1.02503699]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.674102496407691}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5887652757450201
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56597941,  15.1518456 ,   0.61605727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45981621991376054}
episode index:883
target Thresh 47.71383475185117
target distance 44.0
model initialize at round 883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.07900272,  9.04443978]), 'previousTarget': array([90.29527642,  8.26234812]), 'currentState': array([72.7217509 ,  4.01484716,  0.46678744]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.36798910953521724
running average episode reward sum: 0.5885155289506651
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.043243  ,  15.89366424,   4.70895544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3092057610273884}
episode index:884
target Thresh 47.74210677872988
target distance 5.0
model initialize at round 884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.81908777,  19.73419942,   4.37298393]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.879262014145804}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5889469227032632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68234001,  15.91357206,   5.03452122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.967223749033159}
episode index:885
target Thresh 47.77035054771301
target distance 46.0
model initialize at round 885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.37696649, 10.90155308]), 'previousTarget': array([88.7042351, 10.4268235]), 'currentState': array([67.59353578,  7.96627297,  1.5695647 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.29666115373512864
running average episode reward sum: 0.5879473650549129
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([106.58722839,  16.76396094]), 'previousTarget': array([107.82334204,  16.80658603]), 'currentState': array([87.01288598, 20.86824154,  4.67072759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:886
target Thresh 47.79856608704432
target distance 19.0
model initialize at round 886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.76686234,  14.91410718]), 'currentState': array([97.12797078,  6.79471068,  4.53041296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.665609607223917}
done in step count: 20
reward sum = 0.6931863187789915
running average episode reward sum: 0.5880660110004867
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39041841,  14.24752806,   1.84085779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9684026714433035}
episode index:887
target Thresh 47.8267534249394
target distance 6.0
model initialize at round 887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.34673534,   8.40821694,   5.56400895]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.683951014793795}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5884533976411472
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42341259,  14.01584671,   1.86808729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1406185738653019}
episode index:888
target Thresh 47.85491258958553
target distance 46.0
model initialize at round 888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.31496606, 14.19501708]), 'previousTarget': array([89., 15.]), 'currentState': array([69.32478112, 13.56851383,  4.35427034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = -0.07756305735629393
running average episode reward sum: 0.5877042227761332
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6962178 ,  14.98412581,   0.36560147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3041966744652645}
episode index:889
target Thresh 47.88304360914192
target distance 15.0
model initialize at round 889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.16864802,   5.21762151,   6.08706493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.941110547303467}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5880600405876306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73019581,  14.24643763,   1.06126368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8004064894609112}
episode index:890
target Thresh 47.91114651173957
target distance 12.0
model initialize at round 890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.46469765,  20.61754263,   0.28104401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.830432007124159}
done in step count: 13
reward sum = 0.8082210229989678
running average episode reward sum: 0.5883071348439846
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19060038,  14.33829511,   1.29243306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0454573647738268}
episode index:891
target Thresh 47.9392213254814
target distance 11.0
model initialize at round 891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.68194798,   4.83875923,   6.13283211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.965307985223681}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5886820648435181
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52580655,  15.46121553,   0.67932373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.661497691506965}
episode index:892
target Thresh 47.9672680784422
target distance 36.0
model initialize at round 892
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.8465974 , 13.47236156]), 'previousTarget': array([98.87767469, 13.20863052]), 'currentState': array([79.      , 11.      ,  4.371663], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1393
running average episode reward sum: 0.58786685536441
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([107.43175175,  13.17650808]), 'previousTarget': array([105.91890982,  12.63487887]), 'currentState': array([87.98815991,  8.4917737 ,  5.83575523]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:893
target Thresh 47.99528679866875
target distance 12.0
model initialize at round 893
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.13506521,  16.33102639]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  12.      ,   3.130688], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.830680661523916}
done in step count: 13
reward sum = 0.6696140229989678
running average episode reward sum: 0.5879582951492361
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00302223,  14.33675344,   1.57603571]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1974392141503276}
episode index:894
target Thresh 48.02327751417977
target distance 21.0
model initialize at round 894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.98271696,  15.08167719]), 'previousTarget': array([114.,  15.]), 'currentState': array([94.04687107, 16.68231721,  2.55294251]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.026406565284645293
running average episode reward sum: 0.5872718539643937
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.2605021 ,  16.80453934,   4.41299055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.923854867421776}
episode index:895
target Thresh 48.05124025296597
target distance 42.0
model initialize at round 895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.37502675, 14.18891826]), 'previousTarget': array([92.949174, 13.424941]), 'currentState': array([72.38786586, 13.47239934,  2.93256831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5872332508652844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09708742,  15.16531814,   0.70606163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9179222283014934}
episode index:896
target Thresh 48.07917504299009
target distance 3.0
model initialize at round 896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.18758182,  14.0013949 ,   6.06120449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.069316746218154}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5876712294038962
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02246416,  14.61326767,   0.62080484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.051255542555391}
episode index:897
target Thresh 48.10708191218693
target distance 25.0
model initialize at round 897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.39924997,  15.73456467]), 'previousTarget': array([109.74881264,  15.84018998]), 'currentState': array([88.521955  , 17.94661005,  3.26342487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5268848390153413
running average episode reward sum: 0.5876035385460024
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15784564,  14.99991954,   5.17846365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8421543626274687}
episode index:898
target Thresh 48.13496088846335
target distance 45.0
model initialize at round 898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.32892477, 12.30650042]), 'previousTarget': array([89.92145281, 12.77079581]), 'currentState': array([68.43014008, 10.29693133,  2.61656594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.2547771104709797
running average episode reward sum: 0.5872333200498122
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21765421,  15.89037722,   5.31740838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1852580034364413}
episode index:899
target Thresh 48.16281199969835
target distance 27.0
model initialize at round 899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.86989728,  16.1232725 ]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([89.19743626, 19.72803013,  4.56996346]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5874805362144471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25460492,  14.46848649,   5.78219555]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9154891798001602}
episode index:900
target Thresh 48.19063527374302
target distance 6.0
model initialize at round 900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.13582085,  19.27242746,   0.12291658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.255496780597302}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5878322582330868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21230897,  14.93576203,   0.40713163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.790306066401443}
episode index:901
target Thresh 48.21843073842065
target distance 8.0
model initialize at round 901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.15810582,   7.4638896 ,   1.67475533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.535234004908947}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5882243290658671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35687386,  14.2992917 ,   1.18500905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9511063847875533}
episode index:902
target Thresh 48.24619842152671
target distance 43.0
model initialize at round 902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.2606848, 18.025087 ]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([70.40854908, 20.4525769 ,  2.46289206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.31156940884549555
running average episode reward sum: 0.5879179559537736
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1458817 ,  14.60823868,   0.74894536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9396781338251798}
episode index:903
target Thresh 48.273938350828864
target distance 23.0
model initialize at round 903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.44755049,  13.27980085]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([93.44689322,  4.5633602 ,  5.64448446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.24630579416667184
running average episode reward sum: 0.586995142070897
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([111.86086972,  15.18807027]), 'previousTarget': array([110.21651515,  15.27816486]), 'currentState': array([91.89666736, 16.38415727,  5.23975071]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:904
target Thresh 48.301650554067066
target distance 11.0
model initialize at round 904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.54694783,  16.41139138,   1.26992148]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.557835582835306}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5873458458641986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72951135,  14.44248735,   0.60682163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6196648007638371}
episode index:905
target Thresh 48.32933505895352
target distance 12.0
model initialize at round 905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.4358937 ,   4.58561413,   0.9025979 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.310439656904764}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5877263309657911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88885608,  14.11681087,   1.17673641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8901550523314203}
episode index:906
target Thresh 48.35699189317271
target distance 47.0
model initialize at round 906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.01149746, 18.8167478 ]), 'previousTarget': array([87.88777826, 17.88427891]), 'currentState': array([67.19490678, 21.51910491,  1.11054814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.27820296345304485
running average episode reward sum: 0.5867716128903568
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([108.93517535,  16.8761852 ]), 'previousTarget': array([109.5761433 ,  16.31864593]), 'currentState': array([89.82854793, 22.78692024,  1.06319025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:907
target Thresh 48.38462108438151
target distance 5.0
model initialize at round 907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.23947221,  18.63885673,   5.21285892]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.991986576177094}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5871214041481966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28001967,  14.50452499,   5.59111637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5691278438037949}
episode index:908
target Thresh 48.41222266020908
target distance 25.0
model initialize at round 908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.34786711,  15.64525586]), 'previousTarget': array([109.61161351,  16.0776773 ]), 'currentState': array([90.53751502, 18.39297311,  6.00191283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5755146494617834
running average episode reward sum: 0.5871086354411708
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43025274,  14.69381367,   0.29017548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.646809094356783}
episode index:909
target Thresh 48.43979664825702
target distance 2.0
model initialize at round 909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07909998,  15.16438622,   0.41588491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9354569302715973}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5875623622154112
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07909998,  15.16438622,   0.41588491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9354569302715973}
episode index:910
target Thresh 48.4673430760993
target distance 44.0
model initialize at round 910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.66259461,  9.07800507]), 'previousTarget': array([90.59429865, 10.00792472]), 'currentState': array([70.18746136,  4.52617198,  3.19853926]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2907640694998551
running average episode reward sum: 0.5865982278227492
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([107.78847602,  16.85493009]), 'previousTarget': array([108.98563442,  16.27883636]), 'currentState': array([88.41896608, 21.83710679,  1.250133  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:911
target Thresh 48.49486197128237
target distance 13.0
model initialize at round 911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.62803134,  19.60322495,   0.2544241 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.20057909176166}
done in step count: 8
reward sum = 0.85344469442792
running average episode reward sum: 0.5868908226326233
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04393887,  15.36565813,   5.21103229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0236008764841138}
episode index:912
target Thresh 48.522353361325116
target distance 6.0
model initialize at round 912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.00309798,  12.97589761,   0.69737792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.32928308199923}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5872688889253663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00643729,  14.0274273 ,   1.92723668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3903469047899077}
episode index:913
target Thresh 48.54981727371893
target distance 6.0
model initialize at round 913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14793336e+02, 1.04487576e+01, 1.14232183e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.5559321257567476}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5876773431059732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4354106 ,  14.18482232,   2.23172256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9916026596617451}
episode index:914
target Thresh 48.57725373592774
target distance 3.0
model initialize at round 914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.08735632,  16.07126976,   6.2104382 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.103403308154471}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5880135845389269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43894606,  14.73009777,   6.12926779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6225983723064471}
episode index:915
target Thresh 48.60466277538799
target distance 9.0
model initialize at round 915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.78307841,   9.27973822,   5.49932194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.847720426994114}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5883994650682524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10299236,  14.18206127,   1.01552295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2139384101756936}
episode index:916
target Thresh 48.63204441950874
target distance 16.0
model initialize at round 916
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.67925058,  15.11182831]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 15.       ,  4.1980886], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.67960425700358}
done in step count: 15
reward sum = 0.6521513546412885
running average episode reward sum: 0.5884689873033374
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8427719 ,  15.06702311,   5.84769345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.17091745024198712}
episode index:917
target Thresh 48.659398695671634
target distance 43.0
model initialize at round 917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.30836587, 13.61158754]), 'previousTarget': array([91.91402432, 12.85246738]), 'currentState': array([71.34262135, 12.44152521,  0.98865342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.07270909773535847
running average episode reward sum: 0.5877487497379358
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([113.58299239,  16.96860049]), 'previousTarget': array([113.21515427,  15.52680252]), 'currentState': array([94.03322891, 21.18840472,  5.25007311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:918
target Thresh 48.686725631230935
target distance 8.0
model initialize at round 918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50515699,   5.22219939,   4.29957712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.790841042857652}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.588123414153789
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46856923,  14.65536419,   1.7296409 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6333975912307632}
episode index:919
target Thresh 48.7140252535136
target distance 11.0
model initialize at round 919
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.34160058,  16.64793724]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.      ,  14.      ,   3.517994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.64660788452906}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.5883910866112401
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10254519,  14.09010506,   1.50879607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2780195413689257}
episode index:920
target Thresh 48.741297589819226
target distance 12.0
model initialize at round 920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14442473,   4.4728647 ,   0.91408873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.528125952086684}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5887642399894114
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46463975,  14.48361233,   1.65229565]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7438190799052308}
episode index:921
target Thresh 48.768542667420185
target distance 30.0
model initialize at round 921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.5092283 ,  11.47556744]), 'previousTarget': array([103.77752632,  10.88509298]), 'currentState': array([84.38842211,  5.61086194,  0.9927671 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5888585619081433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1384893 ,  14.28639437,   1.66265375]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1186749663762035}
episode index:922
target Thresh 48.79576051356155
target distance 38.0
model initialize at round 922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.38946826, 10.14338176]), 'previousTarget': array([96.21128529,  9.56116153]), 'currentState': array([75.97594741,  5.33555396,  1.21493936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.2706823029943854
running average episode reward sum: 0.5885138422343473
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64653996,  15.34813048,   6.07192154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4961137296186145}
episode index:923
target Thresh 48.82295115546115
target distance 21.0
model initialize at round 923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.01446871,  13.3600414 ]), 'previousTarget': array([112.05721038,  13.59867161]), 'currentState': array([92.51904061,  5.74957891,  1.81285763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.3656932191936614
running average episode reward sum: 0.5882726943739136
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19859604,  14.42756399,   5.67815624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9848509046196726}
episode index:924
target Thresh 48.85011462030964
target distance 35.0
model initialize at round 924
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.75531437, 17.88109727]), 'previousTarget': array([99.7124451, 17.6207237]), 'currentState': array([80.       , 21.       ,  4.8746037], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 26
reward sum = 0.3935150301652426
running average episode reward sum: 0.588062145547742
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1149233 ,  14.38290465,   0.4053013 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.078965915510045}
episode index:925
target Thresh 48.87725093527049
target distance 17.0
model initialize at round 925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.60854147, 22.56909699,  0.19082594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.054670923088906}
done in step count: 13
reward sum = 0.7437099988059679
running average episode reward sum: 0.5882302317823621
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69167987,  15.64343505,   5.04399825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7134913924148966}
episode index:926
target Thresh 48.90436012748001
target distance 11.0
model initialize at round 926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84465615,   5.22375188,   1.31744862]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.777482241233267}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5886112996546583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64412935,  14.00005144,   1.96927796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0613863787216238}
episode index:927
target Thresh 48.93144222404741
target distance 5.0
model initialize at round 927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.86593114,  11.5069011 ,   0.13518703]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.4122329304382}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5890226010558924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3629892 ,  14.18989049,   5.84764606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.030563034270471}
episode index:928
target Thresh 48.95849725205477
target distance 7.0
model initialize at round 928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.44100515,  16.38617769,   5.29633129]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.685043380210228}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5892959278506481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08116218,  15.26288725,   0.54577216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9557053171819707}
episode index:929
target Thresh 48.98552523855713
target distance 43.0
model initialize at round 929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.47120783, 13.17720448]), 'previousTarget': array([91.91402432, 12.85246738]), 'currentState': array([70.52620361, 11.69504163,  3.71092248]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2368837090782997
running average episode reward sum: 0.5889169899810004
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47847243,  15.24392405,   0.48657108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5757516400465242}
episode index:930
target Thresh 49.01252621058249
target distance 46.0
model initialize at round 930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.4485785 , 16.77955729]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.50091004, 18.22542138,  4.80690587]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14991774583743758
running average episode reward sum: 0.5881233973539128
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.0381472 ,  16.22300019]), 'previousTarget': array([109.54403098,  16.53326658]), 'currentState': array([91.92795564, 22.12220168,  5.38144252]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:931
target Thresh 49.039500195131794
target distance 24.0
model initialize at round 931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.49542188,  15.62908332]), 'previousTarget': array([110.93091516,  15.3390904 ]), 'currentState': array([89.62476381, 17.89997478,  1.55212021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5883524823240687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17873863,  14.35532966,   0.68024767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0440642136220675}
episode index:932
target Thresh 49.06644721917906
target distance 42.0
model initialize at round 932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.17995494, 10.62733268]), 'previousTarget': array([92.55604828, 10.19058177]), 'currentState': array([74.60697159,  6.51657574,  5.72063619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = -0.4805758187018056
running average episode reward sum: 0.5872067928267204
{'dynamicTrap': 18, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14360365,  15.42498211,   0.67264344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9560462862308592}
episode index:933
target Thresh 49.09336730967129
target distance 21.0
model initialize at round 933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.14682205,  14.35972702]), 'previousTarget': array([113.23047895,  14.49442256]), 'currentState': array([92.63215157,  9.98049924,  3.52966797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5873162599498921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04396125,  14.56505321,   4.97999863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0503279469209004}
episode index:934
target Thresh 49.12026049352859
target distance 3.0
model initialize at round 934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.59274162,  15.08841789,   2.6395359 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.5942487975488286}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5877258671585018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74827073,  14.56628114,   3.5715671 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.864882153601314}
episode index:935
target Thresh 49.14712679764414
target distance 42.0
model initialize at round 935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.248071  , 16.36554648]), 'previousTarget': array([92.99433347, 15.52394444]), 'currentState': array([72.28399669, 17.5637689 ,  2.94664681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.38525352592705037
running average episode reward sum: 0.5875095505546221
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05412305,  15.60197644,   4.91360008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1211863536161482}
episode index:936
target Thresh 49.173966248884255
target distance 8.0
model initialize at round 936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.15831894,   8.25969937,   1.25042796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.919466923897607}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5878873206707868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.516526  ,  14.41295801,   0.32102776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.760503394968831}
episode index:937
target Thresh 49.200778874088385
target distance 42.0
model initialize at round 937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.61586775, 14.60501695]), 'previousTarget': array([92.99433347, 15.52394444]), 'currentState': array([72.61898072, 14.25215844,  3.75050187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.240521932150946
running average episode reward sum: 0.5870041551560514
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([110.31373592,  13.88121337]), 'previousTarget': array([108.81773333,  13.72490659]), 'currentState': array([90.8604301 ,  9.23698118,  5.01830481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:938
target Thresh 49.22756470006915
target distance 43.0
model initialize at round 938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.38275166, 17.7825785 ]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([73.54640985, 20.33591819,  4.86756456]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.22644548237688827
running average episode reward sum: 0.5866201736088958
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76382681,  14.74602946,   5.16644649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3468123547304437}
episode index:939
target Thresh 49.2543237536124
target distance 20.0
model initialize at round 939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.25909249,  14.8056831 ]), 'previousTarget': array([114.77872706,  14.96680906]), 'currentState': array([93.38252646, 12.58709726,  3.73840463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.4188970119514829
running average episode reward sum: 0.5864417447135155
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51157579,  15.82354219,   4.99648263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9574862631026971}
episode index:940
target Thresh 49.28105606147716
target distance 10.0
model initialize at round 940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.81448597,  21.91264074,   0.62030285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.713880800758071}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5868190437620675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35776263,  15.9069432 ,   5.04810852]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1113121981323049}
episode index:941
target Thresh 49.30776165039575
target distance 11.0
model initialize at round 941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.37123923,   3.57629441,   2.38609171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.722262269399407}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5871658571418145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30889196,  14.2068403 ,   1.74932548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0520136076499582}
episode index:942
target Thresh 49.33444054707378
target distance 30.0
model initialize at round 942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.58989737,  18.14700601]), 'previousTarget': array([104.32469879,  17.84674699]), 'currentState': array([83.20350694, 23.0630884 ,  3.82804835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5873680368897014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48362858,  14.26488464,   5.47623378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.898350734071002}
episode index:943
target Thresh 49.36109277819013
target distance 10.0
model initialize at round 943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11758751e+02, 6.45202450e+00, 1.02069020e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.141858569893994}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5877233087726867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36146287,  14.11797394,   1.6522381 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0888983628947921}
episode index:944
target Thresh 49.38771837039703
target distance 7.0
model initialize at round 944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.17157874,  22.22900224,   6.20096404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.693222953704051}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5880583974142065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91761727,  14.18279211,   4.9973721 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8213498983461605}
episode index:945
target Thresh 49.4143173503201
target distance 29.0
model initialize at round 945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.20734798,  13.44931456]), 'previousTarget': array([105.58520839,  13.05211208]), 'currentState': array([87.59194615,  9.54598127,  5.60357887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.300521619733596
running average episode reward sum: 0.5877544473320916
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21528219,  15.13602657,   4.81733181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7964202820867928}
episode index:946
target Thresh 49.440889744558284
target distance 14.0
model initialize at round 946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.57334988,  20.5729038 ,   0.21124792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.53726900903847}
done in step count: 9
reward sum = 0.7756102474836408
running average episode reward sum: 0.5879528167092316
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52333756,  14.08526274,   5.34929794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.031480169737747}
episode index:947
target Thresh 49.46743557968402
target distance 22.0
model initialize at round 947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.01127677,  13.05681763]), 'previousTarget': array([111.51093912,  13.57265691]), 'currentState': array([91.37512746,  5.79775853,  2.29362512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5881128879715513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00062512,  14.05252985,   1.82348324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9474703557672269}
episode index:948
target Thresh 49.4939548822431
target distance 15.0
model initialize at round 948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.89774337,  14.92904168]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.46633749,  3.5268971 ,  1.83072269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.588408602328556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7482993 ,  14.02682603,   0.84098433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0051969050828649}
episode index:949
target Thresh 49.52044767875488
target distance 23.0
model initialize at round 949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.50312568,  12.56476213]), 'previousTarget': array([109.73169692,  12.25132013]), 'currentState': array([91.21725617,  4.46371427,  1.05186796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5885447306762083
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36918565,  14.73788887,   6.12550671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6831024686880541}
episode index:950
target Thresh 49.54691399571212
target distance 27.0
model initialize at round 950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.6913985 ,  13.06814288]), 'previousTarget': array([107.17596225,  12.68176659]), 'currentState': array([89.56795316,  7.21204876,  1.14442366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.13313979646241658
running average episode reward sum: 0.5880658611344481
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92271098,  15.97564675,   5.0760384 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.978703310614382}
episode index:951
target Thresh 49.57335385958115
target distance 5.0
model initialize at round 951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.81596964,  13.43111826,   2.06260533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.468500868556074}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5884571743160297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48730652,  15.27409428,   5.97700055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5813624347914805}
episode index:952
target Thresh 49.599767296801836
target distance 42.0
model initialize at round 952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.06267414, 17.89061179]), 'previousTarget': array([92.90990945, 17.10381815]), 'currentState': array([73.23407041, 20.50336397,  0.45773065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.12852388843967205
running average episode reward sum: 0.5879745580664217
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04713407,  15.82100406,   6.02206504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2577762680045739}
episode index:953
target Thresh 49.626154333787625
target distance 26.0
model initialize at round 953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.16626192,  15.95760009]), 'previousTarget': array([108.76743395,  15.95885631]), 'currentState': array([90.54754107, 19.84421715,  1.36726087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7783510647739723
running average episode reward sum: 0.5881741141531173
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38637525,  15.96076569,   5.61584112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1400026538868049}
episode index:954
target Thresh 49.65251499692554
target distance 10.0
model initialize at round 954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.37030661,   6.71221806,   0.49317884]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.705051126223259}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5885052219655317
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18597374,  14.1197478 ,   1.40415663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8996833708129022}
episode index:955
target Thresh 49.67884931257627
target distance 37.0
model initialize at round 955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.48027791,  7.99563663]), 'previousTarget': array([96.86920279,  8.6297199 ]), 'currentState': array([76.6555516 ,  1.24066221,  2.87946439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5884449712552697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34306429,  15.64826466,   5.53004905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9229364004186423}
episode index:956
target Thresh 49.70515730707411
target distance 2.0
model initialize at round 956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.03896658,  15.57317966,   3.96011406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1865860594039483}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5888645689864554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65007186,  15.14425121,   3.99201159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6658842489553782}
episode index:957
target Thresh 49.731439006727065
target distance 7.0
model initialize at round 957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.99925831,  15.09848243,   0.59395969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.001434353279695}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5892525976305197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4068544 ,  15.81751945,   6.18526766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0100295806378694}
episode index:958
target Thresh 49.757694437816845
target distance 5.0
model initialize at round 958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.81207543,  21.02239101,   0.7312212 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.127489923360274}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5896100666089102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77866698,  15.65935331,   4.24263018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0203279152728921}
episode index:959
target Thresh 49.78392362659888
target distance 44.0
model initialize at round 959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.19968406, 15.84670316]), 'previousTarget': array([91., 15.]), 'currentState': array([70.21132983, 16.52912198,  2.98364091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5896519557386959
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4910824 ,  15.90302076,   5.09572981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.03655371907458}
episode index:960
target Thresh 49.81012659930233
target distance 40.0
model initialize at round 960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.40891849,  9.66590426]), 'previousTarget': array([94.51219512, 10.3902439 ]), 'currentState': array([73.9926702 ,  4.86910298,  2.97471619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2811251814469003
running average episode reward sum: 0.5887458400912603
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([112.52025895,  14.40712963]), 'previousTarget': array([110.85047846,  13.838137  ]), 'currentState': array([93.06848386,  9.75649032,  1.00249126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:961
target Thresh 49.83630338213022
target distance 39.0
model initialize at round 961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.79181414, 10.3005126 ]), 'previousTarget': array([95.24899352,  9.4292033 ]), 'currentState': array([77.42642226,  5.3023532 ,  1.20012453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 88
reward sum = 0.3480524791527359
running average episode reward sum: 0.5884956390923638
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17995054,  15.18587204,   6.0733341 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8408504837225412}
episode index:962
target Thresh 49.86245400125931
target distance 11.0
model initialize at round 962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.53983945,  22.32082215,   0.59459656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.961984558041825}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5888427305309262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40408393,  15.60360199,   4.67858306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8482047711169122}
episode index:963
target Thresh 49.88857848284022
target distance 33.0
model initialize at round 963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.0840621 ,   9.52103515]), 'previousTarget': array([100.60816786,   9.33049037]), 'currentState': array([83.67215793,  1.71067121,  0.81491047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.3632458686115518
running average episode reward sum: 0.588608708889931
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0463966 ,  15.46651409,   5.83403634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0616001310251506}
episode index:964
target Thresh 49.91467685299744
target distance 10.0
model initialize at round 964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05872616e+02, 1.14182702e+01, 9.22632217e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.804995465270986}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5889454016760385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05566755,  15.71126164,   0.12434876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.182225399974768}
episode index:965
target Thresh 49.94074913782933
target distance 11.0
model initialize at round 965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.81989732,   5.62677489,   1.59092829]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.447221355026116}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5892813973756323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05626478,  14.75331508,   1.06416577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9754432934444213}
episode index:966
target Thresh 49.966795363408195
target distance 46.0
model initialize at round 966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.56107891, 13.68606047]), 'previousTarget': array([88.98112317, 13.86874449]), 'currentState': array([70.58992231, 12.61232681,  5.05450428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.24984662449528022
running average episode reward sum: 0.5884136331337804
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.92950295,  14.54913207,   1.69937423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.161570414839386}
episode index:967
target Thresh 49.99281555578025
target distance 40.0
model initialize at round 967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.74562096, 18.54424341]), 'previousTarget': array([94.61161351, 19.0776773 ]), 'currentState': array([76.07608125, 22.16490738,  4.61321971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.068607
running average episode reward sum: 0.587734892810295
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.48328871, 22.36657975,  5.56168421]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.08502831217478}
episode index:968
target Thresh 50.0188097409657
target distance 35.0
model initialize at round 968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.75447452, 18.43815427]), 'previousTarget': array([99.49717013, 18.54350397]), 'currentState': array([78.1404695 , 22.34850047,  3.17670345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.26299709944834926
running average episode reward sum: 0.5873997660885593
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14023255e+02, 1.53767220e+01, 3.35905194e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0468767204743668}
episode index:969
target Thresh 50.04477794495872
target distance 50.0
model initialize at round 969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.23846814, 19.3543734 ]), 'previousTarget': array([84.80683493, 19.22704311]), 'currentState': array([63.42381216, 22.07088   ,  3.95092869]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.497957355728769
running average episode reward sum: 0.5862808412207063
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([113.60004518,  14.64884435]), 'currentState': array([105.1286877 ,  22.09438819,   1.11624034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.156198023537133}
episode index:970
target Thresh 50.070720193727524
target distance 32.0
model initialize at round 970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.51093605,  15.772962  ]), 'previousTarget': array([102.96105157,  15.75243428]), 'currentState': array([84.56502124, 17.24281996,  1.15636378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5863522916628829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58516837,  15.96296138,   4.9321598 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0485131813944533}
episode index:971
target Thresh 50.09663651321436
target distance 6.0
model initialize at round 971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.9820689 ,  10.15294204,   2.21484244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.529087161648999}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5867274333894644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0299265 ,  14.36456425,   0.38248754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1596642516649167}
episode index:972
target Thresh 50.12252692933554
target distance 9.0
model initialize at round 972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.32847377,   7.79372129,   1.54202133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.525339309658449}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5870823541649193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2352628 ,  14.13103011,   1.20905679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1575541663900937}
episode index:973
target Thresh 50.148391467981504
target distance 29.0
model initialize at round 973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.7538535 ,  16.23260396]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([86.97360957, 19.18928281,  6.15797013]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.18955035721621577
running average episode reward sum: 0.5866742104308857
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04804499,  14.77821794,   5.28615377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9774485295734776}
episode index:974
target Thresh 50.17423015501678
target distance 25.0
model initialize at round 974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.48319083,  15.18877629]), 'previousTarget': array([109.93630557,  15.40509555]), 'currentState': array([91.5119423 , 16.26079869,  4.81845028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.07971456204556204
running average episode reward sum: 0.5861542518171572
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.568594  ,  15.96198283,   5.74567457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.054287481733399}
episode index:975
target Thresh 50.200043016280055
target distance 23.0
model initialize at round 975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.01030186,  14.11196235]), 'previousTarget': array([110.88993593,  13.5704125 ]), 'currentState': array([9.28381888e+01, 8.41722078e+00, 3.71131897e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6049354405550535
running average episode reward sum: 0.5861734948384051
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72022719,  14.17894961,   1.30206439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.867407960737288}
episode index:976
target Thresh 50.22583007758421
target distance 28.0
model initialize at round 976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.83750625,  12.99048333]), 'previousTarget': array([106.40285  ,  12.8507125]), 'currentState': array([88.58103354,  7.58786739,  5.01818705]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.10408107274039136
running average episode reward sum: 0.5854669906750695
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([107.12005131,  11.85255191]), 'previousTarget': array([108.56524687,  12.30270061]), 'currentState': array([88.54683681,  4.4339467 ,  2.24032443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:977
target Thresh 50.2515913647163
target distance 35.0
model initialize at round 977
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.91011787, 15.10600779]), 'previousTarget': array([99.96742669, 15.85900419]), 'currentState': array([80.       , 17.       ,  2.9850445], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5641476949995692
running average episode reward sum: 0.5854451918042356
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08363036,  15.53108492,   6.08630486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.059143289444232}
episode index:978
target Thresh 50.27732690343758
target distance 27.0
model initialize at round 978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.59365269,  16.74216081]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([88.12501097, 21.32167931,  5.79673743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5855803147263949
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67984214,  15.96496574,   5.25653173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1804000238609211}
episode index:979
target Thresh 50.303036719483636
target distance 4.0
model initialize at round 979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.98538179,  10.78541578,   3.66667938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.164806341996608}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5859629838032048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39816874,  14.02483503,   2.11656746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1459265162758885}
episode index:980
target Thresh 50.32872083856427
target distance 20.0
model initialize at round 980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.66072964,  14.94000661]), 'previousTarget': array([114.9007438 ,  14.99007438]), 'currentState': array([93.68076592, 14.0449926 ,  3.48167467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5853656718931097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.15504992,  13.52194928]), 'previousTarget': array([110.76301465,  13.96835462]), 'currentState': array([89.76539445,  8.61876042,  2.50767117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:981
target Thresh 50.35437928636361
target distance 46.0
model initialize at round 981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.31960442, 12.96904892]), 'previousTarget': array([88.98112317, 13.86874449]), 'currentState': array([69.38185796, 11.39225904,  3.93558025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.44785648285211727
running average episode reward sum: 0.5843135108393978
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([114.99118334,  13.82679649]), 'previousTarget': array([114.46923219,  14.64159781]), 'currentState': array([97.8941982 ,  3.44926792,  2.06210111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:982
target Thresh 50.380012088540084
target distance 8.0
model initialize at round 982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.80839356,   7.38304781,   0.89760404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.118191610892424}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5846768543170799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02145934,  14.22291923,   1.73666443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.249558459871148}
episode index:983
target Thresh 50.40561927072652
target distance 6.0
model initialize at round 983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.55926641,  21.11145678,   5.93407011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.072360378706799}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5850298914040616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90703681,  15.08188055,   3.8443166 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.910725099512075}
episode index:984
target Thresh 50.43120085853009
target distance 14.0
model initialize at round 984
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.54557467,  14.32117308]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  17.      ,   3.709326], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.774695094972543}
done in step count: 14
reward sum = 0.5949761620689782
running average episode reward sum: 0.5850399891407773
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24317255,  15.64745333,   0.15536392]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9959837371973639}
episode index:985
target Thresh 50.45675687753239
target distance 40.0
model initialize at round 985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.38188771, 17.32781692]), 'previousTarget': array([94.9439862 , 16.50420104]), 'currentState': array([74.50814952, 19.57159285,  0.87156534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.39705864767830984
running average episode reward sum: 0.5848493386930466
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00782347,  15.53932867,   6.0675758 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.129287243724213}
episode index:986
target Thresh 50.482287353289436
target distance 8.0
model initialize at round 986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.93885368,   8.44412303,   1.07061897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.282193244462931}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5851916845448549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46128505,  14.16027639,   0.44215028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9976720584345088}
episode index:987
target Thresh 50.50779231133172
target distance 8.0
model initialize at round 987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.07759420e+02, 1.03889499e+01, 6.04450703e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.58415841068001}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5854611846324561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03548359,  15.52305318,   0.87083152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0972130736248058}
episode index:988
target Thresh 50.53327177716418
target distance 43.0
model initialize at round 988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.7317468 ,  9.11291972]), 'previousTarget': array([91.57581251, 10.09726308]), 'currentState': array([71.2954541 ,  4.39799382,  3.40625787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.32309522459961515
running average episode reward sum: 0.5845425229446584
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([108.290951  ,  16.11571763]), 'previousTarget': array([108.85531513,  16.40868666]), 'currentState': array([88.56190202, 19.39666667,  5.22761269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:989
target Thresh 50.5587257762663
target distance 29.0
model initialize at round 989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.52693728,  17.21102272]), 'previousTarget': array([105.70920231,  16.60186167]), 'currentState': array([84.95826776, 21.34226628,  1.24810946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.47712338870067206
running average episode reward sum: 0.5834701331349156
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([108.42631852,  15.73028907]), 'previousTarget': array([109.98374253,  15.68603533]), 'currentState': array([88.54860401, 17.93856119,  2.40689508]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:990
target Thresh 50.58415433409208
target distance 5.0
model initialize at round 990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.74208781,  11.47316266,   1.881007  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.142159349538039}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5838218941992669
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18075928,  14.0545181 ,   2.28210813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9626058022360313}
episode index:991
target Thresh 50.609557476070066
target distance 40.0
model initialize at round 991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.69920817, 12.27700556]), 'previousTarget': array([94.9007438 , 12.99007438]), 'currentState': array([73.86065093,  9.74093641,  4.8328768 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.42633389575705205
running average episode reward sum: 0.583663136136321
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24084356,  15.78231223,   5.93409771]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0901059220192948}
episode index:992
target Thresh 50.63493522760342
target distance 38.0
model initialize at round 992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.27952423,  9.98054327]), 'previousTarget': array([96.0716533 ,  9.02262736]), 'currentState': array([76.96186454,  4.80096566,  0.80019176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.16623648301050598
running average episode reward sum: 0.5829079502157302
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([108.60209528,  15.09593698]), 'previousTarget': array([107.27112096,  14.82574679]), 'currentState': array([88.60434342, 15.39580452,  5.92850741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:993
target Thresh 50.66028761406988
target distance 3.0
model initialize at round 993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54944794,  16.77392295,   5.26457757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8302458276392641}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5832879180827163
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23648911,  15.79593587,   5.1927682 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8303258391813229}
episode index:994
target Thresh 50.68561466082184
target distance 35.0
model initialize at round 994
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.25702192,  9.40065799]), 'previousTarget': array([99.07987434,  9.99653193]), 'currentState': array([80.       ,  4.       ,  0.3309087], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.3684532549018354
running average episode reward sum: 0.5830720038483637
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15580515,  15.15595274,   6.04969294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8584790039866332}
episode index:995
target Thresh 50.710916393186366
target distance 30.0
model initialize at round 995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.4251637 ,  17.72247148]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([86.18769692, 23.1923775 ,  6.06055004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5832675353298403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0889919 ,  15.05164287,   0.28069509]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.91247067655746}
episode index:996
target Thresh 50.73619283646517
target distance 5.0
model initialize at round 996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.99803254,  16.08551729,   0.58379889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.118400754689277}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5836173826844815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30319492,  15.16138657,   0.40059776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7152502646906344}
episode index:997
target Thresh 50.7614440159347
target distance 9.0
model initialize at round 997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.5950034 ,  21.46301562,   4.94843781]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.828761136190234}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5839759626110511
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35695503,  15.7008854 ,   5.70287471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9511819927804634}
episode index:998
target Thresh 50.786669956846154
target distance 42.0
model initialize at round 998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.90886877,  9.71115399]), 'previousTarget': array([92.34744447,  9.06718784]), 'currentState': array([74.50950016,  4.84653711,  1.43836957]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.690378510877421
running average episode reward sum: 0.5827003325074591
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([111.77222906,  16.26159292]), 'previousTarget': array([110.27929458,  16.70741146]), 'currentState': array([93.14453937, 23.54233306,  0.87227637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:999
target Thresh 50.81187068442546
target distance 4.0
model initialize at round 999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.67280097,  11.8777433 ,   6.04402542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.5627557739832785}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5830879311749516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25511905,  14.90987795,   0.81541113]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7503130064133587}
episode index:1000
target Thresh 50.83704622387334
target distance 22.0
model initialize at round 1000
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.37049076,  15.02163804]), 'previousTarget': array([112.50265712,  15.56757793]), 'currentState': array([93.        , 20.        ,  0.76560795], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.17572383658552146
running average episode reward sum: 0.582329877460905
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05591323,  16.303435  ,   5.3065985 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.304633696686379}
episode index:1001
target Thresh 50.86219660036535
target distance 43.0
model initialize at round 1001
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([91.82449081, 18.35622165]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([72.       , 21.       ,  0.6171835], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.16062967308022863
running average episode reward sum: 0.5819090189734992
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09660812,  15.16270923,   6.22363733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.917927654672711}
episode index:1002
target Thresh 50.887321839051886
target distance 11.0
model initialize at round 1002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.3284727 ,   4.19591977,   2.01491499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.853647005485668}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5822305275039432
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0841631 ,  14.09597862,   0.964141  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2868612523097127}
episode index:1003
target Thresh 50.912421965058144
target distance 8.0
model initialize at round 1003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.67101836,  13.54808582,   0.66246169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.493386130154926}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5826170498869074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06732095,  14.07585743,   5.8687644 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3129850331602524}
episode index:1004
target Thresh 50.93749700348429
target distance 47.0
model initialize at round 1004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.07578642, 14.81710092]), 'previousTarget': array([87.98191681, 13.85029433]), 'currentState': array([68.07624787, 14.68124191,  0.51547527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4563223786014289
running average episode reward sum: 0.5824913835473198
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16689141,  15.43716816,   0.13452429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9408432011828701}
episode index:1005
target Thresh 50.962546979405346
target distance 8.0
model initialize at round 1005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.51855192,   5.34860639,   3.66082788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.965288920093688}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5828388725774984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26382807,  14.1555477 ,   0.93731707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1202896056158083}
episode index:1006
target Thresh 50.9875719178713
target distance 44.0
model initialize at round 1006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.69706058, 11.3125369 ]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([70.92337753,  8.31229378,  3.66792417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.2730641701043818
running average episode reward sum: 0.5825312512244963
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22636696,  14.11429572,   0.97726678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1760017624567645}
episode index:1007
target Thresh 51.01257184390708
target distance 34.0
model initialize at round 1007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.7795695 ,   9.83980717]), 'previousTarget': array([99.6810367 ,  9.14274933]), 'currentState': array([81.97908863,  3.01764335,  6.07788998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 24
reward sum = 0.6634394623034625
running average episode reward sum: 0.582611517306916
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31992252,  14.96393557,   6.11991838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6810330540925574}
episode index:1008
target Thresh 51.03754678251263
target distance 8.0
model initialize at round 1008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.47474523,  18.99593854,   5.51368088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.651566811353655}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5829214546081566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57486064,  14.11690267,   1.13243413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9801042631463703}
episode index:1009
target Thresh 51.062496758662874
target distance 37.0
model initialize at round 1009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.53468775, 18.20630407]), 'previousTarget': array([97.65140491, 18.28216664]), 'currentState': array([79.95113521, 22.26641118,  0.44324714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5268848390153413
running average episode reward sum: 0.58286597281054
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03233268,  15.7322834 ,   5.8660585 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2135151486160143}
episode index:1010
target Thresh 51.08742179730781
target distance 20.0
model initialize at round 1010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.95552563,  15.0159578 ]), 'previousTarget': array([114.97504678,  14.99875234]), 'currentState': array([93.95785948, 15.32148826,  3.24855423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.470657977252261
running average episode reward sum: 0.5827549856734893
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14110267e+02, 1.53435087e+01, 5.19567728e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9537414564998992}
episode index:1011
target Thresh 51.112321923372456
target distance 3.0
model initialize at round 1011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.31731007,  17.11789543,   4.33157086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4941505257875347}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5831476190868553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66749741,  15.73816072,   4.38018429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9952055287231869}
episode index:1012
target Thresh 51.13719716175696
target distance 13.0
model initialize at round 1012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.48345592,   1.31229208,   2.68131351]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.413622675782774}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5834737490260428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68671189,  14.05193931,   0.8298817 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.998483104150423}
episode index:1013
target Thresh 51.16204753733654
target distance 18.0
model initialize at round 1013
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.49848099,  14.60303895]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       ,  7.       ,  2.2121754], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.43998574624956527
running average episode reward sum: 0.5833322421199516
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2147693 ,  15.97039212,   5.46104039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2482980913184303}
episode index:1014
target Thresh 51.186873074961596
target distance 25.0
model initialize at round 1014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.18977047,  12.76429983]), 'previousTarget': array([109.25928039,  13.39259851]), 'currentState': array([89.18752531,  6.52613672,  3.19858265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5834936606935591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02200353,  14.92483639,   0.77602327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9808805557437963}
episode index:1015
target Thresh 51.21167379945766
target distance 11.0
model initialize at round 1015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.67291321,  21.32534591,   6.00061919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.26962949654856}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5838275691913292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.547263  ,  15.86067716,   5.80328399]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9724895772300531}
episode index:1016
target Thresh 51.236449735625456
target distance 5.0
model initialize at round 1016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.54085626,  19.14778782,   5.53757143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.400909111645095}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5841608210352196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39041741,  15.96194214,   5.58169131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1388255392537918}
episode index:1017
target Thresh 51.26120090824092
target distance 1.0
model initialize at round 1017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01644419,  14.08694526,   1.79738283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9132028071675444}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5845693074585642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01644419,  14.08694526,   1.79738283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9132028071675444}
episode index:1018
target Thresh 51.285927342055246
target distance 7.0
model initialize at round 1018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.746024  ,   8.8797414 ,   4.62787437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.27548953106093}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5848654954509661
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06273775,  14.52668803,   1.15787727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0499927348347922}
episode index:1019
target Thresh 51.310629061794835
target distance 7.0
model initialize at round 1019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.23982689,   8.79071244,   1.57772272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.7951311735391355}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.585205887463178
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00252764,  14.14910592,   0.42421233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3110956605675215}
episode index:1020
target Thresh 51.335306092161446
target distance 27.0
model initialize at round 1020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.81577257,  12.3957223 ]), 'previousTarget': array([106.75497521,  11.94628712]), 'currentState': array([87.75739513,  6.33121496,  0.74106121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = -0.08988300182989001
running average episode reward sum: 0.5845446838497665
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39369567,  15.31231162,   0.15285145]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6820142852292408}
episode index:1021
target Thresh 51.35995845783208
target distance 24.0
model initialize at round 1021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.7725614 ,  15.58339052]), 'previousTarget': array([110.57960839,  15.92091492]), 'currentState': array([90.96032634, 18.31749767,  5.6988132 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5847811064916313
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57032421,  15.91593803,   5.83929767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0117132794513037}
episode index:1022
target Thresh 51.38458618345912
target distance 5.0
model initialize at round 1022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.10199985e+02, 2.03305383e+01, 7.15169907e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.173199034292491}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.585148471988707
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11161082,  15.74731708,   5.63611713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1609126335574695}
episode index:1023
target Thresh 51.4091892936703
target distance 5.0
model initialize at round 1023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.10769745e+02, 1.15094829e+01, 1.08570933e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.484411063894641}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.585505739154636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24346211,  15.04544164,   0.85964468]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7579013967655048}
episode index:1024
target Thresh 51.433767813068734
target distance 44.0
model initialize at round 1024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.36721992, 11.41361282]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([71.59361217,  8.41287346,  4.07979035]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.029791308637205825
running average episode reward sum: 0.5849054493519122
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([90.09861343, 12.7244044 ]), 'previousTarget': array([89.91705812, 13.57431954]), 'currentState': array([70.18160487, 10.90430266,  4.22556169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1025
target Thresh 51.45832176623291
target distance 15.0
model initialize at round 1025
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.66524448,  15.19025169]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   5.       ,   5.1516023], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.046411243539787}
done in step count: 11
reward sum = 0.6874312542587164
running average episode reward sum: 0.5850053770370066
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00105502,  14.03249055,   0.33615201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.390670917075728}
episode index:1026
target Thresh 51.482851177716825
target distance 6.0
model initialize at round 1026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.38686419,  19.98768278,   4.70112417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.793968026471837}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5853163572687221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05920242,  15.95858354,   5.67587686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9604099821586446}
episode index:1027
target Thresh 51.50735607204985
target distance 5.0
model initialize at round 1027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.63430158,  18.84959318,   5.73544624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.113442431802554}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5856908540028964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42595125,  15.52578932,   5.60377661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.778451268701731}
episode index:1028
target Thresh 51.53183647373694
target distance 3.0
model initialize at round 1028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.56407035,  15.15301842,   5.62613878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4440597629330194}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5860741476336031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24507764,  15.22718536,   0.87496468]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7883660087757743}
episode index:1029
target Thresh 51.55629240725844
target distance 19.0
model initialize at round 1029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.27181518,  15.57834512]), 'previousTarget': array([114.76686234,  15.08589282]), 'currentState': array([94.30568081, 21.92545053,  4.17359245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5862305534070963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06362133,  15.18823503,   6.25351173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9551112219703162}
episode index:1030
target Thresh 51.58072389707032
target distance 48.0
model initialize at round 1030
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([86.57157635,  8.11745057]), 'previousTarget': array([86.49464637,  8.46752313]), 'currentState': array([67.       ,  4.       ,  3.7272995], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11682802309987764
running average episode reward sum: 0.5855486343222205
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([110.4818406 ,  14.52251824]), 'previousTarget': array([111.87660764,  14.58293918]), 'currentState': array([90.5925976 , 12.42061125,  1.8858964 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1031
target Thresh 51.60513096760406
target distance 1.0
model initialize at round 1031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16680014,  15.92420306,   2.96522817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9391344831714391}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.585950234482761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16680014,  15.92420306,   2.96522817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9391344831714391}
episode index:1032
target Thresh 51.62951364326673
target distance 29.0
model initialize at round 1032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.19267433,  15.59165536]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([87.24985738, 17.10296346,  4.73995707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5859570265562545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05323758,  15.52137054,   6.08687558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0808266873809116}
episode index:1033
target Thresh 51.65387194844102
target distance 11.0
model initialize at round 1033
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.74369355,  15.41873814]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,   5.       ,   4.1695547], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.460614502032985}
done in step count: 93
reward sum = -0.4530013937457647
running average episode reward sum: 0.5849522311787864
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87233598,  15.94255161,   5.75389723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9511580521972758}
episode index:1034
target Thresh 51.67820590748522
target distance 10.0
model initialize at round 1034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.40886564,   6.92058987,   1.5887801 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.7934073850276}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5852786007084957
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00985794,  14.77164802,   1.15888819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0161328301716903}
episode index:1035
target Thresh 51.702515544733316
target distance 22.0
model initialize at round 1035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.29864536,  15.49367458]), 'previousTarget': array([112.6773982 ,  15.42229124]), 'currentState': array([91.47420007, 18.13779428,  2.76874089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.585519175187976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1464024 ,  15.45703084,   0.73141873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9682489631781379}
episode index:1036
target Thresh 51.72680088449493
target distance 50.0
model initialize at round 1036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.47660455, 19.01983496]), 'previousTarget': array([84.80683493, 19.22704311]), 'currentState': array([66.67230995, 21.810877  ,  5.16057289]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.14555451864432195
running average episode reward sum: 0.5848141860907414
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([108.61696777,  15.15797666]), 'previousTarget': array([106.90963224,  15.31413731]), 'currentState': array([88.62309032, 15.65281453,  5.14628069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1037
target Thresh 51.75106195105539
target distance 40.0
model initialize at round 1037
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.63944207, 12.78051787]), 'previousTarget': array([94.77872706, 11.96680906]), 'currentState': array([75.       ,  9.       ,  6.0773454], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = -0.27093958277444535
running average episode reward sum: 0.5839897604945322
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28132883,  15.76678652,   6.26045997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0509280745083436}
episode index:1038
target Thresh 51.775298768675796
target distance 48.0
model initialize at round 1038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.63023285, 16.86174941]), 'previousTarget': array([86.96105157, 16.75243428]), 'currentState': array([68.67989323, 18.27027656,  5.46416718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.03569900223746957
running average episode reward sum: 0.5833933324264552
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.35848408,  15.26398599]), 'previousTarget': array([113.18173837,  15.43078188]), 'currentState': array([93.61219891, 18.43955235,  3.66276106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1039
target Thresh 51.79951136159294
target distance 8.0
model initialize at round 1039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.56888084,   6.8371845 ,   0.85754221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.854610983996789}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5837285939798019
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69711018,  14.22020577,   1.15602764]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8365532171476779}
episode index:1040
target Thresh 51.82369975401944
target distance 43.0
model initialize at round 1040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.54544799, 16.41913883]), 'previousTarget': array([91.99459386, 15.53500945]), 'currentState': array([72.58527175, 17.68063188,  1.95557606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.032285537719266684
running average episode reward sum: 0.5831368416919065
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.45822534,  18.04031339,   0.48748201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.066720460091181}
episode index:1041
target Thresh 51.84786397014367
target distance 27.0
model initialize at round 1041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.94143172,  13.38518126]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([87.33127264,  9.45559338,  3.29376221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5825772094062137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.84224262,  15.82449602]), 'previousTarget': array([109.46108263,  15.55619882]), 'currentState': array([87.97362206, 18.11314562,  3.81322008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1042
target Thresh 51.87200403412986
target distance 48.0
model initialize at round 1042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.08564879, 17.15635491]), 'previousTarget': array([86.98266146, 16.16738911]), 'currentState': array([67.14505707, 18.6967451 ,  0.50944757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.31785746583915225
running average episode reward sum: 0.5817138971576563
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.66280894,  14.25413845,   0.64700517]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5311398940806504}
episode index:1043
target Thresh 51.89611997011807
target distance 15.0
model initialize at round 1043
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.332006  ,  15.09625351]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  19.       ,   5.2363763], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.891782482244983}
done in step count: 14
reward sum = 0.7987458127689782
running average episode reward sum: 0.5819217821342956
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14273352e+02, 1.52640686e+01, 3.47163876e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7731425115549366}
episode index:1044
target Thresh 51.92021180222425
target distance 44.0
model initialize at round 1044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.86531916, 15.44776391]), 'previousTarget': array([90.99483671, 15.54557189]), 'currentState': array([72.86941005, 15.85226246,  5.58046055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.33063793939101804
running average episode reward sum: 0.5816813191268857
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00946894,  15.54152986,   5.88851873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1288960821003815}
episode index:1045
target Thresh 51.944279554540216
target distance 48.0
model initialize at round 1045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.57270785, 15.09017203]), 'previousTarget': array([87., 15.]), 'currentState': array([68.57282428, 15.15841323,  1.0047124 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.44612778984710233
running average episode reward sum: 0.5815517268426794
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40969562,  15.14432642,   0.56075153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.607691844798115}
episode index:1046
target Thresh 51.96832325113375
target distance 10.0
model initialize at round 1046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.66316334,  15.09013877,   0.76718703]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.337323941155423}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5818954980199079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17380612,  15.18994219,   0.69251522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8477466392849818}
episode index:1047
target Thresh 51.992342916048514
target distance 24.0
model initialize at round 1047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.59195717,  13.78872761]), 'previousTarget': array([109.97366596,  13.32455532]), 'currentState': array([92.74683239,  7.09087184,  5.44170076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.3697566461702231
running average episode reward sum: 0.5816930754513491
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00082458,  15.71144627,   5.81367543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.226583592933675}
episode index:1048
target Thresh 52.01633857330421
target distance 12.0
model initialize at round 1048
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.10299453,  15.47969643]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  23.      ,   5.677576], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.594660149823373}
done in step count: 9
reward sum = 0.8435172474836408
running average episode reward sum: 0.581942669514297
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16769936,  14.99459638,   6.12475709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8323181838571144}
episode index:1049
target Thresh 52.04031024689646
target distance 26.0
model initialize at round 1049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.3922518 ,  16.62617594]), 'previousTarget': array([108.48782391,  16.50280987]), 'currentState': array([90.18360393, 22.19644048,  6.06397683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5821518961047967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12515688,  14.74186471,   0.55273463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9121317446309405}
episode index:1050
target Thresh 52.06425796079698
target distance 37.0
model initialize at round 1050
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.57076799, 10.12129111]), 'previousTarget': array([97.43335206, 10.72703158]), 'currentState': array([78.       ,  6.       ,  3.5600975], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.24553534221395365
running average episode reward sum: 0.5818316139412469
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08201075,  15.13045211,   0.86331278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9272119602344516}
episode index:1051
target Thresh 52.08818173895345
target distance 22.0
model initialize at round 1051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.42042646,  13.39149734]), 'previousTarget': array([111.79586847,  13.83486126]), 'currentState': array([93.17760213,  5.19397574,  5.48061931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5820405483286973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12409973,  14.04720296,   1.12530532]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2942269876386738}
episode index:1052
target Thresh 52.112081605289674
target distance 45.0
model initialize at round 1052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.50701204, 13.75483317]), 'previousTarget': array([89.95570316, 13.33038021]), 'currentState': array([68.52906538, 12.8158724 ,  3.64517677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5818424702015612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41420613,  15.90172753,   5.73238779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.075298563238346}
episode index:1053
target Thresh 52.135957583705505
target distance 6.0
model initialize at round 1053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.18826934,  18.75428539,   0.13451677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.1030656811392685}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5821659068469373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11157115,  15.14846576,   6.10625637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9007485256802086}
episode index:1054
target Thresh 52.15980969807694
target distance 44.0
model initialize at round 1054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.77201036, 17.11120097]), 'previousTarget': array([90.8721051 , 17.74180624]), 'currentState': array([69.84167623, 18.77906834,  2.94329882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5823082172382239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01555432,  15.83293275,   5.84839213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2895387824732594}
episode index:1055
target Thresh 52.183637972256086
target distance 28.0
model initialize at round 1055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.16143115,  16.48587114]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([88.61744466, 20.7323511 ,  5.11548376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.4884248260218059
running average episode reward sum: 0.5822193125116932
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25768595,  15.97059925,   5.84705402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2219218664144051}
episode index:1056
target Thresh 52.20744243007122
target distance 49.0
model initialize at round 1056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.22929961, 18.51034443]), 'previousTarget': array([85.79898987, 19.17157288]), 'currentState': array([67.38719125, 21.01847831,  0.35615378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4191764255214129
running average episode reward sum: 0.5820650619090534
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04022996,  15.18640692,   0.13822899]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9777044863987325}
episode index:1057
target Thresh 52.2312230953268
target distance 7.0
model initialize at round 1057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88915317,   9.67133253,   2.58968109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.402341197703415}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5823958750338152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28730632,  14.07235351,   1.62542376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.169812075716953}
episode index:1058
target Thresh 52.254979991803495
target distance 6.0
model initialize at round 1058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.99847953,   8.7483737 ,   2.46715605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9348364474719935}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5827085486621909
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52181945,  14.10445425,   1.80671836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0152136893645571}
episode index:1059
target Thresh 52.278713143258194
target distance 40.0
model initialize at round 1059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.01885947, 18.94409593]), 'previousTarget': array([94.61161351, 19.0776773 ]), 'currentState': array([73.33324272, 22.47630394,  4.34337139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -1.0292933504053134
running average episode reward sum: 0.5811877921536367
{'dynamicTrap': 23, 'scaleFactor': 20, 'currentTarget': array([104.21861872,  17.58400025]), 'previousTarget': array([97.09279573, 17.86320595]), 'currentState': array([84.76942801, 22.24543576,  4.92447234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:1060
target Thresh 52.30242257342407
target distance 51.0
model initialize at round 1060
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.97584994, 11.98255744]), 'previousTarget': array([83.93876756, 12.56382491]), 'currentState': array([64.       , 11.       ,  2.4378688], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.12305245606657073
running average episode reward sum: 0.5807559963609062
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03817564,  14.89742265,   0.33007859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9672787700343566}
episode index:1061
target Thresh 52.32610830601054
target distance 46.0
model initialize at round 1061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.2795369 , 15.61164866]), 'previousTarget': array([89., 15.]), 'currentState': array([70.28565606, 16.10634937,  1.56917446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.65378432527442
running average episode reward sum: 0.5808247612657212
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31813165,  15.70005763,   5.4166519 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9772538707650382}
episode index:1062
target Thresh 52.349770364703346
target distance 7.0
model initialize at round 1062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.4140898 ,  19.56703834,   5.33425045]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.854596133971151}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.581137736323311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80780998,  15.91973296,   5.78205131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9395987051650317}
episode index:1063
target Thresh 52.37340877316455
target distance 47.0
model initialize at round 1063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.7177802 , 18.12113708]), 'previousTarget': array([87.92796014, 17.30400339]), 'currentState': array([66.83846535, 20.31495593,  1.41074634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3778439003289263
running average episode reward sum: 0.5809466706879779
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14085498e+02, 1.49673083e+01, 7.39929040e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.915086006223254}
episode index:1064
target Thresh 52.397023555032554
target distance 40.0
model initialize at round 1064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.53074315, 16.81714788]), 'previousTarget': array([94.9439862 , 16.50420104]), 'currentState': array([73.60199904, 18.50390748,  1.78564   ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5810887896541436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03835542,  15.59729858,   6.03853626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1320450086906872}
episode index:1065
target Thresh 52.420614733922136
target distance 35.0
model initialize at round 1065
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.93580627, 14.6011335 ]), 'previousTarget': array([99.96742669, 14.14099581]), 'currentState': array([80.       , 13.       ,  4.6653595], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = -0.05453418393672127
running average episode reward sum: 0.5804925204481484
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00424727,  15.28273777,   6.22664607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0351155241380439}
episode index:1066
target Thresh 52.44418233342451
target distance 44.0
model initialize at round 1066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.21042138, 10.76339433]), 'previousTarget': array([90.81660336, 11.70226409]), 'currentState': array([70.49625234,  7.39418981,  3.39840841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 64
reward sum = 0.4446400060389093
running average episode reward sum: 0.5803651985039971
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43880795,  15.95360407,   5.48086422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1064796616225796}
episode index:1067
target Thresh 52.46772637710723
target distance 5.0
model initialize at round 1067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.9923078 ,  17.11299041,   0.66979229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.435228572838752}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5807212198630758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16949314,  15.89104954,   6.22231642]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2180767308027571}
episode index:1068
target Thresh 52.49124688851438
target distance 28.0
model initialize at round 1068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.74035819,  15.23261032]), 'previousTarget': array([106.98725709,  15.28616939]), 'currentState': array([88.75415281, 15.97530418,  5.38520641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5808983217582508
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20005896,  15.03973982,   0.20812482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8009275349028471}
episode index:1069
target Thresh 52.514743891166454
target distance 21.0
model initialize at round 1069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.48951333,  13.37814045]), 'previousTarget': array([111.00530293,  12.52709229]), 'currentState': array([95.69026166,  2.52525404,  1.15546232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 17
reward sum = 0.7091321691909267
running average episode reward sum: 0.5810181664754777
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3384145 ,  14.03855107,   0.71258024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1670815857087902}
episode index:1070
target Thresh 52.53821740856047
target distance 12.0
model initialize at round 1070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.67666096,   3.13169604,   0.98559492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.092424912462734}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5811946603870833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01071477,  14.48734683,   0.51437213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1142255357086739}
episode index:1071
target Thresh 52.56166746416992
target distance 9.0
model initialize at round 1071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.75167119,  19.5552736 ,   0.16488617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.56088710965706}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5815219651329041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02302579,  15.55174496,   6.22731608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1220076203600564}
episode index:1072
target Thresh 52.58509408144491
target distance 35.0
model initialize at round 1072
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.80527093, 17.2158945 ]), 'previousTarget': array([99.79898987, 17.17157288]), 'currentState': array([80.       , 20.       ,  0.7211777], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6923427143471035
running average episode reward sum: 0.5816252463530478
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03386096,  15.54887764,   6.14392877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1111666469963102}
episode index:1073
target Thresh 52.608497283812
target distance 36.0
model initialize at round 1073
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.82477948, 11.64161289]), 'previousTarget': array([98.72787848, 12.28797975]), 'currentState': array([79.       ,  9.       ,  1.4795443], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.24983392045263128
running average episode reward sum: 0.5808510758066738
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([109.87562476,  16.47790056]), 'previousTarget': array([111.21036099,  16.02250051]), 'currentState': array([90.65886874, 22.02012828,  3.54884477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1074
target Thresh 52.631877094674444
target distance 25.0
model initialize at round 1074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.52385164,  12.32444533]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([91.55399823,  3.54467493,  0.69998604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5808568299519119
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14129207e+02, 1.54351335e+01, 1.00848076e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9734588803373989}
episode index:1075
target Thresh 52.65523353741203
target distance 39.0
model initialize at round 1075
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.99400498, 14.48965775]), 'previousTarget': array([95.99342862, 14.51265202]), 'currentState': array([76.      , 14.      ,  4.129415], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.29079564488323895
running average episode reward sum: 0.5805872563598407
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01975151,  15.68442759,   5.91421786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1955451618703843}
episode index:1076
target Thresh 52.678566635381216
target distance 48.0
model initialize at round 1076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.8954975 ,  7.66849839]), 'previousTarget': array([86.40285  ,  7.8507125]), 'currentState': array([68.58929937,  2.4463615 ,  0.67479246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.20066976645735377
running average episode reward sum: 0.5798618552244486
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.61402798,  14.32798585,   6.0797924 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5402991416780052}
episode index:1077
target Thresh 52.70187641191509
target distance 25.0
model initialize at round 1077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.32171802,  14.1909993 ]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([91.78858206,  9.89488479,  0.45300835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.4166557856335885
running average episode reward sum: 0.5797104581283531
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08295686,  15.64722582,   6.11383888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.122439035269239}
episode index:1078
target Thresh 52.725162890323425
target distance 52.0
model initialize at round 1078
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.93168692, 18.34837763]), 'previousTarget': array([82.90818058, 18.08575187]), 'currentState': array([63.       , 20.       ,  0.4113123], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.4997933329276084
running average episode reward sum: 0.5796363922106509
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49989408,  15.23228116,   0.30438274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5514167869370954}
episode index:1079
target Thresh 52.74842609389272
target distance 16.0
model initialize at round 1079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.52228  ,  14.6118525]), 'currentState': array([99.461565  ,  3.74417982,  2.09799278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.186882241892903}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5799040861185754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39609915,  15.32545666,   0.68639524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6860162387977893}
episode index:1080
target Thresh 52.771666045886164
target distance 44.0
model initialize at round 1080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.1520613 ,  9.22868293]), 'previousTarget': array([90.29527642,  8.26234812]), 'currentState': array([71.71319409,  4.52438259,  0.1231916 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.1397160154049562
running average episode reward sum: 0.5794968816128273
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21655137,  15.52886294,   0.48318184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9452448154588122}
episode index:1081
target Thresh 52.794882769543726
target distance 44.0
model initialize at round 1081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.63527375,  9.33353054]), 'previousTarget': array([90.59429865, 10.00792472]), 'currentState': array([72.19870855,  4.61972812,  6.16245115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.21297008355778743
running average episode reward sum: 0.57876447221803
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.59567767,  14.67880681,   0.41292752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4405853934348527}
episode index:1082
target Thresh 52.81807628808212
target distance 26.0
model initialize at round 1082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.37269114,  15.35507735]), 'previousTarget': array([108.98522349,  15.23133756]), 'currentState': array([90.43131524, 16.88528231,  5.8271038 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.578992915571324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05585511,  15.46542209,   6.22385371]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0526287588690815}
episode index:1083
target Thresh 52.84124662469486
target distance 5.0
model initialize at round 1083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.44330105,  20.68613279,   2.49078718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.895372572398956}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5793449479462583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0874936 ,  15.95665117,   5.15402116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3220625520662628}
episode index:1084
target Thresh 52.864393802552314
target distance 9.0
model initialize at round 1084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.42931291,   5.91170251,   2.18774939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.223026034816789}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5796700358724894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70901356,  14.22122755,   2.1401695 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8313601071832665}
episode index:1085
target Thresh 52.887517844801636
target distance 30.0
model initialize at round 1085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.06520495,  13.19127796]), 'previousTarget': array([104.47682419,  12.54459231]), 'currentState': array([86.46282724,  9.22305569,  1.23437142]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5400839524304888
running average episode reward sum: 0.5796335845986017
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13743492,  15.12116699,   0.78500617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8710338401440814}
episode index:1086
target Thresh 52.91061877456686
target distance 37.0
model initialize at round 1086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.4031557 , 17.63518504]), 'previousTarget': array([97.74210955, 17.79857683]), 'currentState': array([79.68264819, 20.96708893,  0.9904434 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5494869489407621
running average episode reward sum: 0.5796058508031483
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08649568,  15.44266538,   5.81806482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0151072776929404}
episode index:1087
target Thresh 52.93369661494895
target distance 34.0
model initialize at round 1087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.01291356,  10.49378162]), 'previousTarget': array([99.85980667,  9.65640235]), 'currentState': array([81.97645801,  4.36080844,  1.85493965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 35
reward sum = 0.6444416714626943
running average episode reward sum: 0.579665442550078
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10998705,  15.85481562,   5.00038662]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8618624609142674}
episode index:1088
target Thresh 52.95675138902574
target distance 18.0
model initialize at round 1088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.56593702, 21.47636936,  5.66664511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.664138419293565}
done in step count: 13
reward sum = 0.6716930929989678
running average episode reward sum: 0.5797499491161469
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28180878,  15.16791634,   5.5569657 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7375598430384501}
episode index:1089
target Thresh 52.97978311985199
target distance 45.0
model initialize at round 1089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.68611684, 10.79803599]), 'previousTarget': array([89.61161351,  9.9223227 ]), 'currentState': array([70.97826173,  7.39209308,  6.26280076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.26717751325929573
running average episode reward sum: 0.5794631854135259
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1589371 ,  15.01031285,   0.22655907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8411261239226783}
episode index:1090
target Thresh 53.002791830459444
target distance 28.0
model initialize at round 1090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.43706232,  14.44131672]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([88.50913692, 12.74491197,  1.46851557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5796449985885814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15013386,  15.14296569,   0.37964099]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8618072006242639}
episode index:1091
target Thresh 53.025777543856805
target distance 47.0
model initialize at round 1091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.51943147, 13.62955603]), 'previousTarget': array([87.92796014, 12.69599661]), 'currentState': array([68.54616133, 12.59588307,  2.24469963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5683971595933045
running average episode reward sum: 0.5796346983697214
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15691376,  15.17758313,   0.22098868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8615858459858777}
episode index:1092
target Thresh 53.0487402830298
target distance 25.0
model initialize at round 1092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.98248075,  15.81679035]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([90.24232592, 19.03024446,  4.61748213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5293517255541653
running average episode reward sum: 0.5795886938200274
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01720134,  15.00975461,   0.12133203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9828470663675288}
episode index:1093
target Thresh 53.07168007094117
target distance 45.0
model initialize at round 1093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.26128784, 14.05711252]), 'previousTarget': array([90., 15.]), 'currentState': array([70.27579868, 13.29538865,  4.117926  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.19575676002469455
running average episode reward sum: 0.5792378419609823
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19307768,  15.17966129,   5.96476835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8266812046911909}
episode index:1094
target Thresh 53.094596930530685
target distance 29.0
model initialize at round 1094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.25983623,  13.91816763]), 'previousTarget': array([105.89383588,  14.05798302]), 'currentState': array([87.45237264, 11.14970487,  0.4803459 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.468336979731843
running average episode reward sum: 0.5791365626347456
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12640152,  14.20011048,   0.22776978]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1844819765526453}
episode index:1095
target Thresh 53.11749088471524
target distance 21.0
model initialize at round 1095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.90949605,  14.27689823]), 'previousTarget': array([113.23047895,  14.49442256]), 'currentState': array([92.43544216,  9.72044952,  3.65197289]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.42745779001122364
running average episode reward sum: 0.5789981695940306
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10152509,  15.7780023 ,   5.56201683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1885052548186807}
episode index:1096
target Thresh 53.140361956388766
target distance 25.0
model initialize at round 1096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.55451209,  12.85233943]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([88.33798781,  7.30930826,  1.97869933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6349867464396457
running average episode reward sum: 0.5790492074945279
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23710164,  15.83725926,   5.48674304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1327033934142532}
episode index:1097
target Thresh 53.163210168422346
target distance 3.0
model initialize at round 1097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.5466628 ,  12.70404084,   0.92396468]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7172812647621756}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5794144632254072
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71547226,  14.21634742,   1.1296495 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8337070264596503}
episode index:1098
target Thresh 53.18603554366418
target distance 34.0
model initialize at round 1098
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.55634055,  18.81078239]), 'previousTarget': array([100.46834337,  18.41921333]), 'currentState': array([81.       , 23.       ,  2.4136314], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = -0.18291253296209975
running average episode reward sum: 0.578720808087839
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33716095,  15.84786889,   5.54277717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0762143200394438}
episode index:1099
target Thresh 53.20883810493967
target distance 22.0
model initialize at round 1099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.63108426,  15.06821331]), 'previousTarget': array([112.9793708 ,  14.90815322]), 'currentState': array([92.63937072, 15.64387792,  0.77675247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5788089973978139
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50962213,  15.99157739,   5.3116235 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1062080199769306}
episode index:1100
target Thresh 53.231617875051356
target distance 11.0
model initialize at round 1100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.53190428,   3.75470281,   0.85122221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.4992282196209}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5791047040986413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6842487 ,  14.24864081,   1.36667816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8150089107681112}
episode index:1101
target Thresh 53.25437487677902
target distance 19.0
model initialize at round 1101
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.92531335,  16.27318563]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 18.       ,  4.9743705], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5350060671375363
running average episode reward sum: 0.5790646871866983
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72368212,  14.28806918,   6.06347089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7636733979314865}
episode index:1102
target Thresh 53.27710913287968
target distance 53.0
model initialize at round 1102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.2508574 , 16.81642408]), 'previousTarget': array([81.98577525, 16.2458198 ]), 'currentState': array([63.28350911, 17.95879106,  5.94935924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 70
reward sum = 0.2824515302181505
running average episode reward sum: 0.5787957722665092
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07636704,  15.94809294,   5.21625881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9511635788334917}
episode index:1103
target Thresh 53.29982066608757
target distance 32.0
model initialize at round 1103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.3411112 ,  15.71746377]), 'previousTarget': array([102.99024152,  15.37530495]), 'currentState': array([84.38626594, 17.06065054,  6.03597314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5789348190032736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02759417,  15.07197738,   5.465462  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.975066065689095}
episode index:1104
target Thresh 53.32250949911423
target distance 4.0
model initialize at round 1104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.05596473,  15.21834239,   0.51982749]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9500743793582958}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.579271520569696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17781248,  15.0423182 ,   5.62311604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8232758629068985}
episode index:1105
target Thresh 53.3451756546485
target distance 26.0
model initialize at round 1105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.52029066,  16.31045529]), 'previousTarget': array([108.31231517,  16.80053053]), 'currentState': array([88.91716636, 20.27498775,  5.59824991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5794510412196322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29082998,  14.11692047,   1.78509709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9297373421715532}
episode index:1106
target Thresh 53.367819155356536
target distance 44.0
model initialize at round 1106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.50256925, 14.31729768]), 'previousTarget': array([90.9793708 , 13.90815322]), 'currentState': array([69.50973459, 13.78198279,  1.64835954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1160674380670707
running average episode reward sum: 0.5788227499104301
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.78788868,  15.91662887,   4.87253193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5196783675373342}
episode index:1107
target Thresh 53.390440023881844
target distance 2.0
model initialize at round 1107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.88787363,  14.7837828 ,   3.67703354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9002149145900231}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5791415609194523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99968896,  14.69740251,   3.36193746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0444822886911649}
episode index:1108
target Thresh 53.4130382828453
target distance 31.0
model initialize at round 1108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.11356063,  16.77414213]), 'previousTarget': array([103.74482241,  16.81535122]), 'currentState': array([85.42801636, 20.30675404,  5.48478109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.579356858824482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39523182,  15.62141394,   4.91011922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8671215812657778}
episode index:1109
target Thresh 53.435613954845145
target distance 15.0
model initialize at round 1109
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.92799108,  16.3197698 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.000000e+02, 9.000000e+00, 7.257245e-02], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.734292653224415}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.5795784636852335
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00664056,  14.30367384,   0.74998859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2131088553491594}
episode index:1110
target Thresh 53.45816706245707
target distance 14.0
model initialize at round 1110
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.55532038,  14.28725685]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  16.       ,   3.6551695], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.649328454727133}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.5797372933797189
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12031133,  14.82039398,   5.49741827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8978365511623598}
episode index:1111
target Thresh 53.480697628234175
target distance 5.0
model initialize at round 1111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.61186728,  11.39367588,   2.16710135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.452799608126971}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5800711537722731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10961124,  14.06883578,   1.8656288 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9375934257014265}
episode index:1112
target Thresh 53.50320567470703
target distance 49.0
model initialize at round 1112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.50873268, 12.03125663]), 'previousTarget': array([85.89668285, 12.03027376]), 'currentState': array([67.62433793,  9.88396875,  5.20187349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.40978163750610025
running average episode reward sum: 0.5799181533084221
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35238374,  14.04007406,   1.7280908 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1579570930364045}
episode index:1113
target Thresh 53.5256912243837
target distance 19.0
model initialize at round 1113
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.48998204,  15.68902959]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([96.        ,  6.        ,  0.32240015], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.994418376658988}
done in step count: 22
reward sum = 0.39199163534604586
running average episode reward sum: 0.5797494580499281
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60162611,  14.00091866,   1.6922009 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0755767210826523}
episode index:1114
target Thresh 53.54815429974971
target distance 45.0
model initialize at round 1114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.83430063, 16.63634287]), 'previousTarget': array([89.92145281, 17.22920419]), 'currentState': array([68.87329594, 17.88465835,  3.48373085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5792295033790312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.19383185,  15.61788317,   5.02501193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9089324244196382}
episode index:1115
target Thresh 53.570594923268146
target distance 42.0
model initialize at round 1115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.75992522, 18.80650811]), 'previousTarget': array([92.64677133, 19.25775784]), 'currentState': array([72.02291925, 22.03924613,  4.88782084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.5923704648028388
running average episode reward sum: 0.5792412784340704
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07698237,  14.66077881,   5.37688044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9833781324174239}
episode index:1116
target Thresh 53.59301311737964
target distance 34.0
model initialize at round 1116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.12742575,   9.63122436]), 'previousTarget': array([99.6810367 ,  9.14274933]), 'currentState': array([82.47549665,  2.41280759,  1.28280037]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.34708437027416283
running average episode reward sum: 0.5790334387669621
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14082799e+02, 1.43448742e+01, 4.96448030e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.127141317001391}
episode index:1117
target Thresh 53.61540890450238
target distance 23.0
model initialize at round 1117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.6639099,  15.1534262]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([93.79448434, 17.43507653,  0.87760609]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.5153391866142187
running average episode reward sum: 0.5789764671639633
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62339373,  14.13980322,   1.67061235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9390265113378358}
episode index:1118
target Thresh 53.63778230703217
target distance 6.0
model initialize at round 1118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.81534199,  19.21123817,   5.29656774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.374693309030275}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5793175033952734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47608519,  15.47954818,   4.90189952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7102486770533537}
episode index:1119
target Thresh 53.66013334734238
target distance 22.0
model initialize at round 1119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.04293246,  15.32718932]), 'previousTarget': array([112.9793708 ,  15.09184678]), 'currentState': array([92.16424628, 17.52669739,  1.16217768]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5795681648696003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02354139,  15.08515979,   5.11364168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9801650950031155}
episode index:1120
target Thresh 53.68246204778411
target distance 8.0
model initialize at round 1120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.35387814,  21.44600044,   5.32835364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.784541942453627}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5797955918067819
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50109749,  14.07331727,   1.92673651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0524469568857358}
episode index:1121
target Thresh 53.704768430686
target distance 41.0
model initialize at round 1121
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.98676932, 12.72735962]), 'previousTarget': array([93.94667447, 13.45951277]), 'currentState': array([74.       , 12.       ,  1.0786442], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5643662770294444
running average episode reward sum: 0.5797818401893332
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18800521,  14.07438143,   5.32566207]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.231302268036733}
episode index:1122
target Thresh 53.72705251835447
target distance 37.0
model initialize at round 1122
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.96788766, 14.86709996]), 'previousTarget': array([97.9926994 , 15.45965677]), 'currentState': array([78.     , 16.     ,  3.30481], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.09999573281269108
running average episode reward sum: 0.5793546041186505
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64429487,  14.25536294,   0.28600391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8252335930406354}
episode index:1123
target Thresh 53.749314333073606
target distance 24.0
model initialize at round 1123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.17885413,  14.46065535]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([91.37515166, 11.66541504,  0.38044941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.579581614045102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1253058 ,  15.66140202,   5.87113706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0966050188591556}
episode index:1124
target Thresh 53.77155389710522
target distance 40.0
model initialize at round 1124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.98990689,  9.79104101]), 'previousTarget': array([94.51219512, 10.3902439 ]), 'currentState': array([73.52751658,  5.1850272 ,  4.65709472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.0754567310821827
running average episode reward sum: 0.5789993577383221
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([91.30043403, 16.55271361]), 'previousTarget': array([92.52014724, 15.81615084]), 'currentState': array([71.34322051, 17.86024124,  3.14735197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1125
target Thresh 53.793771232688876
target distance 3.0
model initialize at round 1125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.15489842,  13.5462994 ,   0.69790131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3489668556105863}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5793382535218583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84737744,  14.081173  ,   1.11844588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9314164989124314}
episode index:1126
target Thresh 53.81596636204191
target distance 9.0
model initialize at round 1126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.59650354,   6.36961054,   1.15995258]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.74655242810464}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5796512323101326
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5737812 ,  14.2942252 ,   1.44380903]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8244880415253452}
episode index:1127
target Thresh 53.83813930735947
target distance 6.0
model initialize at round 1127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.43468485,  19.33199338,   5.90701616]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.293510077213068}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5798921955537361
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9476217 ,  14.12470079,   1.8015255 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.876864980016645}
episode index:1128
target Thresh 53.86029009081447
target distance 36.0
model initialize at round 1128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.27671593,  9.52639622]), 'previousTarget': array([97.81107799,  8.79288928]), 'currentState': array([80.38850846,  2.95101647,  1.61053294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = -0.0672751004686915
running average episode reward sum: 0.5793189738566391
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06020093,  15.74513437,   5.45823571]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1993529615663054}
episode index:1129
target Thresh 53.882418734557746
target distance 15.0
model initialize at round 1129
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.14600127,  13.79495726]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  15.       ,   3.2013123], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.19090748356406}
done in step count: 12
reward sum = 0.6784778717161293
running average episode reward sum: 0.579406725093683
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04834058,  14.13203909,   5.13947842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2880263195872463}
episode index:1130
target Thresh 53.90452526071789
target distance 2.0
model initialize at round 1130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88485882,  16.85943388,   4.48817843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8629953947776892}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5797523416055365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55184393,  15.90608239,   4.24887031]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0108556544791092}
episode index:1131
target Thresh 53.92660969140147
target distance 5.0
model initialize at round 1131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.68504797,  16.67227994,   5.74540322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7128731665469066}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.580080290111097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1264232 ,  15.00672047,   5.46968344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8736026458660593}
episode index:1132
target Thresh 53.9486720486929
target distance 46.0
model initialize at round 1132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.32036704, 18.71188275]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([69.52608383, 21.57306298,  6.07560778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.17416677927933216
running average episode reward sum: 0.579722025759083
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02295464,  15.23079335,   5.16350951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.003933863259628}
episode index:1133
target Thresh 53.97071235465453
target distance 4.0
model initialize at round 1133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.50687576,  12.84894116,   1.80808127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6263527295926608}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5800750927557681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.83005034,  14.16092449,   2.02759157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.18026745669706}
episode index:1134
target Thresh 53.99273063132668
target distance 15.0
model initialize at round 1134
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.00173967,  13.99768008]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   2.       ,   3.7894125], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4347046020847181
running average episode reward sum: 0.5799470130283046
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21386204,  15.567516  ,   4.88054803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9695809875846997}
episode index:1135
target Thresh 54.01472690072764
target distance 46.0
model initialize at round 1135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.22889693, 18.80213673]), 'previousTarget': array([88.83200822, 18.41321632]), 'currentState': array([70.46040846, 21.83641809,  1.53012579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5800557284173639
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18912467,  14.34238433,   0.99802178]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0440197170121062}
episode index:1136
target Thresh 54.03670118485364
target distance 46.0
model initialize at round 1136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.30465394,  7.58456913]), 'previousTarget': array([88.45157783,  8.65146426]), 'currentState': array([69.03430635,  2.23165368,  5.19612658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22720455507507248
running average episode reward sum: 0.5793457369631049
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([112.14725146,  14.7105551 ]), 'previousTarget': array([111.46023292,  14.87730225]), 'currentState': array([92.24940829, 12.69168481,  6.20239614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1137
target Thresh 54.05865350567902
target distance 51.0
model initialize at round 1137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.42277026, 17.42394941]), 'previousTarget': array([83.96548746, 16.82555956]), 'currentState': array([65.48959714, 19.05753727,  1.35518103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.27650803732921453
running average episode reward sum: 0.5785936686201415
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([113.13799389,  15.26653837]), 'previousTarget': array([116.40206633,  15.4614978 ]), 'currentState': array([93.33980491, 18.10056611,  4.0680515 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1138
target Thresh 54.080583885156074
target distance 50.0
model initialize at round 1138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.41347281,  6.406553  ]), 'previousTarget': array([84.35645014,  7.03267704]), 'currentState': array([64.11493377,  1.15618244,  3.7955842 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.04558609111870876
running average episode reward sum: 0.5781257076214571
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18390263,  14.81821233,   5.32386895]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8360990787387227}
episode index:1139
target Thresh 54.10249234521518
target distance 13.0
model initialize at round 1139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.24594103,   3.13137927,   1.74724596]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.933839580688591}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5784280049783049
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69096253,  14.15336128,   1.31684757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9012775855994517}
episode index:1140
target Thresh 54.12437890776481
target distance 48.0
model initialize at round 1140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.95910104, 10.88486854]), 'previousTarget': array([86.79065962, 10.88613786]), 'currentState': array([68.18674736,  7.87587705,  5.23266632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.07383332700666512
running average episode reward sum: 0.5779857659967347
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40911388,  14.14932756,   1.73072502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.035755766599908}
episode index:1141
target Thresh 54.14624359469153
target distance 13.0
model initialize at round 1141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.03987953,   3.27622258,   6.16027493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.0490248230625}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5781676332251152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15583354,  15.04142267,   0.73034392]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8451821411879202}
episode index:1142
target Thresh 54.16808642786001
target distance 3.0
model initialize at round 1142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.15618273,  18.62726774,   1.55746776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.80817644170853}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5784938120673505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71812436,  15.80843983,   4.02815911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0813313805884368}
episode index:1143
target Thresh 54.189907429113106
target distance 20.0
model initialize at round 1143
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.4285769 ,  12.81043868]), 'previousTarget': array([112.14985851,  13.28991511]), 'currentState': array([95.        ,  3.        ,  0.65064466], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = -0.42532519968198873
running average episode reward sum: 0.577616347896241
{'dynamicTrap': 18, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73751074,  14.09549451,   1.23816466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9418231242381997}
episode index:1144
target Thresh 54.21170662027182
target distance 15.0
model initialize at round 1144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.58836553,  7.11140615,  4.07283592]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.209109232575123}
done in step count: 22
reward sum = 0.7363860151855566
running average episode reward sum: 0.5777550113611224
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90765188,  14.00067602,   2.0640568 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3499927279187471}
episode index:1145
target Thresh 54.23348402313533
target distance 19.0
model initialize at round 1145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.71721531,  14.2869415 ]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([96.23638627,  4.56991438,  3.87992883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6806027908385961
running average episode reward sum: 0.5778447563693925
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00862209,  15.8798737 ,   5.74658659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3255217412334606}
episode index:1146
target Thresh 54.25523965948106
target distance 38.0
model initialize at round 1146
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.6335341 , 18.18891895]), 'previousTarget': array([96.66906377, 18.37675141]), 'currentState': array([77.        , 22.        ,  0.19618483], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.045898783772223783
running average episode reward sum: 0.5773809848152537
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51067436,  14.4731651 ,   0.9524477 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7337188243397106}
episode index:1147
target Thresh 54.27697355106462
target distance 7.0
model initialize at round 1147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.32452403,   8.02442097,   2.18070626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.655085825795664}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5776981443662865
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49870152,  14.22127742,   0.99162385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9247226959670669}
episode index:1148
target Thresh 54.29868571961993
target distance 5.0
model initialize at round 1148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11524137e+02, 1.52374620e+01, 4.50271070e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4839652857874275}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5779590868194047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14389017e+02, 1.43958521e+01, 5.55911327e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8592408040060215}
episode index:1149
target Thresh 54.32037618685916
target distance 26.0
model initialize at round 1149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.85811595,  15.20416517]), 'previousTarget': array([108.94108971,  15.46607002]), 'currentState': array([89.87386324, 15.9976658 ,  0.15623444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.23430560287330005
running average episode reward sum: 0.5776602577029297
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22768766,  14.14886786,   1.1696724 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1493007768162815}
episode index:1150
target Thresh 54.342044974472756
target distance 13.0
model initialize at round 1150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.96528216,   3.45792653,   1.88067677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.674948078361076}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.577944116797027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93165122,  14.6707708 ,   1.48726746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9881122732582965}
episode index:1151
target Thresh 54.363692104129534
target distance 14.0
model initialize at round 1151
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.45230002,  14.31436496]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  15.       ,   3.6905107], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.467503717436479}
done in step count: 11
reward sum = 0.6874312542587164
running average episode reward sum: 0.5780391577149625
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20369718,  15.30324502,   5.49877239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8520890375630679}
episode index:1152
target Thresh 54.3853175974766
target distance 48.0
model initialize at round 1152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.37881286, 15.58739975]), 'previousTarget': array([87., 15.]), 'currentState': array([68.38367978, 16.02859481,  5.98344964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5780840705280486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27570018,  14.77361905,   0.54860169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.758853449082547}
episode index:1153
target Thresh 54.406921476139466
target distance 54.0
model initialize at round 1153
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.99902117, 13.80213049]), 'previousTarget': array([80.99657153, 14.37030688]), 'currentState': array([61.       , 14.       ,  0.1472707], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.16362728907473612
running average episode reward sum: 0.5777249225371878
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27914165,  14.0276027 ,   4.98893823]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2104516774973237}
episode index:1154
target Thresh 54.42850376172201
target distance 29.0
model initialize at round 1154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.41293247,  15.16647029]), 'previousTarget': array([105.98811999,  15.31075448]), 'currentState': array([87.41774495, 15.60519112,  0.73840445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5778716300452349
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18303342,  14.98312495,   5.0489317 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8171408427479383}
episode index:1155
target Thresh 54.45006447580652
target distance 18.0
model initialize at round 1155
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.2692658 ,  15.60201192]), 'previousTarget': array([114.88854382,  14.94427191]), 'currentState': array([97.       ,  6.       ,  0.3762999], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.89147010405888}
done in step count: 14
reward sum = 0.6615248827689781
running average episode reward sum: 0.5779439944507052
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67452575,  14.76406148,   1.9965905 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4019956154973633}
episode index:1156
target Thresh 54.47160363995371
target distance 5.0
model initialize at round 1156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.4988781 ,  15.62060628,   5.79286415]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5557005904266346}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5782582002890373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60604476,  14.04883224,   6.19085442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0295245705560558}
episode index:1157
target Thresh 54.49312127570275
target distance 13.0
model initialize at round 1157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.58910796,   2.83780513,   1.59573531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.376686256674457}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5785242854975235
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97834413,  14.19161142,   1.25839384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.80867859642183}
episode index:1158
target Thresh 54.51461740457127
target distance 41.0
model initialize at round 1158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.83832553,  9.22131896]), 'previousTarget': array([93.31685674,  9.18257132]), 'currentState': array([72.43316388,  4.37986245,  1.91942954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.14721946543375275
running average episode reward sum: 0.57815215019117
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84467579,  14.33494433,   1.62049642]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0750703353984283}
episode index:1159
target Thresh 54.53609204805541
target distance 22.0
model initialize at round 1159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.56133015,  15.627713  ]), 'previousTarget': array([112.81660336,  15.29773591]), 'currentState': array([91.88645465, 19.21926895,  3.17101014]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5782362682074365
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04433262,  15.11503617,   5.90370904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9625660780120514}
episode index:1160
target Thresh 54.557545227629795
target distance 50.0
model initialize at round 1160
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([84.43188518,  6.73305805]), 'previousTarget': array([84.35645014,  7.03267704]), 'currentState': array([65.       ,  2.       ,  3.8764462], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.32299854295511315
running average episode reward sum: 0.5780164252054965
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09013025,  14.37426317,   5.23037056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1042687845357355}
episode index:1161
target Thresh 54.57897696474763
target distance 22.0
model initialize at round 1161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.27819755,  15.56415748]), 'previousTarget': array([111.79586847,  16.16513874]), 'currentState': array([94.27240481, 21.79150382,  4.64060838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5781243694996394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47114373,  14.46772083,   1.39749169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7108428335815631}
episode index:1162
target Thresh 54.60038728084064
target distance 5.0
model initialize at round 1162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.47362743,  12.93659835,   2.93460912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.899018579878495}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.578412755465232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14033944e+02, 1.51769993e+01, 6.42656982e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9821364760747272}
episode index:1163
target Thresh 54.62177619731916
target distance 6.0
model initialize at round 1163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.64373384,  10.51636979,   2.81155783]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.205023390782867}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5787085732822103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78173033,  14.08305508,   2.29223125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9425654557511679}
episode index:1164
target Thresh 54.643143735572075
target distance 25.0
model initialize at round 1164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.16416999,  14.46459886]), 'previousTarget': array([109.98401917,  14.79936077]), 'currentState': array([89.24781158, 12.63739675,  3.23969424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5787860524112124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00031617,  14.43733802,   4.87959503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1471513693285051}
episode index:1165
target Thresh 54.664489916966964
target distance 5.0
model initialize at round 1165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16720134,  10.27383576,   1.81945634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.729120922787015}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5791135052050279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29730216,  14.61002905,   1.98079878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8036551444440146}
episode index:1166
target Thresh 54.68581476284998
target distance 48.0
model initialize at round 1166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.15242758, 16.61059143]), 'previousTarget': array([86.93091516, 17.3390904 ]), 'currentState': array([68.18831898, 17.80824244,  0.2231571 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5791102843350809
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07566947,  14.25063047,   4.76569058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1899334536863695}
episode index:1167
target Thresh 54.70711829454599
target distance 6.0
model initialize at round 1167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.50507511,  20.28779456,   4.4899987 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.338396585141706}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5793657729811973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7987071 ,  14.0713316 ,   0.24532479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2248910244314832}
episode index:1168
target Thresh 54.72840053335852
target distance 11.0
model initialize at round 1168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1490979 ,   5.81792721,   2.23286313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.183283228690785}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5796674834815616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90445414,  14.03456138,   1.23119652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9701550083092954}
episode index:1169
target Thresh 54.749661500569815
target distance 42.0
model initialize at round 1169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.80850062,  9.22070028]), 'previousTarget': array([92.23047895,  8.49442256]), 'currentState': array([74.51317794,  3.95852176,  1.26385754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5446213381712781
running average episode reward sum: 0.579637529511211
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42942802,  14.27823129,   5.56162888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9200556800852562}
episode index:1170
target Thresh 54.77090121744082
target distance 47.0
model initialize at round 1170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.58467313,  8.1618919 ]), 'previousTarget': array([87.27622241,  7.33172109]), 'currentState': array([69.22290065,  3.14973006,  1.7645685 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.579638835134274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92399475,  15.68665925,   5.46897036]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.690852897953398}
episode index:1171
target Thresh 54.7921197052113
target distance 10.0
model initialize at round 1171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.54778017,   6.69345841,   0.42294621]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.159489842027357}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5799395403499504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17372716,  14.20087762,   0.78204172]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8177883058633563}
episode index:1172
target Thresh 54.81331698509969
target distance 24.0
model initialize at round 1172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.85102749,  13.33988002]), 'previousTarget': array([109.97366596,  13.32455532]), 'currentState': array([92.28231322,  5.91001794,  0.30549877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = 0.24492161641293494
running average episode reward sum: 0.5796539325716581
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26267031,  15.47491642,   0.39289351]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8770408637771455}
episode index:1173
target Thresh 54.83449307830332
target distance 49.0
model initialize at round 1173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.85919095, 17.79528441]), 'previousTarget': array([85.96262067, 16.77779873]), 'currentState': array([65.95057372, 19.70498594,  0.62849247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5639465274968708
running average episode reward sum: 0.5796405531806234
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7538767 ,  15.55062523,   4.56803016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6031291949343687}
episode index:1174
target Thresh 54.85564800599826
target distance 46.0
model initialize at round 1174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.35131549, 11.47862834]), 'previousTarget': array([88.77237675, 11.00883994]), 'currentState': array([70.55234035,  8.65009808,  1.4065954 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5797223306239252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26463295,  15.92536116,   4.76649243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1819720724110676}
episode index:1175
target Thresh 54.87678178933944
target distance 8.0
model initialize at round 1175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.62529967,  17.37077508,   0.53578287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.801277734507562}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5800299478167628
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.417992  ,  14.22795764,   5.2562822 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9668416159675243}
episode index:1176
target Thresh 54.89789444946065
target distance 49.0
model initialize at round 1176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.18925369, 13.45543465]), 'previousTarget': array([85.93369232, 12.62724019]), 'currentState': array([65.21604497, 12.42057541,  1.06506443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1433044500784072
running average episode reward sum: 0.5794153901295112
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12425613,  14.39706418,   1.19325434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6156063543963961}
episode index:1177
target Thresh 54.91898600747456
target distance 28.0
model initialize at round 1177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.67604995,  13.69696877]), 'previousTarget': array([106.55604828,  13.19058177]), 'currentState': array([87.98526128, 10.19370722,  1.89077061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.579426893572866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32552015,  15.83831176,   4.81139687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.075959887656118}
episode index:1178
target Thresh 54.940056484472706
target distance 52.0
model initialize at round 1178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.46497225, 18.96550771]), 'previousTarget': array([82.82121323, 19.33175976]), 'currentState': array([64.63152412, 21.54122802,  5.01906515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4475702653528226
running average episode reward sum: 0.5793150558898974
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74504555,  15.63176207,   4.54404341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.681267268902667}
episode index:1179
target Thresh 54.96110590152559
target distance 38.0
model initialize at round 1179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.48539512, 17.03674033]), 'previousTarget': array([96.82908591, 17.39090975]), 'currentState': array([78.63578371, 19.48478582,  0.62795049]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.559241778970996
running average episode reward sum: 0.5792980446382712
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28475259,  14.5285077 ,   5.00571139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8566702019893079}
episode index:1180
target Thresh 54.98213427968262
target distance 7.0
model initialize at round 1180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.10284425,  12.33325388,   5.76633418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.394747617719391}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5796047187320583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04343369,  15.17785904,   0.6498111 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9729609199704726}
episode index:1181
target Thresh 55.003141639972185
target distance 39.0
model initialize at round 1181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.61719238, 11.26505231]), 'previousTarget': array([95.76743395, 12.04114369]), 'currentState': array([74.94473599,  7.66026946,  3.07344055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3415723480760758
running average episode reward sum: 0.5794033377078147
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01837469,  14.55592494,   4.54359499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0774000703072306}
episode index:1182
target Thresh 55.02412800340163
target distance 47.0
model initialize at round 1182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.20700892,  7.87898602]), 'previousTarget': array([87.27622241,  7.33172109]), 'currentState': array([66.79196705,  3.0773041 ,  1.4273839 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2414190689761428
running average episode reward sum: 0.5791176367198758
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10586294,  15.70229727,   4.82045264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1369707722390068}
episode index:1183
target Thresh 55.04509339095734
target distance 6.0
model initialize at round 1183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20059514,  20.92849793,   5.04219103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.931890605204691}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5794317181499267
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16670447,  15.85281129,   4.3411761 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8689519414416188}
episode index:1184
target Thresh 55.06603782360468
target distance 26.0
model initialize at round 1184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.35936228,  13.4959281 ]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([90.03457976,  8.34298906,  6.20839388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.44221221098895946
running average episode reward sum: 0.5793159210974701
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7184862 ,  14.17362219,   0.89769561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8730122068859629}
episode index:1185
target Thresh 55.08696132228812
target distance 41.0
model initialize at round 1185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.04041837, 18.98566784]), 'previousTarget': array([93.78922128, 18.1040164 ]), 'currentState': array([73.3619197 , 22.55731873,  0.94966245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 43
reward sum = 0.588898543543512
running average episode reward sum: 0.5793240008803083
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00036888,  14.18595031,   5.96228619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2891622367970712}
episode index:1186
target Thresh 55.10786390793112
target distance 25.0
model initialize at round 1186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.25859692,  14.29515099]), 'previousTarget': array([109.93630557,  14.59490445]), 'currentState': array([89.40762929, 11.85812737,  3.22383738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5795319575972041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06485197,  14.67642598,   5.24956742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9895463502776206}
episode index:1187
target Thresh 55.1287456014363
target distance 7.0
model initialize at round 1187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.08950975e+02, 1.46114628e+01, 3.97064050e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.061490757851625}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5797608513796094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57909782,  14.01805148,   2.19427214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0683545936809913}
episode index:1188
target Thresh 55.149606423685334
target distance 7.0
model initialize at round 1188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.94392655,   6.86400935,   3.21303713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.57905123961112}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5800493155032833
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49351524,  14.06494082,   0.37936716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0634201774843817}
episode index:1189
target Thresh 55.17044639553906
target distance 16.0
model initialize at round 1189
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.50537162,  14.64951906]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.      , 16.      ,  0.985114], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.568102301779271}
done in step count: 18
reward sum = 0.6266067614500874
running average episode reward sum: 0.5800884394074403
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36862856,  14.37809399,   1.1193242 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8862262601645271}
episode index:1190
target Thresh 55.19126553783744
target distance 10.0
model initialize at round 1190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.55160835,   6.10886286,   0.11413682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.598743804654326}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5803761440716052
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15609289,  14.3376685 ,   1.22035998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6804763040916711}
episode index:1191
target Thresh 55.21206387139963
target distance 49.0
model initialize at round 1191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.55029416, 18.82070997]), 'previousTarget': array([85.73865762, 19.77736202]), 'currentState': array([66.72824744, 21.48275125,  6.16989183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5804393010149799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31292911,  15.07030171,   0.23598604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6906581872849242}
episode index:1192
target Thresh 55.232841417023955
target distance 7.0
model initialize at round 1192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.70011739,  10.63045483,   1.74077106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.666905889756815}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.580749905163249
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22527472,  14.55561982,   6.15471377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8931254090594488}
episode index:1193
target Thresh 55.253598195487974
target distance 23.0
model initialize at round 1193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.25360611,  15.99335066]), 'previousTarget': array([111.7042351,  15.5731765]), 'currentState': array([90.67772694, 20.0902952 ,  1.49488378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6137199503395537
running average episode reward sum: 0.5807775182664118
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51251425,  14.47530219,   2.00440916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7162053775734909}
episode index:1194
target Thresh 55.27433422754845
target distance 48.0
model initialize at round 1194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.0662134 , 11.65730071]), 'previousTarget': array([86.79065962, 10.88613786]), 'currentState': array([68.21848463,  9.19403812,  1.74603706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5807778437022709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00717664,  15.82506319,   4.74094502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2909018131641128}
episode index:1195
target Thresh 55.29504953394142
target distance 8.0
model initialize at round 1195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.87176025,   8.70854397,   2.60369158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.56398548394368}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5810873856806972
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48403718,  14.33223638,   2.03293593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8438755177038286}
episode index:1196
target Thresh 55.31574413538221
target distance 34.0
model initialize at round 1196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.47660235, 18.71625535]), 'previousTarget': array([100.58914087,  17.96694159]), 'currentState': array([80.02619806, 23.37262445,  1.17783332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3437291947825403
running average episode reward sum: 0.5808890914527121
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14349381e+02, 1.49891117e+01, 4.23074562e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6507100458181462}
episode index:1197
target Thresh 55.33641805256542
target distance 27.0
model initialize at round 1197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.05010383,  16.01213616]), 'previousTarget': array([107.94535509,  15.52256629]), 'currentState': array([87.21024913, 18.53803511,  1.11481166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 28
reward sum = 0.6324806086998763
running average episode reward sum: 0.5809321561582607
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08860376,  15.16094241,   0.21096057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.925497471689328}
episode index:1198
target Thresh 55.35707130616495
target distance 24.0
model initialize at round 1198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.6888946 ,  16.10763532]), 'previousTarget': array([110.2,  16.4]), 'currentState': array([92.72199915, 22.45246604,  1.09077006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5811577821924029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30518358,  15.99172211,   4.89456128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.210901564061657}
episode index:1199
target Thresh 55.377703916834065
target distance 33.0
model initialize at round 1199
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.99274726,  15.53856947]), 'previousTarget': array([102.,  15.]), 'currentState': array([82.       , 15.       ,  4.7434034], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.24120002088704384
running average episode reward sum: 0.5808744840579818
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14068728e+02, 1.49031051e+01, 9.54167780e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9362989217069655}
episode index:1200
target Thresh 55.39831590520537
target distance 4.0
model initialize at round 1200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.71417482,  10.50680158,   3.3682065 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.809077596655099}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.581182656885494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00698426,  14.07721435,   1.77167688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3555860797680166}
episode index:1201
target Thresh 55.41890729189088
target distance 46.0
model initialize at round 1201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.20235595, 17.90276458]), 'previousTarget': array([88.88288926, 17.83881638]), 'currentState': array([67.31051763, 19.97996665,  4.15228069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.46997512317893236
running average episode reward sum: 0.58109013813865
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05249212,  14.38717271,   6.20177695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1284185696020423}
episode index:1202
target Thresh 55.43947809748196
target distance 5.0
model initialize at round 1202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.52669228,   9.78412353,   6.10210484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.434717720858326}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5813664699003666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06912748,  14.5675787 ,   1.62506687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0264072478263366}
episode index:1203
target Thresh 55.46002834254942
target distance 8.0
model initialize at round 1203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.45502202,  21.33701636,   5.91846287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.11013244269587}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.58167346622927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05375923,  15.98584706,   4.6716102 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3664794192896228}
episode index:1204
target Thresh 55.48055804764353
target distance 23.0
model initialize at round 1204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.44143682,  14.05607586]), 'previousTarget': array([110.88993593,  13.5704125 ]), 'currentState': array([93.67765855,  7.13360337,  5.3526544 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.18990386568687162
running average episode reward sum: 0.5813483462288199
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13890403,  14.78883215,   0.7509533 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8866104711322381}
episode index:1205
target Thresh 55.50106723329395
target distance 43.0
model initialize at round 1205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.31149868,  9.88124189]), 'previousTarget': array([91.37605409,  8.956665  ]), 'currentState': array([7.28018565e+01, 5.47966694e+00, 6.41801357e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.4458371174238033
running average episode reward sum: 0.5812359820258307
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60701136,  14.43432816,   5.78914273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6887849424380712}
episode index:1206
target Thresh 55.5215559200099
target distance 5.0
model initialize at round 1206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.03199898,  14.07835187,   0.17306876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.052768490725803}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5815502819661573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34129071,  14.05529108,   5.5810839 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1516826268261504}
episode index:1207
target Thresh 55.54202412828008
target distance 48.0
model initialize at round 1207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.95675113, 11.04151839]), 'previousTarget': array([86.89236818, 12.07212169]), 'currentState': array([67.1530732 ,  8.24610424,  5.67161393]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5815403903440638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23657386,  14.56098425,   0.23268928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8806556075561247}
episode index:1208
target Thresh 55.56247187857267
target distance 26.0
model initialize at round 1208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.92212324,  15.6262232 ]), 'previousTarget': array([108.76743395,  15.95885631]), 'currentState': array([90.0724978 , 18.07415497,  0.27441495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.5521767538602625
running average episode reward sum: 0.5815161028035479
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30824722,  15.93761005,   5.0386002 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1651757430552565}
episode index:1209
target Thresh 55.58289919133544
target distance 10.0
model initialize at round 1209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.56974251,  20.65786201,   0.35345507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.997325077283879}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5818058129234681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20006862,  15.31160572,   5.21546157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8584802500145239}
episode index:1210
target Thresh 55.6033060869957
target distance 50.0
model initialize at round 1210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.26585376, 18.60773625]), 'previousTarget': array([84.80683493, 19.22704311]), 'currentState': array([66.42165645, 21.09928861,  0.44526547]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5818454643010732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14090397,  14.93903421,   6.13962073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8612565296352159}
episode index:1211
target Thresh 55.62369258596035
target distance 2.0
model initialize at round 1211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.48178896,  14.04328986,   6.25757648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.7945080280729582}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.58217405715231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38610541,  14.14653804,   6.18741887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.051315307580077}
episode index:1212
target Thresh 55.64405870861588
target distance 17.0
model initialize at round 1212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.34226644, 18.8890744 ,  5.88596481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.133490630980052}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5823752068363027
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43021498,  14.10149644,   6.15745927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0639377866623032}
episode index:1213
target Thresh 55.66440447532843
target distance 26.0
model initialize at round 1213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.47838292,  10.59314409]), 'previousTarget': array([106.88854382,  10.94427191]), 'currentState': array([87.32810003,  2.19271369,  2.01683283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5823839310863564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65321086,  14.06859576,   5.75597452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9938694918030143}
episode index:1214
target Thresh 55.684729906443756
target distance 48.0
model initialize at round 1214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.68833029,  8.68371018]), 'previousTarget': array([86.40285  ,  7.8507125]), 'currentState': array([68.202634  ,  4.17731031,  6.04794342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5824442399665933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16670008,  15.74192678,   5.67104691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1157258177332172}
episode index:1215
target Thresh 55.705035022287305
target distance 4.0
model initialize at round 1215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.9911464 ,  14.68460057,   6.1350354 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.025339118818324}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5827712595060944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26104796,  14.74069301,   0.15548245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7831284900638735}
episode index:1216
target Thresh 55.72531984316417
target distance 26.0
model initialize at round 1216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.79618442,  11.15262786]), 'previousTarget': array([107.15918769,  11.38116355]), 'currentState': array([87.34351934,  3.43905854,  1.92509615]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2847201278940454
running average episode reward sum: 0.5825263530709162
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57129716,  14.65523387,   5.64657058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5501361783743963}
episode index:1217
target Thresh 55.74558438935921
target distance 30.0
model initialize at round 1217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.00408922,  17.42636099]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([83.40106337, 21.39138024,  4.46763885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5826996600746704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20692256,  15.88200173,   4.94207808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1861276838855637}
episode index:1218
target Thresh 55.76582868113693
target distance 8.0
model initialize at round 1218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.48999853,  20.58932664,   2.38693249]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.030897524107338}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5829271897666858
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89104204,  15.65953997,   1.90478922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1085796726452213}
episode index:1219
target Thresh 55.78605273874165
target distance 27.0
model initialize at round 1219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.01068222,  13.38200435]), 'previousTarget': array([107.35993796,  13.01924318]), 'currentState': array([89.70281454,  8.1660437 ,  1.05947679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5830556923762116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54900844,  14.07445198,   3.17318944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0761270425979652}
episode index:1220
target Thresh 55.806256582397424
target distance 25.0
model initialize at round 1220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.38396141,  13.39200718]), 'previousTarget': array([108.81774824,  12.77438937]), 'currentState': array([91.49709248,  6.81278367,  1.43637496]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5582009682435921
running average episode reward sum: 0.5830353363367909
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0369772 ,  14.51855565,   4.70916648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.076662239429032}
episode index:1221
target Thresh 55.826440232308116
target distance 6.0
model initialize at round 1221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.68106057,  20.1557257 ,   5.48365331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.725678051426773}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5833286626977272
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33940631,  15.86217148,   4.8823345 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0861508605833383}
episode index:1222
target Thresh 55.846603708657334
target distance 3.0
model initialize at round 1222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.96927229,  13.84861169,   0.49503863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.2420680864863547}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5836450734395934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42202562,  14.49452609,   6.24189997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7678269760008126}
episode index:1223
target Thresh 55.8667470316086
target distance 44.0
model initialize at round 1223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.42997195, 14.29397387]), 'previousTarget': array([90.99483671, 14.45442811]), 'currentState': array([69.43759152, 13.74195481,  2.40146756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.35993398648134833
running average episode reward sum: 0.5834623029437125
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49077749,  15.73302163,   3.08371488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8821469557819012}
episode index:1224
target Thresh 55.8868702213052
target distance 22.0
model initialize at round 1224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.45331364,  14.92770383]), 'previousTarget': array([112.9793708 ,  15.09184678]), 'currentState': array([92.46136774, 14.36016592,  3.45944953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5192659254049584
running average episode reward sum: 0.5834098977375584
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59144042,  14.16293861,   5.65524309]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9314465622396209}
episode index:1225
target Thresh 55.90697329787036
target distance 53.0
model initialize at round 1225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.05169772,  8.29903318]), 'previousTarget': array([81.71773129,  9.34829399]), 'currentState': array([62.45292001,  4.3130688 ,  5.87280965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.583442618371452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1388503 ,  15.78186111,   3.48444321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7940945805452218}
episode index:1226
target Thresh 55.92705628140713
target distance 8.0
model initialize at round 1226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.58117453,  11.3431125 ,   0.566248  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.387431666755358}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5837499968487369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31421687,  14.21988495,   6.14745818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.038690512542635}
episode index:1227
target Thresh 55.94711919199853
target distance 9.0
model initialize at round 1227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.88064351,   7.23077783,   1.31257772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.793742719985667}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5840413080478837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45839763,  14.5202965 ,   5.98246405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7234974629058328}
episode index:1228
target Thresh 55.967162049707426
target distance 50.0
model initialize at round 1228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.60200041, 15.24553809]), 'previousTarget': array([85., 15.]), 'currentState': array([66.60274796, 15.41845796,  5.52857203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.4893830293636418
running average episode reward sum: 0.5839642874793856
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36420983,  14.31377024,   6.16443125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9354894044465636}
episode index:1229
target Thresh 55.98718487457672
target distance 26.0
model initialize at round 1229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.05293646,  11.21161148]), 'previousTarget': array([106.88854382,  10.94427191]), 'currentState': array([90.49404878,  1.63635883,  0.77124756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.2927833021775319
running average episode reward sum: 0.58372755497101
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35645398,  15.55655408,   4.87097265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.850825435795722}
episode index:1230
target Thresh 56.00718768662922
target distance 40.0
model initialize at round 1230
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.92738923, 14.70269158]), 'previousTarget': array([94.97504678, 13.99875234]), 'currentState': array([75.       , 13.       ,  5.0340815], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.049529763189050224
running average episode reward sum: 0.5832936006316258
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03116506,  15.43008329,   4.8333286 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0600060257527513}
episode index:1231
target Thresh 56.02717050586773
target distance 18.0
model initialize at round 1231
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.47655767,  15.7314314 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 21.       ,  4.8570657], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.298403615286603}
done in step count: 18
reward sum = 0.49417480725708746
running average episode reward sum: 0.583221263948692
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64888375,  15.04671415,   5.80538994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3542101536501758}
episode index:1232
target Thresh 56.0471333522751
target distance 10.0
model initialize at round 1232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.53477739,  17.36337393,   5.97133791]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.755817519970721}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5834744001938744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73964206,  14.76807456,   1.1432425 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34867702706611914}
episode index:1233
target Thresh 56.067076245814135
target distance 45.0
model initialize at round 1233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.81774227, 11.90722649]), 'previousTarget': array([89.76232938, 11.07414013]), 'currentState': array([70.9793322 ,  9.37000644,  1.9583767 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5835890727512196
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11978242,  15.04300236,   5.3395274 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8812673807055785}
episode index:1234
target Thresh 56.08699920642777
target distance 24.0
model initialize at round 1234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.2173218 ,  12.75713342]), 'previousTarget': array([108.88854382,  11.94427191]), 'currentState': array([92.10957824,  4.26539488,  1.86091202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.583746345857817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69827547,  15.11493014,   2.27080019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7076705260818144}
episode index:1235
target Thresh 56.10690225403895
target distance 45.0
model initialize at round 1235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.55706014, 17.26841223]), 'previousTarget': array([89.95570316, 16.66961979]), 'currentState': array([68.6302474 , 18.97783736,  1.51161265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.609369714593039
running average episode reward sum: 0.5837670767386708
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09953461,  14.5835557 ,   4.66209802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9921006829806732}
episode index:1236
target Thresh 56.12678540855073
target distance 7.0
model initialize at round 1236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.10877607,   9.34419383,   1.78690928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.036147775034176}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5840411087659054
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18202836,  14.10690029,   1.7550946 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2110758431529682}
episode index:1237
target Thresh 56.14664868984626
target distance 45.0
model initialize at round 1237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.30800146, 10.65100668]), 'previousTarget': array([89.82455801, 11.6432744 ]), 'currentState': array([69.58852477,  7.31300742,  3.55631924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.40230329613134397
running average episode reward sum: 0.5838943092403525
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02642051,  15.57691276,   4.21212788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1316736968194445}
episode index:1238
target Thresh 56.16649211778881
target distance 15.0
model initialize at round 1238
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.26590022,  14.74986382]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  23.       ,   1.0138266], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.62206310547592}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.584096478542829
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91585857,  15.89033548,   4.92244378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8943025494939818}
episode index:1239
target Thresh 56.18631571222185
target distance 48.0
model initialize at round 1239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.60844917, 17.28428824]), 'previousTarget': array([86.93091516, 17.3390904 ]), 'currentState': array([68.6829464 , 19.00891564,  1.01529759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5841489028572044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10045255,  15.60717482,   5.04978681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0852865426233498}
episode index:1240
target Thresh 56.20611949296893
target distance 7.0
model initialize at round 1240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.39423569,  21.67511658,   4.35122108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.819169638371144}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5844445041038143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57018373,  15.87778375,   4.18555787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0467157192594712}
episode index:1241
target Thresh 56.22590347983387
target distance 47.0
model initialize at round 1241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.46869494, 14.76380466]), 'previousTarget': array([87.98191681, 13.85029433]), 'currentState': array([68.46948744, 14.58576141,  0.41743541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5845403198774825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63045097,  15.60481648,   4.54712801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7087802563320462}
episode index:1242
target Thresh 56.24566769260063
target distance 8.0
model initialize at round 1242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.16492806,  12.62074654,   1.05875087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.237337593590618}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5848351306015553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14334506e+02, 1.48811936e+01, 6.36143982e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6760158965561036}
episode index:1243
target Thresh 56.265412151033445
target distance 1.0
model initialize at round 1243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08181472,  15.9634927 ,   5.31010809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9669600952972758}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5851688644193997
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08181472,  15.9634927 ,   5.31010809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9669600952972758}
episode index:1244
target Thresh 56.285136874876756
target distance 36.0
model initialize at round 1244
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.68555558, 16.46745114]), 'previousTarget': array([98.80984546, 17.24863257]), 'currentState': array([79.       , 20.       ,  1.8397889], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.3798257173927795
running average episode reward sum: 0.5850039301647598
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15226151,  14.225829  ,   2.04511235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1480423703510239}
episode index:1245
target Thresh 56.30484188385532
target distance 24.0
model initialize at round 1245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.92433886,  16.66668775]), 'previousTarget': array([109.97366596,  16.67544468]), 'currentState': array([89.63689172, 21.95765957,  4.7478143 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6875700503549549
running average episode reward sum: 0.5850862464730986
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24827035,  15.12139766,   6.08236175]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7614688806216909}
episode index:1246
target Thresh 56.32452719767411
target distance 28.0
model initialize at round 1246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.64573266,  14.70365984]), 'previousTarget': array([106.98725709,  14.71383061]), 'currentState': array([88.66744678, 13.77194449,  5.13725937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5851644087413723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87498555,  14.49590885,   0.23456979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5193616239936906}
episode index:1247
target Thresh 56.34419283601845
target distance 8.0
model initialize at round 1247
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.29843237,  11.07072941,   5.96276659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.768537578099257}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5854057712918328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14342611,  14.26209358,   1.44318808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1305860012455422}
episode index:1248
target Thresh 56.36383881855401
target distance 44.0
model initialize at round 1248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.1569869 , 11.90427465]), 'previousTarget': array([90.91786413, 12.81071492]), 'currentState': array([71.32346365,  9.32913281,  3.92574859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.42655117454882907
running average episode reward sum: 0.5852785858660978
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06957734,  14.03821506,   1.23714809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9642983342745778}
episode index:1249
target Thresh 56.38346516492674
target distance 25.0
model initialize at round 1249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.9436737 ,  15.47331366]), 'previousTarget': array([109.74881264,  15.84018998]), 'currentState': array([90.03072699, 17.337329  ,  5.74086738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.4139354838839721
running average episode reward sum: 0.5851415113845121
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03855592,  15.44520685,   4.83403936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0595205774287701}
episode index:1250
target Thresh 56.403071894763
target distance 11.0
model initialize at round 1250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.43652359,   4.8768056 ,   1.55800694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.732074776290283}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5853823134311401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89972052,  15.41406991,   3.48449767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9904296542952864}
episode index:1251
target Thresh 56.42265902766951
target distance 14.0
model initialize at round 1251
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.33587123,  16.81784715]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  10.       ,   5.5007687], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.977599988190796}
done in step count: 14
reward sum = 0.7987458127689782
running average episode reward sum: 0.5855527315616017
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02101384,  14.02008398,   5.58670227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3851531709031144}
episode index:1252
target Thresh 56.44222658323344
target distance 23.0
model initialize at round 1252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.74454786,  13.45438519]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([93.67743877,  4.87653177,  6.25620848]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.40966399856790314
running average episode reward sum: 0.5854123574730193
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04212773,  15.80475783,   5.05779011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2510613261242378}
episode index:1253
target Thresh 56.461774581022304
target distance 6.0
model initialize at round 1253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.46080434,   9.83572649,   1.52965229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.260641083841844}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5857115469885911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94747214,  14.5406537 ,   0.43272075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4623399128172907}
episode index:1254
target Thresh 56.48130304058414
target distance 25.0
model initialize at round 1254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.84485398,  12.73656128]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([91.53223996,  4.69615285,  1.43645447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.3659336900097138
running average episode reward sum: 0.5855364251902015
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22027128,  15.10864708,   5.25056232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7872617540025179}
episode index:1255
target Thresh 56.50081198144738
target distance 44.0
model initialize at round 1255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.04231965, 11.30496329]), 'previousTarget': array([90.81660336, 11.70226409]), 'currentState': array([72.29644168,  8.1268659 ,  0.41729182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 85
reward sum = 0.3596865129305846
running average episode reward sum: 0.5853566083810776
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53288794,  15.35886228,   6.10744108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5890465283332792}
episode index:1256
target Thresh 56.52030142312097
target distance 3.0
model initialize at round 1256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.86352048,  13.29545882,   3.04936844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.525503756125724}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5856474862184038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2930085 ,  14.00077412,   2.57980214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0413003119270294}
episode index:1257
target Thresh 56.53977138509436
target distance 47.0
model initialize at round 1257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.67500712, 12.44142547]), 'previousTarget': array([87.95938166, 13.27400308]), 'currentState': array([67.76210984, 10.57688218,  5.55675912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5857245188962988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13104829,  15.70933347,   5.16463149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1217089794738708}
episode index:1258
target Thresh 56.55922188683753
target distance 30.0
model initialize at round 1258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.52306516,  12.56228508]), 'previousTarget': array([104.32469879,  12.15325301]), 'currentState': array([83.9594953 ,  8.40695904,  1.31543851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.26357977727592985
running average episode reward sum: 0.5854686453922319
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15281131,  15.80141803,   4.95477859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1661901777604895}
episode index:1259
target Thresh 56.578652947800954
target distance 28.0
model initialize at round 1259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.26911885,  12.95250958]), 'previousTarget': array([106.40285  ,  12.8507125]), 'currentState': array([85.69767072,  8.83444958,  1.56175733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6094516908797292
running average episode reward sum: 0.5854876795553171
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14777619,  14.43118761,   0.46578425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5876949365967503}
episode index:1260
target Thresh 56.598064587415706
target distance 9.0
model initialize at round 1260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.47051467,  12.77313953,   1.49404353]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.851882375016696}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.585755131589316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23943   ,  14.30317445,   5.83849844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0315195443198848}
episode index:1261
target Thresh 56.617456825093434
target distance 6.0
model initialize at round 1261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.64105034,  19.24431735,   5.7019192 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.855808201821786}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5860521528875813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11949542,  15.5982533 ,   4.45642614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.064516473340113}
episode index:1262
target Thresh 56.63682968022637
target distance 50.0
model initialize at round 1262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.36904393, 17.33918199]), 'previousTarget': array([84.96409691, 16.80215419]), 'currentState': array([66.43546238, 18.96777849,  1.31716805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5860719985618847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31093612,  14.54215185,   1.94740263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8273052395976299}
episode index:1263
target Thresh 56.65618317218737
target distance 26.0
model initialize at round 1263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.51168591,  15.0221716 ]), 'previousTarget': array([108.98522349,  15.23133756]), 'currentState': array([90.51192993, 15.12096742,  0.36738628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.586242535421835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02720878,  14.14679153,   4.48291181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2939425994010592}
episode index:1264
target Thresh 56.67551732032993
target distance 26.0
model initialize at round 1264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.66366958,  13.33782818]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([90.56854501,  7.39003719,  0.63912361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5863464785026069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10461067,  14.7765812 ,   5.28055551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.922842354702539}
episode index:1265
target Thresh 56.6948321439882
target distance 4.0
model initialize at round 1265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06865282,  17.00535244,   4.84775078]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2110734884558996}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5866653201467597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15780065,  15.72436385,   4.71465819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1108567534743268}
episode index:1266
target Thresh 56.71412766247701
target distance 12.0
model initialize at round 1266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.96960944,   4.37558919,   1.9668104 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.031622064725877}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5869379326390725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74664219,  14.40115618,   2.12951281]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6502338823137772}
episode index:1267
target Thresh 56.73340389509187
target distance 49.0
model initialize at round 1267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.97347357, 18.98166871]), 'previousTarget': array([85.89668285, 17.96972624]), 'currentState': array([66.15902461, 21.69968499,  0.50865674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.4716331335750318
running average episode reward sum: 0.5868469982549525
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15433248,  14.53687863,   3.55114047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4881597250290621}
episode index:1268
target Thresh 56.75266086110902
target distance 28.0
model initialize at round 1268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.50315112,  15.97696499]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.72551401, 18.95103127,  0.90688449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 26
reward sum = 0.7104411018285138
running average episode reward sum: 0.5869443931356252
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51913227,  15.49594188,   5.07642462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6907909416870834}
episode index:1269
target Thresh 56.771898579785436
target distance 9.0
model initialize at round 1269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.68952241,   7.87737986,   1.22049511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.884418376737278}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5872161419189098
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44262442,  15.19706285,   3.22972713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48451021265268246}
episode index:1270
target Thresh 56.79111707035882
target distance 29.0
model initialize at round 1270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.90041276,  17.32158773]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([87.67459308, 22.83229165,  5.17336941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.10232538415977155
running average episode reward sum: 0.5868346385689813
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35503472,  15.8706966 ,   5.47867919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0835556219175728}
episode index:1271
target Thresh 56.81031635204768
target distance 29.0
model initialize at round 1271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.91315392,  10.24470225]), 'previousTarget': array([104.48033665,  10.64703585]), 'currentState': array([84.30173906,  2.92245966,  2.30224371]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5869098901070642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19738792,  14.11963425,   2.97574092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1913144045536375}
episode index:1272
target Thresh 56.82949644405129
target distance 36.0
model initialize at round 1272
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.83454343,  9.72755334]), 'previousTarget': array([97.97366596,  9.32455532]), 'currentState': array([79.       ,  3.       ,  5.1185393], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.28891069000897973
running average episode reward sum: 0.5866757980410012
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43545093,  15.84164176,   4.4930054 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0134478294634588}
episode index:1273
target Thresh 56.84865736554974
target distance 26.0
model initialize at round 1273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.43165048,  15.98945636]), 'previousTarget': array([108.48782391,  16.50280987]), 'currentState': array([89.74011247, 19.48850352,  6.17772675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5867351436006111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63940597,  14.9593107 ,   0.5986155 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3628824505421959}
episode index:1274
target Thresh 56.867799135703976
target distance 24.0
model initialize at round 1274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.91483049,  14.66778717]), 'previousTarget': array([110.84555753,  14.48069469]), 'currentState': array([89.95737411, 13.3639716 ,  1.27157116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5867892017001982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08248779,  14.94968908,   5.13284442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9188905551097364}
episode index:1275
target Thresh 56.88692177365574
target distance 30.0
model initialize at round 1275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.09903258,  11.11607367]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([86.48033186,  3.8123771 ,  5.78455264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5868806268516867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0460422 ,  15.06541776,   5.14385599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9561981853419703}
episode index:1276
target Thresh 56.90602529852768
target distance 3.0
model initialize at round 1276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.45934195,  16.53844634,   4.04840547]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.1204942962790145}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5871963037296416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86233919,  15.69256016,   4.14819179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.106014671613724}
episode index:1277
target Thresh 56.92510972942334
target distance 49.0
model initialize at round 1277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.1905261 ,  8.52370717]), 'previousTarget': array([85.42594623,  7.75737459]), 'currentState': array([67.71175316,  3.98747474,  0.25263572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.401253110578882
running average episode reward sum: 0.5870508082733421
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5665551 ,  14.58748787,   1.0078495 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5983650589903019}
episode index:1278
target Thresh 56.94417508542714
target distance 31.0
model initialize at round 1278
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.89467757,  14.04983029]), 'previousTarget': array([103.90700027,  13.9264839 ]), 'currentState': array([84.       , 12.       ,  0.9585349], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6245152023787424
running average episode reward sum: 0.5870801002155668
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85742971,  14.71416351,   1.04787799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3194194541607485}
episode index:1279
target Thresh 56.96322138560443
target distance 52.0
model initialize at round 1279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.04345252,  9.0030517 ]), 'previousTarget': array([82.76743395, 10.04114369]), 'currentState': array([63.38657675,  5.31425309,  5.867172  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.362545276747314
running average episode reward sum: 0.5869046823847323
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64561795,  14.86168727,   1.05283013]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3804169445033339}
episode index:1280
target Thresh 56.98224864900152
target distance 30.0
model initialize at round 1280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.32585077,  13.90646998]), 'previousTarget': array([104.9007438 ,  13.99007438]), 'currentState': array([86.48291174, 11.40491635,  0.62837857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5870416363519161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26889556,  15.32818755,   1.10124174]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8013867788768899}
episode index:1281
target Thresh 57.00125689464567
target distance 41.0
model initialize at round 1281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.01789207, 11.39377255]), 'previousTarget': array([93.78922128, 11.8959836 ]), 'currentState': array([75.33585019,  7.84169887,  4.79279199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5870700948219155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20513173,  15.92415116,   5.00485313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2189630585749593}
episode index:1282
target Thresh 57.020246141545144
target distance 27.0
model initialize at round 1282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.54932948,  15.62883878]), 'previousTarget': array([107.87767469,  15.79136948]), 'currentState': array([89.68111559, 17.92101609,  0.96305149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 26
reward sum = 0.5180695744124447
running average episode reward sum: 0.5870163142136463
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47250263,  14.68673019,   5.56969938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6135074978629876}
episode index:1283
target Thresh 57.03921640868917
target distance 47.0
model initialize at round 1283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.21446009, 12.8801883 ]), 'previousTarget': array([87.95938166, 13.27400308]), 'currentState': array([69.28170317, 11.2415299 ,  0.47366982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.39728326265384956
running average episode reward sum: 0.5868685470395343
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19847127,  15.75047632,   5.06596697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.098026871631238}
episode index:1284
target Thresh 57.05816771504802
target distance 24.0
model initialize at round 1284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.12469492,  13.65065542]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([90.84934228,  8.31579079,  3.62659979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5869324405893632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04212111,  15.22537011,   5.13299441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9840343755686565}
episode index:1285
target Thresh 57.07710007957302
target distance 7.0
model initialize at round 1285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.35119402,   9.7507537 ,   2.19641334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.502103951725903}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5872230032405379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40281467,  15.44046538,   2.61816217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5968830790879673}
episode index:1286
target Thresh 57.09601352119652
target distance 8.0
model initialize at round 1286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.35219713,  22.73335716,   4.31095183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.174085455320604}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5874765341218458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14445360e+02, 1.40732769e+01, 9.00422186e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0800188409121547}
episode index:1287
target Thresh 57.11490805883196
target distance 46.0
model initialize at round 1287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.36657738, 10.55806174]), 'previousTarget': array([88.7042351, 10.4268235]), 'currentState': array([70.68401549,  7.00887062,  5.46439064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5874854467556535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44377069,  14.4935743 ,   5.74164744]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7522353619152737}
episode index:1288
target Thresh 57.13378371137389
target distance 3.0
model initialize at round 1288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.34813267,  14.27741089,   1.41992432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8029976985365825}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5877900352376119
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85206498,  14.98431443,   6.1697861 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1487642648105257}
episode index:1289
target Thresh 57.15264049769795
target distance 46.0
model initialize at round 1289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.52829562,  9.13559392]), 'previousTarget': array([88.35234545,  8.04843794]), 'currentState': array([69.00171646,  4.80977537,  0.7955277 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6694273361027975
running average episode reward sum: 0.5878533199669647
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17689467,  15.14953542,   5.90858275]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8365782861077238}
episode index:1290
target Thresh 57.17147843666094
target distance 29.0
model initialize at round 1290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.11782897,  11.43774702]), 'previousTarget': array([104.48033665,  10.64703585]), 'currentState': array([86.30291687,  4.65548466,  0.37982059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.13697270079940566
running average episode reward sum: 0.5875040708429
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00869115,  14.4217079 ,   6.00043655]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1476563035833718}
episode index:1291
target Thresh 57.190297547100805
target distance 3.0
model initialize at round 1291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.21243989,  11.99021517,   2.13077295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.102352506461196}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.587785406740003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23995536,  14.18480762,   1.02085033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8497747919277469}
episode index:1292
target Thresh 57.20909784783664
target distance 44.0
model initialize at round 1292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.74053176,  9.08538723]), 'previousTarget': array([90.40285  ,  8.8507125]), 'currentState': array([72.35739771,  4.15648124,  1.12477368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.3500254469188494
running average episode reward sum: 0.5876015243271483
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22620681,  15.72313863,   4.70177959]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0590964954956568}
episode index:1293
target Thresh 57.22787935766876
target distance 9.0
model initialize at round 1293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.97421432,   4.15890967,   4.27358866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.028737330711843}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5878605221402092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95354799,  14.58188306,   2.00266233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42068939167663627}
episode index:1294
target Thresh 57.24664209537868
target distance 26.0
model initialize at round 1294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.30474865,  12.06089917]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([89.99158836,  4.02173503,  0.23136538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3962997685170399
running average episode reward sum: 0.587712598778338
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80559684,  15.28514464,   3.60250635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3451087597535424}
episode index:1295
target Thresh 57.26538607972912
target distance 42.0
model initialize at round 1295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.44945162, 15.80398716]), 'previousTarget': array([93., 15.]), 'currentState': array([72.46215064, 16.51658771,  2.85862297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 46
reward sum = 0.5088073394845136
running average episode reward sum: 0.5876517150906113
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66784094,  14.85717443,   0.25074226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36156435173555923}
episode index:1296
target Thresh 57.284111329464096
target distance 25.0
model initialize at round 1296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.3610391,  13.3276868]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([91.54624914,  6.54508561,  0.72386807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.530843566644202
running average episode reward sum: 0.5876079154387636
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0557664 ,  15.35487121,   4.32884461]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0087173382354915}
episode index:1297
target Thresh 57.30281786330882
target distance 37.0
model initialize at round 1297
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.16847083,  8.70698923]), 'previousTarget': array([97.02445638,  9.17009396]), 'currentState': array([78.       ,  3.       ,  1.8646872], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.01659515029437858
running average episode reward sum: 0.5871679980542147
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40763459,  14.08560687,   0.21969998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.089500608268253}
episode index:1298
target Thresh 57.32150569996986
target distance 27.0
model initialize at round 1298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.34788516,  17.20028281]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([86.9648316 , 22.12950536,  1.34525025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5873456261831932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11789652,  15.52204817,   4.8132014 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0250077263051076}
episode index:1299
target Thresh 57.34017485813505
target distance 6.0
model initialize at round 1299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4872237 ,   7.18801923,   4.56290984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.8271597942800835}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5876180373548991
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11601415,  14.20448498,   2.77821243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1892330012148031}
episode index:1300
target Thresh 57.35882535647354
target distance 30.0
model initialize at round 1300
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.87372967,   9.61682161]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([85.       ,  3.       ,  2.2507334], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.41262438486706465
running average episode reward sum: 0.5874835303199354
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40244265,  15.07705005,   0.3095386 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.60250435462371}
episode index:1301
target Thresh 57.37745721363583
target distance 13.0
model initialize at round 1301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.02745557e+02, 2.32037250e+01, 3.59714031e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.74694808983496}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5877199778805643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56148642,  15.46453422,   4.48108763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6388162502478402}
episode index:1302
target Thresh 57.39607044825378
target distance 44.0
model initialize at round 1302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.17459786, 12.67873536]), 'previousTarget': array([90.81660336, 11.70226409]), 'currentState': array([71.26884982, 10.73935532,  0.54422736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5877379343361685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08427494,  14.77549497,   0.16255793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9428440471677375}
episode index:1303
target Thresh 57.41466507894063
target distance 45.0
model initialize at round 1303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.24593088, 10.98762582]), 'previousTarget': array([89.69125016, 10.50066669]), 'currentState': array([71.52528356,  7.65654955,  5.67965824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5872872150613708
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.39796227,  13.88735729,   1.24174757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.829942429309119}
episode index:1304
target Thresh 57.43324112429101
target distance 35.0
model initialize at round 1304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.82732691, 18.27261879]), 'previousTarget': array([99.7124451, 17.6207237]), 'currentState': array([80.27692766, 22.48947652,  0.37697244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5868371865440825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.15062247,  16.23402482,   5.35036256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4795149246532575}
episode index:1305
target Thresh 57.45179860288096
target distance 2.0
model initialize at round 1305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01125357,  12.99747012,   2.07427168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2333260951579574}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5863878471975709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.9009742 ,  13.40675411,   1.67447451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6352119010631028}
episode index:1306
target Thresh 57.47033753326799
target distance 17.0
model initialize at round 1306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.79140314,  14.86502556]), 'currentState': array([97.95230855,  5.68229415,  0.58913779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.42790328711587}
done in step count: 99
reward sum = -0.06590361045807
running average episode reward sum: 0.5858887718665413
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.27722274,  17.85581937,   0.88794541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.691990492685855}
episode index:1307
target Thresh 57.488857933990985
target distance 24.0
model initialize at round 1307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.16859965,  12.66998206]), 'previousTarget': array([108.88854382,  11.94427191]), 'currentState': array([92.15409075,  3.98220506,  1.71511143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3554283978508633
running average episode reward sum: 0.5851691104218032
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14509925e+02, 1.91051795e+01, 4.76582113e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.134328498157583}
episode index:1308
target Thresh 57.507359823570376
target distance 38.0
model initialize at round 1308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.97930088, 19.36941506]), 'previousTarget': array([96.5709957 , 18.87979038]), 'currentState': array([76.48700114, 23.84716573,  2.19369018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13247291395107
running average episode reward sum: 0.5846208735811822
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15121751,  18.493867  ,   5.46489741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5954886062831517}
episode index:1309
target Thresh 57.52584322050806
target distance 44.0
model initialize at round 1309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.56671366, 13.64879356]), 'previousTarget': array([90.91786413, 12.81071492]), 'currentState': array([71.59987986, 12.49746915,  0.17977643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.584174598105166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.24477661,  16.41411712,   5.8152479 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8839310092292096}
episode index:1310
target Thresh 57.544308143287424
target distance 8.0
model initialize at round 1310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.79440243,  16.85156601,   0.85118663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.411903960201592}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.583729003446047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.87529805,  16.82839412,   5.48524257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.619115852165799}
episode index:1311
target Thresh 57.56275461037338
target distance 53.0
model initialize at round 1311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.96997341,  7.19129121]), 'previousTarget': array([81.42421688,  6.76443056]), 'currentState': array([63.53907895,  2.45416757,  1.19815558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.10686365559571592
running average episode reward sum: 0.58320263709007
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18205857,  18.51846128,   5.95413813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.612284313930897}
episode index:1312
target Thresh 57.581182640212425
target distance 27.0
model initialize at round 1312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.44805072,  15.85016154]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([89.6784893 , 18.87744436,  5.20029921]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5827584614334896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.38215006,  19.34111631,   5.7699211 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.632788495805632}
episode index:1313
target Thresh 57.599592251232565
target distance 45.0
model initialize at round 1313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.60776613, 12.30352222]), 'previousTarget': array([89.87767469, 12.20863052]), 'currentState': array([68.71134174, 10.27071717,  1.92452693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5823149618433576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74388525,  17.44888186,   5.22870689]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.559372509160526}
episode index:1314
target Thresh 57.61798346184343
target distance 25.0
model initialize at round 1314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.1108616 ,  13.42584216]), 'previousTarget': array([108.81774824,  12.77438937]), 'currentState': array([91.07329775,  7.29631016,  6.1524207 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.11860806751351616
running average episode reward sum: 0.5817819405282574
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.05752815,  12.66751623,   3.99642998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1102897937366496}
episode index:1315
target Thresh 57.63635629043621
target distance 26.0
model initialize at round 1315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.18940604,  13.90555426]), 'previousTarget': array([108.86817872,  14.29248216]), 'currentState': array([89.5350075 , 10.20357938,  5.19502583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.29080268504684303
running average episode reward sum: 0.5811188823021365
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.24959301,  17.47243489,   5.67552308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7702738096942308}
episode index:1316
target Thresh 57.65471075538377
target distance 8.0
model initialize at round 1316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.39411697,  21.37461436,   4.18252671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.80936886284669}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5806776378964401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.4906578 ,  12.66114465,   3.59715962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7735005005113127}
episode index:1317
target Thresh 57.67304687504055
target distance 13.0
model initialize at round 1317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.26107662,   6.7985091 ,   4.09077299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.867137136907115}
done in step count: 99
reward sum = -0.06792093
running average episode reward sum: 0.5801855297265641
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.01784128,  14.63064205,   6.14258195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.082786299070584}
episode index:1318
target Thresh 57.691364667742675
target distance 47.0
model initialize at round 1318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.01630046, 15.20840305]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([69.01694372, 15.36880849,  1.1216436 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.0950798550849469
running average episode reward sum: 0.5796735771982765
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.42980003,  16.39760401,   5.79078001]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9994061845522328}
episode index:1319
target Thresh 57.70966415180794
target distance 7.0
model initialize at round 1319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09890707e+02, 1.84720949e+01, 6.50768836e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.177403866494589}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5792344305488839
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28264322,  12.69798896,   2.71608107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.319297737664263}
episode index:1320
target Thresh 57.72794534553584
target distance 30.0
model initialize at round 1320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.98334375,  11.09860532]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([86.34704163,  3.83993854,  1.53484088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2817499002795916
running average episode reward sum: 0.5785826634551455
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64461219,  17.29972086,   5.67013679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.32701880573119}
episode index:1321
target Thresh 57.74620826720755
target distance 50.0
model initialize at round 1321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.45043569, 16.54212229]), 'previousTarget': array([84.98401917, 16.20063923]), 'currentState': array([66.47954887, 17.62086204,  0.88976228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5786214236425495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91327098,  15.9907478 ,   5.9501329 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3474587501951567}
episode index:1322
target Thresh 57.76445293508602
target distance 49.0
model initialize at round 1322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.67271557, 18.86958313]), 'previousTarget': array([85.85172777, 18.56917619]), 'currentState': array([67.87025949, 21.67363964,  0.53850634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3621471670580059
running average episode reward sum: 0.5779103362724055
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.32447991,  15.62211998,   4.68049659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.406291782255898}
episode index:1323
target Thresh 57.782679367415895
target distance 16.0
model initialize at round 1323
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.65079556,  15.32750891]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.      , 20.      ,  4.327444], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.258772057041316}
done in step count: 99
reward sum = -0.664025220188985
running average episode reward sum: 0.5769723184805163
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.91610964,  14.10136662,   1.7710327 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4079631596627225}
episode index:1324
target Thresh 57.80088758242363
target distance 12.0
model initialize at round 1324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.49374926,   6.88199281,   5.89656526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.277173850561594}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5771541559289063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14004557e+02, 1.58695523e+01, 9.57610160e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3217521766328917}
episode index:1325
target Thresh 57.81907759831742
target distance 45.0
model initialize at round 1325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.94498119, 16.80709271]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([69.99679924, 18.24585492,  5.59479976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.577039854245241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77623807,  14.01376014,   2.65295437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0113053263408103}
episode index:1326
target Thresh 57.8372494332873
target distance 8.0
model initialize at round 1326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.72773594,  21.76417298,   4.73621225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.8827822809869685}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5770655342642972
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3369223 ,  14.11412033,   2.58369291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1065508671607067}
episode index:1327
target Thresh 57.8554031055051
target distance 31.0
model initialize at round 1327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.42805776,  15.88927184]), 'previousTarget': array([103.95850618,  15.71235444]), 'currentState': array([85.51381457, 17.73938494,  1.46138531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5309195053813703
running average episode reward sum: 0.5770307857485721
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14164808,  15.95164989,   1.09566109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2815637075383788}
episode index:1328
target Thresh 57.8735386331245
target distance 4.0
model initialize at round 1328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.97391425,  17.20007531,   0.73401558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.587994969196801}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5772120319124914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40335391,  15.8812252 ,   5.3944746 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9691502630956446}
episode index:1329
target Thresh 57.891656034281
target distance 24.0
model initialize at round 1329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.96578558,  14.53327841]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([92.19827269, 11.49264787,  1.41040188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4539134615436384
running average episode reward sum: 0.5771193262204847
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07981093,  15.94290862,   1.42917607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3175069599713725}
episode index:1330
target Thresh 57.909755327092036
target distance 8.0
model initialize at round 1330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50884259,   8.02676536,   2.46660262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9917753235278415}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5773190436263175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03823063,  14.51721236,   1.57363623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0761432220238714}
episode index:1331
target Thresh 57.9278365296569
target distance 5.0
model initialize at round 1331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.98912509,  18.16735881,   0.19247649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.370072030774488}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5768856209208924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.05979662,  13.67128255,   4.02078856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6996055220827968}
episode index:1332
target Thresh 57.94589966005679
target distance 44.0
model initialize at round 1332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.49033288, 10.02758788]), 'previousTarget': array([90.50265712,  9.43242207]), 'currentState': array([69.85978604,  6.20114952,  1.32017469]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.4255901233886546
running average episode reward sum: 0.5767721209227437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04882511,  15.65910461,   0.69103318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1572175949224297}
episode index:1333
target Thresh 57.96394473635481
target distance 29.0
model initialize at round 1333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.47692982,  16.73284735]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([8.79872652e+01, 2.12220536e+01, 7.77452290e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5768618069025988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54210119,  15.9488913 ,   5.08742965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0928258791678411}
episode index:1334
target Thresh 57.98197177659609
target distance 42.0
model initialize at round 1334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.2527808, 12.1167979]), 'previousTarget': array([92.79898987, 11.82842712]), 'currentState': array([71.39858007,  9.70625574,  3.61774981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5768738703029725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67006591,  15.9925087 ,   5.07534858]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1975232102584212}
episode index:1335
target Thresh 57.999980798807634
target distance 27.0
model initialize at round 1335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.77132717,  15.71343326]), 'previousTarget': array([107.98629668,  15.25976679]), 'currentState': array([87.86802815, 17.67778769,  0.63929296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.576864163207276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29180774,  15.96620629,   5.96043406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0093098362169846}
episode index:1336
target Thresh 58.01797182099848
target distance 3.0
model initialize at round 1336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.9644486 ,  12.13063929,   0.82649612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.177056759846649}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5770028906202452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00468726,  15.66229385,   1.34715313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1955252391836106}
episode index:1337
target Thresh 58.03594486115965
target distance 55.0
model initialize at round 1337
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.63474306,  6.80484755]), 'previousTarget': array([79.54031523,  7.2633415 ]), 'currentState': array([60.       ,  3.       ,  0.3563138], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.07854111514327805
running average episode reward sum: 0.5766303481871533
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91930353,  15.9998023 ,   5.88533979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.358206029289243}
episode index:1338
target Thresh 58.05389993726418
target distance 44.0
model initialize at round 1338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.33160799, 17.58873395]), 'previousTarget': array([90.81660336, 18.29773591]), 'currentState': array([7.24607628e+01, 1.98579876e+01, 1.20086034e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.4092496423907612
running average episode reward sum: 0.5765053439259162
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70008151,  15.90902061,   5.49161054]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.147358964325992}
episode index:1339
target Thresh 58.071837067267154
target distance 10.0
model initialize at round 1339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37725136,   6.74107058,   2.18922853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.267540977208235}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.57675002805359
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25871423,  14.13234489,   2.77237354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1411967305596167}
episode index:1340
target Thresh 58.089756269105706
target distance 13.0
model initialize at round 1340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68296977,   2.00091232,   1.01054209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.002953071884857}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5770080404819079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9578801 ,  14.05922863,   1.39174298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9417137852575841}
episode index:1341
target Thresh 58.107657560699025
target distance 9.0
model initialize at round 1341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.59191875,  12.36525906,   0.57778614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.862666702342442}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5769358597189344
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3733505 ,  15.94601435,   1.39161138]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1347390668602777}
episode index:1342
target Thresh 58.12554095994842
target distance 23.0
model initialize at round 1342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.12391144,  13.92919782]), 'previousTarget': array([110.88993593,  13.5704125 ]), 'currentState': array([91.84601924,  8.60351736,  0.65652966]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2625918310078108
running average episode reward sum: 0.5767017986402218
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77137766,  15.90608984,   5.50592311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1899673521699161}
episode index:1343
target Thresh 58.143406484737284
target distance 4.0
model initialize at round 1343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.99215473,  15.05737085,   0.17544007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.0086647431750473}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5770093121829001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58488407,  15.56910254,   0.44904553]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7044138943483965}
episode index:1344
target Thresh 58.161254152931136
target distance 8.0
model initialize at round 1344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.29562714,  22.60830922,   4.2616713 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.94524975498021}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5770301276140931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09863848,  14.71332361,   0.88976904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9458519624978741}
episode index:1345
target Thresh 58.17908398237767
target distance 18.0
model initialize at round 1345
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.75471496,  16.50425842]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 15.      ,  5.237367], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.814944241502225}
done in step count: 24
reward sum = 0.5777711408072188
running average episode reward sum: 0.5770306781439543
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48767023,  15.98975036,   5.47659584]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1033712147052277}
episode index:1346
target Thresh 58.1968959909067
target distance 21.0
model initialize at round 1346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.1410892 ,  14.76740234]), 'previousTarget': array([113.64677133,  14.74224216]), 'currentState': array([94.83642554,  9.53959581,  4.22251987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.2844477588470616
running average episode reward sum: 0.5768134673649662
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19217553,  15.96093383,   5.80420454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9799618667646336}
episode index:1347
target Thresh 58.21469019633022
target distance 20.0
model initialize at round 1347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.97189934,  15.00522772]), 'previousTarget': array([114.9007438 ,  15.00992562]), 'currentState': array([95.30925951, 18.66317728,  0.39165163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7776986190304375
running average episode reward sum: 0.5769624919581898
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21545391,  15.88857518,   5.47764774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9143228317609455}
episode index:1348
target Thresh 58.23246661644248
target distance 2.0
model initialize at round 1348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.925594  ,  16.90125248,   4.35430932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.706043838585145}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5770063933622711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03795119,  14.19787138,   1.01223345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2525766415736017}
episode index:1349
target Thresh 58.25022526901985
target distance 10.0
model initialize at round 1349
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.58375759,  15.00905881]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.       ,   5.       ,   4.1373124], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.308974435329667}
done in step count: 36
reward sum = 0.5571132180495735
running average episode reward sum: 0.5769916576768543
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30273687,  14.16031074,   1.77921768]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0914457963194069}
episode index:1350
target Thresh 58.26796617182102
target distance 13.0
model initialize at round 1350
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04023064,   3.81029866,   0.77629185]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.189773661649596}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5772544805415695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9390615 ,  14.22985427,   2.42289758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7725528725190391}
episode index:1351
target Thresh 58.285689342586885
target distance 11.0
model initialize at round 1351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.4434793 ,   3.35997548,   2.52170753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.446598818432095}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5775100206405979
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00440883,  14.09100967,   1.79055907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9090010188459897}
episode index:1352
target Thresh 58.30339479904062
target distance 11.0
model initialize at round 1352
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.33610783,  13.96240951]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  17.       ,   3.4568994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.704586270470694}
done in step count: 10
reward sum = 0.8343820750088045
running average episode reward sum: 0.5776998743393178
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8094085 ,  15.91674722,   6.16636493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9363496088643574}
episode index:1353
target Thresh 58.321082558887674
target distance 57.0
model initialize at round 1353
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.85129238,  9.43437689]), 'previousTarget': array([77.80587954,  9.77977257]), 'currentState': array([58.      ,  7.      ,  3.615479], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 56
reward sum = 0.43099420247715914
running average episode reward sum: 0.5775915245078095
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26391522,  15.9950591 ,   2.41331635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2377250967646087}
episode index:1354
target Thresh 58.338752639815816
target distance 2.0
model initialize at round 1354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88445229,  12.9677505 ,   1.16750544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.035531696066035}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5778813455229328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.435114  ,  14.03727335,   2.38594177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1162162791223973}
episode index:1355
target Thresh 58.35640505949512
target distance 37.0
model initialize at round 1355
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.67495618,  9.15863198]), 'previousTarget': array([96.86920279,  8.6297199 ]), 'currentState': array([78.     ,  2.     ,  4.76633], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -1.2227866292363079
running average episode reward sum: 0.5765534192878595
{'dynamicTrap': 23, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.87100616,  18.15907054,   5.52557539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.8095067213504854}
episode index:1356
target Thresh 58.374039835578024
target distance 27.0
model initialize at round 1356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.60899462,  12.13672605]), 'previousTarget': array([106.97366596,  12.32455532]), 'currentState': array([86.47843708,  6.30390784,  2.04341137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5766736454883756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30171758,  14.10631098,   2.07352728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1341421467683905}
episode index:1357
target Thresh 58.391656985699285
target distance 54.0
model initialize at round 1357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.42101299, 19.45381448]), 'previousTarget': array([80.87767469, 18.79136948]), 'currentState': array([62.60532481, 22.16278142,  0.86056784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.46633896860718127
running average episode reward sum: 0.5765923975672554
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48903657,  15.88912914,   5.57426034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0147449938351536}
episode index:1358
target Thresh 58.409256527476074
target distance 33.0
model initialize at round 1358
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.96887944,  16.88471803]), 'previousTarget': array([101.91786413,  16.18928508]), 'currentState': array([82.       , 18.       ,  4.6972322], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.686480465987303
running average episode reward sum: 0.5766732570730834
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85214454,  15.63125863,   5.73279683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0604894063306634}
episode index:1359
target Thresh 58.42683847850792
target distance 41.0
model initialize at round 1359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.76199645,  8.36068428]), 'previousTarget': array([93.06461275,  8.04487721]), 'currentState': array([72.5314926 ,  2.86634885,  1.63611972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4111400476022456
running average episode reward sum: 0.5759469237608221
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.79066035,  14.0230199 ,   4.51873653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3547505267517113}
episode index:1360
target Thresh 58.44440285637678
target distance 15.0
model initialize at round 1360
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.19779717,  15.03615984]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.      ,  21.      ,   6.279507], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.4827221027612}
done in step count: 17
reward sum = 0.7729431933839268
running average episode reward sum: 0.5760916675298325
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12070073,  14.3047501 ,   5.24849512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.120954787425959}
episode index:1361
target Thresh 58.46194967864703
target distance 58.0
model initialize at round 1361
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.89224084, 17.92665622]), 'previousTarget': array([76.92609538, 18.28223316]), 'currentState': array([57.       , 20.       ,  1.5520109], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.22489055580131745
running average episode reward sum: 0.575833810619606
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45177512,  14.1037107 ,   2.55439211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0037107510572043}
episode index:1362
target Thresh 58.4794789628655
target distance 3.0
model initialize at round 1362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37665736,  12.53789589,   1.85804331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.53978595688164}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5761376742948667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31896858,  14.02731714,   0.13815975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0236468661318936}
episode index:1363
target Thresh 58.49699072656148
target distance 6.0
model initialize at round 1363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.93670857,  21.44482356,   2.53672832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.729531249269264}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5762521652738692
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62155298,  14.77762343,   2.28196235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6601359296649802}
episode index:1364
target Thresh 58.51448498724672
target distance 10.0
model initialize at round 1364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.58217477,   4.54013006,   0.72713679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.7797160148904}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5764859279764223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82155605,  15.40409147,   2.19094297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9155568046325387}
episode index:1365
target Thresh 58.531961762415484
target distance 47.0
model initialize at round 1365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.34982189, 14.7664743 ]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([69.35065071, 14.58439679,  5.757855  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.06394620732385486
running average episode reward sum: 0.5760170903956754
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.20588324,  17.51944462,   5.77471987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7622718767826835}
episode index:1366
target Thresh 58.549421069544564
target distance 22.0
model initialize at round 1366
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.79564068,  15.16451235]), 'previousTarget': array([112.0585156 ,  15.93592685]), 'currentState': array([93.       , 22.       ,  5.5457706], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.43998574624956527
running average episode reward sum: 0.5759175795367536
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99921361,  15.57224398,   0.71477648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.151473407551677}
episode index:1367
target Thresh 58.56686292609325
target distance 23.0
model initialize at round 1367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.07271198,  15.41620261]), 'previousTarget': array([111.7042351,  15.5731765]), 'currentState': array([93.52336426, 19.63793257,  1.40657824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5758965554598925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75889684,  14.54228788,   1.38080282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5173307605911849}
episode index:1368
target Thresh 58.58428734950342
target distance 45.0
model initialize at round 1368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.07118134,  8.54724426]), 'previousTarget': array([89.21428366,  7.55079306]), 'currentState': array([70.70930682,  3.5354766 ,  1.9179444 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.1453664369514908
running average episode reward sum: 0.5755820703477608
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62405429,  15.24617374,   2.60038945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.670854137525727}
episode index:1369
target Thresh 58.601694357199484
target distance 41.0
model initialize at round 1369
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.97677292, 18.71911285]), 'previousTarget': array([93.78922128, 18.1040164 ]), 'currentState': array([75.31308874, 22.37144423,  1.3682583 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5052832118675581
running average episode reward sum: 0.5747931175870197
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55804821,  16.04570857,   4.80644184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1352655177155369}
episode index:1370
target Thresh 58.61908396658846
target distance 58.0
model initialize at round 1370
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.9996334 , 15.87890619]), 'previousTarget': array([76.99702801, 15.65522365]), 'currentState': array([57.     , 16.     ,  4.99261], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.049600455087194684
running average episode reward sum: 0.5744100448937303
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3475452 ,  14.76785964,   2.50251863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.692521777461651}
episode index:1371
target Thresh 58.63645619505995
target distance 27.0
model initialize at round 1371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.50637209,  14.42559513]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([89.61480835, 12.34576533,  0.52121812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 47
reward sum = 0.4949870589573907
running average episode reward sum: 0.5743521564200157
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.96707516,  15.46117627,   2.76561752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.071409315240968}
episode index:1372
target Thresh 58.65381105998619
target distance 8.0
model initialize at round 1372
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.27181259,  21.13282182,   4.2633878 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.177108391935457}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5745060719221186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73730427,  15.83535989,   2.26904498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1142009380936084}
episode index:1373
target Thresh 58.67114857872204
target distance 50.0
model initialize at round 1373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.14500638, 19.62538357]), 'previousTarget': array([84.74881264, 19.84018998]), 'currentState': array([63.35256399, 22.49927012,  4.37966871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5745195074202841
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7032753 ,  15.04888921,   1.92829005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3007252973433982}
episode index:1374
target Thresh 58.68846876860503
target distance 26.0
model initialize at round 1374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.35823266,  13.62098657]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([89.93018481,  8.87220181,  6.02793307]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.38544950567271064
running average episode reward sum: 0.5743820019644676
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29039696,  15.91608077,   4.98081679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.158766781746219}
episode index:1375
target Thresh 58.705771646955334
target distance 3.0
model initialize at round 1375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.94816358,  11.60185018,   6.26990989]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5279507299747137}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5746768551607144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.94239814,  14.02086953,   3.28467533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3589741487565437}
episode index:1376
target Thresh 58.723057231075856
target distance 47.0
model initialize at round 1376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.21817683,  8.65054367]), 'previousTarget': array([87.37835421,  7.94766491]), 'currentState': array([69.79843205,  3.86791754,  0.3523145 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4163522658908704
running average episode reward sum: 0.5745618772454857
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0681286 ,  15.29752507,   0.85624012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9782154568414658}
episode index:1377
target Thresh 58.740325538252165
target distance 51.0
model initialize at round 1377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.63666371, 16.47555124]), 'previousTarget': array([83.98463901, 16.21628867]), 'currentState': array([65.66186811, 17.47931437,  5.63481254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.07410600243737286
running average episode reward sum: 0.5741987017195
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81824033,  14.604144  ,   4.86350646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4355898866199496}
episode index:1378
target Thresh 58.75757658575258
target distance 4.0
model initialize at round 1378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.2596517 ,  17.60988005,   4.82652283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8979641643623126}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.574287327184569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17309901,  15.97277221,   3.1304489 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9880531602116427}
episode index:1379
target Thresh 58.77481039082814
target distance 24.0
model initialize at round 1379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.50016195,  14.76172371]), 'previousTarget': array([110.93091516,  14.6609096 ]), 'currentState': array([89.51890546, 13.89605103,  3.52408588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6610341286339985
running average episode reward sum: 0.5743501871856194
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99026857,  14.10841158,   5.44956081]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8916415285241622}
episode index:1380
target Thresh 58.792026970712655
target distance 29.0
model initialize at round 1380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.46540786,  12.24376519]), 'previousTarget': array([104.90745963,  11.51981367]), 'currentState': array([86.25209007,  6.68962983,  0.63601387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5743723855056425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54793765,  14.96445359,   4.65157798]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45345773003856527}
episode index:1381
target Thresh 58.80922634262271
target distance 48.0
model initialize at round 1381
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([86.8569824 , 19.61248034]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([67.       , 22.       ,  2.8880055], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 47
reward sum = 0.31361053543517925
running average episode reward sum: 0.5741837010989345
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29723971,  14.18078441,   1.15580704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0793452647162018}
episode index:1382
target Thresh 58.82640852375767
target distance 29.0
model initialize at round 1382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.7043407 ,  12.43878837]), 'previousTarget': array([105.10128274,  11.9279843 ]), 'currentState': array([87.59440325,  6.53876374,  1.33586329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5742720810822682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62275264,  15.02677411,   2.4176218 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6233279228522308}
episode index:1383
target Thresh 58.84357353129972
target distance 58.0
model initialize at round 1383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.7255022 , 17.75432686]), 'previousTarget': array([76.9732997 , 16.96689829]), 'currentState': array([55.77450392, 19.15349358,  1.37645125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3771768487803585
running average episode reward sum: 0.5741296712323392
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48796287,  15.40145435,   3.1236797 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6318808153523796}
episode index:1384
target Thresh 58.86072138241388
target distance 22.0
model initialize at round 1384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.36633689,  16.01824908]), 'previousTarget': array([112.50265712,  15.56757793]), 'currentState': array([92.10819001, 21.41489422,  1.12926435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5742492168656648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45089097,  14.9956422 ,   2.83087477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5491263248757922}
episode index:1385
target Thresh 58.877852094247984
target distance 25.0
model initialize at round 1385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.54576691,  15.22910778]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([91.58961456, 16.55273289,  5.60782308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6087676737447901
running average episode reward sum: 0.5742741219572081
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1041835 ,  15.07651261,   2.813587  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.12926090653499125}
episode index:1386
target Thresh 58.89496568393275
target distance 45.0
model initialize at round 1386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.81916256,  8.79126879]), 'previousTarget': array([89.32469879,  8.15325301]), 'currentState': array([71.44752778,  3.81736027,  5.89177686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.36378746695616215
running average episode reward sum: 0.5741223651763854
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29703924,  14.45460877,   5.94843354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8897221061935483}
episode index:1387
target Thresh 58.9120621685818
target distance 5.0
model initialize at round 1387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36885074,  10.75373029,   1.90776086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.292919268194388}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5744148562677568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06629888,  14.17424892,   2.53494656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8284083419230335}
episode index:1388
target Thresh 58.92914156529158
target distance 8.0
model initialize at round 1388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.91666141,   8.58824267,   0.32492387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.692101536799021}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5746928844561889
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70297069,  14.66787794,   1.74766743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44556870994877307}
episode index:1389
target Thresh 58.9462038911415
target distance 20.0
model initialize at round 1389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.61161351,  15.0776773 ]), 'currentState': array([95.94091578, 20.39537292,  1.98751992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.808047362072482}
done in step count: 40
reward sum = 0.5429913355209538
running average episode reward sum: 0.5746700775864513
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19964242,  14.07638397,   1.99967782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9449463859228648}
episode index:1390
target Thresh 58.96324916319389
target distance 27.0
model initialize at round 1390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.800346  ,  11.92679228]), 'previousTarget': array([106.75497521,  11.94628712]), 'currentState': array([89.40603277,  4.07508923,  0.42814988]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.574696279715816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4229908 ,  15.55058963,   3.44347158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7975516017973678}
episode index:1391
target Thresh 58.98027739849403
target distance 45.0
model initialize at round 1391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.61850581, 15.76652501]), 'previousTarget': array([89.99506356, 15.55566525]), 'currentState': array([71.62924467, 16.42184107,  1.26335448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5948470136803151
running average episode reward sum: 0.5747107558178021
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96426433,  14.86044894,   2.35651383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.14405393674716221}
episode index:1392
target Thresh 58.99728861407015
target distance 26.0
model initialize at round 1392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.01033791,  15.35792089]), 'previousTarget': array([109.,  15.]), 'currentState': array([89.04595088, 16.5509216 ,  0.60615563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5748622040482324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06311551,  15.17914934,   4.40740872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9538590169911946}
episode index:1393
target Thresh 59.01428282693347
target distance 45.0
model initialize at round 1393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.61095543,  7.7875438 ]), 'previousTarget': array([89.32469879,  8.15325301]), 'currentState': array([71.43201263,  2.11584622,  0.45686978]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5748709376048244
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11830686,  15.45782398,   5.06786437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47286288620465766}
episode index:1394
target Thresh 59.03126005407818
target distance 6.0
model initialize at round 1394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51469953,   8.08992268,   2.79626083]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.927097882350392}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5751474430330648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.681834  ,  14.53538954,   0.91017024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5631096548897881}
episode index:1395
target Thresh 59.048220312481554
target distance 21.0
model initialize at round 1395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.52575568,  15.35153556]), 'previousTarget': array([113.90990945,  15.10381815]), 'currentState': array([92.72461153, 18.16484156,  3.33482075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5752815370669574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55954281,  15.27092421,   5.11417241]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5171097157732184}
episode index:1396
target Thresh 59.06516361910382
target distance 50.0
model initialize at round 1396
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([84.86937801, 10.28206432]), 'previousTarget': array([84.80683493, 10.77295689]), 'currentState': array([65.       ,  8.       ,  1.9338759], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.39891553675007385
running average episode reward sum: 0.5751552908247836
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20857716,  15.3421528 ,   3.26616886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4007155708870093}
episode index:1397
target Thresh 59.08208999088829
target distance 12.0
model initialize at round 1397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.10842042,   4.5300707 ,   0.7584717 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.556252439042682}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5754039241606943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15366935e+02, 1.40923012e+01, 2.85338312e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9790599038754105}
episode index:1398
target Thresh 59.09899944476135
target distance 47.0
model initialize at round 1398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.23888585, 10.71387881]), 'previousTarget': array([87.64310384,  9.76144542]), 'currentState': array([67.47307911,  7.66217697,  0.97417915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5428418651984278
running average episode reward sum: 0.5753806489219793
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24464448,  15.97363032,   3.36417806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.00389587500569}
episode index:1399
target Thresh 59.11589199763243
target distance 42.0
model initialize at round 1399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.55816051, 17.46670703]), 'previousTarget': array([92.949174, 16.575059]), 'currentState': array([72.67789098, 19.6518577 ,  2.77337772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.3940414304347778
running average episode reward sum: 0.5752511209087742
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10373908,  14.80900313,   5.08879213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9163860745014275}
episode index:1400
target Thresh 59.132767666394116
target distance 12.0
model initialize at round 1400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.76419141,   4.84794119,   1.14933625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.655268898111268}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5755125263538079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77878188,  14.21294792,   2.55570755]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8175502623962997}
episode index:1401
target Thresh 59.14962646792206
target distance 12.0
model initialize at round 1401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35243765,   3.68261261,   3.33594179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.32287373595041}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5757536138867108
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41638367,  14.47612174,   3.05031407]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7842553522208106}
episode index:1402
target Thresh 59.16646841907506
target distance 59.0
model initialize at round 1402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.73274193, 19.62564766]), 'previousTarget': array([75.89737675, 18.97653796]), 'currentState': array([56.87727351, 22.02572558,  6.13643533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4172183598980941
running average episode reward sum: 0.5756406165567118
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05505741,  14.10159061,   5.34219098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3038619244455434}
episode index:1403
target Thresh 59.18329353669509
target distance 27.0
model initialize at round 1403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.42621401,  17.28481285]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([89.53474363, 23.85081216,  1.18076223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5884174998741436
running average episode reward sum: 0.575649716900955
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28835851,  15.01918952,   5.38814599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7119001635749617}
episode index:1404
target Thresh 59.20010183760724
target distance 56.0
model initialize at round 1404
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([79.03060295, 19.10614138]), 'previousTarget': array([78.92075411, 18.22136124]), 'currentState': array([59.15966005, 21.37453917,  0.4451592 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5757019965532449
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18320006,  14.74220762,   0.31140494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31625807975567294}
episode index:1405
target Thresh 59.21689333861984
target distance 7.0
model initialize at round 1405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77653463,  23.32302758,   3.10755789]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.35917425200907}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5758133773306995
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68555939,  14.73168672,   5.56007689]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41335809490659886}
episode index:1406
target Thresh 59.233668056524365
target distance 7.0
model initialize at round 1406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58876107,  22.40368548,   3.87298226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.427058519907073}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5758562857235446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1839826,  14.1541894,   5.0591391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.175278679245191}
episode index:1407
target Thresh 59.250426008095566
target distance 1.0
model initialize at round 1407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.65174781,  15.0072756 ,   1.48995185]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3482634570529046}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5761504218842524
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12265716,  15.85008707,   5.81428475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.221629440569133}
episode index:1408
target Thresh 59.26716721009137
target distance 42.0
model initialize at round 1408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.56537312,  8.90527264]), 'previousTarget': array([92.10571781,  7.91367456]), 'currentState': array([73.26490701,  3.66199048,  0.40273452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.41518919907000534
running average episode reward sum: 0.5760361839688413
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98506056,  14.21601176,   2.97058373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7841305716045676}
episode index:1409
target Thresh 59.283891679252996
target distance 26.0
model initialize at round 1409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.97382899,  15.27173532]), 'previousTarget': array([109.,  15.]), 'currentState': array([90.00299432, 16.35144015,  1.9423272 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.3578900990321261
running average episode reward sum: 0.5758814704334252
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34954831,  15.45714275,   4.20201608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5754680895202907}
episode index:1410
target Thresh 59.3005994323049
target distance 57.0
model initialize at round 1410
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.93350868, 13.62948814]), 'previousTarget': array([77.97235659, 13.05117666]), 'currentState': array([58.      , 12.      ,  5.196951], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = -0.19863837722720995
running average episode reward sum: 0.5753325548787401
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01404943,  15.17565115,   0.63382706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0014748380848855}
episode index:1411
target Thresh 59.31729048595485
target distance 23.0
model initialize at round 1411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.75471184,  13.86076492]), 'previousTarget': array([110.88993593,  13.5704125 ]), 'currentState': array([91.4381459 ,  8.67710983,  2.49227417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.12376143451405436
running average episode reward sum: 0.5750127453034111
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46580694,  14.51622445,   2.37842133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6715764141749542}
episode index:1412
target Thresh 59.3339648568939
target distance 59.0
model initialize at round 1412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.10515295, 17.76540746]), 'previousTarget': array([75.97419539, 16.98436295]), 'currentState': array([57.15819589, 19.22105297,  1.8218109 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5750470783887528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23644545,  14.24586537,   4.97798454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.073188987825772}
episode index:1413
target Thresh 59.35062256179641
target distance 10.0
model initialize at round 1413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.6757277 ,   6.69250841,   0.8196466 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.62650904089588}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5753062248321844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15273164e+02, 1.41657228e+01, 1.08537734e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8778592838373163}
episode index:1414
target Thresh 59.3672636173201
target distance 4.0
model initialize at round 1414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.97667482,  17.29593094,   5.67253387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.0602522222455173}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5754953675661433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64609727,  14.65825005,   2.34248267]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7309136179553016}
episode index:1415
target Thresh 59.383888040106015
target distance 4.0
model initialize at round 1415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92843477,   9.92479886,   2.91868448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.075705685035802}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.575708662520545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3935318 ,  15.37546947,   1.94726318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7132888647016291}
episode index:1416
target Thresh 59.40049584677859
target distance 39.0
model initialize at round 1416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.02641611, 18.7493641 ]), 'previousTarget': array([95.68542414, 18.46671874]), 'currentState': array([77.44786703, 22.83353058,  1.00563323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5758512967455828
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68234652,  14.79368073,   4.67766929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3787761526129683}
episode index:1417
target Thresh 59.41708705394562
target distance 22.0
model initialize at round 1417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.62971679,  15.16232933]), 'previousTarget': array([113.,  15.]), 'currentState': array([91.65287507, 16.12451178,  3.32906175]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6479846564357029
running average episode reward sum: 0.5759021665337987
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89860163,  15.27508227,   2.9312371 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2931755191482274}
episode index:1418
target Thresh 59.433661678198334
target distance 5.0
model initialize at round 1418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.59063187,  18.3675767 ,   5.47668266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.650601511526816}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.576166499080216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19071286,  15.48444209,   1.84293294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9432018956388102}
episode index:1419
target Thresh 59.45021973611134
target distance 28.0
model initialize at round 1419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.61633925,  11.49609429]), 'previousTarget': array([105.83483823,  11.72672794]), 'currentState': array([88.16318397,  3.78369776,  0.20228213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.28979957140743723
running average episode reward sum: 0.5759648322297422
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00009405,  14.99789946,   4.69379017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9999081569693602}
episode index:1420
target Thresh 59.46676124424271
target distance 1.0
model initialize at round 1420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94502271,  13.9472175 ,   3.17589152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0542170070104535}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5761014109162836
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22856576,  14.90453729,   5.55965563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24770029303236174}
episode index:1421
target Thresh 59.48328621913394
target distance 4.0
model initialize at round 1421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89349245,  12.19131241,   3.32930458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.947380995749283}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5762714569969314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25173612,  14.64992634,   4.19478767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8261055658471879}
episode index:1422
target Thresh 59.49979467731001
target distance 20.0
model initialize at round 1422
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.14694607,  14.29476763]), 'previousTarget': array([112.52431817,  13.638375  ]), 'currentState': array([95.       ,  4.       ,  0.8796203], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.3896612204005223
running average episode reward sum: 0.5761403183907499
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06081568,  15.1837207 ,   2.71674868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9569851003406692}
episode index:1423
target Thresh 59.51628663527939
target distance 25.0
model initialize at round 1423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.07122708,  15.80483357]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([90.33265606, 19.02800186,  4.90026646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.33457059922284954
running average episode reward sum: 0.5759706767340309
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19579674,  14.40086415,   4.91134639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0028492625959784}
episode index:1424
target Thresh 59.53276210953402
target distance 46.0
model initialize at round 1424
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.99528808,  8.55963539]), 'previousTarget': array([88.45157783,  8.65146426]), 'currentState': array([70.62740781,  3.57112718,  0.75232714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.152220507188844
running average episode reward sum: 0.5756733081939992
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18071648,  15.32709379,   4.41738354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37369612355289306}
episode index:1425
target Thresh 59.549221116549404
target distance 47.0
model initialize at round 1425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.49210599, 10.51924718]), 'previousTarget': array([87.83899559, 11.53263774]), 'currentState': array([67.75227063,  7.30383117,  5.52391434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.1482235233098365
running average episode reward sum: 0.575373553786647
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97969886,  14.16155517,   1.23355225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8386905708806938}
episode index:1426
target Thresh 59.56566367278452
target distance 43.0
model initialize at round 1426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.68476093,  8.43324558]), 'previousTarget': array([91.14422855,  7.78779003]), 'currentState': array([73.49825379,  2.78718998,  1.47774332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5753499458834797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29397955,  15.3538046 ,   4.71311059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7897104340755803}
episode index:1427
target Thresh 59.58208979468194
target distance 51.0
model initialize at round 1427
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.97924564, 11.91090272]), 'previousTarget': array([83.93876756, 12.56382491]), 'currentState': array([64.       , 11.       ,  2.4977126], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.1609143294356563
running average episode reward sum: 0.5750597248635583
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01127677,  15.32701156,   4.66613882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0413981912150925}
episode index:1428
target Thresh 59.59849949866779
target distance 7.0
model initialize at round 1428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.90954539,  14.41596877,   6.12249774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.123848221195743}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5752182769032192
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92318919,  14.11607869,   4.30022312]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8872523761274468}
episode index:1429
target Thresh 59.614892801151775
target distance 14.0
model initialize at round 1429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.55271866,  18.4103805 ,   6.0570178 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.87299789068202}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5752791606543247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45730638,  14.77769717,   2.55675491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5864596464972274}
episode index:1430
target Thresh 59.63126971852719
target distance 23.0
model initialize at round 1430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.32381582,  14.95921682]), 'previousTarget': array([111.92481176,  14.73259233]), 'currentState': array([92.32613778, 14.65446615,  2.37501249]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.5752229478653281
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87700733,  14.5723439 ,   4.15421059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44499094485231505}
episode index:1431
target Thresh 59.64763026717096
target distance 12.0
model initialize at round 1431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.40029164,   2.47719866,   2.44746113]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.648416996427837}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.5751465915994792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14437372,  15.97679821,   4.68388182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.298549600797642}
episode index:1432
target Thresh 59.66397446344364
target distance 28.0
model initialize at round 1432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.63836315,  13.12449486]), 'previousTarget': array([106.23047895,  12.49442256]), 'currentState': array([88.25744843,  8.18686944,  1.68659061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.40350097614716984
running average episode reward sum: 0.5750268109885565
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06360635,  14.0071611 ,   5.80456864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3647571709871644}
episode index:1433
target Thresh 59.68030232368942
target distance 15.0
model initialize at round 1433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.41742338,  23.69083405,   5.87765772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.125042148033188}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5752501801958578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22350558,  14.83736108,   5.73301649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7933441862047714}
episode index:1434
target Thresh 59.696613864236156
target distance 59.0
model initialize at round 1434
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.95505467, 13.34007199]), 'previousTarget': array([75.97419539, 13.01563705]), 'currentState': array([56.       , 12.       ,  2.8461504], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = -0.07881997017246134
running average episode reward sum: 0.5747943821816639
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35300945,  15.02673733,   4.04526693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6475427800872807}
episode index:1435
target Thresh 59.71290910139541
target distance 40.0
model initialize at round 1435
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.92890222, 11.68489064]), 'previousTarget': array([94.84555753, 12.48069469]), 'currentState': array([75.        , 10.        ,  0.95566285], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.20146547622627103
running average episode reward sum: 0.5745344038348983
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61805389,  15.05620757,   3.48953615]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6206044602683087}
episode index:1436
target Thresh 59.7291880514624
target distance 10.0
model initialize at round 1436
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.05229543,  18.87437462,   0.19094485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.75049720321272}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5747514187742728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35248429,  15.51363701,   2.31814236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6229511625989206}
episode index:1437
target Thresh 59.74545073071609
target distance 47.0
model initialize at round 1437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.63247538,  7.01527218]), 'previousTarget': array([87.27622241,  7.33172109]), 'currentState': array([66.33310156,  1.76797098,  2.26990223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.3377933805696329
running average episode reward sum: 0.5745866357157161
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41406885,  14.06641589,   2.60369308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1022225788872577}
episode index:1438
target Thresh 59.76169715541915
target distance 34.0
model initialize at round 1438
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.79381601,  12.86441053]), 'previousTarget': array([100.78718271,  12.90987981]), 'currentState': array([81.       , 10.       ,  2.2483754], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.45366644640149945
running average episode reward sum: 0.574502605007367
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29492538,  15.69579702,   2.09753397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9905875526394985}
episode index:1439
target Thresh 59.77792734181802
target distance 10.0
model initialize at round 1439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.52883715,   6.56373067,   0.26823509]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.56358338497986}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5746890220826285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45961865,  14.56354963,   4.81041045]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6338282373554361}
episode index:1440
target Thresh 59.79414130614287
target distance 59.0
model initialize at round 1440
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.94017955, 11.54571655]), 'previousTarget': array([75.9285661 , 11.68886153]), 'currentState': array([56.      , 10.      ,  3.872067], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.1960908054768068
running average episode reward sum: 0.5744262891078847
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74017403,  14.33170763,   3.97263119]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7170245631729465}
episode index:1441
target Thresh 59.81033906460769
target distance 18.0
model initialize at round 1441
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.6409008 ,  13.09416155]), 'previousTarget': array([113.21358457,  13.70981108]), 'currentState': array([97.       ,  2.       ,  4.0441265], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.47002866411977007
running average episode reward sum: 0.574353891309696
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27083567,  15.174316  ,   6.06038887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7497110737250041}
episode index:1442
target Thresh 59.82652063341021
target distance 47.0
model initialize at round 1442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.83702633, 17.63758873]), 'previousTarget': array([87.83899559, 18.46736226]), 'currentState': array([68.93789233, 19.643696  ,  6.17261744]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5744336523592868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17721838,  14.45485415,   5.1543679 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.98699219428027}
episode index:1443
target Thresh 59.84268602873202
target distance 2.0
model initialize at round 1443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.61844927,  16.03273965,   4.75647378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.535737054781836}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5747145847329992
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74518693,  14.81015537,   5.43175948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3177588434777864}
episode index:1444
target Thresh 59.85883526673852
target distance 41.0
model initialize at round 1444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.27974812, 11.15667986]), 'previousTarget': array([93.62981184, 10.83020719]), 'currentState': array([72.55989549,  7.82090228,  3.57414675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5552673412284759
running average episode reward sum: 0.5747011264329961
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04846404,  14.02162404,   5.35611902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3647857698826462}
episode index:1445
target Thresh 59.87496836357894
target distance 35.0
model initialize at round 1445
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.84253729, 13.50473832]), 'previousTarget': array([99.87065345, 13.27093182]), 'currentState': array([80.      , 11.      ,  2.526185], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 60
reward sum = 0.0990110184424271
running average episode reward sum: 0.5743721567870828
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28330606,  15.9349789 ,   4.82659699]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1780644019623545}
episode index:1446
target Thresh 59.89108533538637
target distance 52.0
model initialize at round 1446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.78673393,  8.15040914]), 'previousTarget': array([82.64012894,  8.77694787]), 'currentState': array([64.25157165,  3.86351815,  4.54875102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 70
reward sum = 0.29133347404412535
running average episode reward sum: 0.5741765529980413
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48902633,  15.17177136,   2.39538901]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5390728050658706}
episode index:1447
target Thresh 59.907186198277806
target distance 16.0
model initialize at round 1447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.76224028,  20.70001161,   0.94628149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.336359871300038}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.574344875086853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1567104 ,  15.24999697,   2.4479097 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2950536104534539}
episode index:1448
target Thresh 59.92327096835409
target distance 25.0
model initialize at round 1448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.36683463,  14.84676337]), 'previousTarget': array([109.93630557,  14.59490445]), 'currentState': array([91.38460007, 14.00396928,  5.97367722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.5572454639590148
running average episode reward sum: 0.5743330742510159
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01550179,  14.47213238,   2.58728299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1170859198245573}
episode index:1449
target Thresh 59.9393396617
target distance 7.0
model initialize at round 1449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98242923,   9.68287829,   0.57123685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.664282264234259}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5743757310867489
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42068654,  14.97140633,   2.59378209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42165716290771993}
episode index:1450
target Thresh 59.95539229438424
target distance 36.0
model initialize at round 1450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.31977367, 17.76145353]), 'previousTarget': array([98.63230779, 18.18260682]), 'currentState': array([77.55934861, 20.84781084,  4.18782616]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5745000202363815
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70405822,  14.68042663,   0.67217782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7731915098499225}
episode index:1451
target Thresh 59.971428882459435
target distance 23.0
model initialize at round 1451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.96040038,  15.20979242]), 'previousTarget': array([112.,  15.]), 'currentState': array([90.98731731, 16.24707379,  3.26284003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7191088373142187
running average episode reward sum: 0.5745996130856087
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49542745,  14.4913023 ,   3.96682405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.716496199269307}
episode index:1452
target Thresh 59.987449441962184
target distance 34.0
model initialize at round 1452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.63381605, 10.32412871]), 'previousTarget': array([99.85980667,  9.65640235]), 'currentState': array([80.50006329,  4.50180058,  0.88214922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 42
reward sum = 0.6064178819241738
running average episode reward sum: 0.5746215114124074
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14046996e+02, 1.53581780e+01, 2.75659973e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.018090096136075}
episode index:1453
target Thresh 60.00345398891303
target distance 19.0
model initialize at round 1453
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.91490758,  16.49817446]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.      , 10.      ,  5.373287], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 44
reward sum = 0.5726116020847181
running average episode reward sum: 0.5746201290813705
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6925332 ,  14.90720384,   2.38827879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.698722661220936}
episode index:1454
target Thresh 60.019442539316536
target distance 20.0
model initialize at round 1454
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.98845008,  13.32039475]), 'previousTarget': array([114.97504678,  14.99875234]), 'currentState': array([95.       , 14.       ,  0.6152692], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5922820409839835
running average episode reward sum: 0.5746322678524375
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06353699,  14.81872368,   4.18862126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19208865654431673}
episode index:1455
target Thresh 60.03541510916125
target distance 38.0
model initialize at round 1455
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.96932522, 14.10727159]), 'previousTarget': array([96.97235659, 14.05117666]), 'currentState': array([77.       , 13.       ,  2.2705362], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = -0.0811144274705804
running average episode reward sum: 0.5741818923748805
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10142654,  14.46653687,   0.5549124 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0449962551833232}
episode index:1456
target Thresh 60.05137171441974
target distance 19.0
model initialize at round 1456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([97.29226776,  7.01817125,  0.15539038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.42352623759554}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5743605690180343
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10244761,  15.65637438,   4.29732632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6643213397445028}
episode index:1457
target Thresh 60.06731237104862
target distance 21.0
model initialize at round 1457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.51536857,  14.24686028]), 'previousTarget': array([113.45612429,  14.63241055]), 'currentState': array([93.37535328,  8.44515237,  3.35045552]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5745220006361436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22038848,  14.3614282 ,   5.89677978]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0077539713779056}
episode index:1458
target Thresh 60.08323709498853
target distance 53.0
model initialize at round 1458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.42412604, 16.08519665]), 'previousTarget': array([81.99644096, 15.62270866]), 'currentState': array([63.43592711, 16.77214919,  1.47479969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.33308614480841686
running average episode reward sum: 0.5743565202688867
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18881791,  14.87193893,   6.01167165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8212283598131468}
episode index:1459
target Thresh 60.09914590216423
target distance 11.0
model initialize at round 1459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.47546778,  22.61057741,   5.66554803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.191702200324274}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5745763707716195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3853205 ,  14.75806654,   3.34941   ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4549765804157912}
episode index:1460
target Thresh 60.1150388084845
target distance 47.0
model initialize at round 1460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.22409764, 18.07382628]), 'previousTarget': array([87.78180356, 19.05377394]), 'currentState': array([68.35459555, 20.35481002,  5.93467617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = -0.010146768187364319
running average episode reward sum: 0.5741761495950563
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67424245,  15.4635178 ,   5.70955878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5665392632002804}
episode index:1461
target Thresh 60.13091582984225
target distance 41.0
model initialize at round 1461
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.0419577 ,  8.11586845]), 'previousTarget': array([93.06461275,  8.04487721]), 'currentState': array([74.       ,  2.       ,  1.1477997], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.20408699169076122
running average episode reward sum: 0.5739230106361615
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64367881,  15.39816849,   5.07243288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5343247428409172}
episode index:1462
target Thresh 60.14677698211451
target distance 43.0
model initialize at round 1462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.24761689, 11.47175638]), 'previousTarget': array([91.80809791, 11.76392064]), 'currentState': array([70.44775321,  8.64945267,  2.46147835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6228986989560288
running average episode reward sum: 0.5739564868414382
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70956214,  15.73115316,   2.78118326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0188539514190216}
episode index:1463
target Thresh 60.16262228116243
target distance 7.0
model initialize at round 1463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.57267968,  13.4852769 ,   6.08385324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.603395527990731}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5741947301526312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64317716,  14.33084555,   4.26589743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7583470285426701}
episode index:1464
target Thresh 60.17845174283131
target distance 22.0
model initialize at round 1464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.30885764,  14.99071744]), 'previousTarget': array([113.,  15.]), 'currentState': array([91.30892089, 14.94042117,  2.18226135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.1405624058589391
running average episode reward sum: 0.5738987353920213
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39022601,  14.86926207,   3.30072968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6236318818086226}
episode index:1465
target Thresh 60.19426538295062
target distance 51.0
model initialize at round 1465
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.57743868,  7.08948588]), 'previousTarget': array([83.46834337,  7.58078667]), 'currentState': array([64.       ,  3.       ,  2.0927718], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.41448296459265105
running average episode reward sum: 0.5737899933928403
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0638858 ,  14.24119911,   5.18834752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2050263801608962}
episode index:1466
target Thresh 60.21006321733398
target distance 37.0
model initialize at round 1466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.49342702,  9.14784919]), 'previousTarget': array([97.02445638,  9.17009396]), 'currentState': array([79.64306597,  2.46474448,  5.49046962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.08739423286859871
running average episode reward sum: 0.5734584352738735
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3063843 ,  15.41192898,   1.29970437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8067144589457469}
episode index:1467
target Thresh 60.225845261779256
target distance 22.0
model initialize at round 1467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.04558493,  16.21559322]), 'previousTarget': array([111.79586847,  16.16513874]), 'currentState': array([93.54997244, 23.82560759,  0.22683871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5735819099686487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04518204,  15.33908726,   4.82923076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0132410955312512}
episode index:1468
target Thresh 60.24161153206848
target distance 12.0
model initialize at round 1468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.04037019e+02, 2.38123442e+01, 4.45576906e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.065716138617663}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5738070972831757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3503544 ,  15.8861703 ,   2.87824244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0987889706925809}
episode index:1469
target Thresh 60.257362043967916
target distance 6.0
model initialize at round 1469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.56708531,   8.73928122,   0.84513825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.716820194910711}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5740572150057048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1829494 ,  15.31266541,   5.44782716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8748321793991201}
episode index:1470
target Thresh 60.273096813228086
target distance 7.0
model initialize at round 1470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.7555397 ,   9.89069024,   1.93780994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.258681170631568}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.574281773034259
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23317179,  14.68651167,   5.75897518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8284325204648851}
episode index:1471
target Thresh 60.28881585558377
target distance 40.0
model initialize at round 1471
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.99495829, 15.55095312]), 'previousTarget': array([94.99375293, 15.50015618]), 'currentState': array([75.       , 16.       ,  4.1778426], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.11151328728746557
running average episode reward sum: 0.5739673922694853
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34387965,  14.42953319,   0.41516444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8694402155942077}
episode index:1472
target Thresh 60.30451918675398
target distance 30.0
model initialize at round 1472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.97227834,  11.07161674]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([86.35025504,  3.77639583,  1.53183382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.2920207895816466
running average episode reward sum: 0.5737759824916933
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97219047,  14.82679549,   2.1330544 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.17542284212222473}
episode index:1473
target Thresh 60.32020682244208
target distance 6.0
model initialize at round 1473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.28708599,  11.48542045,   5.90971494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.707432882985432}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5740384112756202
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0499707 ,  14.81583824,   0.49422654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9677144347664647}
episode index:1474
target Thresh 60.3358787783357
target distance 18.0
model initialize at round 1474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.20824347,  14.45731942]), 'previousTarget': array([113.21358457,  13.70981108]), 'currentState': array([97.7113521 ,  3.15012812,  0.23617911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.5838222064438056
running average episode reward sum: 0.5740450443570901
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88596003,  14.57242053,   6.20314658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4425260651558673}
episode index:1475
target Thresh 60.35153507010679
target distance 30.0
model initialize at round 1475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.16934996,  12.48106037]), 'previousTarget': array([104.61161351,  12.9223227 ]), 'currentState': array([85.79524793,  7.51677082,  6.2145443 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5367830840202747
running average episode reward sum: 0.5740197991265097
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42839651,  15.32757127,   2.64970485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5392833303311495}
episode index:1476
target Thresh 60.36717571341164
target distance 1.0
model initialize at round 1476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40305511,  17.57916922,   2.9333545 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.610472615536697}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5741739025729636
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29558825,  14.40367461,   5.85751518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.922930051443472}
episode index:1477
target Thresh 60.3828007238909
target distance 16.0
model initialize at round 1477
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.50528571,  13.34198006]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 14.       ,  0.9623537], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.518397220533245}
done in step count: 25
reward sum = -0.05589086324992143
running average episode reward sum: 0.5737476070615815
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8793748 ,  14.56454512,   5.80358747]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45185328313670114}
episode index:1478
target Thresh 60.3984101171696
target distance 59.0
model initialize at round 1478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.09014017, 20.09710862]), 'previousTarget': array([75.86070472, 19.6436452 ]), 'currentState': array([54.24359165, 22.56986248,  2.21395946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5073973717913223
running average episode reward sum: 0.5737027455096746
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37232989,  15.29159842,   4.87834977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6920978239181023}
episode index:1479
target Thresh 60.4140039088571
target distance 36.0
model initialize at round 1479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.07890043, 16.63268922]), 'previousTarget': array([98.96920706, 15.89059961]), 'currentState': array([79.1832407 , 18.67296471,  0.45170069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.4570337773996901
running average episode reward sum: 0.5736239151258165
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20557086,  15.16997421,   4.30461109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26674071818189266}
episode index:1480
target Thresh 60.42958211454721
target distance 5.0
model initialize at round 1480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.65840361,  18.39540662,   5.51372337]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.650844693212225}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5738983756827876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11359343,  15.12697202,   5.58692944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8954543529896998}
episode index:1481
target Thresh 60.44514474981815
target distance 7.0
model initialize at round 1481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.61070351,   8.6529147 ,   6.13416457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.006031392967191}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5741464065692371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07903498,  14.64896409,   1.1586793 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35982320849859006}
episode index:1482
target Thresh 60.46069183023253
target distance 40.0
model initialize at round 1482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.55262539,  8.83182596]), 'previousTarget': array([94.02068137,  8.18172145]), 'currentState': array([76.48855874,  2.7852267 ,  1.49541109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 36
reward sum = 0.635601011155745
running average episode reward sum: 0.5741878459519656
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69875105,  14.75571037,   5.30385056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3878509441553297}
episode index:1483
target Thresh 60.47622337133745
target distance 20.0
model initialize at round 1483
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.99852621,  16.24948013]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([95.       , 10.       ,  6.1670675], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6321222158051552
running average episode reward sum: 0.5742268852847507
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25484567,  14.03656922,   4.9010351 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2179711946221898}
episode index:1484
target Thresh 60.49173938866444
target distance 55.0
model initialize at round 1484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.13745487, 18.27535324]), 'previousTarget': array([79.94731634, 17.54928608]), 'currentState': array([61.23036065, 20.20086608,  1.39121141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5461256563619067
running average episode reward sum: 0.5742079618982707
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46402594,  15.64983676,   2.44608401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7985035308900235}
episode index:1485
target Thresh 60.507239897729534
target distance 12.0
model initialize at round 1485
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.34288191,  13.2641629 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.        ,   3.        ,   0.55107903], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.297516469054878}
done in step count: 13
reward sum = 0.6696140229989678
running average episode reward sum: 0.5742721651695364
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59885441,  15.18496513,   2.2133459 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6267684640884845}
episode index:1486
target Thresh 60.52272491403323
target distance 22.0
model initialize at round 1486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.53570303,  13.0593689 ]), 'previousTarget': array([110.21853056,  12.17458624]), 'currentState': array([94.08692444,  3.28490597,  6.14186246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5743138015655648
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22932191,  15.09039287,   1.28127202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24649423351771746}
episode index:1487
target Thresh 60.53819445306056
target distance 2.0
model initialize at round 1487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.64618332,  15.20731765,   3.36752272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3602182880290727}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5745799206505341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3411487 ,  14.05329812,   5.90028489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1533990970058534}
episode index:1488
target Thresh 60.55364853028105
target distance 45.0
model initialize at round 1488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.59343322, 17.29441633]), 'previousTarget': array([89.92145281, 17.22920419]), 'currentState': array([71.68883422, 19.24555412,  5.46820027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.49995898312853265
running average episode reward sum: 0.5745298058503179
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25267441,  14.01916142,   4.80940112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0128616289103827}
episode index:1489
target Thresh 60.56908716114878
target distance 10.0
model initialize at round 1489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.64469993,  15.05040386,   5.76754523]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.355452096788891}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5747760812486741
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54201727,  14.39118714,   0.36998782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7618407147117582}
episode index:1490
target Thresh 60.58451036110239
target distance 26.0
model initialize at round 1490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.60917865,  13.36140183]), 'previousTarget': array([108.64012894,  13.77694787]), 'currentState': array([89.23584738,  8.39410522,  5.86821008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.4705951827116873
running average episode reward sum: 0.5747062080772878
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19415941,  15.91719349,   5.17866677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2209107051100776}
episode index:1491
target Thresh 60.59991814556507
target distance 48.0
model initialize at round 1491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.54441414, 18.74069923]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([68.74139214, 21.54075626,  5.23265589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.43310075933160364
running average episode reward sum: 0.5746112982590936
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74479317,  14.38296679,   2.78619032]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9671850083616591}
episode index:1492
target Thresh 60.61531052994462
target distance 8.0
model initialize at round 1492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.66215907,  22.79150025,   5.14839799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.043689751956476}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5748382948761228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84959965,  14.87752763,   3.7697482 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8583816424999102}
episode index:1493
target Thresh 60.63068752963342
target distance 18.0
model initialize at round 1493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.77582686,  14.43048706]), 'previousTarget': array([114.48314552,  14.71285862]), 'currentState': array([95.64214152,  5.99428791,  3.51955056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.04754847864235112
running average episode reward sum: 0.5744853565787776
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15273038e+02, 1.44197127e+01, 6.71881407e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6413137067041398}
episode index:1494
target Thresh 60.64604916000847
target distance 50.0
model initialize at round 1494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.03585131, 10.98602085]), 'previousTarget': array([84.9007438 , 11.99007438]), 'currentState': array([65.21292285,  8.3305534 ,  5.84924507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = -0.12787459839656162
running average episode reward sum: 0.5740155505888276
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97127727,  14.39314751,   5.96339799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6075318388193138}
episode index:1495
target Thresh 60.66139543643139
target distance 6.0
model initialize at round 1495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.4921542 ,  16.22162739,   0.52918785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.670444006157135}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.574280445942712
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34477444,  14.68214641,   5.4326354 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7282523205683668}
episode index:1496
target Thresh 60.67672637424847
target distance 3.0
model initialize at round 1496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.79846483,  17.66490215,   5.36685938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.923249946191455}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5745385057717416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53560899,  15.32406232,   2.21731222]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6260138829776087}
episode index:1497
target Thresh 60.69204198879065
target distance 44.0
model initialize at round 1497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.29486087, 11.32893685]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([70.51207536,  8.38931144,  3.40824699]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.57459265845185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03808949,  14.15194209,   4.58180707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2823704775339086}
episode index:1498
target Thresh 60.707342295373536
target distance 28.0
model initialize at round 1498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.64547664,  14.70329168]), 'previousTarget': array([106.98725709,  14.71383061]), 'currentState': array([88.66724292, 13.77045884,  5.13636834]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5747387702765278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86517915,  14.66393985,   4.09948874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3620954092617198}
episode index:1499
target Thresh 60.72262730929746
target distance 5.0
model initialize at round 1499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.75428561,  13.33506381,   5.57586479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.503592641814809}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5750024770963433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04582941,  14.53671401,   5.35965395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0606957216085553}
episode index:1500
target Thresh 60.7378970458474
target distance 36.0
model initialize at round 1500
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.86280863, 14.33855369]), 'previousTarget': array([98.93091516, 13.6609096 ]), 'currentState': array([79.      , 12.      ,  3.195694], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = -0.010414036189231646
running average episode reward sum: 0.5746124594325954
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03246177,  15.35394465,   4.68086833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0302461085311028}
episode index:1501
target Thresh 60.75315152029312
target distance 43.0
model initialize at round 1501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.52207833, 16.83415844]), 'previousTarget': array([91.91402432, 17.14753262]), 'currentState': array([73.59460901, 18.53591229,  0.72678774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5747174467230227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97075782,  14.18527551,   0.59801286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2673384440861402}
episode index:1502
target Thresh 60.7683907478891
target distance 25.0
model initialize at round 1502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.21033859,  12.77056413]), 'previousTarget': array([108.56953382,  12.42781353]), 'currentState': array([89.20849814,  6.53116836,  1.12973261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5748030955908048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97489092,  15.40309494,   0.37820936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40387621083677133}
episode index:1503
target Thresh 60.78361474387455
target distance 27.0
model initialize at round 1503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.19466569,  17.1331199 ]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([86.75690232, 21.84197914,  4.87454987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5748981271313682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43174128,  15.60910851,   4.90730228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8330252990352557}
episode index:1504
target Thresh 60.79882352347347
target distance 20.0
model initialize at round 1504
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.49428159,  15.50755678]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([95.       , 10.       ,  0.5774427], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.29693325967455}
done in step count: 32
reward sum = 0.2993236554671293
running average episode reward sum: 0.5747150211701295
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01345496,  15.82718983,   5.71853505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.287444801863704}
episode index:1505
target Thresh 60.81401710189466
target distance 9.0
model initialize at round 1505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10173317,   7.45489417,   1.34432244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.545791647149029}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.574964871786816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59145534,  14.57098894,   0.57961905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7306640141873788}
episode index:1506
target Thresh 60.82919549433168
target distance 4.0
model initialize at round 1506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.51078938,  11.04580527,   0.6868717 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.68398540736055}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5752272036568977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59494294,  15.16259555,   2.14873788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4364728284796564}
episode index:1507
target Thresh 60.84435871596293
target distance 52.0
model initialize at round 1507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.4660311 , 10.23438614]), 'previousTarget': array([82.86817872, 11.29248216]), 'currentState': array([62.67720611,  7.33569589,  5.50993681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4364357101923879
running average episode reward sum: 0.5751351668575181
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46705789,  15.05933538,   5.1188295 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5362350004654844}
episode index:1508
target Thresh 60.85950678195164
target distance 26.0
model initialize at round 1508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.09980315,  12.69608123]), 'previousTarget': array([107.89972147,  12.54221128]), 'currentState': array([90.46975138,  5.42138735,  0.6349532 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5751391637079227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66186764,  14.03039716,   5.4379219 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0268705642465559}
episode index:1509
target Thresh 60.87463970744585
target distance 21.0
model initialize at round 1509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.87288284,  14.14084244]), 'previousTarget': array([112.05721038,  13.59867161]), 'currentState': array([94.32843805,  6.65061096,  0.36438203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.2624202314533866
running average episode reward sum: 0.5749320650772906
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79746707,  15.5505577 ,   2.42204724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5866288131268373}
episode index:1510
target Thresh 60.88975750757854
target distance 43.0
model initialize at round 1510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.39033709, 19.83743748]), 'previousTarget': array([91.66260099, 19.34184168]), 'currentState': array([71.79737363, 23.85188427,  1.17587203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5767079916266481
running average episode reward sum: 0.5749332404092227
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50960364,  15.36768121,   3.22138427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6283990306933006}
episode index:1511
target Thresh 60.90486019746746
target distance 34.0
model initialize at round 1511
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.67922247,  12.56766074]), 'previousTarget': array([100.69567118,  12.47570668]), 'currentState': array([81.       ,  9.       ,  0.9651792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.3041884865467659
running average episode reward sum: 0.5747541764185728
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56512012,  14.40547746,   0.39944511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7365986441974554}
episode index:1512
target Thresh 60.91994779221531
target distance 60.0
model initialize at round 1512
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.91963471, 19.20886825]), 'previousTarget': array([74.9007438 , 19.00992562]), 'currentState': array([55.      , 21.      ,  4.924528], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.3559092060986452
running average episode reward sum: 0.5746095333449972
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09454827,  14.5244181 ,   5.63770628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0227516749135976}
episode index:1513
target Thresh 60.935020306909706
target distance 37.0
model initialize at round 1513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.59843419,  9.64489958]), 'previousTarget': array([97.17072713,  9.69940536]), 'currentState': array([79.58614964,  3.43740121,  4.93233431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5746100255620592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3884163 ,  14.53778239,   6.03180878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7666027256588277}
episode index:1514
target Thresh 60.950077756623145
target distance 3.0
model initialize at round 1514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.73767846,  13.70195296,   0.9354769 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.168975003675856}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5748776757102031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88935962,  14.66749758,   4.52845347]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3504270954405435}
episode index:1515
target Thresh 60.96512015641309
target distance 49.0
model initialize at round 1515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.20723447, 15.29023631]), 'previousTarget': array([85.99583637, 14.40807829]), 'currentState': array([65.20818344, 15.48506384,  1.05063963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 52
reward sum = 0.41771265589462736
running average episode reward sum: 0.5747740048528049
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14451645e+02, 1.41784040e+01, 1.74681097e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9877819778303666}
episode index:1516
target Thresh 60.980147521321946
target distance 33.0
model initialize at round 1516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.94673657,  17.72676211]), 'previousTarget': array([101.77431008,  17.00389241]), 'currentState': array([81.3129077 , 21.53632427,  0.98134542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.4201411211350574
running average episode reward sum: 0.5746720715082316
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10933211,  14.50562496,   0.59279092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5063202412289928}
episode index:1517
target Thresh 60.99515986637707
target distance 46.0
model initialize at round 1517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.29437165, 19.21646675]), 'previousTarget': array([88.77237675, 18.99116006]), 'currentState': array([70.57943569, 22.58118069,  5.62577612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.1682946267975337
running average episode reward sum: 0.5744043656816765
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34508122,  14.61935219,   5.95210414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7575033769459136}
episode index:1518
target Thresh 61.01015720659081
target distance 13.0
model initialize at round 1518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.7782584 ,   3.77577622,   0.68472999]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.871694429908075}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5746215991967042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25021791,  15.14974927,   3.15295702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7645901059547591}
episode index:1519
target Thresh 61.025139556960504
target distance 9.0
model initialize at round 1519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.74075487,  10.9825255 ,   0.45570838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.296791023487588}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5748692100195353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06316931,  14.62776299,   0.76011684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3775589418957788}
episode index:1520
target Thresh 61.04010693246851
target distance 56.0
model initialize at round 1520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.80470593,  8.09660376]), 'previousTarget': array([78.55604828,  7.19058177]), 'currentState': array([58.14052311,  4.44695804,  1.09678411]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 70
reward sum = 0.4007096030661095
running average episode reward sum: 0.5747547066619065
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.8454838 ,  15.55097036,   3.82868344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0091636113045825}
episode index:1521
target Thresh 61.055059348082196
target distance 28.0
model initialize at round 1521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.01947479,  15.08741682]), 'previousTarget': array([106.98725709,  14.71383061]), 'currentState': array([88.02104285, 15.33785634,  6.19209022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5957127374909834
running average episode reward sum: 0.574768476721584
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25188425,  14.93186472,   3.60056637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.751212080323612}
episode index:1522
target Thresh 61.06999681875399
target distance 38.0
model initialize at round 1522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.94823229, 11.12309062]), 'previousTarget': array([96.5709957 , 11.12020962]), 'currentState': array([75.31189064,  7.3265018 ,  2.00318599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4182465380822748
running average episode reward sum: 0.5746657046016633
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20697943,  14.85556503,   5.57685799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8060664262063872}
episode index:1523
target Thresh 61.08491935942136
target distance 23.0
model initialize at round 1523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.70593847,  12.47604942]), 'previousTarget': array([109.41125677,  11.84114513]), 'currentState': array([93.46380183,  2.34151816,  1.2049492 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5748417396992894
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17976152,  14.8726518 ,   1.07417418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22029926432500258}
episode index:1524
target Thresh 61.09982698500684
target distance 38.0
model initialize at round 1524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.15069643, 18.69212453]), 'previousTarget': array([96.66906377, 18.37675141]), 'currentState': array([75.48795844, 22.34954642,  1.90440845]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.36295534305880434
running average episode reward sum: 0.5747027977998531
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14416804e+02, 1.41418780e+01, 7.72682319e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0375407926768863}
episode index:1525
target Thresh 61.114719710418065
target distance 9.0
model initialize at round 1525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.5462085 ,   9.26406088,   0.5919295 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.405317926541786}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5749248256174703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77492503,  15.30462973,   3.39281812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.378758516233942}
episode index:1526
target Thresh 61.129597550547764
target distance 8.0
model initialize at round 1526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.20663038,  20.93753003,   1.06605422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.79749319570591}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5751526054922643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04032183,  15.06485958,   2.13707871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.0763715619246659}
episode index:1527
target Thresh 61.14446052027378
target distance 25.0
model initialize at round 1527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.86676129,  16.21260763]), 'previousTarget': array([109.25928039,  16.60740149]), 'currentState': array([91.67562114, 21.84289534,  5.17969954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5752319645318961
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23140051,  14.58885401,   5.92645983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.871657158794395}
episode index:1528
target Thresh 61.15930863445907
target distance 28.0
model initialize at round 1528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.9100776 ,  11.67775517]), 'previousTarget': array([105.83483823,  11.72672794]), 'currentState': array([88.40935826,  4.08016449,  0.43175524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5753543391230113
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0011123 ,  14.49156793,   5.84293139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1208388818980866}
episode index:1529
target Thresh 61.17414190795176
target distance 45.0
model initialize at round 1529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.39692083, 10.05735677]), 'previousTarget': array([89.76232938, 11.07414013]), 'currentState': array([69.75949576,  6.26637579,  3.68299735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5704621178424103
running average episode reward sum: 0.5753511415927625
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40161856,  15.97268107,   4.76916908]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1420021056583853}
episode index:1530
target Thresh 61.18896035558512
target distance 25.0
model initialize at round 1530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.67560959,  15.94507819]), 'previousTarget': array([109.74881264,  15.84018998]), 'currentState': array([91.13677748, 20.21521164,  6.09191913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5755204182876399
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33738757,  14.53642274,   5.86907328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8086773840288193}
episode index:1531
target Thresh 61.20376399217759
target distance 25.0
model initialize at round 1531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.8982486 ,  15.90794449]), 'previousTarget': array([109.74881264,  15.84018998]), 'currentState': array([91.37092925, 20.23042049,  1.43507242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.2902499002341634
running average episode reward sum: 0.5753342103776833
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08948957,  14.33013092,   6.12721861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6758202178340271}
episode index:1532
target Thresh 61.218552832532836
target distance 31.0
model initialize at round 1532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.87332442,  15.18045958]), 'previousTarget': array([103.98960229,  14.64482588]), 'currentState': array([84.87649927, 15.53680738,  0.19675446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5754561989647475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7120387 ,  14.51446496,   1.47714836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8618256174756207}
episode index:1533
target Thresh 61.23332689143968
target distance 26.0
model initialize at round 1533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.41668709,  14.76164591]), 'previousTarget': array([108.94108971,  14.53392998]), 'currentState': array([90.44367731, 13.7229543 ,  5.77590233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6757541660644253
running average episode reward sum: 0.5755215822549038
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26556746,  14.64171005,   5.74260018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8171675736378974}
episode index:1534
target Thresh 61.248086183672186
target distance 50.0
model initialize at round 1534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.14746325,  8.13911649]), 'previousTarget': array([84.44774604,  7.66745905]), 'currentState': array([63.59587023,  3.92779721,  1.54768825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = -0.20584491714072312
running average episode reward sum: 0.575012548704809
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14085975,  14.56693972,   6.05424159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9621139064837554}
episode index:1535
target Thresh 61.26283072398964
target distance 7.0
model initialize at round 1535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42855491,   9.14322398,   6.22655529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.8724343056417965}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5752210940860291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26541988,  14.02150433,   0.54024332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0138547668701177}
episode index:1536
target Thresh 61.2775605271366
target distance 5.0
model initialize at round 1536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.5194241 ,  11.74503084,   2.03260273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.0924419360258035}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5754781389174629
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34112629,  15.92564974,   0.69668549]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.136196288734503}
episode index:1537
target Thresh 61.292275607842846
target distance 4.0
model initialize at round 1537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.63126027,  17.23394621,   3.72048855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.766139140076555}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5757039299158443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0350004 ,  15.89994403,   3.4661272 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3195163831618348}
episode index:1538
target Thresh 61.30697598082349
target distance 3.0
model initialize at round 1538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42548848,  19.59466857,   2.00921774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.614327656350387}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5759477805461134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20687049,  14.91107559,   5.60166238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7980989725029031}
episode index:1539
target Thresh 61.32166166077887
target distance 51.0
model initialize at round 1539
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.98840812, 12.68083845]), 'previousTarget': array([83.96548746, 13.17444044]), 'currentState': array([64.       , 12.       ,  2.9400547], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 42
reward sum = 0.4602450793064104
running average episode reward sum: 0.5758726489219318
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01405889,  14.46226114,   5.91536016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.123050729744886}
episode index:1540
target Thresh 61.33633266239471
target distance 15.0
model initialize at round 1540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.66917204,   5.07516269,   1.05499953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.61966816171325}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.576079959502942
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17934789,  15.04077793,   5.84337318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8216646084674286}
episode index:1541
target Thresh 61.350989000341976
target distance 44.0
model initialize at round 1541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.39113341, 17.68207428]), 'previousTarget': array([90.8721051 , 17.74180624]), 'currentState': array([72.5303944 , 20.03814008,  1.03491658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5761812717014837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05064373,  14.78023704,   5.57287739]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9744604069088775}
episode index:1542
target Thresh 61.365630689277026
target distance 27.0
model initialize at round 1542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.54509688,  12.63459844]), 'previousTarget': array([106.75497521,  11.94628712]), 'currentState': array([88.48170146,  6.58588331,  0.40297294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.570048463582285
running average episode reward sum: 0.5761772971012769
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43851704,  14.19014953,   5.90835743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9854546668741099}
episode index:1543
target Thresh 61.38025774384155
target distance 60.0
model initialize at round 1543
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.86013372, 18.63883741]), 'previousTarget': array([74.9007438 , 19.00992562]), 'currentState': array([55.       , 21.       ,  5.9613895], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29258717993646827
running average episode reward sum: 0.5759936247456001
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31197203,  15.62499313,   3.01005691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6985291384466273}
episode index:1544
target Thresh 61.39487017866259
target distance 58.0
model initialize at round 1544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.07912708, 17.81335033]), 'previousTarget': array([76.9732997 , 16.96689829]), 'currentState': array([58.13693901, 19.33293415,  1.47158748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3513962244312947
running average episode reward sum: 0.5758482542599598
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1223902 ,  14.3686552 ,   5.74279726]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0811083307484028}
episode index:1545
target Thresh 61.4094680083526
target distance 29.0
model initialize at round 1545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.0789826 ,  15.63558092]), 'previousTarget': array([105.98811999,  15.31075448]), 'currentState': array([87.14305763, 17.23523574,  6.09732217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5760101691173826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0810804 ,  15.68389226,   5.09261918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1454788796538098}
episode index:1546
target Thresh 61.4240512475094
target distance 43.0
model initialize at round 1546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.73998064, 10.34397623]), 'previousTarget': array([91.48016054,  9.53026989]), 'currentState': array([73.16363045,  6.24928295,  1.78946369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3498832803640186
running average episode reward sum: 0.5758639978900048
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49305583,  14.36357461,   6.27652079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8136520508154226}
episode index:1547
target Thresh 61.43861991071623
target distance 44.0
model initialize at round 1547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.11003329, 13.10931424]), 'previousTarget': array([90.95367385, 13.36047776]), 'currentState': array([69.16315143, 11.65263853,  2.68992186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 36
reward sum = 0.3337622898556132
running average episode reward sum: 0.5757076014377861
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97543928,  14.33572832,   5.93772135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6647255812874909}
episode index:1548
target Thresh 61.45317401254176
target distance 46.0
model initialize at round 1548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.31941389, 17.40733804]), 'previousTarget': array([88.95760212, 16.69841725]), 'currentState': array([70.41388038, 19.34891868,  1.13786018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5758182951065361
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58737316,  14.90426582,   6.14810692]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42358699522738635}
episode index:1549
target Thresh 61.46771356754009
target distance 20.0
model initialize at round 1549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.40285  ,  15.1492875]), 'currentState': array([96.32922823, 21.12805446,  1.60790568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.650719321191122}
done in step count: 14
reward sum = 0.7349347885759783
running average episode reward sum: 0.5759209509087745
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23428357,  14.58553789,   0.49884036]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8706896657355416}
episode index:1550
target Thresh 61.48223859025077
target distance 5.0
model initialize at round 1550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53827166,  10.2620391 ,   1.17872685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.768438931464198}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5761752243124438
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45121182,  15.24143733,   2.86634761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5995502051801743}
episode index:1551
target Thresh 61.49674909519884
target distance 58.0
model initialize at round 1551
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.61280476,  5.91636175]), 'previousTarget': array([76.51579154,  6.37422914]), 'currentState': array([57.       ,  2.       ,  1.1820507], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 82
reward sum = -0.04002773103620483
running average episode reward sum: 0.5757781863257502
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90153031,  15.11445945,   5.40634079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.15098756848207628}
episode index:1552
target Thresh 61.5112450968948
target distance 5.0
model initialize at round 1552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6995506 ,  18.23512188,   4.65486395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3098919329772536}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5760322241967574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58162968,  15.14671496,   4.13370061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44334975155149775}
episode index:1553
target Thresh 61.525726609834635
target distance 29.0
model initialize at round 1553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.65856954,  17.20894392]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([87.32499155, 22.32878394,  0.54110115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5761826074940704
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39052889,  14.87644642,   5.85272384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6218685680858004}
episode index:1554
target Thresh 61.54019364849989
target distance 46.0
model initialize at round 1554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.22533877, 15.20307859]), 'previousTarget': array([88.98112317, 16.13125551]), 'currentState': array([69.22595953, 15.36065372,  5.8529619 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5762130530165123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04326762,  14.45475466,   5.97533187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1011945031404795}
episode index:1555
target Thresh 61.55464622735757
target distance 60.0
model initialize at round 1555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.05870331, 20.32210318]), 'previousTarget': array([74.9007438 , 19.00992562]), 'currentState': array([55.23392409, 22.96371798,  1.71449828]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5001774247187486
running average episode reward sum: 0.5761641869314881
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56538558,  14.62054476,   0.29316385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.576954047082492}
episode index:1556
target Thresh 61.56908436086029
target distance 59.0
model initialize at round 1556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.22344832,  7.07917243]), 'previousTarget': array([75.53149897,  6.30355062]), 'currentState': array([5.76491028e+01, 2.97490646e+00, 2.27557421e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5761749783633893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30367619,  15.74209512,   5.91219247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0176305848441842}
episode index:1557
target Thresh 61.58350806344615
target distance 31.0
model initialize at round 1557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.18665273,  10.61230493]), 'previousTarget': array([102.84855491,  10.6881969 ]), 'currentState': array([82.12516474,  4.55758153,  3.04497188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.510221504709964
running average episode reward sum: 0.5761326462236888
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54618457,  14.62246993,   0.13280551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5903197423754118}
episode index:1558
target Thresh 61.59791734953889
target distance 41.0
model initialize at round 1558
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.97864933,  8.30958553]), 'previousTarget': array([93.06461275,  8.04487721]), 'currentState': array([74.       ,  2.       ,  5.2505503], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.05110924010259765
running average episode reward sum: 0.5757958768804424
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76002105,  15.59233189,   2.76967087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.963581370857775}
episode index:1559
target Thresh 61.612312233547776
target distance 46.0
model initialize at round 1559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.04351833, 17.37547274]), 'previousTarget': array([88.95760212, 16.69841725]), 'currentState': array([70.13350839, 19.27059909,  6.14911515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5758731956888842
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04570615,  15.60068756,   5.0141365 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1276091040264895}
episode index:1560
target Thresh 61.62669272986769
target distance 25.0
model initialize at round 1560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.80625841,  13.41657778]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([89.42943591,  8.46291757,  0.93266797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5759371648454321
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14221373e+02, 1.52677035e+01, 9.18076376e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8233617881574627}
episode index:1561
target Thresh 61.64105885287914
target distance 51.0
model initialize at round 1561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.34493029, 11.52666717]), 'previousTarget': array([83.9045705 , 11.95142848]), 'currentState': array([65.48071613,  9.20008261,  4.77787915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.19240881074672744
running average episode reward sum: 0.5756916281270591
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33824956,  15.79306728,   3.93671648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.032893681025088}
episode index:1562
target Thresh 61.655410616948245
target distance 43.0
model initialize at round 1562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.12252184, 17.48794428]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([7.32506071e+01, 1.97478125e+01, 6.86212142e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5757644096099395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6544236 ,  15.52675705,   6.2144829 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6299968542486554}
episode index:1563
target Thresh 61.669748036426775
target distance 6.0
model initialize at round 1563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.74265636,  15.5525379 ,   6.19644165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.286299300667988}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.576010465620419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.678996  ,  15.36696352,   5.76275009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48755080910659093}
episode index:1564
target Thresh 61.684071125652146
target distance 57.0
model initialize at round 1564
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.98763096, 15.29671571]), 'previousTarget': array([77.99692284, 15.64917679]), 'currentState': array([58.      , 16.      ,  1.361858], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3663822164757462
running average episode reward sum: 0.5758765178573872
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36672185,  15.49467847,   5.58051331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8035844684877639}
episode index:1565
target Thresh 61.698379898947444
target distance 25.0
model initialize at round 1565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.07992805,  14.64146312]), 'previousTarget': array([109.85753677,  14.38290441]), 'currentState': array([89.11650613, 13.43241987,  1.11346579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 38
reward sum = 0.5121675013596911
running average episode reward sum: 0.5758358352159455
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48152936,  15.88473881,   5.5745225 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0254630960211775}
episode index:1566
target Thresh 61.71267437062146
target distance 2.0
model initialize at round 1566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35479375,  13.95030276,   2.22347467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2321344885042684}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5761001390862608
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18299333,  15.31747672,   1.15746944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8765223115448425}
episode index:1567
target Thresh 61.72695455496864
target distance 9.0
model initialize at round 1567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.47300286,  16.9762758 ,   6.21335286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.782117443364828}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5763453532896496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07228153,  15.40398274,   5.76231043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.011861458877142}
episode index:1568
target Thresh 61.7412204662692
target distance 8.0
model initialize at round 1568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.19120707,   9.50219576,   0.39369714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.751314914660057}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5765720709407761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57493359,  14.60767204,   5.84595104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5784485116181732}
episode index:1569
target Thresh 61.75547211878903
target distance 45.0
model initialize at round 1569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.97328143,  9.54362015]), 'previousTarget': array([89.61161351,  9.9223227 ]), 'currentState': array([68.3688195 ,  5.5857075 ,  4.40031672]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.41088330531328804
running average episode reward sum: 0.5764665366951535
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00325803,  15.03884048,   4.52451393]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9974984387704825}
episode index:1570
target Thresh 61.7697095267798
target distance 7.0
model initialize at round 1570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.85705991,   9.70272301,   4.92052197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.714351075910177}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5766638112559561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07264416,  15.35223728,   1.22594891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9919979568990712}
episode index:1571
target Thresh 61.7839327044789
target distance 4.0
model initialize at round 1571
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.44033415,  13.78615014,   1.47846979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.832899703086635}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5769204500528671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14016599e+02, 1.52164859e+01, 1.06992345e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0069476937635087}
episode index:1572
target Thresh 61.79814166610953
target distance 2.0
model initialize at round 1572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.69066993,  12.9641349 ,   2.89171004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4205561220351046}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5771767625448868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10559808,  15.50272893,   6.20569176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0260073980089461}
episode index:1573
target Thresh 61.81233642588064
target distance 57.0
model initialize at round 1573
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.98310805, 11.82182286]), 'previousTarget': array([77.95093522, 12.40006563]), 'currentState': array([58.       , 11.       ,  0.9067551], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.19888232226610925
running average episode reward sum: 0.576936423002143
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10499278,  15.33042719,   6.00681682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9540545341594072}
episode index:1574
target Thresh 61.82651699798699
target distance 13.0
model initialize at round 1574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.41426485,   1.94189014,   0.96893471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.311658779049802}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.577150125112925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84149868,  14.15050223,   1.38201446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8641580465210927}
episode index:1575
target Thresh 61.84068339660916
target distance 35.0
model initialize at round 1575
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.78181769, 17.05386883]), 'previousTarget': array([99.79898987, 17.17157288]), 'currentState': array([80.       , 20.       ,  3.8560483], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.46380910786968027
running average episode reward sum: 0.5770782082238113
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63177203,  14.54557095,   5.41708142]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5848910962757358}
episode index:1576
target Thresh 61.854835635913545
target distance 20.0
model initialize at round 1576
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.47820408,  16.53867444]), 'previousTarget': array([114.77872706,  14.96680906]), 'currentState': array([95.       , 12.       ,  5.9030566], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.592466098832096
running average episode reward sum: 0.577087965922358
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53383455,  14.92364947,   6.09280464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4723765723599952}
episode index:1577
target Thresh 61.86897373005239
target distance 26.0
model initialize at round 1577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.58816611,  13.75699516]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([90.09571237,  9.27991495,  1.86960119]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7041395353470851
running average episode reward sum: 0.5771684802249085
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83284722,  15.85649774,   5.38576896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8726559609579367}
episode index:1578
target Thresh 61.88309769316378
target distance 49.0
model initialize at round 1578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.34023997, 16.60904273]), 'previousTarget': array([85.98336106, 16.18435261]), 'currentState': array([67.37399494, 17.77053297,  5.92991656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.637147039265866
running average episode reward sum: 0.5772064653794626
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41894376,  15.87712092,   5.48980841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0521252098842249}
episode index:1579
target Thresh 61.89720753937169
target distance 30.0
model initialize at round 1579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.28034928,  10.15899634]), 'previousTarget': array([103.77752632,  10.88509298]), 'currentState': array([83.58835764,  3.04496453,  4.71001434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6054180832922643
running average episode reward sum: 0.5772243208338378
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63207843,  15.12990866,   6.09102707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3901827053729727}
episode index:1580
target Thresh 61.91130328278597
target distance 45.0
model initialize at round 1580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.41071887, 18.51189398]), 'previousTarget': array([89.76232938, 18.92585987]), 'currentState': array([71.62874389, 21.45696883,  0.62996298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.31500365863016055
running average episode reward sum: 0.5770584633624882
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90102473,  15.58569067,   4.0333739 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.59399466792901}
episode index:1581
target Thresh 61.92538493750236
target distance 40.0
model initialize at round 1581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.77589378,  8.87600588]), 'previousTarget': array([94.28410786,  9.30312966]), 'currentState': array([73.49453119,  3.56290352,  4.43416286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5770610600443817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3308844 ,  15.03594299,   5.54438982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.670080284169859}
episode index:1582
target Thresh 61.939452517602504
target distance 37.0
model initialize at round 1582
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.97972794, 15.09973802]), 'previousTarget': array([97.9926994 , 15.45965677]), 'currentState': array([78.       , 16.       ,  1.8791323], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5726116020847181
running average episode reward sum: 0.577058249268665
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53664408,  15.89747451,   3.9069486 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0100293091867008}
episode index:1583
target Thresh 61.953506037154
target distance 26.0
model initialize at round 1583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.30967256,  16.34598474]), 'previousTarget': array([108.76743395,  15.95885631]), 'currentState': array([87.6091404 , 19.7940324 ,  1.74626386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5772051366543673
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43024898,  15.78989771,   4.2679353 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9739376893758951}
episode index:1584
target Thresh 61.967545510210364
target distance 45.0
model initialize at round 1584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.8916615 , 16.31719941]), 'previousTarget': array([89.99506356, 15.55566525]), 'currentState': array([70.9214465 , 17.40830552,  2.00140808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.39354967672130575
running average episode reward sum: 0.5770892657017281
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07972557,  15.82639637,   4.56001825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2368653876210312}
episode index:1585
target Thresh 61.98157095081106
target distance 2.0
model initialize at round 1585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14653033,  11.40085458,   4.99798441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.602126997077363}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.577325016511437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30947034,  14.354444  ,   2.53821677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7159011361936926}
episode index:1586
target Thresh 61.99558237298155
target distance 2.0
model initialize at round 1586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48650182,  16.32673082,   3.72680163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4131166598691571}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5775788129723624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10307063,  15.44033684,   3.89588982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45223897136850305}
episode index:1587
target Thresh 62.00957979073324
target distance 43.0
model initialize at round 1587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.13326155, 18.14529721]), 'previousTarget': array([91.86614761, 17.68998284]), 'currentState': array([73.33700298, 20.99277796,  1.64870947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.19010356521171723
running average episode reward sum: 0.5773348109271731
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35666129,  15.19720672,   4.53428131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.672885716930742}
episode index:1588
target Thresh 62.02356321806356
target distance 23.0
model initialize at round 1588
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.64401159,  15.5757613 ]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([90.8164627 , 18.19650639,  1.34074545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5775019653528853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83778707,  14.50948347,   5.97708345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5166425279892839}
episode index:1589
target Thresh 62.037532668955926
target distance 50.0
model initialize at round 1589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.00916697,  7.36950059]), 'previousTarget': array([84.53288933,  8.29723565]), 'currentState': array([65.62667811,  2.43805806,  6.1039443 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5775006149029633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33065753,  14.82209373,   5.44220106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6925821122170919}
episode index:1590
target Thresh 62.0514881573798
target distance 22.0
model initialize at round 1590
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.16447864,  14.36969031]), 'previousTarget': array([111.51093912,  13.57265691]), 'currentState': array([93.       ,  6.       ,  5.2937126], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.3538219476230418
running average episode reward sum: 0.5773600249172437
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74749691,  14.44780684,   5.71518947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6071862094004853}
episode index:1591
target Thresh 62.06542969729067
target distance 10.0
model initialize at round 1591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.53096618,   6.76794423,   1.97384804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.88317120947378}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5775597599859255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78860699,  14.08607679,   0.34913441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9380525806232846}
episode index:1592
target Thresh 62.07935730263008
target distance 22.0
model initialize at round 1592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.34171958,  14.61851011]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([91.44958632, 12.54413447,  3.81406212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5777106370591278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30127105,  15.74970943,   5.22117225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0248347985051012}
episode index:1593
target Thresh 62.09327098732562
target distance 30.0
model initialize at round 1593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.38553834,  10.53468076]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([83.53191108,  3.86079591,  1.63332772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.2376275778580229
running average episode reward sum: 0.5774972850771949
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0310693 ,  15.62776334,   5.33371558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1545187371854617}
episode index:1594
target Thresh 62.107170765291
target distance 7.0
model initialize at round 1594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.72372982,  21.91101568,   5.29602427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.276231416831128}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.577719584803107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74769034,  15.15430233,   4.524077  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29575221880541064}
episode index:1595
target Thresh 62.121056650425984
target distance 43.0
model initialize at round 1595
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([91.97233854, 15.94848047]), 'previousTarget': array([91.97840172, 16.07077201]), 'currentState': array([72.       , 17.       ,  5.4713154], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5098026283684022
running average episode reward sum: 0.5776770303191254
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25569266,  15.53231953,   5.88087244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9150724002256468}
episode index:1596
target Thresh 62.13492865661647
target distance 53.0
model initialize at round 1596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([80.59228264, 18.70996312]), 'previousTarget': array([81.91159005, 18.12154811]), 'currentState': array([60.70753788, 20.8540074 ,  1.57182729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.2678686182712199
running average episode reward sum: 0.5774830363228525
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38632517,  14.87969103,   6.09931349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6253567370005187}
episode index:1597
target Thresh 62.148786797734445
target distance 25.0
model initialize at round 1597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.47756124,  13.55645515]), 'previousTarget': array([109.25928039,  13.39259851]), 'currentState': array([88.95009554,  9.23464031,  1.26550674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.577579920135951
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67185579,  14.70674008,   0.13103573]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4400908969034043}
episode index:1598
target Thresh 62.16263108763807
target distance 26.0
model initialize at round 1598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.02769315,  11.82425677]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([89.82677373,  3.53411007,  6.23592329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5776630804280938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06159152,  14.55560402,   5.87264692]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0383151078104675}
episode index:1599
target Thresh 62.176461540171616
target distance 9.0
model initialize at round 1599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.09296643,  11.68040035,   0.50552869]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.505524120551303}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5778904660962018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24757342,  15.83459397,   5.96302098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8705398819446618}
episode index:1600
target Thresh 62.19027816916557
target distance 22.0
model initialize at round 1600
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.24384992,  13.38554663]), 'previousTarget': array([111.79586847,  13.83486126]), 'currentState': array([91.30519639,  6.95690738,  2.17794931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.5912309934667488
running average episode reward sum: 0.5778987987179198
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30392378,  15.48578693,   5.45380874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8488292202570659}
episode index:1601
target Thresh 62.20408098843653
target distance 31.0
model initialize at round 1601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.735101  ,  11.35774631]), 'previousTarget': array([103.36554647,  11.99756038]), 'currentState': array([83.56263791,  5.66418336,  3.38597727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5779771688154739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32930704,  15.5807091 ,   4.84774161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8871595664578215}
episode index:1602
target Thresh 62.21787001178733
target distance 5.0
model initialize at round 1602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.34094574,  14.44740079,   0.99784386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.68597053775287}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5782039329955022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9117174 ,  15.05949755,   4.50185809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10646020622538603}
episode index:1603
target Thresh 62.231645253007
target distance 46.0
model initialize at round 1603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.87035777, 13.69897027]), 'previousTarget': array([88.92481176, 12.73259233]), 'currentState': array([69.89710814, 12.66490096,  1.65838623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.578244087402665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17961067,  15.42605996,   4.51257695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9244272490975473}
episode index:1604
target Thresh 62.24540672587077
target distance 7.0
model initialize at round 1604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.34993001,  20.9949741 ,   0.37002676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.018741015467259}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5784823128996104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8289485 ,  15.90362567,   3.73963921]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9196727492235459}
episode index:1605
target Thresh 62.25915444414012
target distance 3.0
model initialize at round 1605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.1612972 ,  13.34667472,   3.56667328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.721156016261494}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5787323861792495
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.687366  ,  15.03622622,   1.59728909]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6883199571223484}
episode index:1606
target Thresh 62.27288842156277
target distance 47.0
model initialize at round 1606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.93381152, 11.26417932]), 'previousTarget': array([87.88777826, 12.11572109]), 'currentState': array([69.13610813,  8.42676142,  4.9094224 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4867341117682312
running average episode reward sum: 0.5786751377197529
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83138333,  14.65382416,   5.82521664]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3850575174644103}
episode index:1607
target Thresh 62.286608671872706
target distance 27.0
model initialize at round 1607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.36743003,  15.41831959]), 'previousTarget': array([107.98629668,  15.25976679]), 'currentState': array([89.42236023, 16.89959961,  5.837138  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5788239137147015
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65382945,  15.82533761,   4.2595492 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8949950961956638}
episode index:1608
target Thresh 62.30031520879017
target distance 60.0
model initialize at round 1608
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.87019705, 20.7250782 ]), 'previousTarget': array([74.82455801, 20.3567256 ]), 'currentState': array([55.       , 23.       ,  3.7534308], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.19356162673698823
running average episode reward sum: 0.5785844716469715
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34803905,  15.72095526,   4.9021138 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9720234390118162}
episode index:1609
target Thresh 62.314008046021684
target distance 47.0
model initialize at round 1609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.66500075, 11.65444309]), 'previousTarget': array([87.78180356, 10.94622606]), 'currentState': array([68.82446083,  9.13393208,  1.95212525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.30384660406586306
running average episode reward sum: 0.5784138270087222
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67068179,  15.31222162,   4.45034265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4537982171378953}
episode index:1610
target Thresh 62.32768719726012
target distance 8.0
model initialize at round 1610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.58514493,   6.43457659,   0.66736763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.221047306597914}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5786391940617279
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01168217,  14.98351956,   0.96200451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9884552259250516}
episode index:1611
target Thresh 62.3413526761846
target distance 17.0
model initialize at round 1611
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.08305134,  16.31082622]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 11.       ,  4.8634224], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.84679338346621}
done in step count: 28
reward sum = 0.5474983572036325
running average episode reward sum: 0.5786198759247192
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0618908 ,  15.42713926,   6.14781188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.030774864647852}
episode index:1612
target Thresh 62.35500449646063
target distance 6.0
model initialize at round 1612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.53107082,   9.92591276,   1.87111354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.3000508609427275}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5788627024120566
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55252704,  14.67531369,   2.81028283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.552859159828238}
episode index:1613
target Thresh 62.36864267174001
target distance 50.0
model initialize at round 1613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.89794465,  9.0200771 ]), 'previousTarget': array([84.68366648,  9.54305997]), 'currentState': array([66.30725272,  4.99455979,  0.29452973]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6302525484071753
running average episode reward sum: 0.5788945424653373
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67242316,  15.41134211,   5.5660354 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5258411488787416}
episode index:1614
target Thresh 62.38226721566094
target distance 9.0
model initialize at round 1614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89541641,   7.43682775,   1.45727372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.563895304854861}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5791249421603433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61836399,  15.5019927 ,   0.2146861 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.63058917935038}
episode index:1615
target Thresh 62.39587814184794
target distance 4.0
model initialize at round 1615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.34365092,  18.70184599,   4.32969213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.938154587413211}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.579367005314947
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64814087,  15.21401474,   3.72226423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41183389383191965}
episode index:1616
target Thresh 62.409475463911946
target distance 45.0
model initialize at round 1616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.71034908, 15.79798282]), 'previousTarget': array([89.95570316, 16.66961979]), 'currentState': array([70.72113341, 16.45468435,  6.12099671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5794308195324458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58272273,  15.78282081,   4.70001937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8870900414578589}
episode index:1617
target Thresh 62.423059195450286
target distance 8.0
model initialize at round 1617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.11718973,  17.32914897,   0.46922701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.266224064954221}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5796604605895332
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53537893,  15.9582615 ,   5.89702478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0649590833973195}
episode index:1618
target Thresh 62.436629350046694
target distance 8.0
model initialize at round 1618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.06464965,   8.48406792,   1.32729447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.797275330324949}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5798898179640302
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9797553 ,  15.83977643,   2.6409471 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8400204168422142}
episode index:1619
target Thresh 62.45018594127132
target distance 28.0
model initialize at round 1619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.55676052,  16.89276969]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([88.17365153, 21.82177422,  0.2226569 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.7383238442884296
running average episode reward sum: 0.5799876167457119
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15150426,  14.83071022,   4.53346342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22718400248479365}
episode index:1620
target Thresh 62.463728982680756
target distance 14.0
model initialize at round 1620
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.3912391 ,  14.35512627]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   7.       ,   1.2856826], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.40974284742019}
done in step count: 10
reward sum = 0.7650820750088044
running average episode reward sum: 0.5801018020993596
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47078235,  15.22427991,   1.30874364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5747806547996439}
episode index:1621
target Thresh 62.47725848781805
target distance 60.0
model initialize at round 1621
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.91376079, 12.85529819]), 'previousTarget': array([74.95570316, 12.33038021]), 'currentState': array([55.      , 11.      ,  5.485826], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.24145468335058873
running average episode reward sum: 0.579893018425655
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86505658,  15.23627234,   4.43095145]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2720925288650672}
episode index:1622
target Thresh 62.490774470212706
target distance 12.0
model initialize at round 1622
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.6108008 ,  14.85612735]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  17.       ,   3.9960349], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.778609810716384}
done in step count: 13
reward sum = 0.6709793022989679
running average episode reward sum: 0.5799491405968646
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31049949,  15.49184412,   6.17914759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8469483993350755}
episode index:1623
target Thresh 62.504276943380695
target distance 4.0
model initialize at round 1623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.74585913,  17.27007052,   4.53222752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.3800320496569265}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5800758210157134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97876236,  14.78028908,   3.59845427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0031194548273046}
episode index:1624
target Thresh 62.51776592082451
target distance 12.0
model initialize at round 1624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.67915243,   3.11329251,   1.07736796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.023257416744672}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5802810157396937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00762532,  14.72692687,   1.42908772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27317957383174946}
episode index:1625
target Thresh 62.531241416033126
target distance 1.0
model initialize at round 1625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.00864258,  16.00827727,   4.66470432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4261777265043127}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5805329954348107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10581707,  15.02883406,   3.25656092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10967522614052982}
episode index:1626
target Thresh 62.54470344248203
target distance 7.0
model initialize at round 1626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.00537627,  20.58999426,   6.25221419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.935196677057547}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5807725565931176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41424785,  15.97950009,   5.18445718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1412826129843272}
episode index:1627
target Thresh 62.55815201363326
target distance 49.0
model initialize at round 1627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.57421799, 13.36472155]), 'previousTarget': array([85.93369232, 12.62724019]), 'currentState': array([67.60967559, 12.17432391,  0.47877634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5488919258698032
running average episode reward sum: 0.580752973896113
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61753511,  15.93792074,   3.97604567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0129040934937683}
episode index:1628
target Thresh 62.57158714293538
target distance 25.0
model initialize at round 1628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.52209299,  12.4149098 ]), 'previousTarget': array([108.03046115,  11.65462135]), 'currentState': array([91.43495428,  3.87937153,  1.55979412]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5366334300936386
running average episode reward sum: 0.5807258900754854
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36305765,  15.59279623,   6.17582154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8701166209932869}
episode index:1629
target Thresh 62.58500884382353
target distance 43.0
model initialize at round 1629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.11133846, 11.2542765 ]), 'previousTarget': array([91.7401454 , 11.21351204]), 'currentState': array([70.33406208,  8.27781235,  3.90194654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.48215661769164303
running average episode reward sum: 0.5806654181292377
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88967142,  15.15794805,   4.20248372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.903583323861713}
episode index:1630
target Thresh 62.5984171297194
target distance 51.0
model initialize at round 1630
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.96018126, 11.26141355]), 'previousTarget': array([83.9045705 , 11.95142848]), 'currentState': array([64.       , 10.       ,  2.1482527], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = -0.16323720052718926
running average episode reward sum: 0.5802093159718763
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06007709,  15.93029516,   4.80647023]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.32246140663085}
episode index:1631
target Thresh 62.61181201403129
target distance 48.0
model initialize at round 1631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.53878213, 18.79476299]), 'previousTarget': array([86.72787848, 19.71202025]), 'currentState': array([66.71421809, 21.43799218,  5.37670076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.579937924286608
running average episode reward sum: 0.5802091496779515
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41364636,  15.94432497,   4.1838187 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1115575744689228}
episode index:1632
target Thresh 62.62519351015407
target distance 50.0
model initialize at round 1632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.53201669, 11.54121395]), 'previousTarget': array([84.85753677, 11.38290441]), 'currentState': array([66.67801844,  9.12900471,  1.08672827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.586497460816985
running average episode reward sum: 0.5802130004502349
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14988362e+02, 1.48187299e+01, 4.90753238e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18164327645248984}
episode index:1633
target Thresh 62.638561631469244
target distance 21.0
model initialize at round 1633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.9675954 ,  15.81560632]), 'previousTarget': array([113.45612429,  15.36758945]), 'currentState': array([92.65398527, 21.01026355,  1.48773384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.4410162165117189
running average episode reward sum: 0.5801278126999665
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47312199,  15.3617565 ,   5.52279257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6391151685461741}
episode index:1634
target Thresh 62.65191639134494
target distance 13.0
model initialize at round 1634
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.33779148,  14.73646459]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  14.       ,   1.1472366], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.361685423855677}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.5803002515594203
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41676984,  15.90833093,   0.15062817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0794547213058818}
episode index:1635
target Thresh 62.665257803135916
target distance 37.0
model initialize at round 1635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.25351391, 10.43376305]), 'previousTarget': array([97.54828331, 11.22665585]), 'currentState': array([76.82165799,  5.70058489,  3.0959146 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5802937117983676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06810375,  15.76891368,   4.01128632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.208163429764664}
episode index:1636
target Thresh 62.67858588018359
target distance 48.0
model initialize at round 1636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.31480875, 17.87820667]), 'previousTarget': array([86.93091516, 17.3390904 ]), 'currentState': array([68.43013658, 20.02292415,  1.49731391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.580373283890899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6846338 ,  15.05885515,   4.51813847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3208111081990101}
episode index:1637
target Thresh 62.69190063581602
target distance 25.0
model initialize at round 1637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.85739051,  17.13002956]), 'previousTarget': array([109.25928039,  16.60740149]), 'currentState': array([89.96122982, 23.68251851,  0.58383512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6874775665036326
running average episode reward sum: 0.5804386711208214
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63826587,  15.94845061,   4.04470136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.01509120301567}
episode index:1638
target Thresh 62.705202083347984
target distance 24.0
model initialize at round 1638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.450265  ,  12.72213755]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([92.56643261,  3.76844897,  0.79692345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5805051814409851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26065414,  15.79667126,   3.55850551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0868842578908549}
episode index:1639
target Thresh 62.718490236080925
target distance 27.0
model initialize at round 1639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.82955652,  16.77081557]), 'previousTarget': array([107.35993796,  16.98075682]), 'currentState': array([89.60553601, 22.28779257,  1.09505623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.58061605798544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37223389,  15.0240902 ,   0.55857861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6282281673855433}
episode index:1640
target Thresh 62.73176510730299
target distance 6.0
model initialize at round 1640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.98448774,  10.52247122,   2.55272496]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.591244836234791}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5808476118867286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63773285,  15.30679582,   2.35237217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47472219198283905}
episode index:1641
target Thresh 62.74502671028907
target distance 41.0
model initialize at round 1641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.48802388, 13.57933834]), 'previousTarget': array([93.97624702, 13.97445107]), 'currentState': array([72.52773017, 12.31970551,  2.56678665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.24624539276697238
running average episode reward sum: 0.5806438346521855
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57765096,  14.57700227,   5.80463227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5977506083145532}
episode index:1642
target Thresh 62.75827505830074
target distance 13.0
model initialize at round 1642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.22688847,  21.66760598,   0.42557025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.408655302362046}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5808464356338237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33020258,  14.82720652,   5.69965636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6917269502612888}
episode index:1643
target Thresh 62.771510164586374
target distance 24.0
model initialize at round 1643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.69020052,  12.39529508]), 'previousTarget': array([108.58583933,  11.52566297]), 'currentState': array([91.73427543,  3.58707423,  2.03183225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5809568348301213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31209763,  14.94361493,   4.52796558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6902093517682544}
episode index:1644
target Thresh 62.784732042381066
target distance 26.0
model initialize at round 1644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.15109686,  14.39405523]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([89.25756958, 12.33309125,  5.86944044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5810103393430329
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9009106 ,  15.33917046,   4.04589184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35334871422871633}
episode index:1645
target Thresh 62.797940704906694
target distance 21.0
model initialize at round 1645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.30014059,  14.37209987]), 'previousTarget': array([113.64677133,  14.74224216]), 'currentState': array([92.8200212 ,  9.84165302,  4.9277401 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6484082779398093
running average episode reward sum: 0.5810512858427879
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23378547,  15.63234229,   5.56872613]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9934492876458069}
episode index:1646
target Thresh 62.81113616537194
target distance 29.0
model initialize at round 1646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.44471903,  11.11403221]), 'previousTarget': array([104.69995053,  11.09308468]), 'currentState': array([84.48795579,  4.73899281,  1.67699552]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.27704383196700255
running average episode reward sum: 0.5808667032964152
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75020279,  15.62818008,   3.51099229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6760243015410915}
episode index:1647
target Thresh 62.82431843697225
target distance 11.0
model initialize at round 1647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.04489388,   2.60734105,   5.11159194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.54062713748828}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5810308969055161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07360035,  15.34201734,   5.66308802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3498469259487767}
episode index:1648
target Thresh 62.83748753288988
target distance 10.0
model initialize at round 1648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.61495117,  20.41073284,   6.05565155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.833059192515927}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5812381217675674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4007678 ,  15.79985472,   4.80766035]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9994232362309887}
episode index:1649
target Thresh 62.85064346629396
target distance 59.0
model initialize at round 1649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.58213767,  7.43753136]), 'previousTarget': array([75.71877115,  8.3421646 ]), 'currentState': array([55.94035746,  3.66918022,  3.68933332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.2460704798774181
running average episode reward sum: 0.5810349898633915
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34915039,  14.80915479,   5.07592378]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3979043718586893}
episode index:1650
target Thresh 62.86378625034042
target distance 14.0
model initialize at round 1650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.5437165 ,  18.65112032,   1.40914648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.980357396771856}
done in step count: 14
reward sum = 0.7994458127689782
running average episode reward sum: 0.5811672798833222
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96585777,  15.72334228,   3.90101697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7241476021562893}
episode index:1651
target Thresh 62.87691589817203
target distance 46.0
model initialize at round 1651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.02781316, 15.08166682]), 'previousTarget': array([88.99527578, 14.43467991]), 'currentState': array([70.02792011, 15.1470727 ,  1.85012644]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5811120274578505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2032092 ,  15.89053944,   0.21944097]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9134300623904785}
episode index:1652
target Thresh 62.89003242291843
target distance 5.0
model initialize at round 1652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.33954427,  16.37191462,   5.20599675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.824337624297314}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5813416003450509
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68941627,  15.71572256,   0.20984095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7802057615810951}
episode index:1653
target Thresh 62.903135837696176
target distance 24.0
model initialize at round 1653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.29546559,  14.93378175]), 'previousTarget': array([110.93091516,  14.6609096 ]), 'currentState': array([90.29744646, 14.65230146,  2.7766788 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6592132918856305
running average episode reward sum: 0.5813886811742773
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70988463,  15.07614429,   4.0614923 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29994145864575283}
episode index:1654
target Thresh 62.91622615560867
target distance 17.0
model initialize at round 1654
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.11166431,  13.73643736]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 15.       ,  1.3023304], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.155687112224104}
done in step count: 25
reward sum = 0.6385213593991467
running average episode reward sum: 0.5814232024300023
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41714424,  15.80858973,   4.43282946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9098498064247856}
episode index:1655
target Thresh 62.92930338974622
target distance 50.0
model initialize at round 1655
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.00583066, 17.54204927]), 'previousTarget': array([84.96409691, 16.80215419]), 'currentState': array([66.08225851, 19.28883847,  1.88230294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.45223804932959843
running average episode reward sum: 0.5813451920718499
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18958019,  14.60766371,   4.94407166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4357389319209091}
episode index:1656
target Thresh 62.942367553186074
target distance 12.0
model initialize at round 1656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.89343277,  22.90946436,   0.52201527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.833640465239345}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5815401449281787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62924285,  14.24816991,   3.84387374]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9804055557464132}
episode index:1657
target Thresh 62.95541865899239
target distance 15.0
model initialize at round 1657
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.28409232,  14.21649958]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  14.       ,   3.4478781], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.285531454030295}
done in step count: 16
reward sum = 0.5756298410948755
running average episode reward sum: 0.5815365802093407
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86277527,  14.36310412,   3.89568994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.072388698721064}
episode index:1658
target Thresh 62.96845672021628
target distance 48.0
model initialize at round 1658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.33732328, 13.16316719]), 'previousTarget': array([86.93091516, 12.6609096 ]), 'currentState': array([68.38461545, 11.78859446,  1.52772158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5040163063386074
running average episode reward sum: 0.5814898531003168
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91267848,  15.16006351,   4.65146372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18233314149811294}
episode index:1659
target Thresh 62.981481749895806
target distance 27.0
model initialize at round 1659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.51871723,  14.1685747 ]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([89.74490323, 11.16919481,  0.98288065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.5894065658334623
running average episode reward sum: 0.5814946222043729
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92532502,  15.78472106,   4.3040811 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7882661341717031}
episode index:1660
target Thresh 62.99449376105599
target distance 10.0
model initialize at round 1660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.76717998,  18.16517313,   5.531672  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.820297410299895}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5817113503965442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92489746,  15.92775394,   5.31076006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3100239213209945}
episode index:1661
target Thresh 63.00749276670886
target distance 40.0
model initialize at round 1661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.70133057,  7.56893422]), 'previousTarget': array([94.02068137,  8.18172145]), 'currentState': array([73.72719904,  1.24577578,  2.84477663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 59
reward sum = 0.3566930513290492
running average episode reward sum: 0.5815759603249031
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73772013,  15.18646426,   4.56391139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32180685305195594}
episode index:1662
target Thresh 63.02047877985341
target distance 60.0
model initialize at round 1662
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.9745674 , 14.00829412]), 'previousTarget': array([74.98889814, 13.6662966 ]), 'currentState': array([55.        , 13.        ,  0.29320616], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3479566693323602
running average episode reward sum: 0.5814354796929172
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25725388,  14.73578571,   5.01187718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3687665251144222}
episode index:1663
target Thresh 63.033451813475665
target distance 11.0
model initialize at round 1663
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.17752065,  14.14795912]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.      ,  23.      ,   2.119996], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.258176444098993}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.581598526096003
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5238297 ,  15.92070565,   4.20525261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0592905366050263}
episode index:1664
target Thresh 63.046411880548646
target distance 3.0
model initialize at round 1664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29309925,  11.22924025,   2.93582714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.836448590526762}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5817978766794191
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73329783,  15.85142018,   5.08310294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8922143107280642}
episode index:1665
target Thresh 63.05935899403243
target distance 9.0
model initialize at round 1665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.7373581 ,  11.47737779,   6.14997339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.98221122103135}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5819969879464084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19141256,  15.88353435,   0.11671806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.904030817101255}
episode index:1666
target Thresh 63.072293166874125
target distance 45.0
model initialize at round 1666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.33531364, 17.52742338]), 'previousTarget': array([89.95570316, 16.66961979]), 'currentState': array([70.43949762, 19.56617423,  0.31389046]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5057682404974045
running average episode reward sum: 0.5819512598435597
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61069991,  15.26234429,   4.46520237]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46944550519384065}
episode index:1667
target Thresh 63.085214412007915
target distance 49.0
model initialize at round 1667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.68087945, 19.35883083]), 'previousTarget': array([85.85172777, 18.56917619]), 'currentState': array([66.91365987, 22.40136761,  6.26621765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5176024448965927
running average episode reward sum: 0.5819126814173325
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48249534,  15.77983258,   3.71798784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9359219668037978}
episode index:1668
target Thresh 63.098122742355045
target distance 52.0
model initialize at round 1668
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.81881466, 11.68599805]), 'previousTarget': array([82.86817872, 11.29248216]), 'currentState': array([63.       ,  9.       ,  2.9508908], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.06504700907559541
running average episode reward sum: 0.581602995574108
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94906886,  15.33935605,   4.29182982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.343156684409441}
episode index:1669
target Thresh 63.11101817082383
target distance 41.0
model initialize at round 1669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.51127676, 16.88447847]), 'previousTarget': array([93.97624702, 16.02554893]), 'currentState': array([73.58774215, 18.63169591,  2.82827324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5816318702062212
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23529546,  15.59846433,   4.04437301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9710471554444577}
episode index:1670
target Thresh 63.12390071030972
target distance 63.0
model initialize at round 1670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.8803969 , 10.01252015]), 'previousTarget': array([71.90990945, 10.89618185]), 'currentState': array([53.01915418,  7.66070435,  4.35286945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.4429780685645633
running average episode reward sum: 0.5815488936642453
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3951089 ,  15.02096618,   4.39953826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.605254350776443}
episode index:1671
target Thresh 63.13677037369525
target distance 41.0
model initialize at round 1671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.02676966,  8.62405126]), 'previousTarget': array([93.31685674,  9.18257132]), 'currentState': array([74.8914611 ,  2.80683833,  4.34405841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5815521758940738
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3927803 ,  14.83587558,   4.6733089 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6290092151981693}
episode index:1672
target Thresh 63.14962717385007
target distance 44.0
model initialize at round 1672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.41122515, 17.62047411]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([72.54445996, 19.92517385,  6.02904374]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.270777484615489
running average episode reward sum: 0.5813664169632438
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00344034,  15.2209683 ,   4.28924964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0207635057201023}
episode index:1673
target Thresh 63.16247112363101
target distance 9.0
model initialize at round 1673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.07011022e+02, 1.67517867e+01, 3.31007799e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.17878472287498}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5815872196113542
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59445798,  15.88491223,   5.77776659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9734135746063832}
episode index:1674
target Thresh 63.175302235881986
target distance 46.0
model initialize at round 1674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.7968131 , 14.17651939]), 'previousTarget': array([89., 15.]), 'currentState': array([69.80748025, 13.52339453,  6.22280979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5816557724462427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92189374,  14.7766148 ,   5.35456574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.236646435047692}
episode index:1675
target Thresh 63.18812052343414
target distance 26.0
model initialize at round 1675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.61996652,  13.40601925]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([90.44390403,  7.72459095,  5.74318141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5817545292015442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76995089,  15.34151892,   1.08860488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41177392635875204}
episode index:1676
target Thresh 63.20092599910575
target distance 28.0
model initialize at round 1676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.97893408,  13.94140566]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([88.20245828, 10.95962689,  0.29674452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5818714444252756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86808767,  15.1489913 ,   5.84069205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19899564770901873}
episode index:1677
target Thresh 63.21371867570229
target distance 11.0
model initialize at round 1677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.46042419,   5.59126614,   2.02034211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.05692648605218}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5820801416263971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07772677,  14.38336426,   6.1722762 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.109426677277872}
episode index:1678
target Thresh 63.22649856601643
target distance 46.0
model initialize at round 1678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.28958636,  9.10561983]), 'previousTarget': array([88.35234545,  8.04843794]), 'currentState': array([69.79533799,  4.63636031,  1.72367501]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.28240513053975025
running average episode reward sum: 0.5819016574029983
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20202686,  15.9785229 ,   4.01090691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.999160606949083}
episode index:1679
target Thresh 63.23926568282809
target distance 17.0
model initialize at round 1679
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.61650107,  14.81027456]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 16.       ,  3.9333234], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.654478246553683}
done in step count: 18
reward sum = 0.6952137614500875
running average episode reward sum: 0.5819691050839787
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51105222,  15.45220085,   0.24839556]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.665999655822748}
episode index:1680
target Thresh 63.25202003890436
target distance 6.0
model initialize at round 1680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.24482086,  10.19378321,   2.25427574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.304615022540237}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5821943441707818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14526364,  14.93824729,   2.78671616]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8569641967810477}
episode index:1681
target Thresh 63.26476164699961
target distance 11.0
model initialize at round 1681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.54268558,   5.23595151,   0.23776722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.593286537950526}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5823443557149431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96728068,  15.88550672,   5.52810752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8861110002339937}
episode index:1682
target Thresh 63.27749051985544
target distance 5.0
model initialize at round 1682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.00483847,  19.25030227,   6.16700298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.833213954493372}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5825748694667465
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78051754,  15.83321338,   4.75107074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8616362824768095}
episode index:1683
target Thresh 63.290206670200725
target distance 56.0
model initialize at round 1683
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.98540676, 15.23611748]), 'previousTarget': array([78.99681199, 15.64291407]), 'currentState': array([59.       , 16.       ,  1.6956611], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.44256090766840217
running average episode reward sum: 0.5824917257839684
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20677336,  15.13870488,   4.26068009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8052624034708762}
episode index:1684
target Thresh 63.30291011075163
target distance 50.0
model initialize at round 1684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.0582181 ,  8.50125001]), 'previousTarget': array([84.53288933,  8.29723565]), 'currentState': array([66.54412585,  4.11944632,  5.48580194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5825198159355525
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95254421,  15.83851051,   5.15136734]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2690312636704928}
episode index:1685
target Thresh 63.31560085421159
target distance 23.0
model initialize at round 1685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.77151143,  14.60856464]), 'previousTarget': array([111.98112317,  14.86874449]), 'currentState': array([90.85665846, 12.76502704,  2.95549965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5826174744636641
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44613058,  15.33724026,   5.88560778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5592526143227141}
episode index:1686
target Thresh 63.32827891327134
target distance 8.0
model initialize at round 1686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.74519321,  23.79880783,   3.21084058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.970212854100213}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5828301968554467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26398122,  14.43333128,   4.83275479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6251396084741657}
episode index:1687
target Thresh 63.34094430060895
target distance 29.0
model initialize at round 1687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.24022224,  17.02476554]), 'previousTarget': array([105.70920231,  16.60186167]), 'currentState': array([86.75399741, 21.5288794 ,  0.13960075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.5602740325424751
running average episode reward sum: 0.5828168341988631
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81903823,  15.71790464,   3.99153853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7403608851807797}
episode index:1688
target Thresh 63.35359702888981
target distance 44.0
model initialize at round 1688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.9211735 , 14.66735636]), 'previousTarget': array([90.9793708 , 13.90815322]), 'currentState': array([70.92308171, 14.39108716,  0.6598537 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5829010044189691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95409725,  15.32475754,   4.42558776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3279855478733848}
episode index:1689
target Thresh 63.36623711076664
target distance 10.0
model initialize at round 1689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.90854555,   6.82342451,   1.56419535]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.741480270227827}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5830912299045252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15671304,  15.83641882,   0.36344488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8509732239112899}
episode index:1690
target Thresh 63.378864558879535
target distance 61.0
model initialize at round 1690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.44274757, 17.21847211]), 'previousTarget': array([73.95713898, 17.69133515]), 'currentState': array([52.46986676, 18.2596408 ,  2.61084473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5831188658603494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25278933,  15.81290827,   4.94166938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8513062298809404}
episode index:1691
target Thresh 63.39147938585593
target distance 45.0
model initialize at round 1691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.12093874, 18.76835519]), 'previousTarget': array([89.82455801, 18.3567256 ]), 'currentState': array([71.36542168, 21.88597379,  1.52351755]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5832114081224818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65017076,  14.48287794,   3.98178016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8307449901525211}
episode index:1692
target Thresh 63.404081604310655
target distance 12.0
model initialize at round 1692
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.5647195 ,  13.35510065]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.        ,   3.        ,   0.35297424], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.523235705023659}
done in step count: 12
reward sum = 0.6784778717161293
running average episode reward sum: 0.5832676789220056
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42986403,  15.69779136,   5.44658813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.819570534964199}
episode index:1693
target Thresh 63.416671226845956
target distance 26.0
model initialize at round 1693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.42165344,  12.30095096]), 'previousTarget': array([107.89972147,  12.54221128]), 'currentState': array([87.34368671,  6.29835275,  1.95337319]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5833965826472813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55604859,  15.97516267,   5.67609373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1225561319301094}
episode index:1694
target Thresh 63.42924826605142
target distance 24.0
model initialize at round 1694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.49226855,  12.27933083]), 'previousTarget': array([108.58583933,  11.52566297]), 'currentState': array([91.56070238,  3.42162602,  2.20511888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5834758357150991
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.397899  ,  15.36787556,   5.1579217 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.541900394272137}
episode index:1695
target Thresh 63.4418127345041
target distance 45.0
model initialize at round 1695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.75041751, 13.16287758]), 'previousTarget': array([89.87767469, 12.20863052]), 'currentState': array([69.80314628, 11.71154351,  0.7440331 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5917130132502888
running average episode reward sum: 0.583480692541476
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44478556,  14.3995424 ,   4.2334801 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8178095154090709}
episode index:1696
target Thresh 63.45436464476847
target distance 6.0
model initialize at round 1696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.46769828,  13.40342774,   5.40057588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.724582416190716}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5836188341119274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40276108,  14.787738  ,   5.28593382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6338371170642993}
episode index:1697
target Thresh 63.46690400939644
target distance 10.0
model initialize at round 1697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.78060356,  14.47240885,   6.13904214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.234480126953041}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5838240440729374
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98605677,  15.72557194,   5.54167297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7257059003331949}
episode index:1698
target Thresh 63.47943084092737
target distance 29.0
model initialize at round 1698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.8876224 ,  12.64597774]), 'previousTarget': array([105.27985236,  12.31857996]), 'currentState': array([87.67994733,  7.07235965,  0.98287803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6366187357648234
running average episode reward sum: 0.5838551180527444
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68234943,  15.45817016,   3.71206429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5575139276941079}
episode index:1699
target Thresh 63.49194515188811
target distance 61.0
model initialize at round 1699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.24171417, 20.29947257]), 'previousTarget': array([73.86960143, 19.7198818 ]), 'currentState': array([55.41704956, 22.94194733,  1.49988097]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3249411461113514
running average episode reward sum: 0.5837028157163082
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60650318,  14.99299745,   4.02280268]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.39355912652873903}
episode index:1700
target Thresh 63.50444695479295
target distance 39.0
model initialize at round 1700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.4233347 , 16.35902495]), 'previousTarget': array([95.97375327, 15.97570496]), 'currentState': array([74.46681443, 17.67709124,  3.72227228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.172312861026444
running average episode reward sum: 0.5834609638910937
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33323478,  15.20575214,   6.05451011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6977892298937709}
episode index:1701
target Thresh 63.51693626214371
target distance 35.0
model initialize at round 1701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.55450736,  18.40157735]), 'previousTarget': array([99.61161351, 18.0776773 ]), 'currentState': array([81.08695687, 22.98573244,  6.009785  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6761892283866643
running average episode reward sum: 0.5835154458326305
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76499564,  14.57097392,   6.19337628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48917320686965254}
episode index:1702
target Thresh 63.529413086429685
target distance 34.0
model initialize at round 1702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.0588194 ,  11.72007244]), 'previousTarget': array([100.46834337,  11.58078667]), 'currentState': array([82.67180398,  6.80645534,  0.89474314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5835942568054817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47328474,  15.73314185,   3.64385618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9027324801674279}
episode index:1703
target Thresh 63.54187744012771
target distance 50.0
model initialize at round 1703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.36022892, 15.3843027 ]), 'previousTarget': array([84.9960012 , 14.39992002]), 'currentState': array([65.36190982, 15.64359647,  0.34406114]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5836140313853742
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21762061,  15.71935867,   4.24776405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0628237892309889}
episode index:1704
target Thresh 63.554329335702135
target distance 2.0
model initialize at round 1704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.09729467,  17.85212385,   0.84392166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4285416764122316}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5838184016589939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53272597,  15.51445366,   4.62672193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6949874743949098}
episode index:1705
target Thresh 63.566768785604864
target distance 22.0
model initialize at round 1705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.77141643,  15.27055804]), 'previousTarget': array([112.9793708 ,  15.09184678]), 'currentState': array([92.91719525, 17.68093172,  2.63001776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5839752828837511
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13031884,  14.70128068,   5.5826788 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9195534555955348}
episode index:1706
target Thresh 63.579195802275336
target distance 15.0
model initialize at round 1706
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.82933589,  16.77464886]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   6.       ,   3.9955585], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.33036445179367}
done in step count: 59
reward sum = -0.13845509498519426
running average episode reward sum: 0.5835520664936698
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90189297,  15.71432218,   1.25670359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7210278500283166}
episode index:1707
target Thresh 63.591610398140574
target distance 4.0
model initialize at round 1707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.3988058 ,  13.27714484,   5.73467076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.992095887293485}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5837842374149264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32290695,  15.24044244,   0.49797093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7185176172117612}
episode index:1708
target Thresh 63.60401258561518
target distance 61.0
model initialize at round 1708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.6881056, 16.1804067]), 'previousTarget': array([74., 15.]), 'currentState': array([53.6962648 , 16.75163451,  2.58156246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.598380189090555
running average episode reward sum: 0.5830925086691654
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.3919174 ,  23.5111089 ,   1.07395779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.600382024666212}
episode index:1709
target Thresh 63.616402377101345
target distance 13.0
model initialize at round 1709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.58174248,   3.56311053,   2.77745295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.815735400555756}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5832298270486554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17801501,  15.89774174,   0.37335525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2172097423810686}
episode index:1710
target Thresh 63.62877978498885
target distance 16.0
model initialize at round 1710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.59693096,  20.76180003,   5.26477212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.51279269835161}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5834070070864505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77587215,  15.26984582,   4.15950583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3507849177475907}
episode index:1711
target Thresh 63.64114482165511
target distance 27.0
model initialize at round 1711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.58027644,  11.55425587]), 'previousTarget': array([106.0200334 ,  10.67631238]), 'currentState': array([88.07034808,  3.97912867,  1.78068656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6738081548421239
running average episode reward sum: 0.5834598114951863
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69906635,  15.25632643,   5.45330656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7445784015818631}
episode index:1712
target Thresh 63.65349749946515
target distance 47.0
model initialize at round 1712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.86649169,  7.77574778]), 'previousTarget': array([87.37835421,  7.94766491]), 'currentState': array([69.58948012,  2.44688064,  0.6751191 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5741385450437418
running average episode reward sum: 0.5834543700086414
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78350345,  14.83071709,   4.60506484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2748225918818125}
episode index:1713
target Thresh 63.66583783077167
target distance 36.0
model initialize at round 1713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.43187586,  17.63018244]), 'previousTarget': array([98.72787848, 17.71202025]), 'currentState': array([80.7500777 , 21.18360631,  5.51388276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6514350782916389
running average episode reward sum: 0.5834940320321438
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3255837 ,  15.65414392,   3.83682952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7306907778550905}
episode index:1714
target Thresh 63.678165827915
target distance 25.0
model initialize at round 1714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.83983737,  11.64080463]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([88.34558001,  4.02749756,  2.11736012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5835322877734477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90752494,  14.11575654,   0.3857273 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8890658719204018}
episode index:1715
target Thresh 63.690481503223126
target distance 57.0
model initialize at round 1715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.16324492, 20.53419153]), 'previousTarget': array([77.85086911, 19.56217397]), 'currentState': array([58.38520192, 23.50555743,  0.3629868 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.21576562423779327
running average episode reward sum: 0.583317971535956
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3611902 ,  14.32701654,   4.49156235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9278926101377777}
episode index:1716
target Thresh 63.70278486901172
target distance 4.0
model initialize at round 1716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0101681 ,  12.36141728,   3.11097813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6386023122627797}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5835490618262671
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34959389,  15.46012348,   1.34590565]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.796706797585318}
episode index:1717
target Thresh 63.715075937584174
target distance 29.0
model initialize at round 1717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.58226961,  11.00627975]), 'previousTarget': array([104.25018649,  10.18111808]), 'currentState': array([87.16945289,  3.19806738,  1.80748385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4579384101299916
running average episode reward sum: 0.5834759473607861
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65823946,  14.59676285,   5.28488021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5285834476133169}
episode index:1718
target Thresh 63.72735472123153
target distance 58.0
model initialize at round 1718
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.62052343,  8.87750695]), 'previousTarget': array([76.70920231,  8.39813833]), 'currentState': array([57.       ,  5.       ,  3.9520316], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = -0.10249580381293011
running average episode reward sum: 0.5830768945677822
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87616016,  15.59673838,   3.93548896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6094530309126786}
episode index:1719
target Thresh 63.739621232232594
target distance 41.0
model initialize at round 1719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.37344457, 15.78308934]), 'previousTarget': array([93.99405381, 15.51234015]), 'currentState': array([72.38541186, 16.47486125,  1.84555125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.23256550719741637
running average episode reward sum: 0.5828731088774506
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53323667,  15.38111767,   4.40119153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6025933022017825}
episode index:1720
target Thresh 63.75187548285386
target distance 24.0
model initialize at round 1720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.92781832,  15.72068418]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([92.45639593, 20.28836524,  1.13823733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5830291720164497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76249181,  15.57178022,   3.69731138]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.619146797163143}
episode index:1721
target Thresh 63.764117485349594
target distance 35.0
model initialize at round 1721
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.85752247, 13.38302355]), 'previousTarget': array([99.87065345, 13.27093182]), 'currentState': array([80.      , 11.      ,  2.459402], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.391049006466161
running average episode reward sum: 0.5829176852768734
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43009157,  14.99599609,   4.36922786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5699224938353317}
episode index:1722
target Thresh 63.776347251961795
target distance 52.0
model initialize at round 1722
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.99965478, 16.11751023]), 'previousTarget': array([82.9963028 , 15.61545572]), 'currentState': array([63.       , 16.       ,  4.4112096], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.41153401962777997
running average episode reward sum: 0.5828182171018014
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82926488,  15.31292038,   3.57128027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3564682943943183}
episode index:1723
target Thresh 63.78856479492022
target distance 28.0
model initialize at round 1723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.91851192,  14.01678286]), 'previousTarget': array([106.68855151,  13.51581277]), 'currentState': array([88.10854341, 11.26630154,  6.12491483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5829049254269478
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36444588,  15.77224061,   4.36587397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.853918238220998}
episode index:1724
target Thresh 63.80077012644243
target distance 8.0
model initialize at round 1724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2729223 ,   7.22953076,   2.10323465]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.804411197670554}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5830860461972851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66818665,  15.16720934,   5.8421742 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37156300169379913}
episode index:1725
target Thresh 63.812963258733745
target distance 35.0
model initialize at round 1725
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.6444783 , 16.24573943]), 'previousTarget': array([99.79898987, 17.17157288]), 'currentState': array([80.      , 20.      ,  2.202409], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7078213593991467
running average episode reward sum: 0.583158314629036
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97027866,  15.43814512,   4.3496717 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43915202399600234}
episode index:1726
target Thresh 63.825144203987314
target distance 63.0
model initialize at round 1726
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([71.77147589,  9.01475392]), 'previousTarget': array([71.79898987,  8.82842712]), 'currentState': array([52.       ,  6.       ,  5.0729127], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.3347616415372768
running average episode reward sum: 0.583014483318618
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37572456,  15.20753644,   5.43857981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6578686771876057}
episode index:1727
target Thresh 63.837312974384055
target distance 8.0
model initialize at round 1727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15349799e+02, 2.38165841e+01, 1.08662009e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.823520496527514}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5832274321418711
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85194755,  15.65453665,   4.59788752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0743522946886301}
episode index:1728
target Thresh 63.849469582092766
target distance 63.0
model initialize at round 1728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.53392137, 17.57563637]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([50.56738863, 18.73217028,  1.70734453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5832435627418658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55143949,  14.62825735,   5.42027448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5825797244942443}
episode index:1729
target Thresh 63.861614039270044
target distance 41.0
model initialize at round 1729
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.99936725, 13.15908983]), 'previousTarget': array([93.97624702, 13.97445107]), 'currentState': array([74.       , 13.       ,  0.4843274], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6194490858690778
running average episode reward sum: 0.5832644907899163
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77348977,  15.30905314,   0.12101209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3831719311120033}
episode index:1730
target Thresh 63.87374635806036
target distance 1.0
model initialize at round 1730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.95279761,  12.84468672,   3.5098635 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.3962487747860592}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5834880809165541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14909414e+02, 1.47199689e+01, 3.35655212e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2943181937881958}
episode index:1731
target Thresh 63.88586655059602
target distance 13.0
model initialize at round 1731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.65571178,   2.24745903,   1.12546104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.183761404842203}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5836733545851986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35346649,  15.04253239,   0.35701202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6479310061920236}
episode index:1732
target Thresh 63.89797462899721
target distance 48.0
model initialize at round 1732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.46655408, 16.84208347]), 'previousTarget': array([86.93091516, 17.3390904 ]), 'currentState': array([68.5145788 , 18.22724859,  0.50877815]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5837343907832852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91107955,  15.20533032,   4.22291897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22375743458202677}
episode index:1733
target Thresh 63.91007060537203
target distance 22.0
model initialize at round 1733
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([110.99371441,  14.73076411]), 'previousTarget': array([111.51093912,  13.57265691]), 'currentState': array([93.       ,  6.       ,  4.5441084], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.32859184461861024
running average episode reward sum: 0.5835872497532018
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.130336  ,  15.24059536,   4.79653838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27363041430289314}
episode index:1734
target Thresh 63.92215449181644
target distance 10.0
model initialize at round 1734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9014058 ,   6.51031811,   1.55663979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.49025437718171}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5837990092921913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37583334,  14.63866162,   2.07719922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7212138705320434}
episode index:1735
target Thresh 63.934226300414345
target distance 8.0
model initialize at round 1735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.69176791,   7.24277386,   1.10792368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.99841734958135}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5840105248685783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14109170e+02, 1.41069432e+01, 1.04977678e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2613991234816369}
episode index:1736
target Thresh 63.946286043237535
target distance 25.0
model initialize at round 1736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.02189061,  12.7359874 ]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([90.31826358,  5.65260291,  2.39054236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5841176248230611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13246443,  15.16715219,   5.18135137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8834918300196515}
episode index:1737
target Thresh 63.95833373234577
target distance 59.0
model initialize at round 1737
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.83963747, 19.47239542]), 'previousTarget': array([75.86070472, 19.6436452 ]), 'currentState': array([56.      , 22.      ,  5.499973], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.19284702267120796
running average episode reward sum: 0.5838924978943201
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39461231,  15.21044944,   4.05909935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4472223645764032}
episode index:1738
target Thresh 63.97036937978674
target distance 7.0
model initialize at round 1738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.09945337,  14.71442902,   0.56182611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.906453107220385}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5840981262160605
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88219531,  14.645623  ,   4.2746884 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37344477724177166}
episode index:1739
target Thresh 63.98239299759608
target distance 56.0
model initialize at round 1739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([79.17781452,  6.21711626]), 'previousTarget': array([78.55604828,  7.19058177]), 'currentState': array([59.75313478,  1.45457312,  6.1135937 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 51
reward sum = 0.531714285766161
running average episode reward sum: 0.5840680205606296
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95917039,  15.66403948,   2.54963284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1666003024077052}
episode index:1740
target Thresh 63.99440459779741
target distance 52.0
model initialize at round 1740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.54090965,  7.14388743]), 'previousTarget': array([82.56699406,  8.13917182]), 'currentState': array([62.07041287,  2.57226242,  5.11000526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 49
reward sum = 0.5169881829987676
running average episode reward sum: 0.5840294910732304
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68839672,  14.34623147,   3.7685945 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9493699702867249}
episode index:1741
target Thresh 64.00640419240234
target distance 62.0
model initialize at round 1741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.71798643, 11.68355272]), 'previousTarget': array([72.90700027, 10.9264839 ]), 'currentState': array([51.77644174, 10.15554891,  1.37477279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5839511292607681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41797093,  14.52132108,   4.54841531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7535856585135111}
episode index:1742
target Thresh 64.01839179341047
target distance 5.0
model initialize at round 1742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.19171406,  13.18836261,   6.05717534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.217235094635752}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5841727860999759
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84148162,  15.83158917,   0.50787942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8465628320668598}
episode index:1743
target Thresh 64.0303674128094
target distance 24.0
model initialize at round 1743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.62770173,  16.21211085]), 'previousTarget': array([109.97366596,  16.67544468]), 'currentState': array([92.80654697, 22.97703019,  5.4897092 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5843359587070109
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65160326,  15.37585357,   5.6116222 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5124901843187621}
episode index:1744
target Thresh 64.04233106257473
target distance 23.0
model initialize at round 1744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.18059898,  13.5081513 ]), 'previousTarget': array([111.35234545,  14.04843794]), 'currentState': array([91.075026  ,  7.59400882,  3.12049389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.584365671903204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09868186,  15.42847442,   3.94932579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4396912963307523}
episode index:1745
target Thresh 64.05428275467014
target distance 4.0
model initialize at round 1745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39182093,   9.10422882,   4.23529151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.927056566176508}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5845137689945445
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69883815,  15.81938009,   5.66884669]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0769208358544482}
episode index:1746
target Thresh 64.06622250104729
target distance 29.0
model initialize at round 1746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.78025076,  18.2650074 ]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([84.57684085, 23.85330301,  1.65154541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5846289174615238
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76497398,  15.06738934,   3.9154205 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7679365309868288}
episode index:1747
target Thresh 64.07815031364596
target distance 22.0
model initialize at round 1747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.88434726,  14.05077292]), 'previousTarget': array([111.51093912,  13.57265691]), 'currentState': array([94.63683717,  5.86368701,  0.92691344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 31
reward sum = 0.6756224188789094
running average episode reward sum: 0.5846809732403667
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39973278,  14.58288869,   3.62716698]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5777267003072476}
episode index:1748
target Thresh 64.09006620439396
target distance 7.0
model initialize at round 1748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.18616742,   9.56921762,   0.60536468]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.955755562085053}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5848689871192937
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45617706,  15.40959979,   5.75007473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6808196376822181}
episode index:1749
target Thresh 64.10197018520716
target distance 12.0
model initialize at round 1749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.15405515,   2.77503274,   0.19480341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.342715323930022}
done in step count: 18
reward sum = 0.7672720407500875
running average episode reward sum: 0.5849732174356541
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50276399,  15.06626954,   5.92799854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5016326407767163}
episode index:1750
target Thresh 64.11386226798955
target distance 8.0
model initialize at round 1750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.00683071,   5.52723093,   6.13880813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.93440568069803}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5851768193385469
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90847341,  15.13805348,   2.743532  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16563780143139964}
episode index:1751
target Thresh 64.12574246463322
target distance 4.0
model initialize at round 1751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.77341093,  19.04594179,   5.5291822 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.618153726717405}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5854022321128971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62324736,  15.82587945,   4.92761404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9077551551304897}
episode index:1752
target Thresh 64.13761078701837
target distance 7.0
model initialize at round 1752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.90106691,  17.42143115,   6.27899701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.562035781268724}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5856162616496268
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12066473,  15.23197165,   5.42527315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2614781489964317}
episode index:1753
target Thresh 64.14946724701332
target distance 44.0
model initialize at round 1753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.92256613, 14.27501049]), 'previousTarget': array([90.95367385, 13.36047776]), 'currentState': array([70.93162654, 13.67306837,  0.6016407 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5856915833548427
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61898108,  15.58006764,   4.40641408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6940128863491488}
episode index:1754
target Thresh 64.16131185647453
target distance 42.0
model initialize at round 1754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.64784456, 13.81736314]), 'previousTarget': array([92.949174, 13.424941]), 'currentState': array([71.67344306, 12.80578785,  3.50641108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5857627295906931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67261943,  14.92525126,   3.61532383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6767601265124297}
episode index:1755
target Thresh 64.1731446272466
target distance 48.0
model initialize at round 1755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.45435281, 20.23716039]), 'previousTarget': array([86.72787848, 19.71202025]), 'currentState': array([66.7826853 , 23.84624577,  0.75389552]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5245183449410818
running average episode reward sum: 0.585727852378478
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31662902,  15.52100802,   4.88226079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.859328376683286}
episode index:1756
target Thresh 64.18496557116232
target distance 13.0
model initialize at round 1756
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.7955327 ,  16.16827451]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  21.       ,   4.5349903], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.746770681405208}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.585864227109201
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28479485,  15.14601764,   3.7252313 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3200457116410505}
episode index:1757
target Thresh 64.19677470004261
target distance 64.0
model initialize at round 1757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.33544585, 10.96615009]), 'previousTarget': array([70.88143384, 10.17453183]), 'currentState': array([52.42424426,  9.08358504,  1.28574037]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.22855792507724643
running average episode reward sum: 0.585400960811029
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([89.42853117, 20.00786971]), 'previousTarget': array([88.07826856, 20.1001883 ]), 'currentState': array([69.80136631, 23.8516178 ,  1.28007693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:1758
target Thresh 64.20857202569664
target distance 19.0
model initialize at round 1758
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.23174953,  14.22212316]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([96.       ,  6.       ,  2.3151314], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5244802783297868
running average episode reward sum: 0.5853663271086519
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.633623  ,  14.89405263,   3.73502413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6424196086556996}
episode index:1759
target Thresh 64.2203575599217
target distance 51.0
model initialize at round 1759
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.97346784, 15.9701541 ]), 'previousTarget': array([83.98463901, 16.21628867]), 'currentState': array([64.       , 17.       ,  3.5686688], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.45575411098398344
running average episode reward sum: 0.5852926838040355
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75711038,  14.99159354,   3.02525768]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7571570443894172}
episode index:1760
target Thresh 64.23213131450333
target distance 11.0
model initialize at round 1760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.36016015,   4.28905144,   1.95711684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.76122275187002}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5854790691326441
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0254191 ,  14.69810981,   1.3153002 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30295843904489317}
episode index:1761
target Thresh 64.24389330121532
target distance 63.0
model initialize at round 1761
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([71.95644517, 16.68079719]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([52.      , 18.      ,  1.789424], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.3008936059785202
running average episode reward sum: 0.5853175563839754
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54275712,  14.18057707,   0.251295  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9828729459476576}
episode index:1762
target Thresh 64.25564353181962
target distance 3.0
model initialize at round 1762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.84896825,  12.0363121 ,   5.7548715 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.857777536633992}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5854733934788463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75351568,  14.48680991,   2.30721004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5693141374094437}
episode index:1763
target Thresh 64.26738201806647
target distance 43.0
model initialize at round 1763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.58455205, 11.00415023]), 'previousTarget': array([91.80809791, 11.76392064]), 'currentState': array([70.84713701,  7.77391069,  2.94776201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4937179680628082
running average episode reward sum: 0.5854213779315582
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02913087,  14.71330321,   0.28211812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28817295946561644}
episode index:1764
target Thresh 64.27910877169435
target distance 50.0
model initialize at round 1764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.62420852, 14.06025789]), 'previousTarget': array([84.98401917, 13.79936077]), 'currentState': array([66.63516735, 13.398266  ,  5.51209635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5854611727432538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24996206,  15.07676578,   4.09310961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2614842591952149}
episode index:1765
target Thresh 64.29082380443003
target distance 49.0
model initialize at round 1765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.84661417, 12.92809764]), 'previousTarget': array([85.89668285, 12.03027376]), 'currentState': array([6.69005550e+01, 1.14602000e+01, 6.40782118e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.43834145615824194
running average episode reward sum: 0.5853778659954706
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4405763 ,  15.83448536,   4.08590316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.943648920382567}
episode index:1766
target Thresh 64.30252712798855
target distance 23.0
model initialize at round 1766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.03616253,  16.22309565]), 'previousTarget': array([110.88993593,  16.4295875 ]), 'currentState': array([93.54851807, 23.85244702,  1.2795276 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.585523630187541
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65283439,  14.4765015 ,   2.76503593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8368054815510511}
episode index:1767
target Thresh 64.31421875407322
target distance 3.0
model initialize at round 1767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.76902466,  11.07615047,   5.91396719]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.44078506060933}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5857091469394055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44007117,  15.45431015,   0.64991912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7210534006696651}
episode index:1768
target Thresh 64.32589869437567
target distance 60.0
model initialize at round 1768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.72672058, 10.20820636]), 'previousTarget': array([74.9007438 , 10.99007438]), 'currentState': array([55.87394699,  7.78593872,  4.40965223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.3832217228507566
running average episode reward sum: 0.5855946825956583
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02846638,  15.67830285,   3.82992299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.678899910711272}
episode index:1769
target Thresh 64.33756696057584
target distance 45.0
model initialize at round 1769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.17800235,  9.61927009]), 'previousTarget': array([89.5237412 ,  9.33860916]), 'currentState': array([71.66945844,  5.21282998,  1.13680046]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.48502692254462687
running average episode reward sum: 0.5855378646521266
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08262352,  14.6305828 ,   0.71517445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3785442045511084}
episode index:1770
target Thresh 64.349223564342
target distance 36.0
model initialize at round 1770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.76503227, 17.26946724]), 'previousTarget': array([98.87767469, 16.79136948]), 'currentState': array([79.98330998, 20.21623887,  6.17105967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6205076539902579
running average episode reward sum: 0.5855576104394434
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28405597,  15.32034945,   4.48358636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7843466187682435}
episode index:1771
target Thresh 64.36086851733074
target distance 38.0
model initialize at round 1771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.99051826,  8.98077545]), 'previousTarget': array([95.9232797 ,  8.47375358]), 'currentState': array([75.83831307,  3.21943564,  1.32401586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6509511832956508
running average episode reward sum: 0.585594514261597
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10091584,  14.77280472,   6.14697727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9273456848251298}
episode index:1772
target Thresh 64.37250183118704
target distance 26.0
model initialize at round 1772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.20818851,  12.44763399]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([88.2019185 ,  6.22174446,  1.14440143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.38542369224540274
running average episode reward sum: 0.5854816147567938
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59053558,  15.34490767,   4.44039197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5353712823921478}
episode index:1773
target Thresh 64.38412351754421
target distance 62.0
model initialize at round 1773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.18130528, 20.9353179 ]), 'previousTarget': array([72.83555733, 20.44057325]), 'currentState': array([54.38944206, 23.81319042,  0.06053102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = -0.05789798368751087
running average episode reward sum: 0.5851189430553032
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60200496,  15.87909963,   4.59947941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9649954458580873}
episode index:1774
target Thresh 64.39573358802392
target distance 14.0
model initialize at round 1774
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.7639098,  14.0956924]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  16.       ,   1.6216123], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.90518426393509}
done in step count: 10
reward sum = 0.7650820750088044
running average episode reward sum: 0.585220330735277
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62648301,  15.66757685,   4.06781042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9154997548684152}
episode index:1775
target Thresh 64.40733205423624
target distance 9.0
model initialize at round 1775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.85114998,   8.34107793,   5.63290167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.315595420865675}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5854103782373562
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29635454,  15.5860765 ,   1.58542263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.915752469956495}
episode index:1776
target Thresh 64.41891892777966
target distance 32.0
model initialize at round 1776
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.93927139,  14.55738768]), 'previousTarget': array([102.96105157,  14.24756572]), 'currentState': array([83.       , 13.       ,  6.0322537], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.23270740317109395
running average episode reward sum: 0.5852118959778929
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36021015,  14.81368762,   4.91762777]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6663657792766702}
episode index:1777
target Thresh 64.43049422024104
target distance 24.0
model initialize at round 1777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.20073126,  13.89288395]), 'previousTarget': array([109.97366596,  13.32455532]), 'currentState': array([91.99936789,  8.29756047,  1.84283897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5853246441470883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65931406,  14.53280428,   3.01540372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.808063654565132}
episode index:1778
target Thresh 64.44205794319568
target distance 46.0
model initialize at round 1778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.10879337, 18.60048518]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([69.29941569, 21.35521855,  5.902457  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5853094341945311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86091633,  15.30763752,   3.73056291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3376168108765086}
episode index:1779
target Thresh 64.4536101082073
target distance 28.0
model initialize at round 1779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.36162039,  16.26513988]), 'previousTarget': array([106.68855151,  16.48418723]), 'currentState': array([88.71522267, 20.00934294,  0.96009856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.585413217178582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53816332,  14.32723667,   3.77517484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8615278610641564}
episode index:1780
target Thresh 64.46515072682806
target distance 48.0
model initialize at round 1780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.00187536, 18.13931343]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([67.12642425, 20.36786522,  5.79969406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5854956934011961
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0103449 ,  15.11593665,   4.9557546 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9964228676422735}
episode index:1781
target Thresh 64.4766798105986
target distance 10.0
model initialize at round 1781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.57677058,   5.62915964,   1.36478167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.360920968146385}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5856497689686709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00445493,  15.60527559,   1.50464151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1651045155973343}
episode index:1782
target Thresh 64.48819737104797
target distance 13.0
model initialize at round 1782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.38733895,   1.82126177,   0.87642163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.743477111880889}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5857893449599674
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51635211,  14.2376547 ,   2.1526857 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9028209333459095}
episode index:1783
target Thresh 64.49970341969377
target distance 8.0
model initialize at round 1783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.52387112,  22.11132437,   5.04341852]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.272765459221506}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5859940538752925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96543613,  15.42125847,   3.91324248]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4226740571036189}
episode index:1784
target Thresh 64.51119796804201
target distance 13.0
model initialize at round 1784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.15049507,   3.62343579,   2.77869928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.367049420155247}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5861724225145829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0335045 ,  15.37113135,   6.223466  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0353028705827574}
episode index:1785
target Thresh 64.52268102758728
target distance 47.0
model initialize at round 1785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.49754622, 17.31352171]), 'previousTarget': array([87.95938166, 16.72599692]), 'currentState': array([66.56310685, 18.93158444,  1.62149596]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5861898456492008
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12387896,  15.48415673,   5.78516887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0009974132390458}
episode index:1786
target Thresh 64.53415260981262
target distance 20.0
model initialize at round 1786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.33820476,  15.14027622]), 'previousTarget': array([114.1565257 ,  15.25304229]), 'currentState': array([94.77289483, 19.28740141,  4.73834401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5863431016699014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35658287,  14.55593752,   4.44418474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7817781557038094}
episode index:1787
target Thresh 64.5456127261896
target distance 41.0
model initialize at round 1787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.25407641, 17.87357712]), 'previousTarget': array([93.85291755, 17.57891249]), 'currentState': array([75.46255366, 20.75379018,  1.34930569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6437346468335874
running average episode reward sum: 0.5863751998495232
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27857303,  15.79717638,   3.87774064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8444484091064525}
episode index:1788
target Thresh 64.55706138817837
target distance 6.0
model initialize at round 1788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.73726611,  20.63095781,   5.23139579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.507927341797625}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5865843786142804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32515828,  15.15503606,   3.67951712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3602278294274933}
episode index:1789
target Thresh 64.56849860722757
target distance 7.0
model initialize at round 1789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.6230923 ,  11.10125605,   1.05590885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.641636905412731}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5867773847423767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21206494,  15.08924227,   0.28057904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7929727848608866}
episode index:1790
target Thresh 64.57992439477441
target distance 3.0
model initialize at round 1790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39662366,  17.4487401 ,   4.38444126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4806528129500243}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5869861053594944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92626049,  14.01530676,   5.91086346]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9874504027435594}
episode index:1791
target Thresh 64.5913387622447
target distance 63.0
model initialize at round 1791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.2057068 , 13.97935003]), 'previousTarget': array([72., 15.]), 'currentState': array([51.21113608, 13.51336561,  3.21453285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4585274300216985
running average episode reward sum: 0.586914420830846
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63784588,  14.39422201,   5.12698391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7057780010599242}
episode index:1792
target Thresh 64.60274172105282
target distance 60.0
model initialize at round 1792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.30897277,  8.83577835]), 'previousTarget': array([74.72787848,  8.28797975]), 'currentState': array([53.52406319,  5.910483  ,  1.6679045 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5869279193354204
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44403252,  14.94158358,   4.10683459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5590280157681092}
episode index:1793
target Thresh 64.6141332826017
target distance 56.0
model initialize at round 1793
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.95999479, 15.73563926]), 'previousTarget': array([78.98725709, 16.28616939]), 'currentState': array([59.        , 17.        ,  0.64277583], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5229664464014994
running average episode reward sum: 0.5868922663404741
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62780325,  15.39172749,   4.26001898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.540352523577309}
episode index:1794
target Thresh 64.6255134582829
target distance 33.0
model initialize at round 1794
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.07070042,   9.02564399]), 'previousTarget': array([100.79586847,   9.83486126]), 'currentState': array([82.       ,  3.       ,  2.3963416], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 32
reward sum = 0.5856803359578534
running average episode reward sum: 0.5868915911703445
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23426572,  15.52813396,   5.12292113]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5777593857583414}
episode index:1795
target Thresh 64.63688225947662
target distance 38.0
model initialize at round 1795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.67747387,  8.73729537]), 'previousTarget': array([95.9232797 ,  8.47375358]), 'currentState': array([75.56443846,  2.84731424,  1.59838653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.19316353519815682
running average episode reward sum: 0.586672366194859
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50459553,  15.66762102,   4.08402991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8313503581080439}
episode index:1796
target Thresh 64.64823969755166
target distance 19.0
model initialize at round 1796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.96552065, 20.35939683,  1.96321982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.813972996552057}
done in step count: 28
reward sum = 0.6854192872036325
running average episode reward sum: 0.5867273171803952
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66771754,  15.07004759,   3.87734498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6713816906472282}
episode index:1797
target Thresh 64.65958578386544
target distance 37.0
model initialize at round 1797
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.75602493, 15.88560134]), 'previousTarget': array([97.88414095, 16.85036314]), 'currentState': array([78.      , 19.      ,  2.107442], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.4310359570675462
running average episode reward sum: 0.5866407257676517
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28323902,  15.55728712,   4.17273804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6251346038118337}
episode index:1798
target Thresh 64.67092052976406
target distance 7.0
model initialize at round 1798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.56724204,   7.41775086,   0.65429085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.323120162183802}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5868432545748402
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67548733,  14.31675446,   1.14717918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7563947057315215}
episode index:1799
target Thresh 64.68224394658228
target distance 12.0
model initialize at round 1799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.98510666,   4.62202711,   0.569978  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.112621272550001}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5870196650306369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36880366,  15.75726727,   5.82211155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9858308824922469}
episode index:1800
target Thresh 64.69355604564348
target distance 25.0
model initialize at round 1800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.31821801,  12.25017976]), 'previousTarget': array([108.03046115,  11.65462135]), 'currentState': array([91.31574225,  3.53749554,  1.39782208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6438456510229279
running average episode reward sum: 0.5870512174937087
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37423215,  14.46624076,   5.78632814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8224866744154249}
episode index:1801
target Thresh 64.7048568382598
target distance 4.0
model initialize at round 1801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.82157604,   9.59904308,   3.4849472 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.528021226907881}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5872585120511483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1862912 ,  14.39925268,   0.68128636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0114441924739512}
episode index:1802
target Thresh 64.716146335732
target distance 50.0
model initialize at round 1802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.0514966 , 11.96089994]), 'previousTarget': array([84.93630557, 12.59490445]), 'currentState': array([66.1608081 ,  9.87271625,  4.50243776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5333644024248582
running average episode reward sum: 0.5872286206980556
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19206134,  15.57345388,   4.93652196]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9907644709918051}
episode index:1803
target Thresh 64.72742454934962
target distance 7.0
model initialize at round 1803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.53506799,  21.11823215,   5.03026952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.06607884828743}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5874302622885222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65643294,  15.3760449 ,   3.88773077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7565143549180132}
episode index:1804
target Thresh 64.73869149039083
target distance 12.0
model initialize at round 1804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.46621094,   2.57219752,   0.72610682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.522091812371329}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5876160320570205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40380565,  15.17167509,   0.48839891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6204192406300628}
episode index:1805
target Thresh 64.74994717012261
target distance 40.0
model initialize at round 1805
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.1467512 ,  9.77943927]), 'previousTarget': array([94.28410786,  9.30312966]), 'currentState': array([75.    ,  4.    ,  5.0853], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.2551911889527118
running average episode reward sum: 0.5874319651450026
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50266589,  14.18062816,   5.36802158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9584943541777643}
episode index:1806
target Thresh 64.76119159980061
target distance 64.0
model initialize at round 1806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.57458258, 12.8921648 ]), 'previousTarget': array([70.96105157, 12.24756572]), 'currentState': array([49.59607943, 11.96512011,  3.4875896 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.24106252555887853
running average episode reward sum: 0.587240283108707
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17866447,  14.76658328,   3.09824317]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8538591334325442}
episode index:1807
target Thresh 64.77242479066926
target distance 35.0
model initialize at round 1807
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.21052192, 11.56379796]), 'previousTarget': array([99.36985865, 10.9808208 ]), 'currentState': array([80.        ,  6.        ,  0.40601957], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.42907308115574494
running average episode reward sum: 0.5871528012492198
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.94976969,  15.57547577,   4.17950336]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1105110656357322}
episode index:1808
target Thresh 64.7836467539618
target distance 31.0
model initialize at round 1808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.43019169,  14.24112251]), 'previousTarget': array([103.95850618,  14.28764556]), 'currentState': array([85.49278019, 12.6601031 ,  0.78612297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5872758388760699
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58470227,  14.73823247,   5.80486812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49091185482042593}
episode index:1809
target Thresh 64.79485750090014
target distance 60.0
model initialize at round 1809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.24639132, 18.98634281]), 'previousTarget': array([74.86526278, 19.68238601]), 'currentState': array([56.35136856, 21.03282012,  0.31939977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.3609656655648641
running average episode reward sum: 0.5871508056311466
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69248484,  15.74926656,   4.61328477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.809917249486633}
episode index:1810
target Thresh 64.80605704269504
target distance 47.0
model initialize at round 1810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.34649182, 18.39633419]), 'previousTarget': array([87.88777826, 17.88427891]), 'currentState': array([66.48552491, 20.75047807,  1.83089244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6304430623322028
running average episode reward sum: 0.5871747107977403
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70436592,  14.42641839,   5.2489832 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.64528704201032}
episode index:1811
target Thresh 64.81724539054606
target distance 15.0
model initialize at round 1811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.04483985,   5.30978623,   3.77018106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.820130691763058}
done in step count: 14
reward sum = 0.8015040920689782
running average episode reward sum: 0.5872929941207378
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46155805,  15.64255907,   6.28083242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8383328068240278}
episode index:1812
target Thresh 64.82842255564154
target distance 29.0
model initialize at round 1812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.30847019,  16.04033477]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([87.48894278, 18.72106851,  5.72417981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.3564364404387489
running average episode reward sum: 0.5871656601142943
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33918114,  15.57444028,   4.40013156]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.875593053742411}
episode index:1813
target Thresh 64.83958854915863
target distance 64.0
model initialize at round 1813
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.33842339, 19.66356248]), 'previousTarget': array([70.88143384, 19.82546817]), 'currentState': array([49.44192624, 21.69565521,  4.34449768]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.28064663363157005
running average episode reward sum: 0.5869966860092872
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91565035,  15.98260231,   4.4764156 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9862160883481657}
episode index:1814
target Thresh 64.85074338226336
target distance 39.0
model initialize at round 1814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.56036658, 10.04270545]), 'previousTarget': array([95.24899352,  9.4292033 ]), 'currentState': array([75.12385052,  5.3287004 ,  1.27659726]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4925457759094123
running average episode reward sum: 0.5869446469403616
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63655855,  14.82881614,   5.70120545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40173822350146654}
episode index:1815
target Thresh 64.86188706611053
target distance 47.0
model initialize at round 1815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.69933223, 12.54507069]), 'previousTarget': array([87.92796014, 12.69599661]), 'currentState': array([69.79282124, 10.61353758,  0.30655843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.587004926990532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64877329,  14.51864782,   5.37493134]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5958692154065585}
episode index:1816
target Thresh 64.87301961184384
target distance 42.0
model initialize at round 1816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.83175061, 11.73320327]), 'previousTarget': array([92.85976548, 12.3642578 ]), 'currentState': array([74.06574222,  8.68280782,  4.40187437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.47125427026512096
running average episode reward sum: 0.586941222721558
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7494882 ,  15.59182799,   5.02533893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6426636232895623}
episode index:1817
target Thresh 64.88414103059583
target distance 6.0
model initialize at round 1817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.35675497,  19.90763531,   4.79548546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.175436080312399}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5871520905858477
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42805554,  14.41468352,   4.79325944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8183616871686442}
episode index:1818
target Thresh 64.89525133348793
target distance 45.0
model initialize at round 1818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.32576494, 15.01988586]), 'previousTarget': array([89.99506356, 14.44433475]), 'currentState': array([71.32577199, 15.03668544,  1.67363661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.2196264774340253
running average episode reward sum: 0.5869500424202887
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97061166,  15.54750735,   3.7436319 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1143838158972652}
episode index:1819
target Thresh 64.90635053163044
target distance 35.0
model initialize at round 1819
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.48443054, 10.51186952]), 'previousTarget': array([99.36985865, 10.9808208 ]), 'currentState': array([80.       ,  6.       ,  1.9207324], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 52
reward sum = -0.026808408725328636
running average episode reward sum: 0.5866128125020768
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95084634,  15.16832291,   3.68787075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1753530239816838}
episode index:1820
target Thresh 64.91743863612255
target distance 2.0
model initialize at round 1820
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26628269,  11.80438065,   0.1508438 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.2787686852767064}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5868288955265127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.60129089,  15.15431761,   1.11509376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6207774635833851}
episode index:1821
target Thresh 64.92851565805239
target distance 18.0
model initialize at round 1821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.06563667,  14.42900019]), 'currentState': array([98.17113034,  5.1548797 ,  1.78841179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.49710870184958}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5869210417348976
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33391121,  14.64517492,   0.29921561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7547020049884139}
episode index:1822
target Thresh 64.93958160849695
target distance 29.0
model initialize at round 1822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.62215059,  16.47014874]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([86.92315042, 19.92693805,  0.14964455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5870432616068045
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7066545 ,  15.59018059,   4.98792073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6590635147347732}
episode index:1823
target Thresh 64.9506364985222
target distance 62.0
model initialize at round 1823
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([72.81333128, 19.27384818]), 'previousTarget': array([72.87373448, 19.75619127]), 'currentState': array([53.       , 22.       ,  0.6163768], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5500963978066877
running average episode reward sum: 0.5864198297759856
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([71.46328702, 20.20584963]), 'previousTarget': array([70.01672705, 19.79594035]), 'currentState': array([51.60475075, 22.58041   ,  5.70877863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1824
target Thresh 64.96168033918302
target distance 46.0
model initialize at round 1824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.07861484, 16.92131505]), 'previousTarget': array([88.88288926, 17.83881638]), 'currentState': array([68.12935446, 18.34504619,  4.86517847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5864725063596757
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18061639,  14.55490406,   5.16280497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9324697862826226}
episode index:1825
target Thresh 64.97271314152326
target distance 34.0
model initialize at round 1825
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.80819916,  14.76319492]), 'previousTarget': array([100.922597  ,  13.75787621]), 'currentState': array([81.       , 12.       ,  3.9416606], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.34916231754074567
running average episode reward sum: 0.5863425445914288
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40774118,  15.65968296,   5.60567494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8865394015944925}
episode index:1826
target Thresh 64.98373491657574
target distance 6.0
model initialize at round 1826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.59429846,  15.41622009,   6.06679559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.4217016068146}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5865527013814718
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10951054,  15.0327972 ,   0.33366027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.891093228432774}
episode index:1827
target Thresh 64.99474567536221
target distance 10.0
model initialize at round 1827
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.33055293,  16.65018873]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.       ,  20.       ,   3.5246933], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.860090208101944}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.5867034194594398
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51019558,  15.77134184,   5.47993064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9248068783251354}
episode index:1828
target Thresh 65.00574542889343
target distance 63.0
model initialize at round 1828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.63288453, 19.51025845]), 'previousTarget': array([71.90990945, 19.10381815]), 'currentState': array([53.75071057, 21.67801276,  6.03438855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.3378556915620443
running average episode reward sum: 0.586567362746538
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73750759,  15.97150472,   3.34366205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0063417348438985}
episode index:1829
target Thresh 65.01673418816918
target distance 43.0
model initialize at round 1829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.37693575, 15.74240546]), 'previousTarget': array([92., 15.]), 'currentState': array([71.38680509, 16.37063819,  2.92040324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 37
reward sum = 0.5777819447462886
running average episode reward sum: 0.5865625619716744
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63450913,  14.70661602,   3.15349439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46867657924117356}
episode index:1830
target Thresh 65.02771196417818
target distance 19.0
model initialize at round 1830
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.58076892,  14.0735106 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 10.       ,  1.7015831], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6194490858690778
running average episode reward sum: 0.5865805229350264
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25097048,  14.20613498,   0.79160339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8325910500963168}
episode index:1831
target Thresh 65.03867876789825
target distance 25.0
model initialize at round 1831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.98486857,  15.38483312]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([91.07610445, 17.29300277,  1.73442695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5867298012274424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87882162,  14.062025  ,   6.23392987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9457702183149256}
episode index:1832
target Thresh 65.04963461029615
target distance 9.0
model initialize at round 1832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.04302094,  17.32079348,   6.17555732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.288522071626236}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5869337653347925
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22194625,  14.57097493,   5.47640735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8884988219758779}
episode index:1833
target Thresh 65.06057950232777
target distance 13.0
model initialize at round 1833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.12933231,   3.44025201,   1.10453606]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.868181670392532}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.587106856016185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69482139,  14.77655771,   0.56210968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3782333150505267}
episode index:1834
target Thresh 65.07151345493797
target distance 43.0
model initialize at round 1834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.47633364, 10.42995874]), 'previousTarget': array([91.7401454 , 11.21351204]), 'currentState': array([70.81481459,  6.76598973,  3.00476766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5871233046728205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78595175,  14.62535408,   3.83120676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.870677736656333}
episode index:1835
target Thresh 65.08243647906072
target distance 13.0
model initialize at round 1835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.78783265,   3.42241692,   2.06335566]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.714810094001328}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5873061049940378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69099673,  14.04103732,   2.78893522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0075179629089057}
episode index:1836
target Thresh 65.09334858561904
target distance 15.0
model initialize at round 1836
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.35746693,  14.8649734 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  19.       ,   4.0324492], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.872023270508254}
done in step count: 29
reward sum = 0.22446517038022884
running average episode reward sum: 0.587108586793377
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03973666,  14.85376867,   5.09002623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9713337615269546}
episode index:1837
target Thresh 65.10424978552504
target distance 23.0
model initialize at round 1837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.97114969,  14.92171727]), 'previousTarget': array([111.92481176,  14.73259233]), 'currentState': array([92.98602093, 14.15059556,  1.87227219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6396339979033353
running average episode reward sum: 0.5871371642749385
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10942542,  14.25618128,   5.55560728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.160340192903731}
episode index:1838
target Thresh 65.11514008967991
target distance 5.0
model initialize at round 1838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.544445  ,  18.73222079,   5.67125599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.086288665820382}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5873455176385736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49504836,  14.28951667,   5.68826146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8716436853125809}
episode index:1839
target Thresh 65.12601950897397
target distance 8.0
model initialize at round 1839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.75890563,  21.67620358,   0.63589233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.139089297711758}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5875379821123575
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62249016,  14.28253916,   5.72315785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8107180423183311}
episode index:1840
target Thresh 65.13688805428663
target distance 40.0
model initialize at round 1840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.85269778, 19.38624464]), 'previousTarget': array([94.61161351, 19.0776773 ]), 'currentState': array([76.35767932, 23.85214377,  1.39816767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.2686626263206815
running average episode reward sum: 0.5873647744231715
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72259322,  14.29059285,   2.88692459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0126201010677547}
episode index:1841
target Thresh 65.14774573648643
target distance 4.0
model initialize at round 1841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.08077309,  17.35994802,   3.79672098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.5956550479742377}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5875779857291307
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68714217,  15.57266525,   3.85254031]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6525530704963896}
episode index:1842
target Thresh 65.15859256643107
target distance 15.0
model initialize at round 1842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.44537382,   7.0169516 ,   0.29840964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.730764527595586}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5877401164323248
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3772905 ,  15.76857603,   6.22554615]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9891795724837983}
episode index:1843
target Thresh 65.16942855496738
target distance 13.0
model initialize at round 1843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.5559522 ,   3.62333288,   2.84780544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.595675872679877}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5879167851584916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36100702,  15.59303723,   6.05850366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8717827666631326}
episode index:1844
target Thresh 65.18025371293133
target distance 7.0
model initialize at round 1844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.75737495,  18.49707909,   6.1891799 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.155412612477222}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5881187793182973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52899031,  14.20796102,   6.024376  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9215073897852252}
episode index:1845
target Thresh 65.1910680511481
target distance 6.0
model initialize at round 1845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.44491625,  10.29677368,   0.97882155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.5474518548975364}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5883205546328594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75256264,  14.52848015,   1.85875592]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8880774131685546}
episode index:1846
target Thresh 65.20187158043201
target distance 38.0
model initialize at round 1846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.6658368 ,  9.38997792]), 'previousTarget': array([96.34149075, 10.08986599]), 'currentState': array([77.54111469,  3.53805481,  4.0568862 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.5359307226882801
running average episode reward sum: 0.5882921898077677
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15354172,  14.15136305,   6.18905956]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8624150575386641}
episode index:1847
target Thresh 65.21266431158661
target distance 18.0
model initialize at round 1847
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.68186624,  14.93905699]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 15.      ,  4.095373], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.681960587563534}
done in step count: 18
reward sum = 0.5074241059121727
running average episode reward sum: 0.5882484300221098
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33651081,  15.20031141,   5.590168  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6930675023449808}
episode index:1848
target Thresh 65.22344625540461
target distance 40.0
model initialize at round 1848
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.05483481, 10.82824665]), 'previousTarget': array([94.61161351, 10.9223227 ]), 'currentState': array([73.40670247,  7.09315688,  2.07319117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.3003938909106666
running average episode reward sum: 0.5880927488219414
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1230686 ,  15.42852014,   4.98144029]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9760318541005502}
episode index:1849
target Thresh 65.23421742266798
target distance 51.0
model initialize at round 1849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.76244081,  8.41470762]), 'previousTarget': array([83.46834337,  7.58078667]), 'currentState': array([65.2205127 ,  4.15876522,  1.76945179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.25889745350364635
running average episode reward sum: 0.5879148054190666
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00913328,  14.57350037,   4.94097334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.07875798525647}
episode index:1850
target Thresh 65.24497782414787
target distance 39.0
model initialize at round 1850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.78796959,  9.9547449 ]), 'previousTarget': array([95.48782391, 10.49719013]), 'currentState': array([74.33076522,  5.32687301,  2.72999763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.4485958573476586
running average episode reward sum: 0.5878395385643549
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05208069,  14.61705579,   5.64924747]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.022348910145604}
episode index:1851
target Thresh 65.2557274706047
target distance 33.0
model initialize at round 1851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.887681  ,  12.05492602]), 'previousTarget': array([101.5646825 ,  12.15008417]), 'currentState': array([83.45389946,  7.32966042,  5.07616386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.606429049060283
running average episode reward sum: 0.5878495760970202
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8901333 ,  15.70781475,   2.5594997 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7162907331187576}
episode index:1852
target Thresh 65.26646637278809
target distance 1.0
model initialize at round 1852
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.3301895 ,  14.7609293 ,   5.24029088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3515024594071976}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5880559708211989
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28394296,  14.88553489,   1.49443865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7251482202968819}
episode index:1853
target Thresh 65.27719454143697
target distance 37.0
model initialize at round 1853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.83317531, 17.57016829]), 'previousTarget': array([97.65140491, 18.28216664]), 'currentState': array([79.08122304, 20.71029274,  0.13689392]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.5677518199242515
running average episode reward sum: 0.5880450192834983
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20646202,  15.08410363,   3.33343599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22293493982628643}
episode index:1854
target Thresh 65.28791198727949
target distance 59.0
model initialize at round 1854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.76573433, 16.35302479]), 'previousTarget': array([75.99712788, 15.66106563]), 'currentState': array([54.77703362, 17.02521818,  3.45392776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3427513398490386
running average episode reward sum: 0.5875432422704888
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.38378386,  23.36809628,   5.31318272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.667677883347736}
episode index:1855
target Thresh 65.2986187210331
target distance 25.0
model initialize at round 1855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.12240501,  14.20667834]), 'previousTarget': array([109.61161351,  13.9223227 ]), 'currentState': array([89.30213243, 11.53145974,  0.98837018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7414265047141557
running average episode reward sum: 0.5876261535110295
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14442519,  14.09251457,   5.71974132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2472121146752042}
episode index:1856
target Thresh 65.30931475340455
target distance 25.0
model initialize at round 1856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.15819106,  15.48217999]), 'previousTarget': array([109.93630557,  15.40509555]), 'currentState': array([91.31387817, 17.97281158,  1.54464191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.6217005041524029
running average episode reward sum: 0.5876445026497702
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7165835 ,  15.52688394,   4.58012545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5982738502896497}
episode index:1857
target Thresh 65.32000009508985
target distance 13.0
model initialize at round 1857
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.33423224,  13.84358954]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,  16.      ,   3.272921], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.495527583794903}
done in step count: 10
reward sum = 0.7650820750088044
running average episode reward sum: 0.5877400018813951
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03062132,  15.242007  ,   4.77883453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9991308319033213}
episode index:1858
target Thresh 65.33067475677437
target distance 45.0
model initialize at round 1858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.94870007,  8.59467958]), 'previousTarget': array([89.32469879,  8.15325301]), 'currentState': array([71.62232993,  3.44769819,  5.54244382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5878099268575743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01887478,  14.07864386,   5.43378701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3459211795042656}
episode index:1859
target Thresh 65.34133874913273
target distance 23.0
model initialize at round 1859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.67667615,  13.87827263]), 'previousTarget': array([111.35234545,  14.04843794]), 'currentState': array([92.72701297,  7.48215935,  4.14907837]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.3596408942328817
running average episode reward sum: 0.5876872553346577
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01988701,  15.44723356,   5.42544306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0773297187022}
episode index:1860
target Thresh 65.35199208282899
target distance 63.0
model initialize at round 1860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.19174938,  7.55247524]), 'previousTarget': array([71.79898987,  8.82842712]), 'currentState': array([52.48771724,  4.12448496,  4.52875203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = -0.25952286094936067
running average episode reward sum: 0.5872320107799646
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15633937,  15.32558027,   4.82737102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9043040240242582}
episode index:1861
target Thresh 65.36263476851641
target distance 28.0
model initialize at round 1861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.39726718,  13.99184753]), 'previousTarget': array([106.949174,  14.424941]), 'currentState': array([87.57081982, 11.36278212,  5.96935618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5873385876489373
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12855187,  14.59415538,   4.70034248]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9613176888591013}
episode index:1862
target Thresh 65.37326681683774
target distance 29.0
model initialize at round 1862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.67331133,  14.91919882]), 'previousTarget': array([106.,  15.]), 'currentState': array([87.67452746, 14.69864551,  0.80060047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5083270719024635
running average episode reward sum: 0.5872961767440814
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14210279,  14.8954419 ,   0.75138565]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8642453425803961}
episode index:1863
target Thresh 65.38388823842499
target distance 5.0
model initialize at round 1863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.5212147 ,  14.15197209,   0.39884787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5806561515387196}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5874811387457782
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05407018,  15.36038431,   5.8111928 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0122549443401445}
episode index:1864
target Thresh 65.3944990438996
target distance 60.0
model initialize at round 1864
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.99284137, 12.53506462]), 'previousTarget': array([74.97504678, 12.99875234]), 'currentState': array([55.       , 12.       ,  0.9077155], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.14086987656190725
running average episode reward sum: 0.5872416689001032
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04934433,  15.22632008,   5.23716539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9772241184485198}
episode index:1865
target Thresh 65.40509924387236
target distance 6.0
model initialize at round 1865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.54561122,  13.51957546,   1.33429402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.693957410594064}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5874417516123754
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45322819,  14.57285064,   5.96385711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6938414727102303}
episode index:1866
target Thresh 65.41568884894349
target distance 63.0
model initialize at round 1866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.14100389,  7.43644421]), 'previousTarget': array([71.64677133,  6.74224216]), 'currentState': array([53.45971393,  3.88020664,  1.51587993]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3962420568228599
running average episode reward sum: 0.5873393414919739
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13309867,  14.78782037,   1.45619404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25047045470112783}
episode index:1867
target Thresh 65.42626786970257
target distance 6.0
model initialize at round 1867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.61937255,   7.07723801,   4.23086548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.04215703178383}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5875289243655869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00942868,  14.22447308,   0.65311527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2580435403802321}
episode index:1868
target Thresh 65.43683631672866
target distance 61.0
model initialize at round 1868
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.89190304, 18.92341781]), 'previousTarget': array([73.90394822, 19.0422346 ]), 'currentState': array([54.      , 21.      ,  5.430386], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = -0.03898335631135902
running average episode reward sum: 0.5871937118023569
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36403059,  15.78451687,   4.47271931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0099127730711144}
episode index:1869
target Thresh 65.44739420059017
target distance 53.0
model initialize at round 1869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.23947482,  8.46234777]), 'previousTarget': array([81.58267675,  8.06432914]), 'currentState': array([63.6501781 ,  4.4300464 ,  1.22692674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5747590144539432
running average episode reward sum: 0.5871870622315823
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00885836,  15.22474002,   4.2261974 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0163020339374973}
episode index:1870
target Thresh 65.45794153184501
target distance 14.0
model initialize at round 1870
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.34770194,  13.34278785]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   3.       ,   1.0197405], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.885923356275732}
done in step count: 11
reward sum = 0.6874312542587164
running average episode reward sum: 0.5872406401001163
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13565287,  15.91747275,   0.23943102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2604968154302536}
episode index:1871
target Thresh 65.4684783210405
target distance 62.0
model initialize at round 1871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.26329538, 14.65054105]), 'previousTarget': array([73., 15.]), 'currentState': array([51.26393376, 14.49074493,  2.65143514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.02737397269085584
running average episode reward sum: 0.5869415660256456
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37517802,  15.21703234,   5.31288033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6614420208839321}
episode index:1872
target Thresh 65.47900457871344
target distance 12.0
model initialize at round 1872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.87069827,   4.44023352,   2.03703536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.9589690900192}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5871110484116483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27287036,  15.68477866,   5.95364477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9988189608645476}
episode index:1873
target Thresh 65.48952031539008
target distance 60.0
model initialize at round 1873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.60193826, 18.07962135]), 'previousTarget': array([74.95570316, 17.66961979]), 'currentState': array([56.66595404, 19.67853754,  6.03098315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.41528929648108986
running average episode reward sum: 0.5870193612441292
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32062134,  15.62065891,   4.4648433 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9202026093941412}
episode index:1874
target Thresh 65.50002554158615
target distance 25.0
model initialize at round 1874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.18464836,  15.62651159]), 'previousTarget': array([109.85753677,  15.61709559]), 'currentState': array([91.44895797, 18.86727151,  1.53974038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5871295452027423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77412414,  15.82510439,   1.97247545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8554631254890699}
episode index:1875
target Thresh 65.5105202678069
target distance 26.0
model initialize at round 1875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.11309906,  15.2579183 ]), 'previousTarget': array([109.,  15.]), 'currentState': array([90.14089563, 16.31200081,  6.22544116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5872396116944485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11990733,  15.6802379 ,   5.54656431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1123338992160805}
episode index:1876
target Thresh 65.52100450454702
target distance 22.0
model initialize at round 1876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.70289167,  15.21422825]), 'previousTarget': array([112.91786413,  15.18928508]), 'currentState': array([93.97020794, 18.47324544,  1.85404682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7046770584866437
running average episode reward sum: 0.5873021782617325
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12655288,  15.69351185,   5.55901453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1152885516068447}
episode index:1877
target Thresh 65.5314782622908
target distance 52.0
model initialize at round 1877
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.99029627, 17.37705938]), 'previousTarget': array([82.96679883, 16.8480693 ]), 'currentState': array([63.      , 18.      ,  4.006515], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.34289553569845893
running average episode reward sum: 0.587172036279537
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4082832 ,  15.35851861,   5.37235784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.691855741694863}
episode index:1878
target Thresh 65.54194155151195
target distance 39.0
model initialize at round 1878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.40000311, 16.25022914]), 'previousTarget': array([95.97375327, 15.97570496]), 'currentState': array([74.43673534, 17.46181472,  3.55211258]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.4898129682194893
running average episode reward sum: 0.5871202219804097
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09570862,  15.90500012,   5.52280717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9100468969789932}
episode index:1879
target Thresh 65.5523943826738
target distance 4.0
model initialize at round 1879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.55331618,  17.68963918,   3.70804036]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7085822668672064}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5872746904915899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97434894,  15.54079727,   4.17038939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1143686761576492}
episode index:1880
target Thresh 65.56283676622914
target distance 5.0
model initialize at round 1880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.03617075,  14.3261743 ,   1.91754788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.020694393828273}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5874731600926044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68780005,  15.8973503 ,   6.09936792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9501086069363378}
episode index:1881
target Thresh 65.57326871262039
target distance 12.0
model initialize at round 1881
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.64131099,  14.54588251]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.       ,   3.       ,   3.7927954], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.701748390235188}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.5875995496219169
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66202174,  15.25717758,   5.90186139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4246994407195498}
episode index:1882
target Thresh 65.58369023227948
target distance 26.0
model initialize at round 1882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.19625874,  11.91147976]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([87.32391637,  5.29070234,  1.9906491 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5399388542394628
running average episode reward sum: 0.5875742385781662
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02264888,  14.93297427,   5.88775177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07074899784838892}
episode index:1883
target Thresh 65.59410133562794
target distance 26.0
model initialize at round 1883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.02871096,  13.54608629]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([88.44997883,  9.4627974 ,  0.92044902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5876629567568421
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70361084,  15.9442306 ,   5.49574834]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9896554742443948}
episode index:1884
target Thresh 65.60450203307686
target distance 10.0
model initialize at round 1884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40411433,   3.04341448,   4.99657825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.963412790602954}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5878358237545752
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67003145,  14.59040499,   1.47866837]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5259727373387868}
episode index:1885
target Thresh 65.61489233502695
target distance 3.0
model initialize at round 1885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.21438891,  12.30618178,   1.24861019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4871442267096495}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5880386144100607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74654303,  15.01356105,   2.14695346]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25381950067790743}
episode index:1886
target Thresh 65.62527225186851
target distance 24.0
model initialize at round 1886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.57164215,  15.40521525]), 'previousTarget': array([110.98266146,  15.16738911]), 'currentState': array([89.62713336, 16.89402999,  1.68399155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7182193154225054
running average episode reward sum: 0.5881076025928971
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53498485,  15.68032032,   3.95205205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8654735884685711}
episode index:1887
target Thresh 65.63564179398145
target distance 13.0
model initialize at round 1887
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.31891268,  14.92041674]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,  13.       ,   5.3404903], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.480670042227135}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.5882477705440808
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48036724,  14.8302515 ,   5.39025846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5466559755938747}
episode index:1888
target Thresh 65.64600097173533
target distance 48.0
model initialize at round 1888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.37573954, 10.05792281]), 'previousTarget': array([86.57960839,  9.07908508]), 'currentState': array([67.68832045,  6.53577441,  0.13944769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5882664458348946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40188784,  14.96889905,   4.52700108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5989202164474868}
episode index:1889
target Thresh 65.65634979548932
target distance 57.0
model initialize at round 1889
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.97904143, 17.08462933]), 'previousTarget': array([77.97235659, 16.94882334]), 'currentState': array([58.      , 18.      ,  5.014733], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5033116020847181
running average episode reward sum: 0.5882214961821167
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68351738,  14.82035736,   5.17110927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3639130751183673}
episode index:1890
target Thresh 65.66668827559224
target distance 25.0
model initialize at round 1890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.041866  ,  14.15138188]), 'previousTarget': array([109.74881264,  14.15981002]), 'currentState': array([91.48626984,  9.95869076,  5.49780732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.12415498241807332
running average episode reward sum: 0.587976088189645
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46852665,  15.15200094,   1.2006501 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4925662451973743}
episode index:1891
target Thresh 65.67701642238256
target distance 63.0
model initialize at round 1891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.56392027, 10.47060268]), 'previousTarget': array([71.87767469, 10.20863052]), 'currentState': array([50.66701612,  8.44249883,  1.81107759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.587945924053686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70631902,  15.22879517,   4.32746296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37228449364050925}
episode index:1892
target Thresh 65.68733424618847
target distance 59.0
model initialize at round 1892
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.9960687 , 13.39653048]), 'previousTarget': array([75.98851894, 13.67757691]), 'currentState': array([56.       , 13.       ,  3.5614936], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.18449453084545048
running average episode reward sum: 0.5877327960065607
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09000652,  14.12717311,   5.44700939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2609182844470048}
episode index:1893
target Thresh 65.69764175732774
target distance 33.0
model initialize at round 1893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.48518255,  16.64793279]), 'previousTarget': array([101.77431008,  17.00389241]), 'currentState': array([83.68690554, 19.48134548,  0.56576341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.27338897258904266
running average episode reward sum: 0.5875668277787797
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3447008 ,  14.93637873,   5.30151996]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6583803624897666}
episode index:1894
target Thresh 65.70793896610792
target distance 29.0
model initialize at round 1894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.6088463 ,  15.14946035]), 'previousTarget': array([106.,  15.]), 'currentState': array([87.61293415, 15.55380804,  1.30261558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5876883792879186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17934332,  15.44882217,   4.14577377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9353709014305752}
episode index:1895
target Thresh 65.71822588282622
target distance 61.0
model initialize at round 1895
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.99397049, 12.49106427]), 'previousTarget': array([73.97585674, 12.98241918]), 'currentState': array([54.       , 12.       ,  0.5859066], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.19797728991145863
running average episode reward sum: 0.5874828354644078
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41637378,  15.70838937,   4.70716511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8216949650887496}
episode index:1896
target Thresh 65.72850251776953
target distance 11.0
model initialize at round 1896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05106799e+02, 9.66636312e+00, 4.48316892e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.239355399331675}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5876644814909985
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74216061,  14.81839189,   5.89874388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3153770038570367}
episode index:1897
target Thresh 65.73876888121451
target distance 44.0
model initialize at round 1897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.53005442, 16.3706082 ]), 'previousTarget': array([90.9793708 , 16.09184678]), 'currentState': array([72.56715774, 17.58829294,  5.63229651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5876866938986446
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27164082,  15.85642786,   5.45247875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1242667664573602}
episode index:1898
target Thresh 65.74902498342752
target distance 64.0
model initialize at round 1898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.18623467, 20.22404522]), 'previousTarget': array([70.91268452, 19.13318583]), 'currentState': array([50.32075628, 22.5398104 ,  0.97624254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3337607123620159
running average episode reward sum: 0.5875529782685568
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51094322,  14.79224133,   4.83628985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5313569381891191}
episode index:1899
target Thresh 65.75927083466465
target distance 43.0
model initialize at round 1899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.61610405, 17.09628034]), 'previousTarget': array([91.91402432, 17.14753262]), 'currentState': array([73.71151727, 19.04754282,  5.40679878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5876409605364173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5480594 ,  15.60581175,   4.98554963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7558162332262596}
episode index:1900
target Thresh 65.76950644517176
target distance 28.0
model initialize at round 1900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.88562638,  11.3062651 ]), 'previousTarget': array([105.3829006 ,  10.87838597]), 'currentState': array([88.6828488 ,  3.02019919,  1.02200239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5877451358022094
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15878533,  15.14779748,   5.02720067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8540996541945007}
episode index:1901
target Thresh 65.77973182518447
target distance 41.0
model initialize at round 1901
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.88435844, 11.14762412]), 'previousTarget': array([93.78922128, 11.8959836 ]), 'currentState': array([74.        ,  9.        ,  0.34525034], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4400048568341018
running average episode reward sum: 0.5876674595251494
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20756002,  15.40835402,   4.4395624 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4580765902044177}
episode index:1902
target Thresh 65.78994698492815
target distance 10.0
model initialize at round 1902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.55943058,   4.36709846,   0.62445706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.17569296122441}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5878484358196223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80910593,  15.3173151 ,   0.17884697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37030989873905296}
episode index:1903
target Thresh 65.80015193461796
target distance 10.0
model initialize at round 1903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44500805,   5.86275131,   1.54825228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.154088140839864}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5880341667616293
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03007738,  15.40781864,   0.30748391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4089262663169312}
episode index:1904
target Thresh 65.81034668445886
target distance 9.0
model initialize at round 1904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.96133634,   7.4342305 ,   1.91483457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.33361767060652}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5882197027105215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68706221,  15.03630346,   1.87514491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3150365048202804}
episode index:1905
target Thresh 65.8205312446456
target distance 29.0
model initialize at round 1905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.53551154,  16.58067811]), 'previousTarget': array([105.81242258,  16.26725206]), 'currentState': array([86.87537487, 20.2520574 ,  6.23382694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6117039591664646
running average episode reward sum: 0.5882320239363641
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64726291,  15.73195717,   4.70137204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8125175393588147}
episode index:1906
target Thresh 65.83070562536273
target distance 9.0
model initialize at round 1906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.81527008,  21.78039003,   4.92208672]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.235293724330209}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5884074369780481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77411907,  15.42058874,   4.98181051]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4774066217903545}
episode index:1907
target Thresh 65.84086983678465
target distance 63.0
model initialize at round 1907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.54274167, 13.35990694]), 'previousTarget': array([71.95980905, 12.26728946]), 'currentState': array([52.5576472 , 12.58789725,  0.22306538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.25365087765848143
running average episode reward sum: 0.588231988047587
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93302614,  15.64858828,   4.85669505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.652037001514332}
episode index:1908
target Thresh 65.85102388907553
target distance 5.0
model initialize at round 1908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.05265283,  11.3396293 ,   1.27653623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.69948602977549}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.588437262019275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27881055,  14.0840541 ,   2.06128633]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1657920540998556}
episode index:1909
target Thresh 65.86116779238948
target distance 46.0
model initialize at round 1909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.78553618, 10.20571846]), 'previousTarget': array([88.54352728,  9.24859289]), 'currentState': array([69.11184895,  6.60765915,  2.47893238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.4770013366799504
running average episode reward sum: 0.5883789186028671
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56273816,  15.30942432,   5.49892323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.535669046394735}
episode index:1910
target Thresh 65.87130155687036
target distance 16.0
model initialize at round 1910
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.14438311,  13.76599214]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 16.       ,  3.3085294], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.289322230471306}
done in step count: 17
reward sum = 0.7729431933839268
running average episode reward sum: 0.5884754985478075
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54147856,  15.68590358,   3.67817176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8738779971445494}
episode index:1911
target Thresh 65.88142519265197
target distance 30.0
model initialize at round 1911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.00325245,  16.49226764]), 'previousTarget': array([104.61161351,  17.0776773 ]), 'currentState': array([85.22242745, 19.44505653,  5.86447048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5128888786683202
running average episode reward sum: 0.5884359657968244
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54348812,  14.22294776,   5.71445692]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9012287585567611}
episode index:1912
target Thresh 65.89153870985793
target distance 45.0
model initialize at round 1912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.31065834, 19.57385753]), 'previousTarget': array([89.69125016, 19.49933331]), 'currentState': array([71.67333495, 23.36536514,  5.53915149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5890899170811437
running average episode reward sum: 0.588436307642765
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29674526,  15.56057854,   5.00033464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8993417176393136}
episode index:1913
target Thresh 65.90164211860174
target distance 59.0
model initialize at round 1913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.14258391, 10.65547458]), 'previousTarget': array([75.9285661 , 11.68886153]), 'currentState': array([55.2603492 ,  8.48827754,  5.26735663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.1474940951851595
running average episode reward sum: 0.5882059303112824
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88089028,  15.16390514,   4.4784294 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20261298410518805}
episode index:1914
target Thresh 65.91173542898684
target distance 8.0
model initialize at round 1914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.28615123,  15.98473348,   0.33241242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.7856809055151865}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.588390407710285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54152399,  15.47300121,   4.95177372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.719012087669719}
episode index:1915
target Thresh 65.92181865110652
target distance 41.0
model initialize at round 1915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.63824368, 16.58455404]), 'previousTarget': array([93.97624702, 16.02554893]), 'currentState': array([72.68826678, 17.99820939,  1.55398536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5884153529495092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80352782,  14.99248947,   4.15987715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19661567795162768}
episode index:1916
target Thresh 65.93189179504402
target distance 62.0
model initialize at round 1916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.98314334,  6.28176383]), 'previousTarget': array([72.57433899,  6.10429689]), 'currentState': array([51.36426649,  2.39593429,  1.89410734]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.34525449347146875
running average episode reward sum: 0.5879283055596182
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([96.60668742, 18.64928835]), 'previousTarget': array([95.20270049, 19.42931748]), 'currentState': array([76.98907216, 22.54148202,  4.73100586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:1917
target Thresh 65.94195487087248
target distance 29.0
model initialize at round 1917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.99231553,  15.47729868]), 'previousTarget': array([105.89383588,  15.94201698]), 'currentState': array([87.0277488 , 16.66728829,  0.09611719]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5880355453813513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33345206,  14.86014853,   5.60076384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36159191259974466}
episode index:1918
target Thresh 65.95200788865495
target distance 10.0
model initialize at round 1918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.71158156,   4.44293546,   0.48002857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.899737896364993}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5882148209428549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00239364,  15.48348653,   0.5131106 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48349245379959815}
episode index:1919
target Thresh 65.9620508584445
target distance 32.0
model initialize at round 1919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.54610594,  11.0692867 ]), 'previousTarget': array([102.08959956,  10.96549986]), 'currentState': array([84.62904428,  4.57738574,  0.75617092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5882748380647594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46986464,  15.53584559,   4.49489402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7537731721518917}
episode index:1920
target Thresh 65.97208379028405
target distance 8.0
model initialize at round 1920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93171334,  21.31841592,   5.68180275]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.38674169611239}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5884538024113717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06294389,  14.93149621,   3.3431454 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.0930306521515557}
episode index:1921
target Thresh 65.98210669420656
target distance 56.0
model initialize at round 1921
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.99034097, 15.37849541]), 'previousTarget': array([78.99681199, 15.64291407]), 'currentState': array([59.      , 16.      ,  3.596235], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.38162834973424675
running average episode reward sum: 0.5883461929146615
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89230818,  15.44085117,   4.18734717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4538141536306879}
episode index:1922
target Thresh 65.99211958023494
target distance 22.0
model initialize at round 1922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.98954405,  14.53941676]), 'previousTarget': array([112.50265712,  14.43242207]), 'currentState': array([92.21958654, 11.51472175,  1.07222247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5883744120561956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57309329,  15.7248989 ,   4.72667452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8412655625376302}
episode index:1923
target Thresh 66.00212245838206
target distance 28.0
model initialize at round 1923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.3382518 ,  15.78466357]), 'previousTarget': array([106.949174,  15.575059]), 'currentState': array([88.47556115, 18.12421908,  1.32461541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5885111497687936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15292574,  15.09094915,   4.78061497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8519428131233432}
episode index:1924
target Thresh 66.01211533865079
target distance 14.0
model initialize at round 1924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.28405386,  22.07642802,   5.97085017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.552357876557302}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5886799841052689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5492612 ,  15.11252762,   4.65438182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4645728455872456}
episode index:1925
target Thresh 66.02209823103405
target distance 44.0
model initialize at round 1925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.2005037 ,  9.94825549]), 'previousTarget': array([90.50265712,  9.43242207]), 'currentState': array([72.6740802 ,  5.62173435,  5.81554693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5737094899354991
running average episode reward sum: 0.5886722112630208
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55344615,  15.37545047,   2.28603784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6687792608948299}
episode index:1926
target Thresh 66.03207114551469
target distance 64.0
model initialize at round 1926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.62960705, 15.46201804]), 'previousTarget': array([70.99755904, 14.31246186]), 'currentState': array([50.63069121, 15.67026186,  0.82562935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5886653002815543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27414006,  15.4543429 ,   3.4507975 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8563294488894247}
episode index:1927
target Thresh 66.04203409206566
target distance 6.0
model initialize at round 1927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.21207221,  14.49970868,   1.06334138]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.8063393340219305}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5888582104007029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18430919,  15.38776892,   0.56718391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4293420726442056}
episode index:1928
target Thresh 66.0519870806499
target distance 50.0
model initialize at round 1928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.46147217, 18.22562846]), 'previousTarget': array([84.80683493, 19.22704311]), 'currentState': array([65.57966412, 20.39673618,  6.03210139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5577110417821357
running average episode reward sum: 0.5888420636051515
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78071464,  15.33533325,   6.13724372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8496844923584502}
episode index:1929
target Thresh 66.06193012122037
target distance 14.0
model initialize at round 1929
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.2937653 ,  16.52761927]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  14.      ,   4.155433], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.531927244734538}
done in step count: 11
reward sum = 0.6874312542587164
running average episode reward sum: 0.5888931460873553
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71662437,  15.03362247,   5.81623882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28536330452560954}
episode index:1930
target Thresh 66.07186322372013
target distance 64.0
model initialize at round 1930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.96899007, 20.00479243]), 'previousTarget': array([70.91268452, 19.13318583]), 'currentState': array([52.10290533, 22.31535011,  6.20384533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5198624765680987
running average episode reward sum: 0.5888573974236996
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41894553,  15.71302972,   3.84262287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8269986366126724}
episode index:1931
target Thresh 66.0817863980823
target distance 11.0
model initialize at round 1931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.54684689,   3.85695904,   0.91778963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.999913937726557}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5890302169356064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09986683,  14.70889836,   1.16992929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30775566828919826}
episode index:1932
target Thresh 66.09169965423004
target distance 57.0
model initialize at round 1932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.33920101,  6.33682356]), 'previousTarget': array([77.5709957 ,  7.12020962]), 'currentState': array([56.82317518,  1.96363987,  2.85360861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5890612942307087
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26993705,  15.98539849,   5.78839448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.021702597187397}
episode index:1933
target Thresh 66.1016030020766
target distance 6.0
model initialize at round 1933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.62060786,  19.23697532,   4.4396919 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.981921862095314}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5892584181737126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68860851,  15.05651084,   4.27274707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31647770418027765}
episode index:1934
target Thresh 66.11149645152535
target distance 5.0
model initialize at round 1934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.8559213 ,  18.43517945,   1.57034707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.039187517718572}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5894453595854574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16430948,  14.97089323,   4.70905111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8361972500333847}
episode index:1935
target Thresh 66.12138001246973
target distance 10.0
model initialize at round 1935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.31199978,   6.25807548,   1.06123114]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.049083971423936}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5896175183327933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70213328,  14.83171618,   0.16555512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3421169823151232}
episode index:1936
target Thresh 66.13125369479329
target distance 35.0
model initialize at round 1936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.83640805, 10.12315274]), 'previousTarget': array([99.36985865, 10.9808208 ]), 'currentState': array([79.68896008,  4.34602229,  5.53650427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6234408642195992
running average episode reward sum: 0.5896349800498232
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07779224,  15.76639822,   5.90267723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7703361995981316}
episode index:1937
target Thresh 66.14111750836973
target distance 63.0
model initialize at round 1937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.98989145, 17.86963045]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([51.03227184, 19.17094496,  1.25145888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.4236670686887061
running average episode reward sum: 0.5895493412926709
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1212326 ,  14.19089692,   5.62740922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1945207999457834}
episode index:1938
target Thresh 66.15097146306285
target distance 3.0
model initialize at round 1938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.95203969,  12.51312762,   6.14460194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.2215952023207417}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5897507598892193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33218839,  15.34788179,   2.18584925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7529900991668736}
episode index:1939
target Thresh 66.1608155687266
target distance 32.0
model initialize at round 1939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.20439553,  12.58286689]), 'previousTarget': array([102.40285  ,  11.8507125]), 'currentState': array([83.61153168,  8.56793402,  0.18891954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5898242406159023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78882642,  15.92338515,   3.83513624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9472245818171106}
episode index:1940
target Thresh 66.17064983520513
target distance 44.0
model initialize at round 1940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.65182814, 10.15786391]), 'previousTarget': array([90.50265712,  9.43242207]), 'currentState': array([70.00704146,  6.40521733,  1.28196573]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.589851436577504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37744086,  15.86630468,   5.60081155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9449578798771917}
episode index:1941
target Thresh 66.18047427233265
target distance 34.0
model initialize at round 1941
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.80797711,  8.8014702 ]), 'previousTarget': array([99.6810367 ,  9.14274933]), 'currentState': array([81.       ,  2.       ,  1.6678253], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.5392650943315962
running average episode reward sum: 0.5898253879975628
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52541879,  14.86447417,   4.88184132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4935530084462986}
episode index:1942
target Thresh 66.19028888993363
target distance 18.0
model initialize at round 1942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.99190049, 23.69046546,  0.71098602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.99539541607077}
done in step count: 22
reward sum = 0.6637235895390459
running average episode reward sum: 0.5898634210400443
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00028329,  15.85018796,   5.23471602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8501880041458983}
episode index:1943
target Thresh 66.2000936978227
target distance 8.0
model initialize at round 1943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.50047754,  19.17990408,   4.81910858]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.727573379229681}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5900491857668241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52039946,  15.73512337,   4.79914305]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9006786109005772}
episode index:1944
target Thresh 66.20988870580462
target distance 27.0
model initialize at round 1944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.56455771,  15.82543805]), 'previousTarget': array([107.87767469,  15.79136948]), 'currentState': array([89.79126508, 18.82825315,  0.14765126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5901792083928483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14373101,  15.48155093,   4.59815216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5025434306607525}
episode index:1945
target Thresh 66.21967392367445
target distance 11.0
model initialize at round 1945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.81804433,   5.80193119,   4.94376087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.264485376849413}
done in step count: 12
reward sum = 0.8177778717161293
running average episode reward sum: 0.5902961655682456
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66740081,  15.83798408,   5.94766654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9015761475618029}
episode index:1946
target Thresh 66.22944936121738
target distance 49.0
model initialize at round 1946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.75285548,  8.19377404]), 'previousTarget': array([85.33123116,  7.12869398]), 'currentState': array([66.27336587,  3.66062012,  2.41764283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 47
reward sum = 0.3069596948885352
running average episode reward sum: 0.590150640929992
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17267428,  15.74921695,   5.0159244 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1161513766265516}
episode index:1947
target Thresh 66.23921502820886
target distance 29.0
model initialize at round 1947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.00481656,  14.55679328]), 'previousTarget': array([105.89383588,  14.05798302]), 'currentState': array([87.03547544, 13.44980852,  1.42192066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5902718000587938
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16222635,  15.77526875,   4.99388153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7920599890385635}
episode index:1948
target Thresh 66.24897093441456
target distance 4.0
model initialize at round 1948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.94913142,  13.47493389,   6.07123446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.410810130121286}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.590471814527722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9461316 ,  15.02352334,   5.70974088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.0587805436223103}
episode index:1949
target Thresh 66.25871708959038
target distance 8.0
model initialize at round 1949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.75725219,  21.02380091,   4.75576189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.150658462972659}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5906566956740668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32924373,  15.5069271 ,   3.95062387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6044638262145586}
episode index:1950
target Thresh 66.26845350348248
target distance 5.0
model initialize at round 1950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.52285481,  18.80388658,   5.17417514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.080626305232941}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5908563078239006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87537757,  15.66378937,   4.83075574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6753866173980042}
episode index:1951
target Thresh 66.27818018582727
target distance 25.0
model initialize at round 1951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.11570102,  12.20755971]), 'previousTarget': array([108.03046115,  11.65462135]), 'currentState': array([89.58232976,  4.68997036,  0.90421236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5909481043597516
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34307621,  15.22423317,   5.00279084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6941393096573705}
episode index:1952
target Thresh 66.28789714635141
target distance 26.0
model initialize at round 1952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.66527526,  10.92001924]), 'previousTarget': array([106.88854382,  10.94427191]), 'currentState': array([9.01873255e+01, 1.19781409e+00, 4.18515762e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.415064724419671
running average episode reward sum: 0.5908580463055069
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70244381,  15.53422498,   4.67491246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6115030733657182}
episode index:1953
target Thresh 66.29760439477192
target distance 38.0
model initialize at round 1953
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.81429367, 12.71914807]), 'previousTarget': array([96.82908591, 12.60909025]), 'currentState': array([77.       , 10.       ,  4.3999243], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5053547499769284
running average episode reward sum: 0.5908142882214082
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72290818,  15.19814544,   3.77127941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3406486330450899}
episode index:1954
target Thresh 66.30730194079601
target distance 10.0
model initialize at round 1954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.13817804,   6.39376566,   1.11459255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.68117035416818}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5909985213475866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91948649,  14.52192195,   2.2852999 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4848103193019472}
episode index:1955
target Thresh 66.31698979412124
target distance 41.0
model initialize at round 1955
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.89102971, 11.08493098]), 'previousTarget': array([93.78922128, 11.8959836 ]), 'currentState': array([74.      ,  9.      ,  1.119224], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.4503405226503064
running average episode reward sum: 0.5909266103053078
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4729333 ,  14.33011678,   5.42162877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8523748200857775}
episode index:1956
target Thresh 66.32666796443544
target distance 63.0
model initialize at round 1956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.28211519, 12.56387388]), 'previousTarget': array([71.98992951, 13.63460094]), 'currentState': array([51.31309446, 11.45112442,  3.30421078]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5909338047135001
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04431812,  14.54480832,   0.97530824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0585496294240406}
episode index:1957
target Thresh 66.3363364614168
target distance 29.0
model initialize at round 1957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.72582634,  14.34220273]), 'previousTarget': array([105.98811999,  14.68924552]), 'currentState': array([86.78873091, 12.75720252,  6.1855315 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.591017454091687
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06821026,  15.52012582,   5.31547174]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5245793627860353}
episode index:1958
target Thresh 66.34599529473383
target distance 8.0
model initialize at round 1958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.36549494,  21.51404681,   5.96356797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.99446326987474}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5911820787947968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56891828,  14.83738056,   3.34515111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.591703551034506}
episode index:1959
target Thresh 66.35564447404535
target distance 12.0
model initialize at round 1959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.24131139,   4.49158458,   1.05634558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.982874826894271}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5913559988300581
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81180222,  14.54612339,   0.82344359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4913475196898341}
episode index:1960
target Thresh 66.36528400900053
target distance 63.0
model initialize at round 1960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.33979077, 14.50486752]), 'previousTarget': array([71.99748095, 14.31742033]), 'currentState': array([50.3410198 , 14.28314798,  1.96254563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 89
reward sum = 0.2232423194767035
running average episode reward sum: 0.5911682815024939
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38586209,  15.3192465 ,   5.58811752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6921587205038985}
episode index:1961
target Thresh 66.37491390923891
target distance 62.0
model initialize at round 1961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.79573576, 13.08231756]), 'previousTarget': array([72.95850618, 12.28764556]), 'currentState': array([51.81540823, 12.19546243,  1.34162712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.16654935729659365
running average episode reward sum: 0.5909518600324604
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26965944,  15.349936  ,   4.49876076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4417820942122942}
episode index:1962
target Thresh 66.3845341843904
target distance 3.0
model initialize at round 1962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.21708773,  17.96509844,   5.53585488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.702335311429228}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5911451087028463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28828906,  15.41772694,   4.02896566]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5075493856554585}
episode index:1963
target Thresh 66.39414484407529
target distance 31.0
model initialize at round 1963
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.9485546 ,  15.43358625]), 'previousTarget': array([103.98960229,  14.64482588]), 'currentState': array([84.     , 14.     ,  4.51764], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6482193154225053
running average episode reward sum: 0.5911741688895671
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89936369,  15.14483756,   6.20432805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1763677601295554}
episode index:1964
target Thresh 66.4037458979042
target distance 19.0
model initialize at round 1964
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.21037711,  16.56357972]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 12.       ,  6.0513268], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.773494461843647}
done in step count: 54
reward sum = 0.22802967856400247
running average episode reward sum: 0.5909893625331674
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79639218,  15.06510001,   5.64940536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.213761912002316}
episode index:1965
target Thresh 66.41333735547822
target distance 49.0
model initialize at round 1965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.57858094, 13.2775059 ]), 'previousTarget': array([85.96262067, 13.22220127]), 'currentState': array([67.6179226 , 12.02366428,  0.9621672 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5910290280448847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52457476,  15.0275929 ,   5.16854563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5252999578951177}
episode index:1966
target Thresh 66.4229192263888
target distance 19.0
model initialize at round 1966
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.83836038,  14.46239139]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 17.       ,  1.7489007], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7645137614500874
running average episode reward sum: 0.5911172256724421
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70551678,  14.51796446,   5.69894135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5648704566687848}
episode index:1967
target Thresh 66.43249152021781
target distance 59.0
model initialize at round 1967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.14210876,  8.97716399]), 'previousTarget': array([75.77129198,  9.01595979]), 'currentState': array([54.35592562,  6.0604952 ,  2.09481335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13175127271662335
running average episode reward sum: 0.5907499144435858
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44367083,  16.4882643 ,   0.13027881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5529888751737901}
episode index:1968
target Thresh 66.44205424653754
target distance 9.0
model initialize at round 1968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.26571455,  23.75607276,   5.99278146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.682807090329693}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5909232590009568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20420822,  15.60361128,   3.87612804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6372186205907533}
episode index:1969
target Thresh 66.45160741491073
target distance 64.0
model initialize at round 1969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.48075407, 18.56359253]), 'previousTarget': array([70.93924283, 18.44224665]), 'currentState': array([52.55062947, 20.23396182,  5.42293758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = -0.01097963899681531
running average episode reward sum: 0.5906177245349681
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63814606,  15.32669714,   5.69497582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4875133826684492}
episode index:1970
target Thresh 66.46115103489052
target distance 35.0
model initialize at round 1970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.07200834,  10.23871556]), 'previousTarget': array([98.91891892,  9.48648649]), 'currentState': array([81.01772706,  4.16135062,  6.12439853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.38569334354758233
running average episode reward sum: 0.5905137547830718
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08876163,  15.57923901,   5.28669233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5860003958431929}
episode index:1971
target Thresh 66.47068511602056
target distance 12.0
model initialize at round 1971
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.01685131,  13.94251791]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  14.      ,   6.114994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.017001267297735}
done in step count: 10
reward sum = 0.7650820750088044
running average episode reward sum: 0.5906022782720302
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37376709,  15.14613778,   5.97024488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4013204323375821}
episode index:1972
target Thresh 66.48020966783491
target distance 6.0
model initialize at round 1972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4843214 ,  19.18832741,   4.411057  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.216236913256785}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.590799692221208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55093385,  15.91090254,   3.64838874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0155805443594577}
episode index:1973
target Thresh 66.48972469985816
target distance 59.0
model initialize at round 1973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.43545773, 10.28512282]), 'previousTarget': array([75.89737675, 11.02346204]), 'currentState': array([54.56920171,  7.97603829,  4.63701367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.37012228016508897
running average episode reward sum: 0.5906879002191532
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94040906,  15.51061807,   4.51298279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5140835428191336}
episode index:1974
target Thresh 66.49923022160529
target distance 3.0
model initialize at round 1974
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.94117934,  16.61574334,   4.96471047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.52562937946455}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5908850709025866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58495151,  14.91741853,   3.93651325]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5907520334345154}
episode index:1975
target Thresh 66.50872624258186
target distance 38.0
model initialize at round 1975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.51374148, 17.13911233]), 'previousTarget': array([96.93796297, 16.42595029]), 'currentState': array([77.6617314 , 19.56762953,  2.14977436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 29
reward sum = 0.6881660707947213
running average episode reward sum: 0.5909343021778356
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69130052,  15.85090562,   5.98224808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.905171663564938}
episode index:1976
target Thresh 66.51821277228389
target distance 28.0
model initialize at round 1976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.34630237,  10.55339236]), 'previousTarget': array([105.14017935,  10.42222613]), 'currentState': array([88.55731569,  1.41271662,  0.642811  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5909771927933554
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73670335,  14.92292142,   5.31457311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27434692336333755}
episode index:1977
target Thresh 66.52768982019789
target distance 34.0
model initialize at round 1977
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.95635578,  13.32055449]), 'previousTarget': array([100.922597  ,  13.75787621]), 'currentState': array([81.       , 12.       ,  1.3928728], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.5109370170036236
running average episode reward sum: 0.5909367275882038
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76497792,  15.25291229,   3.23011429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8057020800800954}
episode index:1978
target Thresh 66.53715739580093
target distance 48.0
model initialize at round 1978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.03107496, 10.19554213]), 'previousTarget': array([86.65744374,  9.6857707 ]), 'currentState': array([68.34108203,  6.68781087,  5.81532079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.23415314025809597
running average episode reward sum: 0.5907564428043078
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43102307,  15.00228739,   4.60493503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5689815266209188}
episode index:1979
target Thresh 66.54661550856058
target distance 3.0
model initialize at round 1979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.8085318 ,  12.42848593,   1.78636599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.378641368271831}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5909530809645077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22448844,  14.51286669,   1.48732471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5363710629188496}
episode index:1980
target Thresh 66.55606416793493
target distance 61.0
model initialize at round 1980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.31744896,  9.96022832]), 'previousTarget': array([73.83019061,  9.60068074]), 'currentState': array([52.45542656,  7.61500631,  1.7526629 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.07546740462118073
running average episode reward sum: 0.5906928660849805
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49331912,  15.15249833,   3.72549175]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5291325482443093}
episode index:1981
target Thresh 66.56550338337267
target distance 27.0
model initialize at round 1981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.86907614,  16.25285715]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([89.27402769, 20.25711443,  0.4148193 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5908033781950391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1336051 ,  15.87616551,   6.26759362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8862935906354958}
episode index:1982
target Thresh 66.57493316431302
target distance 41.0
model initialize at round 1982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.5474718 , 19.52857709]), 'previousTarget': array([93.62981184, 19.16979281]), 'currentState': array([75.02041463, 23.85223736,  1.92937439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5908566358046481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13877716,  15.23271135,   4.24546054]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2709495739074605}
episode index:1983
target Thresh 66.58435352018574
target distance 59.0
model initialize at round 1983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.54270993, 17.61854075]), 'previousTarget': array([75.97419539, 16.98436295]), 'currentState': array([54.58447019, 18.91030953,  3.58009219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.5099857462495653
running average episode reward sum: 0.590815874267574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96351092,  14.50001339,   0.33790211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5013163270072474}
episode index:1984
target Thresh 66.5937644604112
target distance 23.0
model initialize at round 1984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.85026345,  15.33881744]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([93.09413472, 18.45255758,  1.6809859 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5909220781543606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54875047,  15.67992371,   5.41108314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8160406794596531}
episode index:1985
target Thresh 66.60316599440033
target distance 24.0
model initialize at round 1985
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.0720097 ,  14.92681278]), 'previousTarget': array([110.93091516,  14.6609096 ]), 'currentState': array([92.07825463, 14.42705452,  1.24324006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.5822874058622718
running average episode reward sum: 0.5909177303838208
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70604798,  15.27973194,   5.61646924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4057804158368688}
episode index:1986
target Thresh 66.61255813155466
target distance 62.0
model initialize at round 1986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.57342983, 17.13847917]), 'previousTarget': array([72.97662792, 17.03338897]), 'currentState': array([54.60135309, 18.19495935,  0.99724704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5909503129153709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21712668,  15.37167116,   5.20395738]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4304456315554894}
episode index:1987
target Thresh 66.62194088126637
target distance 10.0
model initialize at round 1987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.23034581,   6.07794   ,   1.69974333]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.006492404748764}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.591126635770746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.003616  ,  14.79357877,   0.14656198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2064529030564284}
episode index:1988
target Thresh 66.63131425291816
target distance 5.0
model initialize at round 1988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.41421191,  17.48252473,   5.35398507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.1126064455788685}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5912706249045964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.9971107 ,  15.99547618,   3.53815724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.4089721641770305}
episode index:1989
target Thresh 66.64067825588343
target distance 63.0
model initialize at round 1989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.89369025, 17.31083237]), 'previousTarget': array([71.98992951, 16.36539906]), 'currentState': array([52.92374152, 18.40680123,  6.26296288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.591222166630574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80070981,  14.39392829,   4.20880039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0042206540235823}
episode index:1990
target Thresh 66.65003289952617
target distance 65.0
model initialize at round 1990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.74229468, 14.44742797]), 'previousTarget': array([69.99053926, 13.61509352]), 'currentState': array([48.74372149, 14.20853474,  3.28005207]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4462434953618209
running average episode reward sum: 0.5911493496183848
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68944261,  15.47275923,   4.34288088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.565638740529603}
episode index:1991
target Thresh 66.65937819320104
target distance 23.0
model initialize at round 1991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.09883575,  13.50910956]), 'previousTarget': array([111.13347761,  13.82323232]), 'currentState': array([92.41663124,  6.36941529,  3.95254707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6491267556623643
running average episode reward sum: 0.5911784547419008
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39971219,  15.81150976,   4.47407025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9046092721766847}
episode index:1992
target Thresh 66.66871414625331
target distance 25.0
model initialize at round 1992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.81988627,  13.68841575]), 'previousTarget': array([109.25928039,  13.39259851]), 'currentState': array([89.25562221,  9.53635939,  1.05326593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.20339747253790835
running average episode reward sum: 0.5909838832505793
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.910453  ,  15.85622794,   4.6509535 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2498203612769982}
episode index:1993
target Thresh 66.67804076801895
target distance 26.0
model initialize at round 1993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.73195723,  15.57067144]), 'previousTarget': array([108.98522349,  15.23133756]), 'currentState': array([87.79332423, 17.136211  ,  1.37626147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5910584652416212
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86832161,  15.23268697,   2.57218691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2673619692282088}
episode index:1994
target Thresh 66.68735806782459
target distance 32.0
model initialize at round 1994
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.86028691,   8.65504153]), 'previousTarget': array([101.52933154,   9.52754094]), 'currentState': array([83.       ,  2.       ,  1.0022289], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.14751069008351747
running average episode reward sum: 0.5908361355297626
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80034688,  14.86895596,   4.48437953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8110041147157326}
episode index:1995
target Thresh 66.69666605498752
target distance 12.0
model initialize at round 1995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.42915802,   2.90298274,   2.24355686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.318111990225253}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5909710164010609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00608913,  15.10324718,   3.06127391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10342658112920872}
episode index:1996
target Thresh 66.70596473881572
target distance 20.0
model initialize at round 1996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.59208691,  14.99488649]), 'previousTarget': array([114.97504678,  14.99875234]), 'currentState': array([93.59221883, 14.92224739,  3.5716356 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5910724902454487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89177582,  15.61768951,   4.74200039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0848061759909295}
episode index:1997
target Thresh 66.7152541286079
target distance 62.0
model initialize at round 1997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.55269295, 10.47202397]), 'previousTarget': array([72.9352791, 11.6076838]), 'currentState': array([52.66552195,  8.35060238,  3.50231409]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5910450606833675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39220596,  15.08593387,   4.92844518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6138389248884141}
episode index:1998
target Thresh 66.72453423365343
target distance 34.0
model initialize at round 1998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.68590546,   9.70185263]), 'previousTarget': array([100.02889037,  10.15640571]), 'currentState': array([81.92949281,  2.75944785,  0.08222931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.12370331376811361
running average episode reward sum: 0.5908112729160262
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98827548,  15.94631946,   5.087396  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9463920843932669}
episode index:1999
target Thresh 66.73380506323241
target distance 65.0
model initialize at round 1999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.71590037, 15.01158741]), 'previousTarget': array([70., 15.]), 'currentState': array([51.71590109, 15.01694153,  0.91738349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5907867098175515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18769626,  15.10013412,   3.29673802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2127362850548339}
episode index:2000
target Thresh 66.74306662661569
target distance 60.0
model initialize at round 2000
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.97313629, 12.0362562 ]), 'previousTarget': array([74.95570316, 12.33038021]), 'currentState': array([55.       , 11.       ,  1.6491075], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6834314424486804
running average episode reward sum: 0.5901499191367588
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20089609,  18.15201372,   4.45291995]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.158409368705954}
episode index:2001
target Thresh 66.75231893306483
target distance 12.0
model initialize at round 2001
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.895706  ,  13.72999101]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  15.       ,   1.8450804], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.969472731178609}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.590267395827629
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.112483  ,  15.96217063,   4.29645151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9687232520574804}
episode index:2002
target Thresh 66.76156199183212
target distance 55.0
model initialize at round 2002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.23993905, 19.91805971]), 'previousTarget': array([79.83995823, 19.47491441]), 'currentState': array([58.4165641 , 22.57019192,  1.78595042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.3596558196468268
running average episode reward sum: 0.5901522627391712
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55961498,  15.5560951 ,   5.39255246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7889300924856637}
episode index:2003
target Thresh 66.77079581216064
target distance 39.0
model initialize at round 2003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.76049651, 10.21900116]), 'previousTarget': array([95.59205401, 11.01888287]), 'currentState': array([75.2961836 ,  5.62111874,  3.27271557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.017173582510199792
running average episode reward sum: 0.589866345234067
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.300867  ,  15.46258518,   2.38235506]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5518206228708465}
episode index:2004
target Thresh 66.78002040328421
target distance 64.0
model initialize at round 2004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.39679683, 21.17129433]), 'previousTarget': array([70.84555753, 20.51930531]), 'currentState': array([49.57745032, 23.85336515,  1.56785059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5840726362054193
running average episode reward sum: 0.5892808395076632
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([80.1603041 , 20.65114559]), 'previousTarget': array([78.85528343, 20.04869234]), 'currentState': array([60.41832575, 23.85337774,  1.54206194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2005
target Thresh 66.7892357744274
target distance 43.0
model initialize at round 2005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.00475284, 18.58743163]), 'previousTarget': array([91.86614761, 17.68998284]), 'currentState': array([72.24378189, 21.67029182,  0.42431128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.4966985833985035
running average episode reward sum: 0.5892346868376187
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01921968,  15.62614674,   2.32006665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6264416464013643}
episode index:2006
target Thresh 66.7984419348056
target distance 41.0
model initialize at round 2006
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.31235696, 16.96559846]), 'previousTarget': array([93.90549268, 17.05800071]), 'currentState': array([72.38699742, 18.69187986,  2.35996199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5893059716820715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6637605 ,  15.86302547,   4.57466892]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.088756619942949}
episode index:2007
target Thresh 66.80763889362498
target distance 60.0
model initialize at round 2007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.84941608, 19.3960514 ]), 'previousTarget': array([74.82455801, 20.3567256 ]), 'currentState': array([55.97431707, 21.62774116,  0.056616  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = -0.22536664733992873
running average episode reward sum: 0.5889002582263833
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47959038,  15.23535903,   5.73460024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5342291694080796}
episode index:2008
target Thresh 66.81682666008247
target distance 25.0
model initialize at round 2008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.7926792 ,  12.36723641]), 'previousTarget': array([108.03046115,  11.65462135]), 'currentState': array([90.38035552,  4.5578615 ,  0.32132888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 46
reward sum = 0.514738341902935
running average episode reward sum: 0.5888633433850077
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45002921,  15.26279756,   3.71137092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.609532956652267}
episode index:2009
target Thresh 66.82600524336587
target distance 20.0
model initialize at round 2009
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.45612091,  15.37305123]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([95.       , 10.       ,  0.5454003], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.222332808839848}
done in step count: 19
reward sum = 0.4988412597667311
running average episode reward sum: 0.58881855627873
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19346331,  14.10425462,   6.2275352 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.205346932320109}
episode index:2010
target Thresh 66.83517465265373
target distance 52.0
model initialize at round 2010
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.67939506,  7.56670856]), 'previousTarget': array([82.56699406,  8.13917182]), 'currentState': array([63.       ,  4.       ,  3.0769188], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3526463500577931
running average episode reward sum: 0.5887011160966211
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08980764,  15.82469133,   1.00351154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.829566878003386}
episode index:2011
target Thresh 66.8443348971155
target distance 48.0
model initialize at round 2011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.42323948, 15.43894936]), 'previousTarget': array([86.99566113, 14.41657627]), 'currentState': array([67.42577263, 15.75725634,  2.11090779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.5886424110894088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47923167,  15.88609473,   3.07369456]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0277954647988488}
episode index:2012
target Thresh 66.8534859859114
target distance 11.0
model initialize at round 2012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05190966e+02, 2.07144119e+01, 6.37893041e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.352164697991062}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5888037994830474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04459884,  14.84765532,   2.3803668 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.15873864428217135}
episode index:2013
target Thresh 66.86262792819252
target distance 16.0
model initialize at round 2013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.73747691,  14.09173155]), 'previousTarget': array([114.52228  ,  14.6118525]), 'currentState': array([97.50222918,  2.41197421,  1.86317229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5889471546089242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2653644 ,  15.11197809,   0.18531334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28802318437940766}
episode index:2014
target Thresh 66.8717607331008
target distance 8.0
model initialize at round 2014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.35043566,  21.54397102,   5.66832641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.027577793912702}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5891221089487714
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09219299,  14.19886016,   1.13670545]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8064270544857473}
episode index:2015
target Thresh 66.88088440976907
target distance 3.0
model initialize at round 2015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.0447285 ,  10.33586029,   4.08410382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.779713051742799}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5893063717965151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09449456,  14.79072314,   1.44690454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22962148464300822}
episode index:2016
target Thresh 66.88999896732096
target distance 9.0
model initialize at round 2016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.46096982,  19.51148555,   2.18120098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.464059450351794}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5894625818625598
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74800541,  15.72684973,   4.09736246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0429873540405394}
episode index:2017
target Thresh 66.89910441487109
target distance 40.0
model initialize at round 2017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.98308155, 19.30619836]), 'previousTarget': array([94.61161351, 19.0776773 ]), 'currentState': array([76.47692183, 23.72318069,  5.91414887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.10080621844935722
running average episode reward sum: 0.5892204330204324
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5740582 ,  14.94280692,   0.17848232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.429764428917044}
episode index:2018
target Thresh 66.90820076152485
target distance 24.0
model initialize at round 2018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.22347245,  14.5102822 ]), 'previousTarget': array([110.98266146,  14.83261089]), 'currentState': array([90.32776656, 12.47045701,  3.28830147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5893377922035999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21766831,  15.3461624 ,   4.02787513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8554947568477032}
episode index:2019
target Thresh 66.91728801637863
target distance 12.0
model initialize at round 2019
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.68817487,  16.01568535]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  12.       ,   4.6343417], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.417653473711445}
done in step count: 17
reward sum = 0.7729431933839268
running average episode reward sum: 0.5894286859665605
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9837876 ,  15.28849399,   3.93760538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28894917142965015}
episode index:2020
target Thresh 66.92636618851968
target distance 6.0
model initialize at round 2020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.57018013,   8.36949476,   0.61285477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.974139695318251}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5896028826332772
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08906717,  15.35680481,   0.28233677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.978319119915604}
episode index:2021
target Thresh 66.93543528702614
target distance 45.0
model initialize at round 2021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.51538676, 15.33982746]), 'previousTarget': array([89.99506356, 14.44433475]), 'currentState': array([70.51731281, 15.61738523,  2.2456001 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5275263127844209
running average episode reward sum: 0.589572182054717
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59631943,  15.18676235,   4.96166433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4447900373233849}
episode index:2022
target Thresh 66.94449532096715
target distance 23.0
model initialize at round 2022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.57968293,  15.80708857]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([93.60676075, 22.13387479,  5.37863702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.5447686592166991
running average episode reward sum: 0.5895500349846042
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07070362,  15.40462986,   4.53734959]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0135665214167695}
episode index:2023
target Thresh 66.9535462994027
target distance 50.0
model initialize at round 2023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.50396013, 17.30370078]), 'previousTarget': array([84.93630557, 17.40509555]), 'currentState': array([66.56899716, 18.91529979,  0.95606869]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5135758642332351
running average episode reward sum: 0.589512498338976
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89711659,  15.64540777,   4.002639  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6535565633414291}
episode index:2024
target Thresh 66.96258823138383
target distance 63.0
model initialize at round 2024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.20386798, 13.27520666]), 'previousTarget': array([71.95980905, 12.26728946]), 'currentState': array([51.21935963, 12.48817033,  1.06423938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = -0.041842897953971
running average episode reward sum: 0.5892007178963623
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18691559,  14.72650906,   6.13859577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8578482059484603}
episode index:2025
target Thresh 66.97162112595241
target distance 50.0
model initialize at round 2025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.93339265, 20.22309795]), 'previousTarget': array([84.74881264, 19.84018998]), 'currentState': array([66.24867718, 23.76032525,  6.00800962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.26965708727913684
running average episode reward sum: 0.5890429964597298
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42647597,  14.94415473,   5.85292517]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5762365064384601}
episode index:2026
target Thresh 66.98064499214138
target distance 63.0
model initialize at round 2026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.14234488,  8.60001308]), 'previousTarget': array([71.70193542,  7.44002047]), 'currentState': array([52.36168145,  5.64614202,  2.27740189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 96
reward sum = 0.3442553639777603
running average episode reward sum: 0.5889222329508585
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38556496,  14.50862832,   3.5435046 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6245850301444058}
episode index:2027
target Thresh 66.98965983897459
target distance 13.0
model initialize at round 2027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78987229,   2.83929815,   3.54520082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.186327080638593}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.589073325663535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80982408,  15.13835537,   5.13708902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.235178842588293}
episode index:2028
target Thresh 66.99866567546687
target distance 59.0
model initialize at round 2028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.01445689, 17.75934926]), 'previousTarget': array([75.97419539, 16.98436295]), 'currentState': array([55.06190944, 19.13624808,  3.27095544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.374648318020827
running average episode reward sum: 0.5889676455217692
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96500495,  14.91463115,   1.11967475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.09226318147164092}
episode index:2029
target Thresh 67.00766251062409
target distance 47.0
model initialize at round 2029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.44846345, 10.5884611 ]), 'previousTarget': array([87.7164234 , 10.35598696]), 'currentState': array([69.74004966,  7.18575246,  5.51098007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 56
reward sum = 0.440410420799815
running average episode reward sum: 0.5888944646228913
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45306648,  15.60325178,   5.69317635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7544414793997205}
episode index:2030
target Thresh 67.01665035344307
target distance 28.0
model initialize at round 2030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.56224556,  15.14914377]), 'previousTarget': array([107.,  15.]), 'currentState': array([88.56761052, 15.61236046,  5.64559073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.7229194351610663
running average episode reward sum: 0.5889604542686511
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33924981,  15.3326321 ,   5.07453663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7397532849816052}
episode index:2031
target Thresh 67.02562921291167
target distance 11.0
model initialize at round 2031
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.76238381,  16.53860657]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  12.       ,   3.8793285], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.68023351472959}
done in step count: 20
reward sum = 0.6786069375972308
running average episode reward sum: 0.5890045716324939
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26011508,  15.79958536,   2.75251479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0893881055584724}
episode index:2032
target Thresh 67.03459909800871
target distance 36.0
model initialize at round 2032
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.74323243, 18.80550894]), 'previousTarget': array([98.63230779, 18.18260682]), 'currentState': array([79.      , 22.      ,  4.573331], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4347046020847181
running average episode reward sum: 0.5889286739593272
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89651964,  15.61074969,   2.09960617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6194540914396679}
episode index:2033
target Thresh 67.04356001770411
target distance 24.0
model initialize at round 2033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.38038577,  13.9308866 ]), 'previousTarget': array([110.40285  ,  13.8507125]), 'currentState': array([89.73278636, 10.19299449,  1.47675276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5889849763295535
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14993274,  15.88536054,   4.47529366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2273865088890181}
episode index:2034
target Thresh 67.05251198095877
target distance 35.0
model initialize at round 2034
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.96623776, 15.83838493]), 'previousTarget': array([99.96742669, 15.85900419]), 'currentState': array([80.       , 17.       ,  5.2565846], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.459656006466161
running average episode reward sum: 0.5889214240102103
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82468926,  15.8156905 ,   4.21846216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.159941110170123}
episode index:2035
target Thresh 67.06145499672469
target distance 36.0
model initialize at round 2035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.86926873, 16.87634129]), 'previousTarget': array([98.93091516, 16.3390904 ]), 'currentState': array([80.02129891, 19.33766088,  1.9287135 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.37157321457676973
running average episode reward sum: 0.5888146714515494
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40888708,  15.54161671,   4.58669664]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8017251090400251}
episode index:2036
target Thresh 67.07038907394484
target distance 51.0
model initialize at round 2036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.24058297,  9.92638444]), 'previousTarget': array([83.62627499,  8.84828921]), 'currentState': array([64.50722321,  6.67146316,  2.13108861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.36389391790763653
running average episode reward sum: 0.588704253801307
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17530654,  15.10930549,   4.64078058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20659155706656496}
episode index:2037
target Thresh 67.07931422155333
target distance 10.0
model initialize at round 2037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08296872,   6.43010509,   3.12403977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.570296524493983}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5888727332390428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68402565,  14.63598056,   0.91079211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48202690886256533}
episode index:2038
target Thresh 67.0882304484753
target distance 11.0
model initialize at round 2038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59772836,   3.69967169,   2.34257793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.316125619701108}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.588977077454982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31966071,  15.99740391,   3.43724163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0473764932972038}
episode index:2039
target Thresh 67.09713776362698
target distance 17.0
model initialize at round 2039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.70621159,  7.47480476,  5.16016251]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.044897394460595}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5891272545024347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22146442,  15.0803421 ,   2.36153829]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23558722687762956}
episode index:2040
target Thresh 67.10603617591566
target distance 62.0
model initialize at round 2040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.02381721,  7.38168769]), 'previousTarget': array([72.69246532,  7.49382449]), 'currentState': array([51.31733821,  3.96779175,  2.15073156]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5484072646004898
running average episode reward sum: 0.5885699127488322
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.58031884,  18.65787996,   5.42618883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.385765919698324}
episode index:2041
target Thresh 67.1149256942398
target distance 44.0
model initialize at round 2041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.7250774,  9.1550878]), 'previousTarget': array([90.29527642,  8.26234812]), 'currentState': array([71.28077771,  4.47329226,  2.39247617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5886296499253862
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08069294,  15.45218802,   4.32872436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0244996239636086}
episode index:2042
target Thresh 67.12380632748888
target distance 13.0
model initialize at round 2042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.05028083,   3.38939612,   3.18039525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.658010646729757}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5887797765060683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79112153,  14.82079088,   4.1510681 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8111653253776444}
episode index:2043
target Thresh 67.13267808454356
target distance 37.0
model initialize at round 2043
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.47574101, 12.54923205]), 'previousTarget': array([97.65140491, 11.71783336]), 'currentState': array([78.      ,  8.      ,  5.615726], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = -0.03303450190266455
running average episode reward sum: 0.5884755620841462
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55048551,  15.78336984,   6.06605743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9574458747084748}
episode index:2044
target Thresh 67.14154097427557
target distance 5.0
model initialize at round 2044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.7775222 ,   8.84330811,   2.88934708]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.408154195281613}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.588657528073347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20345113,  15.42867122,   2.71224582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4745011822404794}
episode index:2045
target Thresh 67.15039500554782
target distance 40.0
model initialize at round 2045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.23167308, 12.95610956]), 'previousTarget': array([94.9007438 , 12.99007438]), 'currentState': array([73.31925312, 11.08647554,  2.08018732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.20540410740503806
running average episode reward sum: 0.5884702096859237
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.9220646 ,  15.67865585,   4.85130626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1448916478139184}
episode index:2046
target Thresh 67.15924018721434
target distance 45.0
model initialize at round 2046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.26941675, 19.59724569]), 'previousTarget': array([89.69125016, 19.49933331]), 'currentState': array([71.63447291, 23.401057  ,  5.51383382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.46174552759976
running average episode reward sum: 0.5884083021714702
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77803391,  15.09419316,   4.43988284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24112506353711508}
episode index:2047
target Thresh 67.16807652812031
target distance 8.0
model initialize at round 2047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30177221,   8.06666639,   1.69645613]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.968402756993044}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5885715523630016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30382215,  15.60539742,   5.44361115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6773580612593649}
episode index:2048
target Thresh 67.17690403710208
target distance 61.0
model initialize at round 2048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.61251343,  5.55196238]), 'previousTarget': array([73.56072872,  6.16867989]), 'currentState': array([53.0915651 ,  1.20080431,  3.06825781]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2671304971477733
running average episode reward sum: 0.5881539330123375
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.92159454,  17.73786668,   4.59695224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.942596185017658}
episode index:2049
target Thresh 67.18572272298714
target distance 23.0
model initialize at round 2049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.7106631 ,  15.68075856]), 'previousTarget': array([111.83200822,  15.41321632]), 'currentState': array([92.12569821, 19.73404736,  0.58550799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5881933563418776
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70004559,  15.47950037,   2.32683588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5655910639257641}
episode index:2050
target Thresh 67.1945325945942
target distance 46.0
model initialize at round 2050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.18851729, 18.74096991]), 'previousTarget': array([88.83200822, 18.41321632]), 'currentState': array([67.36703243, 21.4071916 ,  1.88721943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5215953103659995
running average episode reward sum: 0.5881608853297002
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12378964,  14.6891713 ,   3.19294147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3345718960303435}
episode index:2051
target Thresh 67.20333366073311
target distance 22.0
model initialize at round 2051
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.64361846,  14.24124303]), 'previousTarget': array([112.81660336,  15.29773591]), 'currentState': array([93.       , 18.       ,  1.6370003], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.5308408710135986
running average episode reward sum: 0.5881329515995267
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79300652,  14.88644678,   4.33653543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8010952937188742}
episode index:2052
target Thresh 67.21212593020495
target distance 4.0
model initialize at round 2052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.02173227,  17.87890758,   5.74177146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.142244263650178}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5882739102314797
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82278988,  15.92060457,   3.85510696]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2347048042622073}
episode index:2053
target Thresh 67.22090941180198
target distance 9.0
model initialize at round 2053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.67721354,  23.13907841,   5.35591835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.948415359219878}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5884147316106264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34491288,  15.73873312,   0.35030425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8152861599799005}
episode index:2054
target Thresh 67.2296841143077
target distance 7.0
model initialize at round 2054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.10121299,  19.22443728,   5.6070888 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.089445754889834}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.5883119685523844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91247204,  15.779958  ,   2.89626072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2003914838893723}
episode index:2055
target Thresh 67.23845004649681
target distance 3.0
model initialize at round 2055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.3243514 ,  17.89615253,   2.58482444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.184588842739491}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5883749153247804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34706804,  15.82468911,   4.3106961 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8947448552118686}
episode index:2056
target Thresh 67.24720721713523
target distance 64.0
model initialize at round 2056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.07606124, 20.36598415]), 'previousTarget': array([70.88143384, 19.82546817]), 'currentState': array([52.23053169, 22.84690273,  5.90393883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.2608968627062163
running average episode reward sum: 0.5879620462056598
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([109.56699429,  16.85561628]), 'previousTarget': array([110.95480808,  16.57855995]), 'currentState': array([90.64046979, 23.31987729,  4.16932172]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2057
target Thresh 67.25595563498014
target distance 25.0
model initialize at round 2057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.65597827,  14.50407549]), 'previousTarget': array([109.98401917,  14.79936077]), 'currentState': array([88.71680814, 12.94539181,  3.03769171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 24
reward sum = 0.6670700732937026
running average episode reward sum: 0.588000485480241
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31267248,  15.79075223,   0.67044073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0477157064045013}
episode index:2058
target Thresh 67.26469530877995
target distance 66.0
model initialize at round 2058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.06664098, 12.48047133]), 'previousTarget': array([68.94285376, 11.51082225]), 'currentState': array([48.0953976 , 11.40835288,  1.1317482 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5877149097223584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06120392,  17.35049604,   2.50756268]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.5310412321878615}
episode index:2059
target Thresh 67.27342624727434
target distance 47.0
model initialize at round 2059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.00103531,  9.8854523 ]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([69.37714579,  6.02501707,  1.59188014]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.430447036084341
running average episode reward sum: 0.5872206563506075
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([103.38114799,  18.33507356]), 'previousTarget': array([102.23673514,  18.49184064]), 'currentState': array([84.15741465, 23.85305113,  1.77698542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:2060
target Thresh 67.28214845919426
target distance 3.0
model initialize at round 2060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.81896968,  16.5194117 ,   4.28283286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.370076495855568}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5874160854353476
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13818585,  15.99968276,   2.58314002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.00918826739464}
episode index:2061
target Thresh 67.29086195326191
target distance 18.0
model initialize at round 2061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.60225317,  14.7617582 ]), 'previousTarget': array([114.88854382,  14.94427191]), 'currentState': array([97.44466717,  4.48473321,  4.07714784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.10454574735954936
running average episode reward sum: 0.5871819097136813
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6674485 ,  14.92259835,   3.5573711 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.671921510328393}
episode index:2062
target Thresh 67.29956673819078
target distance 10.0
model initialize at round 2062
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.39714865,   6.34509165,   2.96647108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.802738347705096}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5873269426569689
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61789437,  14.28150867,   3.27205286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9476408844778248}
episode index:2063
target Thresh 67.30826282268568
target distance 7.0
model initialize at round 2063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92244672,  20.49595511,   4.39762807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.572829668515712}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5874761729435977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18605564,  15.39760318,   3.5205586 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9058662742700239}
episode index:2064
target Thresh 67.31695021544266
target distance 57.0
model initialize at round 2064
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.86034055, 20.64057774]), 'previousTarget': array([77.80587954, 20.22022743]), 'currentState': array([58.       , 23.       ,  4.9613934], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3969267834347576
running average episode reward sum: 0.5869994644901456
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([110.90998666,  16.59305342]), 'previousTarget': array([109.48275956,  16.99489732]), 'currentState': array([92.27373258, 23.85184354,  1.19569499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2065
target Thresh 67.32562892514913
target distance 12.0
model initialize at round 2065
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.34491923,  15.02588294]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  17.      ,   5.336058], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.531594946971397}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.5871280923845976
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88061363,  15.19314215,   3.46383721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22706165562435907}
episode index:2066
target Thresh 67.33429896048382
target distance 58.0
model initialize at round 2066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.21330842,  7.0179246 ]), 'previousTarget': array([76.58520839,  7.05211208]), 'currentState': array([58.6681248 ,  2.77695803,  5.14026547]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.1071235841393709
running average episode reward sum: 0.5868958695939613
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41179801,  15.55380523,   4.59604809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6901288586289313}
episode index:2067
target Thresh 67.34296033011674
target distance 46.0
model initialize at round 2067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.57970593, 13.90780591]), 'previousTarget': array([88.98112317, 13.86874449]), 'currentState': array([70.5996791 , 13.01420213,  0.89344805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.603880194436964
running average episode reward sum: 0.5869040825169994
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91369029,  14.20184533,   1.85651729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2132109513068736}
episode index:2068
target Thresh 67.35161304270926
target distance 66.0
model initialize at round 2068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.81563426, 11.72208776]), 'previousTarget': array([68.91786413, 10.81071492]), 'currentState': array([49.86805561, 10.27498518,  6.24623365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.12159477829024246
running average episode reward sum: 0.5866791867682191
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06487909,  14.45837513,   3.90163526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.545496829825478}
episode index:2069
target Thresh 67.3602571069141
target distance 13.0
model initialize at round 2069
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.321446 ,  15.1218382]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.        ,  12.        ,   0.91754186], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.743977742211307}
done in step count: 13
reward sum = 0.8075210229989678
running average episode reward sum: 0.5867858736456252
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91276451,  15.42203355,   1.48928889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43095515552972824}
episode index:2070
target Thresh 67.36889253137535
target distance 29.0
model initialize at round 2070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.91797242,  15.96425202]), 'previousTarget': array([105.95260657,  15.62395817]), 'currentState': array([87.058815  , 18.33361199,  1.50192264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5868669617255663
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45159488,  15.23022883,   0.29948916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5947717998337897}
episode index:2071
target Thresh 67.3775193247284
target distance 3.0
model initialize at round 2071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47402144,  17.06394344,   0.33335608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.1176777049861575}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5869946599926364
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.81336618,  15.06596897,   5.55998619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8160370403772824}
episode index:2072
target Thresh 67.38613749560005
target distance 3.0
model initialize at round 2072
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29225218,  13.57005714,   0.37676144]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5955073045933847}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5871890668136723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76881727,  15.2304861 ,   2.22430634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3264495336568747}
episode index:2073
target Thresh 67.3947470526085
target distance 3.0
model initialize at round 2073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.68478599,  14.81991329,   0.88422805]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3274860122837002}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5873333270860457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93262281,  14.10556716,   1.53677884]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8969669977243635}
episode index:2074
target Thresh 67.40334800436325
target distance 25.0
model initialize at round 2074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.31305375,  12.4559992 ]), 'previousTarget': array([108.81774824,  12.77438937]), 'currentState': array([88.32586003,  6.17217276,  2.02911043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4570662647100716
running average episode reward sum: 0.5872705477788764
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69286947,  15.95644444,   4.86223494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0045472229776897}
episode index:2075
target Thresh 67.41194035946532
target distance 13.0
model initialize at round 2075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.4791173 ,   2.1160617 ,   2.08281326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.283709219360343}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5874019484565559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8088335 ,  14.78740156,   6.24215981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2859068520281931}
episode index:2076
target Thresh 67.42052412650702
target distance 30.0
model initialize at round 2076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.51810153,  15.90761327]), 'previousTarget': array([104.98889814,  15.3337034 ]), 'currentState': array([84.59265846, 17.63293017,  0.80526137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6502370170036236
running average episode reward sum: 0.5874322012579747
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54237241,  14.79125245,   4.18618218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5811569214122317}
episode index:2077
target Thresh 67.42909931407212
target distance 64.0
model initialize at round 2077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.94270242, 19.60366162]), 'previousTarget': array([70.93924283, 18.44224665]), 'currentState': array([51.05100338, 21.682197  ,  0.53048611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5874495704560658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56777071,  14.99560898,   3.7312496 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5677876863794485}
episode index:2078
target Thresh 67.43766593073583
target distance 42.0
model initialize at round 2078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.23227515, 15.89236891]), 'previousTarget': array([93., 15.]), 'currentState': array([73.24905994, 16.71158177,  2.35546982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5875053656097664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64456576,  15.81464177,   1.12729967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0388003830541326}
episode index:2079
target Thresh 67.44622398506476
target distance 29.0
model initialize at round 2079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.68116092,  13.59225702]), 'previousTarget': array([105.58520839,  13.05211208]), 'currentState': array([86.96151959, 10.25523049,  6.22518236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5853186835320813
running average episode reward sum: 0.5875043143203059
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91459455,  15.60618936,   6.10821146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6121761467560052}
episode index:2080
target Thresh 67.45477348561697
target distance 61.0
model initialize at round 2080
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.75901774,  7.09535426]), 'previousTarget': array([73.68254035,  7.54931055]), 'currentState': array([54.       ,  4.       ,  2.2956583], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.17694777624864888
running average episode reward sum: 0.5873070262193585
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34479969,  15.41452468,   4.2171104 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.775318097012554}
episode index:2081
target Thresh 67.46331444094194
target distance 37.0
model initialize at round 2081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.735294  , 17.46394437]), 'previousTarget': array([97.88414095, 16.85036314]), 'currentState': array([76.91483253, 20.13776318,  1.32253838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.4496190336797875
running average episode reward sum: 0.5872408936581003
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6509952 ,  14.10091801,   3.19915278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1100194457965658}
episode index:2082
target Thresh 67.47184685958064
target distance 6.0
model initialize at round 2082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.25580466,  19.71917349,   0.50925446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.434136032726135}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5873975313699705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90063029,  14.4842585 ,   4.54443297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0378459519034315}
episode index:2083
target Thresh 67.4803707500655
target distance 50.0
model initialize at round 2083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.38143435, 15.59967795]), 'previousTarget': array([85., 15.]), 'currentState': array([66.38582368, 16.01866914,  1.580262  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.40719083984158944
running average episode reward sum: 0.5869202816716924
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.00507051,  23.84284012,   0.96265617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.109723449738537}
episode index:2084
target Thresh 67.48888612092038
target distance 60.0
model initialize at round 2084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.30938364, 10.60304804]), 'previousTarget': array([74.9007438 , 10.99007438]), 'currentState': array([53.41969548,  8.5053577 ,  2.434937  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.4939178005075574
running average episode reward sum: 0.5868756761651388
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68619798,  14.33933556,   4.11838336]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9525466752444375}
episode index:2085
target Thresh 67.49739298066069
target distance 6.0
model initialize at round 2085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.06164248,  18.88110998,   0.16882568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.280954524218577}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5870366871997805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78153554,  15.4653075 ,   0.59478724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9095652086659349}
episode index:2086
target Thresh 67.50589133779327
target distance 34.0
model initialize at round 2086
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.11569912,   8.8813304 ]), 'previousTarget': array([99.85980667,  9.65640235]), 'currentState': array([81.       ,  3.       ,  2.3521488], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.49688548606387095
running average episode reward sum: 0.5869934906491644
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64665388,  15.48836328,   3.30606435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8103455645691631}
episode index:2087
target Thresh 67.51438120081647
target distance 22.0
model initialize at round 2087
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([110.86935991,  11.98253731]), 'previousTarget': array([110.55791146,  12.57704261]), 'currentState': array([93.       ,  3.       ,  1.0371239], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.29664845135643203
running average episode reward sum: 0.5868544365115721
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63357048,  15.61501143,   0.19020985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.882978265018709}
episode index:2088
target Thresh 67.52286257822017
target distance 22.0
model initialize at round 2088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.16763827,  14.48843834]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([91.34347359, 11.84221562,  2.2265563 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5869611255645686
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70939182,  15.96408002,   4.76642251]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.196949051567405}
episode index:2089
target Thresh 67.53133547848574
target distance 5.0
model initialize at round 2089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.41249172,  12.07449539,   0.42032069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.629124418511972}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5871086744299724
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98413721,  14.19329902,   4.00731918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2725142564341603}
episode index:2090
target Thresh 67.53979991008609
target distance 24.0
model initialize at round 2090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.85416635,  14.73682684]), 'previousTarget': array([111.,  15.]), 'currentState': array([89.88027117, 13.71530272,  3.03591752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5957895542805092
running average episode reward sum: 0.5871128259746164
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93749197,  14.34347968,   4.31370142]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.144513049927374}
episode index:2091
target Thresh 67.54825588148564
target distance 35.0
model initialize at round 2091
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.67137519, 19.38932165]), 'previousTarget': array([99.49717013, 18.54350397]), 'currentState': array([80.       , 23.       ,  4.2686076], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.29993977536731464
running average episode reward sum: 0.5869755539618979
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91595702,  14.81553604,   4.32861746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9343469439216676}
episode index:2092
target Thresh 67.55670340114037
target distance 7.0
model initialize at round 2092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.51963388,  14.60074005,   0.24690723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.492653814984984}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5871228844446006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28707129,  14.24738356,   1.05317367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0366768370126291}
episode index:2093
target Thresh 67.56514247749779
target distance 25.0
model initialize at round 2093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.95249667,  13.67954096]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([90.60363792,  8.61775618,  2.14803183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6198942445867487
running average episode reward sum: 0.5871385345688327
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08463841,  14.22018919,   1.19310093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2024939685647853}
episode index:2094
target Thresh 67.57357311899699
target distance 7.0
model initialize at round 2094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.08930549,   9.32725966,   1.16216397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.734427079673127}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.587272953317377
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61302312,  14.17739964,   1.62944541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9090778117819558}
episode index:2095
target Thresh 67.58199533406861
target distance 51.0
model initialize at round 2095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.19742624,  8.5450941 ]), 'previousTarget': array([83.46834337,  7.58078667]), 'currentState': array([64.62261534,  4.4430485 ,  0.15346813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.22459287031776815
running average episode reward sum: 0.5870999189266328
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97051753,  14.25027435,   4.35258562]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.226373849794132}
episode index:2096
target Thresh 67.59040913113485
target distance 30.0
model initialize at round 2096
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.67566896,   9.15677224]), 'previousTarget': array([103.35111251,   9.95214875]), 'currentState': array([85.       ,  2.       ,  3.4239824], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 45
reward sum = 0.45613194052007255
running average episode reward sum: 0.5870374640013077
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79923392,  14.2819465 ,   4.53948776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.07441876288945}
episode index:2097
target Thresh 67.59881451860954
target distance 42.0
model initialize at round 2097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.77824242, 12.11333514]), 'previousTarget': array([92.72787848, 11.28797975]), 'currentState': array([71.93100065,  9.64615171,  1.80836302]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.44119838489090274
running average episode reward sum: 0.5869679506175564
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25207281,  15.05317458,   1.20826537]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25762034002461254}
episode index:2098
target Thresh 67.60721150489803
target distance 13.0
model initialize at round 2098
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.93740059,  18.58179706,   6.2775445 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.583146453292061}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5871063751398914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69005638,  14.11840901,   3.27532905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9344879434209185}
episode index:2099
target Thresh 67.61560009839734
target distance 50.0
model initialize at round 2099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.41687464,  8.73401583]), 'previousTarget': array([84.44774604,  7.66745905]), 'currentState': array([64.8238782 ,  4.71973005,  0.74209654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.395744187626631
running average episode reward sum: 0.5870152502886946
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.9029088 ,  15.17805502,   4.51789456]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9202977164919567}
episode index:2100
target Thresh 67.62398030749604
target distance 43.0
model initialize at round 2100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.2694695 , 13.92637384]), 'previousTarget': array([91.97840172, 13.92922799]), 'currentState': array([70.28828977, 13.05893118,  2.1481061 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5608517170930898
running average episode reward sum: 0.5870027973933136
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75332384,  14.85671941,   4.39503477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7668286187902111}
episode index:2101
target Thresh 67.63235214057434
target distance 13.0
model initialize at round 2101
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.07427937,   3.11853639,   1.81557816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.513211421525817}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.587078995917071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93343962,  15.41388281,   0.23796553]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41920073998185653}
episode index:2102
target Thresh 67.64071560600411
target distance 64.0
model initialize at round 2102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.73721341, 19.81538056]), 'previousTarget': array([70.84555753, 20.51930531]), 'currentState': array([49.84944395, 21.93118442,  4.900177  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.14984526885425964
running average episode reward sum: 0.5868710863939789
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15785608,  15.86868682,   3.93320864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8829129801508304}
episode index:2103
target Thresh 67.64907071214877
target distance 50.0
model initialize at round 2103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.74946339, 12.37231196]), 'previousTarget': array([84.96409691, 13.19784581]), 'currentState': array([65.82967939, 10.58284126,  6.25204325]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.4043500767860835
running average episode reward sum: 0.5867843368646977
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90580059,  15.75983889,   5.11936338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.182298547678598}
episode index:2104
target Thresh 67.65741746736344
target distance 21.0
model initialize at round 2104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.44707138,  12.57530311]), 'previousTarget': array([111.00530293,  12.52709229]), 'currentState': array([92.79432999,  3.17420097,  1.35941672]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.46735781157784606
running average episode reward sum: 0.58672760217335
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06707059,  15.01928599,   4.24487463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.06978834818908268}
episode index:2105
target Thresh 67.6657558799949
target distance 34.0
model initialize at round 2105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.94668386, 18.29703968]), 'previousTarget': array([100.69567118,  17.52429332]), 'currentState': array([80.40980229, 22.57608843,  0.91910219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5868296453772273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96048079,  15.90074687,   0.69551694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9016133821701308}
episode index:2106
target Thresh 67.67408595838154
target distance 8.0
model initialize at round 2106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.57801506,   5.44298788,   3.43772459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.898618204760798}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5869890734973272
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66483982,  15.75010184,   0.73368853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8215747788266526}
episode index:2107
target Thresh 67.68240771085344
target distance 21.0
model initialize at round 2107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.93726999,  15.66149472]), 'previousTarget': array([113.45612429,  15.36758945]), 'currentState': array([92.38804161, 19.88377723,  2.20356846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5871025362821176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16127811,  15.50427422,   5.972204  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5294366058666117}
episode index:2108
target Thresh 67.69072114573237
target distance 30.0
model initialize at round 2108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.92206655,  10.57650872]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([86.608539  ,  2.53818124,  0.72022205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.24531488762108927
running average episode reward sum: 0.5869404748081201
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.96747773,  15.76777755,   5.94627947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2351095183513818}
episode index:2109
target Thresh 67.69902627133175
target distance 47.0
model initialize at round 2109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.62591132,  9.57259617]), 'previousTarget': array([87.47376358,  8.55768935]), 'currentState': array([68.00779041,  5.68295199,  2.57616735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.24172168067834265
running average episode reward sum: 0.586776864005215
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90742956,  14.6795896 ,   5.34578145]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9623363415665679}
episode index:2110
target Thresh 67.7073230959567
target distance 41.0
model initialize at round 2110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.42598822, 12.7603016 ]), 'previousTarget': array([93.78922128, 11.8959836 ]), 'currentState': array([74.54345144, 10.59587756,  1.62098265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = -0.001444589016646125
running average episode reward sum: 0.586498218125053
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69530201,  14.98176874,   5.47748958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30524292572507494}
episode index:2111
target Thresh 67.71561162790405
target distance 50.0
model initialize at round 2111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.53980732, 18.31178128]), 'previousTarget': array([84.93630557, 17.40509555]), 'currentState': array([65.66499442, 20.54601763,  0.15458536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.18148256925957956
running average episode reward sum: 0.5863064493519159
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53031597,  15.61858852,   5.84189097]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8147924834055938}
episode index:2112
target Thresh 67.72389187546234
target distance 3.0
model initialize at round 2112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.67755948,  16.53231466,   4.2137475 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.9840220817304783}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5864928163896103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39150306,  15.29236175,   4.41195583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48862054498630136}
episode index:2113
target Thresh 67.73216384691183
target distance 23.0
model initialize at round 2113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.93164803,  13.92457556]), 'previousTarget': array([110.62485559,  13.28798697]), 'currentState': array([93.05736208,  7.30934087,  6.16465217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7163781408072187
running average episode reward sum: 0.5865542569404226
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85520572,  15.52153644,   5.30653471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5412630076762236}
episode index:2114
target Thresh 67.74042755052447
target distance 45.0
model initialize at round 2114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.28589126, 11.16037161]), 'previousTarget': array([89.76232938, 11.07414013]), 'currentState': array([68.48932876,  8.31500461,  1.9311955 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5865932250262993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70866007,  15.34359997,   5.05300808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45048850557039005}
episode index:2115
target Thresh 67.74868299456398
target distance 49.0
model initialize at round 2115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.10599921,  9.91919919]), 'previousTarget': array([85.59608118,  8.99920024]), 'currentState': array([65.38875377,  6.56804532,  0.93249106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.36759502385683923
running average episode reward sum: 0.586489728711947
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99167029,  15.97562084,   5.52980395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9756564020893641}
episode index:2116
target Thresh 67.75693018728577
target distance 50.0
model initialize at round 2116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.89626071, 16.73295132]), 'previousTarget': array([84.93630557, 17.40509555]), 'currentState': array([65.93162152, 17.9217246 ,  0.15176504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5865132033257174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30918434,  15.45199755,   4.58397172]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8255471274339198}
episode index:2117
target Thresh 67.76516913693709
target distance 7.0
model initialize at round 2117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.46883699,   8.13351873,   2.06430888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.19151516707238}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5866675961699847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40571715,  15.13175763,   5.96703095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42657529133394284}
episode index:2118
target Thresh 67.77339985175685
target distance 26.0
model initialize at round 2118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.44950068,  13.41519152]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([87.87601562,  9.30682414,  1.34138787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5853186835320813
running average episode reward sum: 0.5866669595901649
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49980626,  15.56202962,   5.06196604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7521193958529162}
episode index:2119
target Thresh 67.78162233997578
target distance 45.0
model initialize at round 2119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.43544078, 17.5011377 ]), 'previousTarget': array([89.92145281, 17.22920419]), 'currentState': array([71.54715434, 19.61207653,  0.28958856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5119977709863196
running average episode reward sum: 0.5861487215097044
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([105.92065724,  17.66691121]), 'previousTarget': array([104.66212773,  17.52236973]), 'currentState': array([86.73135527, 23.30346067,  6.14758872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2120
target Thresh 67.78983660981636
target distance 29.0
model initialize at round 2120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.35955803,  14.92512653]), 'previousTarget': array([105.95260657,  14.37604183]), 'currentState': array([86.36030889, 14.75182368,  0.55006945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.527656883555216
running average episode reward sum: 0.5861211440283491
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74700758,  15.28783252,   4.27494114]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8005422457579484}
episode index:2121
target Thresh 67.79804266949287
target distance 11.0
model initialize at round 2121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.60691876,  18.26445295,   0.43014484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.944175598777173}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5862886082156125
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51452913,  14.76781764,   6.27429831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5644899254433182}
episode index:2122
target Thresh 67.80624052721136
target distance 3.0
model initialize at round 2122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.20362194,  10.38058257,   3.47032583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.118101901083571}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5864559146410413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96986796,  15.7163917 ,   4.68311251]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7170251141443083}
episode index:2123
target Thresh 67.81443019116969
target distance 31.0
model initialize at round 2123
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.59741246,  10.00806993]), 'previousTarget': array([102.65136197,  10.21988205]), 'currentState': array([85.27621221,  1.98724569,  0.2601698 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5865280636329185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66054285,  15.81447216,   5.18309597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0486571153430795}
episode index:2124
target Thresh 67.82261166955755
target distance 12.0
model initialize at round 2124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.93032763,   4.00310581,   3.31438148]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.189960928724348}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.586677641991213
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62257475,  14.79985932,   3.96412087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6539538263386497}
episode index:2125
target Thresh 67.83078497055638
target distance 54.0
model initialize at round 2125
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.9756901 , 15.01419822]), 'previousTarget': array([80.99657153, 15.62969312]), 'currentState': array([61.      , 16.      ,  2.666153], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.19344132028395566
running average episode reward sum: 0.5864926766470422
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36950405,  15.43732796,   4.49762077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5725285866083955}
episode index:2126
target Thresh 67.83895010233951
target distance 9.0
model initialize at round 2126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.03195529,   5.65809187,   6.19200492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.278476449891972}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.58664213099512
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5487738 ,  15.97481493,   4.21451462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1186674329051995}
episode index:2127
target Thresh 67.84710707307204
target distance 6.0
model initialize at round 2127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09931354e+02, 2.14585989e+01, 7.51134157e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.21003507709779}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5868044539353982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.9978034 ,  14.75739178,   4.27024225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0268740825865879}
episode index:2128
target Thresh 67.85525589091098
target distance 15.0
model initialize at round 2128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.60561984, 21.94238637,  1.53726196]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.803719531631923}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5868978657188043
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.87466382,  15.24736873,   4.8078048 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9089708974674644}
episode index:2129
target Thresh 67.86339656400514
target distance 24.0
model initialize at round 2129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.67915639,  15.88199572]), 'previousTarget': array([110.57960839,  15.92091492]), 'currentState': array([92.34930458, 21.01588597,  1.64523643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5869301480450275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29181627,  15.80760976,   0.82716283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.074131148217713}
episode index:2130
target Thresh 67.87152910049518
target distance 15.0
model initialize at round 2130
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.69694625,  14.91538104]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  14.       ,   4.0550413], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.7220195132499}
done in step count: 19
reward sum = 0.4185469891845166
running average episode reward sum: 0.5868511320155293
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20458757,  15.38021973,   1.19855293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8816166808107572}
episode index:2131
target Thresh 67.87965350851364
target distance 13.0
model initialize at round 2131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.02392034,   3.23782619,   3.24851239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.93503189664444}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5869958257876884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13585875,  15.33732506,   5.51810235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9276466446544951}
episode index:2132
target Thresh 67.88776979618494
target distance 40.0
model initialize at round 2132
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.22162139, 12.26190356]), 'previousTarget': array([94.77872706, 11.96680906]), 'currentState': array([73.37784069,  9.76703532,  3.50596964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.07338511019761262
running average episode reward sum: 0.5867550331409045
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45296262,  15.42028242,   5.50263591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6898457807512762}
episode index:2133
target Thresh 67.89587797162535
target distance 28.0
model initialize at round 2133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.17202265,  16.23188271]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.48978852, 19.78289097,  5.75702793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7789969860600718
running average episode reward sum: 0.5868451184046904
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92946112,  15.55196628,   5.18485099]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5564553028063067}
episode index:2134
target Thresh 67.90397804294305
target distance 45.0
model initialize at round 2134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.36466196, 14.75824682]), 'previousTarget': array([89.98027613, 13.88801227]), 'currentState': array([70.36562489, 14.5619909 ,  2.35086   ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.2314498406043121
running average episode reward sum: 0.5866786569162593
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01931545,  15.82428594,   5.23158782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8245122216278616}
episode index:2135
target Thresh 67.91207001823814
target distance 55.0
model initialize at round 2135
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.73906946, 19.77988558]), 'previousTarget': array([79.79172879, 20.12120309]), 'currentState': array([60.      , 23.      ,  5.836368], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.4252981904523875
running average episode reward sum: 0.5866031042634204
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23272333,  15.61452308,   4.48264199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9830320995505271}
episode index:2136
target Thresh 67.92015390560256
target distance 47.0
model initialize at round 2136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.8851681 , 11.17521501]), 'previousTarget': array([87.7164234 , 10.35598696]), 'currentState': array([69.09628375,  8.27692992,  6.13455576]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.34629064042020496
running average episode reward sum: 0.5864906510749117
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34974927,  14.2385534 ,   5.48943816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0013126051532768}
episode index:2137
target Thresh 67.92822971312022
target distance 51.0
model initialize at round 2137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.19637814, 11.65437629]), 'previousTarget': array([83.86301209, 11.33682495]), 'currentState': array([65.32121278,  9.42327751,  5.58322424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.17901336542799812
running average episode reward sum: 0.5863000630086597
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27630131,  15.86678234,   5.10244969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9097549308769858}
episode index:2138
target Thresh 67.93629744886691
target distance 7.0
model initialize at round 2138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.38254832,  10.29730605,   0.79723415]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.326055815075649}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.586466112604916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66878307,  15.00182933,   6.05339048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.33122198165439304}
episode index:2139
target Thresh 67.94435712091038
target distance 8.0
model initialize at round 2139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.56768315,   8.86531193,   6.17741293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.888706191868195}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5866232521291324
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30897763,  15.70660645,   6.16685075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7712067503348439}
episode index:2140
target Thresh 67.95240873731031
target distance 53.0
model initialize at round 2140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.49438114, 20.50716904]), 'previousTarget': array([81.77598173, 20.01494615]), 'currentState': array([62.77538436, 23.84800217,  1.94058891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.20059272879096646
running average episode reward sum: 0.5864429482882457
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20587069,  15.37804941,   6.17247366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.879524137262124}
episode index:2141
target Thresh 67.9604523061183
target distance 64.0
model initialize at round 2141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.64349777,  7.77709254]), 'previousTarget': array([70.80513158,  8.78509663]), 'currentState': array([51.91538498,  4.49051901,  6.12058961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.5863933210745591
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14196773e+02, 1.54735689e+01, 3.08180710e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9324384976771541}
episode index:2142
target Thresh 67.96848783537791
target distance 44.0
model initialize at round 2142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.37288145, 11.82208677]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([69.52490494,  9.36082111,  2.54048395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3959797196383301
running average episode reward sum: 0.5863044673174727
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57467653,  14.91723156,   3.97800577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5806063477638388}
episode index:2143
target Thresh 67.9765153331247
target distance 6.0
model initialize at round 2143
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59595949,  10.56959083,   2.11042923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.448794683013781}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5864701276169519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91744839,  15.68012162,   4.18426991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6851132683062031}
episode index:2144
target Thresh 67.98453480738617
target distance 2.0
model initialize at round 2144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14210475e+02, 1.49026922e+01, 2.13980039e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.795498970943945}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5866629154362448
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14210475e+02, 1.49026922e+01, 2.13980039e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.795498970943945}
episode index:2145
target Thresh 67.99254626618176
target distance 6.0
model initialize at round 2145
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.61708827,  22.41562255,   0.22837067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.789077361644993}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5868282543150728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53659798,  15.19618045,   4.63730574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5713354194736255}
episode index:2146
target Thresh 68.00054971752294
target distance 49.0
model initialize at round 2146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.98619369, 13.78555525]), 'previousTarget': array([85.96262067, 13.22220127]), 'currentState': array([67.0049609 , 12.91933583,  6.01411599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6229525510337457
running average episode reward sum: 0.5868450797909547
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21381919,  15.59349205,   5.15132457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9850447064744005}
episode index:2147
target Thresh 68.0085451694132
target distance 22.0
model initialize at round 2147
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.74828149,  15.0356665 ]), 'previousTarget': array([112.0585156 ,  15.93592685]), 'currentState': array([93.    , 22.    ,  1.4288], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6385213593991467
running average episode reward sum: 0.5868691376492453
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09985885,  15.44618774,   0.87067679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.00465794708303}
episode index:2148
target Thresh 68.01653262984794
target distance 39.0
model initialize at round 2148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.60113192,  9.20130472]), 'previousTarget': array([95.24899352,  9.4292033 ]), 'currentState': array([74.29732589,  3.97033228,  4.13508296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5868804210842773
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68569225,  15.16452112,   4.81417499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3547626852106656}
episode index:2149
target Thresh 68.02451210681465
target distance 7.0
model initialize at round 2149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.73865892,  11.5434534 ,   5.83699936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.295190575783845}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5870587553070288
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.218777  ,  14.14377304,   1.22043645]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1590659924971332}
episode index:2150
target Thresh 68.03248360829281
target distance 48.0
model initialize at round 2150
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([86.77470265, 19.00648452]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([67.       , 22.       ,  3.8242626], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6334476949995691
running average episode reward sum: 0.5870803215272485
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.60167908,  15.61447242,   5.45265848]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8599965519040089}
episode index:2151
target Thresh 68.04044714225391
target distance 44.0
model initialize at round 2151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.44545273,  9.71479471]), 'previousTarget': array([90.40285  ,  8.8507125]), 'currentState': array([71.93067624,  5.33603935,  1.97236031]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.4694790713451854
running average episode reward sum: 0.5870256741061602
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87531431,  14.93815918,   5.86133089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.13917905698070135}
episode index:2152
target Thresh 68.04840271666149
target distance 42.0
model initialize at round 2152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.72970938,  8.82498351]), 'previousTarget': array([92.34744447,  9.06718784]), 'currentState': array([71.34721703,  3.89355471,  2.26111436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5870514920011805
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29925224,  15.578332  ,   5.42874349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6511680267227127}
episode index:2153
target Thresh 68.05635033947112
target distance 32.0
model initialize at round 2153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.90037971,   9.82395925]), 'previousTarget': array([101.52933154,   9.52754094]), 'currentState': array([84.51224988,  1.95778601,  0.98209256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5943963696543975
running average episode reward sum: 0.5870549018793852
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38706269,  15.93859769,   4.58596772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.015274913900224}
episode index:2154
target Thresh 68.06429001863043
target distance 14.0
model initialize at round 2154
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.44757408,  14.77263375]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.     ,  14.     ,   5.43861], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.47153011509}
done in step count: 8
reward sum = 0.7834446944279201
running average episode reward sum: 0.5871460340337001
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15309598e+02, 1.59119708e+01, 1.52211746e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9630897930244923}
episode index:2155
target Thresh 68.0722217620791
target distance 24.0
model initialize at round 2155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.60250191,  13.37863098]), 'previousTarget': array([109.46153846,  12.69230769]), 'currentState': array([9.18373541e+01, 6.45987201e+00, 4.00295258e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5870556873482731
running average episode reward sum: 0.5871459921289296
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51759958,  14.68888065,   4.18749894]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6039077547603703}
episode index:2156
target Thresh 68.08014557774887
target distance 3.0
model initialize at round 2156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78176006,  16.74140999,   4.83224094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9088366968749648}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5873327580111136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68295789,  15.66928345,   4.40446776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9562279101671061}
episode index:2157
target Thresh 68.08806147356354
target distance 3.0
model initialize at round 2157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.56547372,  15.77384583,   2.86509943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.7462947429029259}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5875193508016553
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17686315,  14.74563168,   4.71128607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3098125562377373}
episode index:2158
target Thresh 68.09596945743904
target distance 67.0
model initialize at round 2158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.35551713,  8.92342393]), 'previousTarget': array([67.85893586,  9.37121622]), 'currentState': array([49.53042722,  6.28414183,  0.56135994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5159218397646202
running average episode reward sum: 0.5874861884528656
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24195836,  15.60260625,   4.35166021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6493674889253079}
episode index:2159
target Thresh 68.10386953728334
target distance 50.0
model initialize at round 2159
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.22511607, 10.80805556]), 'previousTarget': array([84.74881264, 10.15981002]), 'currentState': array([66.43402523,  7.92487614,  1.54288286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5875239132538456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85727857,  15.12986015,   5.41581349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1929587095240504}
episode index:2160
target Thresh 68.11176172099653
target distance 63.0
model initialize at round 2160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.44698417, 10.83063269]), 'previousTarget': array([71.87767469, 10.20863052]), 'currentState': array([50.53398932,  8.96713168,  1.6781919 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5874856726131853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68909652,  14.76398849,   4.92987574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7283923738870249}
episode index:2161
target Thresh 68.11964601647077
target distance 29.0
model initialize at round 2161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.89371086,  17.61741691]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([85.532502  , 22.63175549,  0.88269854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3243787670422673
running average episode reward sum: 0.5873639765421534
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4230578 ,  14.84248516,   4.24169314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4514297577850623}
episode index:2162
target Thresh 68.12752243159036
target distance 14.0
model initialize at round 2162
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.42355427,  13.55498037]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   2.       ,   2.1473365], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.711842948755574}
done in step count: 15
reward sum = 0.7900583546412885
running average episode reward sum: 0.5874576863794623
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05652617,  15.51242863,   0.26608482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0736507708378085}
episode index:2163
target Thresh 68.13539097423174
target distance 20.0
model initialize at round 2163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.69038354,  14.30359839]), 'previousTarget': array([114.1565257 ,  14.74695771]), 'currentState': array([93.54190107,  8.52989773,  4.46347809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5875456548050721
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21849528,  15.31778864,   4.74365849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3856550336002556}
episode index:2164
target Thresh 68.14325165226344
target distance 59.0
model initialize at round 2164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.34477564, 16.94279196]), 'previousTarget': array([75.98851894, 16.32242309]), 'currentState': array([57.37134225, 17.9733057 ,  1.62723749]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.41822102613072865
running average episode reward sum: 0.5874674448149223
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71694432,  14.45953258,   3.85387883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8978386224490973}
episode index:2165
target Thresh 68.15110447354613
target distance 12.0
model initialize at round 2165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83074724,   1.78953064,   2.93430901]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.211553548167176}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5876137581252611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53620433,  15.85911239,   5.27739591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0127137741862455}
episode index:2166
target Thresh 68.15894944593263
target distance 38.0
model initialize at round 2166
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.90740814, 12.92226455]), 'previousTarget': array([96.89010906, 13.09369569]), 'currentState': array([77.       , 11.       ,  3.9569516], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.31523474027218645
running average episode reward sum: 0.5874880640699528
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86006374,  15.93149601,   5.61521193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9419484965085928}
episode index:2167
target Thresh 68.16678657726796
target distance 36.0
model initialize at round 2167
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.47349708, 18.26730848]), 'previousTarget': array([98.72787848, 17.71202025]), 'currentState': array([79.90213834, 22.38579328,  6.26691038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.21907251943691308
running average episode reward sum: 0.5873181307006571
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8905923 ,  15.04416152,   5.41053088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11798425920065674}
episode index:2168
target Thresh 68.17461587538918
target distance 43.0
model initialize at round 2168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.13503737, 14.69784493]), 'previousTarget': array([91.97840172, 13.92922799]), 'currentState': array([71.13664019, 14.44464457,  3.11946976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5873716713019936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83417272,  15.79376786,   4.91030389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.810904499277285}
episode index:2169
target Thresh 68.18243734812563
target distance 68.0
model initialize at round 2169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.85454753,  9.60531383]), 'previousTarget': array([66.82709532,  8.62417438]), 'currentState': array([4.79842097e+01, 7.33162186e+00, 2.26099491e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.22011847042187574
running average episode reward sum: 0.5872024301955973
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87692655,  15.30697366,   5.27304786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3307263271309159}
episode index:2170
target Thresh 68.19025100329878
target distance 24.0
model initialize at round 2170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.83231362,  14.45691588]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([92.1199226 , 11.07732416,  6.04531414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5872795913457621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15286132,  15.79920675,   4.94933614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1646352914938474}
episode index:2171
target Thresh 68.19805684872229
target distance 23.0
model initialize at round 2171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.08536549,  15.19222999]), 'previousTarget': array([111.83200822,  15.41321632]), 'currentState': array([93.18541207, 17.19019239,  0.29150003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.5618291774932151
running average episode reward sum: 0.5872678738439884
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59223585,  15.73059125,   4.9310287 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9404822534600067}
episode index:2172
target Thresh 68.205854892202
target distance 45.0
model initialize at round 2172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.26569995, 10.42798258]), 'previousTarget': array([89.69125016, 10.50066669]), 'currentState': array([68.55190494,  7.05659024,  2.16379046]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5873023948596993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06983486,  14.78066234,   4.98351139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23018670216496465}
episode index:2173
target Thresh 68.21364514153593
target distance 62.0
model initialize at round 2173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.45984116, 11.82834527]), 'previousTarget': array([72.95850618, 12.28764556]), 'currentState': array([51.51269391, 10.37530823,  4.54923391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.06292725986732306
running average episode reward sum: 0.5870611919457194
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31335925,  15.90648584,   5.04788386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9591197001752713}
episode index:2174
target Thresh 68.22142760451436
target distance 6.0
model initialize at round 2174
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.6879072 ,  19.27547612,   5.38817808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.072383415411258}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5872329320919513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34090067,  14.62117824,   5.82895399]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5096265227834769}
episode index:2175
target Thresh 68.22920228891977
target distance 22.0
model initialize at round 2175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.59644631,  13.80729699]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([94.68093064,  4.91717335,  0.96076554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6426354881057407
running average episode reward sum: 0.5872583928254135
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58658341,  15.10027209,   5.30066523]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42540306915042325}
episode index:2176
target Thresh 68.2369692025268
target distance 35.0
model initialize at round 2176
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.54659472, 17.76554197]), 'previousTarget': array([99.61161351, 18.0776773 ]), 'currentState': array([80.       , 22.       ,  1.1215479], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5099742823695124
running average episode reward sum: 0.5872228925450019
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83571617,  15.39031172,   4.85565603]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42347658432551183}
episode index:2177
target Thresh 68.24472835310239
target distance 26.0
model initialize at round 2177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.52084599,  13.1165224 ]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([90.60713354,  6.61487021,  0.77479761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5873250527725852
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95537707,  15.76408197,   4.87547211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7653838624841063}
episode index:2178
target Thresh 68.25247974840568
target distance 38.0
model initialize at round 2178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.27763198, 16.11583976]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([75.30956532, 17.24558049,  2.03033566]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5798026283684021
running average episode reward sum: 0.5873216005355938
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89048099,  14.9506014 ,   5.02856741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.12014422670699451}
episode index:2179
target Thresh 68.26022339618808
target distance 45.0
model initialize at round 2179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.52482986, 16.54310356]), 'previousTarget': array([89.98027613, 16.11198773]), 'currentState': array([71.56789918, 17.85494116,  5.95923156]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5436315375256878
running average episode reward sum: 0.5873015592222866
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5308805 ,  14.89088844,   5.23426712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48164140177123205}
episode index:2180
target Thresh 68.26795930419323
target distance 5.0
model initialize at round 2180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.63870642,  10.40680755,   1.24411326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.691723063289954}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5874683123129227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92095891,  15.16930232,   4.75696471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9363912589709185}
episode index:2181
target Thresh 68.27568748015705
target distance 53.0
model initialize at round 2181
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.97058575, 13.08429913]), 'previousTarget': array([81.96803691, 13.13026624]), 'currentState': array([62.      , 12.      ,  5.487557], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3862135482929329
running average episode reward sum: 0.5873760782322537
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23126012,  14.73916565,   5.0655367 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8117854157380897}
episode index:2182
target Thresh 68.28340793180769
target distance 54.0
model initialize at round 2182
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([79.4933769 , 20.68888021]), 'previousTarget': array([80.78406925, 20.06902678]), 'currentState': array([59.74524329, 23.85293122,  1.41890216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5873955228282092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95777467,  15.54000008,   5.17626731]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5416484744150422}
episode index:2183
target Thresh 68.29112066686561
target distance 50.0
model initialize at round 2183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.79390649, 17.50263257]), 'previousTarget': array([84.96409691, 16.80215419]), 'currentState': array([63.85791337, 19.10143781,  1.36700225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5873745931364228
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8474425 ,  14.71810983,   5.04072594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3205243495689664}
episode index:2184
target Thresh 68.29882569304358
target distance 18.0
model initialize at round 2184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.53060315, 21.69974429,  5.70198495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.779977672751404}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5874915581708611
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95883834,  15.11256882,   1.67628313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11985833495602843}
episode index:2185
target Thresh 68.30652301804659
target distance 44.0
model initialize at round 2185
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.46881175, 16.12858321]), 'previousTarget': array([90.9793708 , 16.09184678]), 'currentState': array([72.49385454, 17.12912526,  1.05598765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.09031425876397259
running average episode reward sum: 0.5872641211628982
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36174902,  14.9369679 ,   4.6144228 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6413558798718836}
episode index:2186
target Thresh 68.31421264957199
target distance 25.0
model initialize at round 2186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.62831909,  16.34546141]), 'previousTarget': array([109.04848294,  16.90448546]), 'currentState': array([91.5131431 , 22.22849183,  0.51220792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5873584742321624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9498554 ,  15.02581061,   5.1727078 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.05639741784616986}
episode index:2187
target Thresh 68.32189459530937
target distance 51.0
model initialize at round 2187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.49761749,  9.84207224]), 'previousTarget': array([83.81423159, 10.71960041]), 'currentState': array([64.77757301,  6.50742896,  6.20266533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4607990998492268
running average episode reward sum: 0.5873006317393
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39843366,  15.43254742,   5.59492643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5880872820811524}
episode index:2188
target Thresh 68.32956886294073
target distance 43.0
model initialize at round 2188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.06809837, 13.02971204]), 'previousTarget': array([91.97840172, 13.92922799]), 'currentState': array([72.14151304, 11.31763811,  3.95289135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5873569371735318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48103514,  14.28596528,   5.73802466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8827061257666183}
episode index:2189
target Thresh 68.3372354601403
target distance 26.0
model initialize at round 2189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.43287176,  11.84724916]), 'previousTarget': array([107.41934502,  11.79279982]), 'currentState': array([90.40297755,  3.19144679,  0.46710842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.3731789888895241
running average episode reward sum: 0.5872591390236304
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35582451,  15.79416442,   5.38386996]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8702345712243358}
episode index:2190
target Thresh 68.34489439457471
target distance 61.0
model initialize at round 2190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.41192425, 18.8284693 ]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([55.50479699, 20.75364056,  1.47430771]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5872903576824849
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82396461,  14.24932491,   4.6072084 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7710392655733342}
episode index:2191
target Thresh 68.35254567390285
target distance 48.0
model initialize at round 2191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.46612862, 10.02825039]), 'previousTarget': array([86.79065962, 10.88613786]), 'currentState': array([67.78441592,  6.47435323,  6.14152157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5118183011687799
running average episode reward sum: 0.5872559269997689
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61772935,  15.30539172,   4.89734983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4892800366325075}
episode index:2192
target Thresh 68.36018930577606
target distance 22.0
model initialize at round 2192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.21604886,  12.60343821]), 'previousTarget': array([110.21853056,  12.17458624]), 'currentState': array([92.33436735,  3.64545483,  2.58742535]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.1189815384628572
running average episode reward sum: 0.5870423955868473
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55014808,  15.59090631,   4.76670088]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7426553803334238}
episode index:2193
target Thresh 68.36782529783791
target distance 7.0
model initialize at round 2193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.01472877,   9.39618369,   1.21946359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.945086970687892}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.587191199074494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17395198,  15.87631025,   0.70802933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2042736358193824}
episode index:2194
target Thresh 68.37545365772444
target distance 9.0
model initialize at round 2194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.04825591,   7.67229525,   2.57277173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.899876583314732}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5873526063411576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19762459,  14.88823092,   2.23260796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22704141536936487}
episode index:2195
target Thresh 68.38307439306398
target distance 9.0
model initialize at round 2195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.72500474,   4.84743308,   2.92807066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.015906659582187}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5875053349787198
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.83332574,  15.09136644,   4.88642109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8383195206846988}
episode index:2196
target Thresh 68.39068751147728
target distance 64.0
model initialize at round 2196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.70993181, 12.77128554]), 'previousTarget': array([70.93924283, 11.55775335]), 'currentState': array([51.73638451, 11.74298192,  1.75032198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4253040552094123
running average episode reward sum: 0.5874315064490114
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89129794,  14.04873102,   4.64370335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9574595564669809}
episode index:2197
target Thresh 68.39829302057746
target distance 11.0
model initialize at round 2197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66970275,   5.03087211,   1.47233343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.974598096821317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.587588300735389
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17672491,  14.76451545,   1.03794897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8562913309055121}
episode index:2198
target Thresh 68.40589092797002
target distance 20.0
model initialize at round 2198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.38308376,  14.8776204 ]), 'previousTarget': array([114.97504678,  14.99875234]), 'currentState': array([93.44012408, 13.36819687,  2.51643133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6713756051398991
running average episode reward sum: 0.5876264031930537
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27566577,  15.06736146,   5.07980182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7274597142277659}
episode index:2199
target Thresh 68.4134812412529
target distance 25.0
model initialize at round 2199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.41968714,  14.7525515 ]), 'previousTarget': array([109.85753677,  14.38290441]), 'currentState': array([90.44880973, 13.67363752,  2.23228136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.587709319894241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07756707,  14.79054519,   1.24314933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9459142795484486}
episode index:2200
target Thresh 68.42106396801636
target distance 34.0
model initialize at round 2200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.43792277,  10.37735915]), 'previousTarget': array([100.33410456,  11.11785121]), 'currentState': array([81.37534652,  4.32606287,  5.88717437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.43478178704729564
running average episode reward sum: 0.5876398389615527
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78000035,  15.26063581,   5.66130019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34107311499411264}
episode index:2201
target Thresh 68.42863911584317
target distance 48.0
model initialize at round 2201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.62810168, 16.31785778]), 'previousTarget': array([86.98266146, 16.16738911]), 'currentState': array([68.65302707, 17.31605317,  1.19891661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.31900012806770234
running average episode reward sum: 0.5872281041899682
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.48447144,  12.54239866,   5.9691738 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8711425959904915}
episode index:2202
target Thresh 68.43620669230846
target distance 9.0
model initialize at round 2202
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.06708884,  16.30142595]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.       ,  11.       ,   3.0356083], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.37767088280346}
done in step count: 9
reward sum = 0.8435172474836408
running average episode reward sum: 0.5873444406145226
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74686524,  15.4645773 ,   4.14590785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5290645271856917}
episode index:2203
target Thresh 68.44376670497982
target distance 46.0
model initialize at round 2203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.09844724, 18.26848623]), 'previousTarget': array([88.77237675, 18.99116006]), 'currentState': array([70.26853486, 20.87128759,  0.24761265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.11890155034197192
running average episode reward sum: 0.5871318984683009
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37255416,  15.55754766,   4.63009459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8393733839722246}
episode index:2204
target Thresh 68.45131916141725
target distance 39.0
model initialize at round 2204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.58981678, 10.65484782]), 'previousTarget': array([95.68542414, 11.53328126]), 'currentState': array([76.07286583,  6.28579467,  3.80110574]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5871239480392799
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00756696,  14.34318405,   1.28038804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.190096856852236}
episode index:2205
target Thresh 68.45886406917322
target distance 46.0
model initialize at round 2205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.4366158, 17.7050395]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([67.53223802, 19.65843274,  1.70990658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.47574209856036653
running average episode reward sum: 0.5870734576270049
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19024132,  14.92948543,   5.41676328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.812823122601746}
episode index:2206
target Thresh 68.4664014357926
target distance 2.0
model initialize at round 2206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.97459639,  12.38877635,   5.63432801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.805341608372169}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.587251539431433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90805711,  14.30951674,   2.98537749]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6965777939828949}
episode index:2207
target Thresh 68.47393126881282
target distance 46.0
model initialize at round 2207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.60376112, 14.79279779]), 'previousTarget': array([88.98112317, 13.86874449]), 'currentState': array([68.60437727, 14.63580889,  0.79809117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5057682404974045
running average episode reward sum: 0.5872146357634376
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40346355,  14.07842253,   5.22716992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0977981499082698}
episode index:2208
target Thresh 68.48145357576368
target distance 39.0
model initialize at round 2208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.57400507, 17.08084698]), 'previousTarget': array([95.76743395, 17.95885631]), 'currentState': array([75.68776673, 19.21099369,  4.04005402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.2783728786356811
running average episode reward sum: 0.5870748250992784
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60882479,  15.2346363 ,   0.31476104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45614936258756583}
episode index:2209
target Thresh 68.48896836416748
target distance 8.0
model initialize at round 2209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.57511173,   7.90402774,   1.57380056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.932090181251185}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5872351894994149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28285567,  15.17453535,   5.17895025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7380776260955119}
episode index:2210
target Thresh 68.49647564153902
target distance 48.0
model initialize at round 2210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.33766847, 19.27987076]), 'previousTarget': array([86.72787848, 19.71202025]), 'currentState': array([68.59046419, 22.44971667,  4.9400881 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4682458275832602
running average episode reward sum: 0.5871813725107599
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93342698,  15.06707677,   1.16608984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9358339671060417}
episode index:2211
target Thresh 68.50397541538558
target distance 24.0
model initialize at round 2211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.86490451,  13.21753285]), 'previousTarget': array([109.46153846,  12.69230769]), 'currentState': array([90.97080184,  6.65911199,  2.48296696]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5872214031059451
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14311791,  15.08484659,   1.65963591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16637811992020354}
episode index:2212
target Thresh 68.51146769320692
target distance 12.0
model initialize at round 2212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98512653,   4.69706472,   2.565588  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.057562315230175}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5873772295608936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12817991,  14.97346651,   0.69884622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8722237648412702}
episode index:2213
target Thresh 68.51895248249534
target distance 10.0
model initialize at round 2213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.83195649,   5.72636149,   1.22902971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.034154765882406}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5875287053806166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65917071,  15.04827384,   0.68049326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3442309815497593}
episode index:2214
target Thresh 68.52642979073562
target distance 8.0
model initialize at round 2214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.12777744e+02, 8.41755090e+00, 5.89873791e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9474495955486395}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5876971330576457
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35447321,  14.98899449,   0.74246681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.645620598831889}
episode index:2215
target Thresh 68.53389962540506
target distance 4.0
model initialize at round 2215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.60381297,  17.36076685,   4.16762233]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.8540210505591923}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5878742101636666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09550596,  15.2194299 ,   4.36264618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2393133290425278}
episode index:2216
target Thresh 68.54136199397351
target distance 6.0
model initialize at round 2216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.99868356,  10.3949851 ,   6.27865893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.798479850437345}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5880379971910623
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99574972,  15.53545067,   5.14217431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5354675354101952}
episode index:2217
target Thresh 68.54881690390332
target distance 27.0
model initialize at round 2217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.16862599,  13.68170221]), 'previousTarget': array([107.5237412 ,  13.33860916]), 'currentState': array([89.66091049,  9.27159608,  1.17208784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5880868588776532
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48311086,  14.6288782 ,   6.2153993 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6363220695665076}
episode index:2218
target Thresh 68.55626436264943
target distance 27.0
model initialize at round 2218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.74675697,  17.58390554]), 'previousTarget': array([107.35993796,  16.98075682]), 'currentState': array([87.66029867, 23.55944815,  0.77528   ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.367267184686043
running average episode reward sum: 0.5879873457302031
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61477167,  15.71774428,   5.63872091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8145905169295597}
episode index:2219
target Thresh 68.56370437765928
target distance 6.0
model initialize at round 2219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.51329547,  19.70263489,   5.43112218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.854219283318547}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5881595581870814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21038639,  15.17491724,   4.74270499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8087556439303786}
episode index:2220
target Thresh 68.57113695637288
target distance 5.0
model initialize at round 2220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11013320e+02, 1.56562845e+01, 8.53349527e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.040337746150424}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5883360284445388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24469838,  14.99338633,   5.42950895]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7553305718489058}
episode index:2221
target Thresh 68.5785621062228
target distance 53.0
model initialize at round 2221
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.8853077 , 12.13881686]), 'previousTarget': array([81.91159005, 11.87845189]), 'currentState': array([62.       , 10.       ,  2.7716365], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.24846588753227639
running average episode reward sum: 0.5881830715854425
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39781744,  15.4654105 ,   4.97081137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7610721183039484}
episode index:2222
target Thresh 68.58597983463423
target distance 57.0
model initialize at round 2222
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.90168985, 12.98059114]), 'previousTarget': array([77.95093522, 12.40006563]), 'currentState': array([58.      , 11.      ,  5.981156], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = -0.06005264236679886
running average episode reward sum: 0.5878914675755673
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02423064,  15.97166744,   5.16008607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3770488264224592}
episode index:2223
target Thresh 68.59339014902488
target distance 21.0
model initialize at round 2223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.96565544,  15.64749179]), 'previousTarget': array([113.45612429,  15.36758945]), 'currentState': array([93.90768219, 21.7132692 ,  0.67086589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5879839688417849
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41594263,  14.97966815,   1.43720096]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41643925801306036}
episode index:2224
target Thresh 68.60079305680506
target distance 58.0
model initialize at round 2224
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.89482991, 11.04835122]), 'previousTarget': array([76.89383588, 11.05798302]), 'currentState': array([57.       ,  9.       ,  2.2364883], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.21959529148305945
running average episode reward sum: 0.5878184008969046
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6240677 ,  14.80982898,   0.41771911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4212957473892344}
episode index:2225
target Thresh 68.60818856537767
target distance 28.0
model initialize at round 2225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.47896332,  14.46560727]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([86.51817879, 13.21377624,  0.9386797 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.6298795735283752
running average episode reward sum: 0.5878372963023993
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27237545,  15.80478026,   5.8551153 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0849464241517555}
episode index:2226
target Thresh 68.61557668213824
target distance 7.0
model initialize at round 2226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.74852455,  17.97567276,   5.37488687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.03594423382373}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.588004677853229
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37707369,  15.50484591,   5.2361876 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8018145568961943}
episode index:2227
target Thresh 68.62295741447488
target distance 67.0
model initialize at round 2227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.59295588,  9.44163211]), 'previousTarget': array([67.8219647 ,  8.66265197]), 'currentState': array([46.72351542,  7.16011168,  1.46632528]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.6482821450708123
running average episode reward sum: 0.5874497914874641
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.54215574,  19.81760264,   6.21802455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.408354058593907}
episode index:2228
target Thresh 68.6303307697683
target distance 9.0
model initialize at round 2228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.23799043,   7.90661492,   0.23097187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.514984756633703}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5876002154008514
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63832256,  15.07795726,   4.44697213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36998365129011146}
episode index:2229
target Thresh 68.6376967553919
target distance 44.0
model initialize at round 2229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.76989721,  8.18552053]), 'previousTarget': array([90.18035416,  7.66692282]), 'currentState': array([72.57858808,  2.5558088 ,  1.34853762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5875747917809206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23769077,  15.87630619,   5.36335877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.161476602330306}
episode index:2230
target Thresh 68.64505537871163
target distance 44.0
model initialize at round 2230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.2738744 ,  8.24202285]), 'previousTarget': array([90.40285  ,  8.8507125]), 'currentState': array([72.03892272,  2.76327902,  0.13790005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5875745506290411
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14009815,  14.77739389,   5.40985063]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8882480878725225}
episode index:2231
target Thresh 68.65240664708611
target distance 48.0
model initialize at round 2231
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.19168928, 18.57868585]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([68.36754269, 21.22504405,  0.49444264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 51
reward sum = 0.4803479389526448
running average episode reward sum: 0.587526510032412
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00402254,  14.17951317,   5.09848465]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2904145572740673}
episode index:2232
target Thresh 68.65975056786665
target distance 38.0
model initialize at round 2232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.32246427, 16.63266566]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([76.39844005, 18.37429103,  0.97339463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5875629834979458
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50522182,  15.44049281,   6.25264466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6624495223519068}
episode index:2233
target Thresh 68.66708714839714
target distance 35.0
model initialize at round 2233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.21739269,  18.10972896]), 'previousTarget': array([99.7124451, 17.6207237]), 'currentState': array([80.64575696, 22.22689727,  2.09630464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6830194226127921
running average episode reward sum: 0.587605712432196
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70957736,  15.06129064,   0.34007509]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29681955890313566}
episode index:2234
target Thresh 68.67441639601415
target distance 11.0
model initialize at round 2234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.30089847,  17.06773183,   1.69727486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.917060324942788}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5877640455136137
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09500126,  15.55669056,   5.46039581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.062509815741183}
episode index:2235
target Thresh 68.68173831804697
target distance 49.0
model initialize at round 2235
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.53797574, 15.91497249]), 'previousTarget': array([85.99583637, 15.59192171]), 'currentState': array([67.54906722, 16.58095768,  1.3688032 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5878126363778964
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10017629,  15.93161818,   5.52930343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2952201152801472}
episode index:2236
target Thresh 68.68905292181749
target distance 22.0
model initialize at round 2236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.55045084,  14.83816163]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([92.5939591 , 13.51966345,  2.84188849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.58790463532616
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51701673,  15.20654269,   6.19685081]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5252929907193871}
episode index:2237
target Thresh 68.69636021464032
target distance 27.0
model initialize at round 2237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.96240945,  13.02804476]), 'previousTarget': array([107.5237412 ,  13.33860916]), 'currentState': array([88.70414535,  7.63181795,  6.19769382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5879594380928921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56292368,  15.7866969 ,   2.63537641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9673546803062735}
episode index:2238
target Thresh 68.70366020382275
target distance 65.0
model initialize at round 2238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.57432142,  8.11937886]), 'previousTarget': array([69.71961771,  7.33716607]), 'currentState': array([48.79042165,  5.18726191,  1.4359684 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5879315850555685
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17106042,  14.02142179,   1.47171158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9934168189477772}
episode index:2239
target Thresh 68.71095289666478
target distance 35.0
model initialize at round 2239
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.97781754, 14.05829603]), 'previousTarget': array([100.,  15.]), 'currentState': array([80.        , 15.        ,  0.73795784], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 59
reward sum = -0.32646366389057424
running average episode reward sum: 0.5875233728908604
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51794398,  15.14483563,   4.98400131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5033441827546277}
episode index:2240
target Thresh 68.7182383004591
target distance 58.0
model initialize at round 2240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.04946569, 18.51443053]), 'previousTarget': array([76.95260657, 17.62395817]), 'currentState': array([56.13038296, 20.3116904 ,  1.14623833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = -0.13896486978443523
running average episode reward sum: 0.5871991925059095
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0437469 ,  15.56218845,   5.35976367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1092681528905182}
episode index:2241
target Thresh 68.72551642249113
target distance 47.0
model initialize at round 2241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.6165175 , 14.97555133]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([66.61652492, 14.95832395,  1.52578735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5872417239075617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05967765,  15.87670077,   4.93879033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8787295773692044}
episode index:2242
target Thresh 68.73278727003895
target distance 51.0
model initialize at round 2242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.36203296, 17.33393216]), 'previousTarget': array([83.96548746, 16.82555956]), 'currentState': array([65.42375858, 18.90403242,  5.84488994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5872967000570779
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31451415,  15.62376442,   4.65317244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9268079106600189}
episode index:2243
target Thresh 68.74005085037345
target distance 12.0
model initialize at round 2243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.34658076,   5.31687825,   1.9431448 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.738539479222847}
done in step count: 20
reward sum = 0.7492999375972308
running average episode reward sum: 0.5873688940132009
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89325652,  15.09980355,   1.76443452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8988147537736817}
episode index:2244
target Thresh 68.74730717075819
target distance 66.0
model initialize at round 2244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.06064843,  6.17281251]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([50.43565672,  2.31798427,  0.5543192 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.28650728647601464
running average episode reward sum: 0.5872348799341198
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.907514  ,  14.27693707,   5.68169235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7289538154038031}
episode index:2245
target Thresh 68.7545562384495
target distance 18.0
model initialize at round 2245
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.54883211,  13.37978349]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 13.       ,  0.9537728], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.552719699829698}
done in step count: 23
reward sum = 0.6543142836436554
running average episode reward sum: 0.5872647460978373
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16590465,  15.1536177 ,   6.2437656 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8481234880679512}
episode index:2246
target Thresh 68.76179806069644
target distance 44.0
model initialize at round 2246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.71551358,  9.25519481]), 'previousTarget': array([90.29527642,  8.26234812]), 'currentState': array([71.25268924,  4.65101532,  2.14310759]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5461256563619067
running average episode reward sum: 0.5872464376466865
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44356549,  14.99245179,   5.6953373 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5564857039667744}
episode index:2247
target Thresh 68.76903264474083
target distance 15.0
model initialize at round 2247
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.70121843,  13.437591  ]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   4.       ,   2.9330351], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.319289944411274}
done in step count: 14
reward sum = 0.5929178827689783
running average episode reward sum: 0.5872489605315273
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34032025,  14.62129143,   5.83514342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7606559991355246}
episode index:2248
target Thresh 68.77625999781728
target distance 60.0
model initialize at round 2248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.24898693, 15.92436149]), 'previousTarget': array([75., 15.]), 'currentState': array([54.2541302 , 16.3779079 ,  3.07694018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5872650905601443
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72098702,  15.39033639,   6.16523279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4798028205312711}
episode index:2249
target Thresh 68.78348012715311
target distance 6.0
model initialize at round 2249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89345489,  19.59079636,   4.46141863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.592032561731596}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5874353278532287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95558358,  15.68266421,   4.35164914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6841076209302599}
episode index:2250
target Thresh 68.79069303996847
target distance 12.0
model initialize at round 2250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.1845822 ,   4.54966749,   2.81473482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.959453737441718}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5875721128049859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73101216,  14.17700822,   1.37574147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1007698481939419}
episode index:2251
target Thresh 68.79789874347628
target distance 20.0
model initialize at round 2251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.25966878,  14.66065853]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([93.62935811, 10.83300873,  1.72018075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5875825680122363
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01000009,  15.13551603,   4.95759813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1358845004545851}
episode index:2252
target Thresh 68.80509724488222
target distance 27.0
model initialize at round 2252
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.47637417,  15.90990749]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([89.74233354, 19.16069819,  5.56331551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 20
reward sum = 0.6919265145485043
running average episode reward sum: 0.5876288813484707
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61481203,  15.40920158,   5.60457145]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5619748282086932}
episode index:2253
target Thresh 68.81228855138481
target distance 3.0
model initialize at round 2253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.25531417,  17.11286515,   6.13115114]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7400963118395145}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5878073955980944
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12034087,  15.92897506,   4.53973514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2793728289326014}
episode index:2254
target Thresh 68.81947267017536
target distance 26.0
model initialize at round 2254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.96362924,  17.00844765]), 'previousTarget': array([108.48782391,  16.50280987]), 'currentState': array([87.56041285, 21.85769436,  1.62356055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5879130990252507
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78557631,  15.92079743,   4.67131567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.945433988179408}
episode index:2255
target Thresh 68.82664960843798
target distance 43.0
model initialize at round 2255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.55380977, 18.83463348]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([72.8394248 , 22.20257432,  0.12485683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.399043837205405
running average episode reward sum: 0.587829380380827
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20718759,  14.52550213,   5.49275448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9239587397541499}
episode index:2256
target Thresh 68.8338193733496
target distance 51.0
model initialize at round 2256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.70697828, 16.14240747]), 'previousTarget': array([83.98463901, 16.21628867]), 'currentState': array([65.72217041, 16.92180108,  0.89391297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5878806069269584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5082798 ,  15.22681081,   0.48402482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5415089083326619}
episode index:2257
target Thresh 68.84098197208
target distance 37.0
model initialize at round 2257
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.11181079,  8.89395354]), 'previousTarget': array([97.02445638,  9.17009396]), 'currentState': array([78.      ,  3.      ,  2.109456], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3223110099459776
running average episode reward sum: 0.5877629941736453
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12202702,  14.71336372,   6.24918462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9235783213987145}
episode index:2258
target Thresh 68.84813741179178
target distance 33.0
model initialize at round 2258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.51754727,  18.12889852]), 'previousTarget': array([101.5646825 ,  17.84991583]), 'currentState': array([83.11773089, 22.99172942,  1.68787211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7334101988187652
running average episode reward sum: 0.5878274683678221
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2295516 ,  14.21291395,   6.03541271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1014060034510778}
episode index:2259
target Thresh 68.85528569964038
target distance 27.0
model initialize at round 2259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.47611738,  14.63579731]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([89.51944694, 13.32000669,  1.21757382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6696296918562873
running average episode reward sum: 0.587863664041932
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29315959,  14.89428691,   5.80293649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7147017708283077}
episode index:2260
target Thresh 68.86242684277407
target distance 44.0
model initialize at round 2260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.58804391, 18.520505  ]), 'previousTarget': array([90.6773982 , 19.42229124]), 'currentState': array([71.81039776, 21.49451118,  6.17046392]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5510659216189805
running average episode reward sum: 0.5878473890563404
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50206277,  15.97186353,   5.15158699]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.091998258507742}
episode index:2261
target Thresh 68.86956084833402
target distance 8.0
model initialize at round 2261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.26065718,   7.34813299,   2.17210865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.755019355174421}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5880079295783756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88033381,  14.10301942,   0.42736661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9049277060783886}
episode index:2262
target Thresh 68.87668772345421
target distance 2.0
model initialize at round 2262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.16194139,  13.54437204,   5.17884642]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.344634773173556}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.588181191651032
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11672622,  14.37379367,   1.94675064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6369924506171524}
episode index:2263
target Thresh 68.88380747526155
target distance 67.0
model initialize at round 2263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.59089878, 15.28096918]), 'previousTarget': array([68., 15.]), 'currentState': array([49.59128162, 15.40471697,  5.58033556]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.37346428045426916
running average episode reward sum: 0.5880863520259451
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6576179 ,  15.36466212,   1.85622381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7519572894554185}
episode index:2264
target Thresh 68.89092011087575
target distance 13.0
model initialize at round 2264
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.72733124,  16.10122764]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.        ,  13.        ,   0.27669597], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.130453859572022}
done in step count: 15
reward sum = 0.7207583546412885
running average episode reward sum: 0.5881449268615369
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48842268,  15.30483828,   4.81065315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5955146798190241}
episode index:2265
target Thresh 68.89802563740949
target distance 28.0
model initialize at round 2265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.7626049 ,  17.00106777]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([88.48585912, 22.33089629,  0.58688467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5882391394222948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98285277,  14.43520138,   1.01258081]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5650588557049674}
episode index:2266
target Thresh 68.90512406196827
target distance 44.0
model initialize at round 2266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.84438085, 11.20539891]), 'previousTarget': array([90.6773982 , 10.57770876]), 'currentState': array([72.1076368 ,  7.97106214,  5.99297578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5556044648912
running average episode reward sum: 0.5882247438887566
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48668005,  14.82362489,   5.47298947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5427757850134712}
episode index:2267
target Thresh 68.9122153916505
target distance 2.0
model initialize at round 2267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91071052,  14.81278297,   3.18192801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9297547329752334}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5884063026436557
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91071052,  14.81278297,   3.18192801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9297547329752334}
episode index:2268
target Thresh 68.91929963354755
target distance 18.0
model initialize at round 2268
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.46230726,  12.75027309]), 'previousTarget': array([113.64100589,  14.09400392]), 'currentState': array([97.       ,  3.       ,  3.1203754], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6543142836436554
running average episode reward sum: 0.58843534979262
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43129401,  14.60180416,   6.23406922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6942524258252146}
episode index:2269
target Thresh 68.92637679474365
target distance 8.0
model initialize at round 2269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.98737499,  21.44307282,   4.5773747 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.5904115043240585}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5885950655195396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39693421,  15.15459217,   5.26046893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.622564926639217}
episode index:2270
target Thresh 68.93344688231595
target distance 16.0
model initialize at round 2270
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.66716349,  15.05466695]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       ,  4.       ,  2.5439875], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.43155440488690266
running average episode reward sum: 0.588525915074523
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65901055,  14.08880741,   0.24995155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9729058224267574}
episode index:2271
target Thresh 68.94050990333453
target distance 63.0
model initialize at round 2271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.16933077, 12.63688841]), 'previousTarget': array([71.93730785, 11.58232602]), 'currentState': array([51.19833536, 11.56016083,  2.92982817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5885150784879816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64582529,  14.46698496,   5.42034764]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6399568382162001}
episode index:2272
target Thresh 68.94756586486244
target distance 66.0
model initialize at round 2272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.28487917,  7.73529489]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([49.53272661,  4.59643053,  0.23871708]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3036809372160386
running average episode reward sum: 0.5883897665032601
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.000995  ,  15.40241807,   5.06891816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.077010351107996}
episode index:2273
target Thresh 68.95461477395563
target distance 41.0
model initialize at round 2273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.35490521, 19.34148245]), 'previousTarget': array([93.71472851, 18.63407074]), 'currentState': array([72.71263304, 23.10726841,  1.45646632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.1548962027391088
running average episode reward sum: 0.5881991360882363
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02459945,  14.49590044,   5.53678602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0979629296320628}
episode index:2274
target Thresh 68.961656637663
target distance 34.0
model initialize at round 2274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.08065898,  18.22134582]), 'previousTarget': array([100.58914087,  17.96694159]), 'currentState': array([82.67481227, 23.06005571,  0.79867572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5882723317590561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03772009,  15.00524351,   5.67866904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9622941919818305}
episode index:2275
target Thresh 68.9686914630264
target distance 45.0
model initialize at round 2275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.42967944, 12.96143713]), 'previousTarget': array([89.98027613, 13.88801227]), 'currentState': array([69.49293688, 11.37200445,  3.56140721]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.5882401985365889
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40596417,  14.00165132,   0.31787843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1617136685051483}
episode index:2276
target Thresh 68.97571925708071
target distance 25.0
model initialize at round 2276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.09368284,  13.14071969]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([91.39153819,  6.0534222 ,  0.4126603 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 31
reward sum = 0.6050504170799262
running average episode reward sum: 0.5882475811534283
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03341383,  15.03449196,   5.7441234 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9672013834862259}
episode index:2277
target Thresh 68.98274002685368
target distance 35.0
model initialize at round 2277
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.94765822, 14.55399459]), 'previousTarget': array([99.99184173, 15.42880452]), 'currentState': array([80.      , 16.      ,  2.510641], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.3805634527201192
running average episode reward sum: 0.5881564116501652
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48426141,  15.56999772,   5.76833575]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7686895982440856}
episode index:2278
target Thresh 68.98975377936608
target distance 61.0
model initialize at round 2278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.24948872, 10.59578318]), 'previousTarget': array([73.93315042, 11.63386479]), 'currentState': array([53.35984766,  8.49764619,  3.38456941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.44819961614737347
running average episode reward sum: 0.5880950001558682
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26582982,  14.82301493,   0.36134198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7552016752375255}
episode index:2279
target Thresh 68.99676052163169
target distance 10.0
model initialize at round 2279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05833081e+02, 1.04648154e+01, 4.73384857e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.227428723199008}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5882458643434784
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37676484,  15.1299611 ,   0.60356781]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.398549409919603}
episode index:2280
target Thresh 69.00376026065724
target distance 4.0
model initialize at round 2280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.51755177,  17.96222375,   4.52704799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.372812267138642}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5884133580460899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54283772,  14.82159118,   0.19254165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49074133805471976}
episode index:2281
target Thresh 69.01075300344246
target distance 68.0
model initialize at round 2281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.30883592, 13.96256478]), 'previousTarget': array([66.9805647, 12.8814955]), 'currentState': array([46.31337402, 13.53653273,  0.98104191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5884179779621371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3278787 ,  15.15540779,   6.1195129 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6898540587411051}
episode index:2282
target Thresh 69.0177387569801
target distance 8.0
model initialize at round 2282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6465953 ,  21.50312991,   6.13014412]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.512725505293727}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.588576791835084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87243196,  15.35057166,   5.92570329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3730604430471523}
episode index:2283
target Thresh 69.02471752825592
target distance 63.0
model initialize at round 2283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.61087374,  8.81581271]), 'previousTarget': array([71.75271057,  8.13535088]), 'currentState': array([50.80218723,  6.0561137 ,  3.42897952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4691419156287948
running average episode reward sum: 0.5881136925761243
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.65166608,  19.12117809,   0.34254664]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.75194672216182}
episode index:2284
target Thresh 69.03168932424869
target distance 17.0
model initialize at round 2284
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.42588429,  14.1059966 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 12.       ,  1.5915695], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.545846800206768}
done in step count: 31
reward sum = 0.5930033696543975
running average episode reward sum: 0.5881158324785656
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1301705 ,  15.62434698,   5.45408661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.070706553572328}
episode index:2285
target Thresh 69.03865415193019
target distance 42.0
model initialize at round 2285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.16611564, 10.43785953]), 'previousTarget': array([92.45612429,  9.63241055]), 'currentState': array([74.62904139,  6.15969058,  0.65070342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5331024550766803
running average episode reward sum: 0.5880917671341203
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25631748,  15.34456912,   5.07728933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.819628919606432}
episode index:2286
target Thresh 69.04561201826527
target distance 27.0
model initialize at round 2286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.03130198,  14.26608531]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([87.11558969, 12.43185421,  1.1141057 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5881781625751666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00216226,  15.75580111,   4.76425827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2517649412335663}
episode index:2287
target Thresh 69.05256293021179
target distance 62.0
model initialize at round 2287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.17846089,  6.59230861]), 'previousTarget': array([72.57433899,  6.10429689]), 'currentState': array([51.53671058,  2.82380165,  1.67589474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.2686028305497096
running average episode reward sum: 0.5880384880419387
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14845641,  14.36175477,   5.56295849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0641820617991677}
episode index:2288
target Thresh 69.05950689472066
target distance 47.0
model initialize at round 2288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.87248474, 19.67351916]), 'previousTarget': array([87.78180356, 19.05377394]), 'currentState': array([69.18496311, 23.19509435,  6.06284017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.38075214370286525
running average episode reward sum: 0.5879479304428391
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32475514,  15.44079144,   4.3888239 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8063824822577002}
episode index:2289
target Thresh 69.06644391873584
target distance 14.0
model initialize at round 2289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.52545832,  4.10153291,  3.71057439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.92717689692648}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5880705496054269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77842659,  15.15097049,   5.78069811]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26811726405942327}
episode index:2290
target Thresh 69.07337400919438
target distance 23.0
model initialize at round 2290
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.5593782 ,  15.09338659]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([93.60126748, 16.38714727,  0.59036403]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5881568034645286
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30090413,  14.72853944,   6.20573136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7499505835995518}
episode index:2291
target Thresh 69.08029717302634
target distance 60.0
model initialize at round 2291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.58596777, 17.70210868]), 'previousTarget': array([74.97504678, 17.00124766]), 'currentState': array([53.62840302, 19.00426416,  3.41566765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4814214408103042
running average episode reward sum: 0.5881102348071751
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32617024,  15.82546789,   5.46241086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.065572041312307}
episode index:2292
target Thresh 69.08721341715489
target distance 15.0
model initialize at round 2292
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.35299435,  14.5646087 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  21.       ,   1.1460896], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.82284450514105}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.588183469878894
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90938548,  14.80470366,   0.19444206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21529433714673665}
episode index:2293
target Thresh 69.09412274849629
target distance 41.0
model initialize at round 2293
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.69292436, 19.5087638 ]), 'previousTarget': array([93.62981184, 19.16979281]), 'currentState': array([74.       , 23.       ,  4.6844616], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.35399570120323226
running average episode reward sum: 0.5880813827957747
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04271186,  15.33601199,   5.72383094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0145465237404931}
episode index:2294
target Thresh 69.10102517395987
target distance 31.0
model initialize at round 2294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.11368187,  12.12818933]), 'previousTarget': array([103.63559701,  12.80043813]), 'currentState': array([83.67304516,  7.43120687,  5.40253067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6630033696543974
running average episode reward sum: 0.5881140285416827
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47881478,  14.07969242,   1.01550364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0374148771198661}
episode index:2295
target Thresh 69.10792070044803
target distance 5.0
model initialize at round 2295
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.75482535,  11.47847168,   2.05189353]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7351869569655567}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5882847541390077
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38405639,  14.49172516,   1.88037628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6370577820042079}
episode index:2296
target Thresh 69.11480933485632
target distance 61.0
model initialize at round 2296
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.57469378, 18.63017301]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([55.65894062, 20.46396041,  1.17844456]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.24580341236241476
running average episode reward sum: 0.5881356547303109
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06205081,  15.54703536,   5.04871413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0858159863986916}
episode index:2297
target Thresh 69.12169108407338
target distance 29.0
model initialize at round 2297
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.21471099,  13.43855524]), 'previousTarget': array([105.58520839,  13.05211208]), 'currentState': array([87.60522532,  9.50560829,  1.31513947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5772031096878084
running average episode reward sum: 0.5881308973129731
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1977397 ,  14.8628091 ,   5.72478421]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8139059765187358}
episode index:2298
target Thresh 69.12856595498094
target distance 29.0
model initialize at round 2298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.31786401,  16.45864762]), 'previousTarget': array([105.70920231,  16.60186167]), 'currentState': array([87.66892568, 20.18949504,  5.38624424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7628618785849703
running average episode reward sum: 0.5882069003496291
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10859535,  15.90209757,   5.10154981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9086104612948729}
episode index:2299
target Thresh 69.13543395445389
target distance 11.0
model initialize at round 2299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.66668847,  19.2335329 ,   1.14921158]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.24858550682793}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5883604974144341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14429306e+02, 1.50177056e+01, 1.11567732e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5709681190148416}
episode index:2300
target Thresh 69.14229508936023
target distance 43.0
model initialize at round 2300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.63157231, 17.92573526]), 'previousTarget': array([91.7401454 , 18.78648796]), 'currentState': array([72.80048798, 20.51959226,  6.21808052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5884262687642706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94356939,  15.87843345,   4.33499688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8802441383472268}
episode index:2301
target Thresh 69.14914936656108
target distance 68.0
model initialize at round 2301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.62050099,  9.12274596]), 'previousTarget': array([66.78718271,  7.90987981]), 'currentState': array([46.7664669 ,  6.71083173,  0.88268042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3310753231597161
running average episode reward sum: 0.5883144742614015
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50442883,  15.96199378,   4.38168985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0821380814403938}
episode index:2302
target Thresh 69.15599679291073
target distance 11.0
model initialize at round 2302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.35593942,   4.94388425,   1.60812896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.631592379024731}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5884439012685464
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71391342,  14.84324102,   2.88619074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7309211673327315}
episode index:2303
target Thresh 69.16283737525659
target distance 23.0
model initialize at round 2303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.52824936,  14.65510713]), 'previousTarget': array([111.98112317,  14.86874449]), 'currentState': array([90.58747107, 13.11713404,  2.6954236 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5885434946002862
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32657815,  15.20360525,   5.02163911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7035283138603526}
episode index:2304
target Thresh 69.16967112043926
target distance 13.0
model initialize at round 2304
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.45154874,  14.20961313]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,   6.      ,   1.366969], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.090270269772633}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.5886462255155394
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62205543,  14.27772655,   0.55241153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9532218467538472}
episode index:2305
target Thresh 69.17649803529248
target distance 66.0
model initialize at round 2305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.42858907,  6.63794389]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([50.7715382 ,  2.95007853,  0.76653283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.18528216750682402
running average episode reward sum: 0.5884713061495338
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65509794,  14.23854229,   6.2622714 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8359277909029195}
episode index:2306
target Thresh 69.18331812664317
target distance 1.0
model initialize at round 2306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72440423,  14.22188096,   0.59578435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8254830516951691}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.588649688764987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72440423,  14.22188096,   0.59578435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8254830516951691}
episode index:2307
target Thresh 69.19013140131142
target distance 40.0
model initialize at round 2307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.47488092, 13.10326199]), 'previousTarget': array([94.9439862 , 13.49579896]), 'currentState': array([73.55207845, 11.34771598,  2.55485964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3608504789318302
running average episode reward sum: 0.5885509889340367
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76479438,  15.57884397,   5.36268622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6248055874336104}
episode index:2308
target Thresh 69.19693786611052
target distance 24.0
model initialize at round 2308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.06768834,  15.66430394]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([92.56197311, 20.08324878,  0.98207939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.6989156712611155
running average episode reward sum: 0.5885987865443992
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58897318,  15.63412718,   5.29362433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7556853335692837}
episode index:2309
target Thresh 69.2037375278469
target distance 31.0
model initialize at round 2309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.26090636,  16.9767417 ]), 'previousTarget': array([103.83555733,  16.44057325]), 'currentState': array([84.59134924, 20.5973111 ,  0.29479694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.588694513419584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02886857,  15.29153528,   5.14504797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0139472715505755}
episode index:2310
target Thresh 69.21053039332024
target distance 1.0
model initialize at round 2310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87417894,  15.10116679,   5.56471086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.161448620015315}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5888724906963388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87417894,  15.10116679,   5.56471086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.161448620015315}
episode index:2311
target Thresh 69.2173164693234
target distance 47.0
model initialize at round 2311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.17873144, 15.13986937]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([69.17902486, 15.24820434,  6.14551658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.44792746431850494
running average episode reward sum: 0.5888115283146875
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14129518e+02, 1.43113675e+01, 7.78657158e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1099344113622198}
episode index:2312
target Thresh 69.22409576264248
target distance 6.0
model initialize at round 2312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.10385089,  10.3666341 ,   1.77072424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.088641089626159}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5889764602090608
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68622323,  14.90401371,   2.16217441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3281298964070926}
episode index:2313
target Thresh 69.23086828005675
target distance 15.0
model initialize at round 2313
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.65972015,  18.57098474,   1.21362537]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.809960124316705}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5891206988582479
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21967711,  14.75865368,   0.12623804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8167936504690829}
episode index:2314
target Thresh 69.23763402833875
target distance 63.0
model initialize at round 2314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.65386129, 13.66978525]), 'previousTarget': array([71.97736275, 12.95130299]), 'currentState': array([50.66285293, 13.07013137,  1.5074476 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5891382811184401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56270493,  14.35606566,   0.49451986]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7783819190472892}
episode index:2315
target Thresh 69.2443930142542
target distance 27.0
model initialize at round 2315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.24032067,  13.74659535]), 'previousTarget': array([107.5237412 ,  13.33860916]), 'currentState': array([87.49623615, 10.55737497,  0.87364364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.4987050182452608
running average episode reward sum: 0.5890992339410338
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66097841,  14.81061475,   5.18014878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3883328626195536}
episode index:2316
target Thresh 69.25114524456211
target distance 10.0
model initialize at round 2316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.46350269,   5.85549969,   1.75769413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.240448523109844}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5892513189282845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56159957,  14.53819537,   1.67641377]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6367562010465225}
episode index:2317
target Thresh 69.2578907260147
target distance 13.0
model initialize at round 2317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03786805,   3.68254394,   2.5582937 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.358279344021224}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5893992110891899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60058733,  15.50730002,   0.40634871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6456653892369075}
episode index:2318
target Thresh 69.26462946535746
target distance 66.0
model initialize at round 2318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.32400989, 17.07844551]), 'previousTarget': array([68.99082358, 16.39421747]), 'currentState': array([50.34561839, 18.00789325,  1.65287274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5894112382258234
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27542484,  15.65142376,   4.75915691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.974352132037795}
episode index:2319
target Thresh 69.27136146932911
target distance 6.0
model initialize at round 2319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.58571055,  14.61317653,   2.17547053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.428090151600788}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.589571231661933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14009362e+02, 1.49910433e+01, 7.17225522e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9906785087274377}
episode index:2320
target Thresh 69.27808674466168
target distance 40.0
model initialize at round 2320
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.81039418, 13.7474138 ]), 'previousTarget': array([94.9007438 , 12.99007438]), 'currentState': array([75.       , 11.       ,  6.1931577], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = -0.1422073764036964
running average episode reward sum: 0.5892559457472127
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36811328,  15.62944537,   5.09610605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8918981449543599}
episode index:2321
target Thresh 69.28480529808043
target distance 39.0
model initialize at round 2321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.99770355, 18.78969683]), 'previousTarget': array([95.68542414, 18.46671874]), 'currentState': array([77.4266508 , 22.90963546,  5.85598213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5579625507334982
running average episode reward sum: 0.5892424688329088
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14476373e+02, 1.53475313e+01, 2.50595212e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6284613754912362}
episode index:2322
target Thresh 69.29151713630391
target distance 43.0
model initialize at round 2322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.75477467, 18.93017522]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([7.30597929e+01, 2.24097862e+01, 6.05874062e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5929820409839834
running average episode reward sum: 0.589244078635815
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11713193,  14.92042227,   0.3836987 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1416068642636782}
episode index:2323
target Thresh 69.29822226604398
target distance 7.0
model initialize at round 2323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.35397014,  12.99957638,   1.64594239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.9899372126904815}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5893997352499561
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99807717,  15.07626551,   0.33351761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07628974090782394}
episode index:2324
target Thresh 69.30492069400574
target distance 43.0
model initialize at round 2324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.5549623 , 15.83143735]), 'previousTarget': array([91.99459386, 15.53500945]), 'currentState': array([73.56997698, 16.60626757,  1.37852162]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5902188779410219
running average episode reward sum: 0.5894000875693932
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50545553,  14.77386373,   1.32788685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5437939394787136}
episode index:2325
target Thresh 69.31161242688765
target distance 17.0
model initialize at round 2325
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.3841792 ,  15.00079877]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 13.       ,  4.1321697], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.492734801418184}
done in step count: 25
reward sum = 0.6385213593991467
running average episode reward sum: 0.5894212059149778
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81884018,  14.53607653,   0.20155543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.498040021378431}
episode index:2326
target Thresh 69.31829747138143
target distance 36.0
model initialize at round 2326
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.14689127,  9.77897523]), 'previousTarget': array([98.12703142,  9.84437071]), 'currentState': array([79.       ,  4.       ,  2.3682134], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.21910098512311288
running average episode reward sum: 0.5892620652958149
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44222963,  14.78372723,   5.57410682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5982321432822241}
episode index:2327
target Thresh 69.32497583417212
target distance 12.0
model initialize at round 2327
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.30437452,  15.58330481]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.      ,  18.      ,   5.159349], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.58397609485614}
done in step count: 18
reward sum = 0.7645137614500874
running average episode reward sum: 0.5893373452340256
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14761812e+02, 1.45902187e+01, 9.51379647e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4739768154493649}
episode index:2328
target Thresh 69.3316475219381
target distance 43.0
model initialize at round 2328
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.1047193 , 13.57793069]), 'previousTarget': array([91.99459386, 14.46499055]), 'currentState': array([72.14318691, 12.33808205,  3.78833437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5993639024593417
running average episode reward sum: 0.5893416503251485
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62136709,  15.14193813,   5.68533856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40436284594867467}
episode index:2329
target Thresh 69.33831254135103
target distance 16.0
model initialize at round 2329
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.91422103,  13.25615945]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 15.       ,  2.4032934], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.009478783753437}
done in step count: 14
reward sum = 0.7294458127689782
running average episode reward sum: 0.5894017808669699
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45156031,  15.60156518,   5.99956878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7521883946009086}
episode index:2330
target Thresh 69.34497089907596
target distance 54.0
model initialize at round 2330
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.20852705, 18.79840108]), 'previousTarget': array([80.91481348, 18.15603579]), 'currentState': array([62.34136924, 21.0997138 ,  6.04962761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2860015762250443
running average episode reward sum: 0.5892716220490197
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14254112e+02, 1.47154092e+01, 1.10411163e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7983363376292547}
episode index:2331
target Thresh 69.35162260177123
target distance 66.0
model initialize at round 2331
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.58835218, 15.95286104]), 'previousTarget': array([68.99770471, 15.69700447]), 'currentState': array([50.59295385, 16.38186643,  5.50846762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5892917394864189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21841657,  15.85435997,   5.65645048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8818371433615145}
episode index:2332
target Thresh 69.35826765608854
target distance 34.0
model initialize at round 2332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.71926545,  9.95794838]), 'previousTarget': array([99.6810367 ,  9.14274933]), 'currentState': array([80.72647396,  3.69106149,  0.77263534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5305714154613974
running average episode reward sum: 0.5892665700376298
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2916811 ,  15.04809199,   5.23060316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7099496440443268}
episode index:2333
target Thresh 69.36490606867297
target distance 33.0
model initialize at round 2333
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.98625525,  13.74135082]), 'previousTarget': array([101.96336993,  14.20990121]), 'currentState': array([82.       , 13.       ,  1.4464022], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6404411018285138
running average episode reward sum: 0.5892884957153465
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46278803,  15.85862703,   3.98080206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9754040886833886}
episode index:2334
target Thresh 69.3715378461629
target distance 64.0
model initialize at round 2334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.34353337, 14.14807586]), 'previousTarget': array([70.99024152, 13.62469505]), 'currentState': array([49.34701421, 13.774952  ,  1.96444488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.40913383554058336
running average episode reward sum: 0.5888609058518536
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.47684396,  23.4607872 ,   0.67899619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.683467828044032}
episode index:2335
target Thresh 69.37816299519011
target distance 5.0
model initialize at round 2335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.02854102,  18.73723441,   5.10883176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.774566946417865}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5890200390299992
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71781289,  15.70952504,   1.22139021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7635806072641259}
episode index:2336
target Thresh 69.3847815223798
target distance 51.0
model initialize at round 2336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.38121013, 10.10606205]), 'previousTarget': array([83.75839053, 10.09935538]), 'currentState': array([65.64875652,  6.84565189,  5.33263028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.34786560757842633
running average episode reward sum: 0.5889168492861174
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93291178,  15.57553235,   0.80168676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5794292965777846}
episode index:2337
target Thresh 69.39139343435043
target distance 18.0
model initialize at round 2337
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.40428179,  15.17169164]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 23.       ,  3.9389863], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.654251669197291
running average episode reward sum: 0.588944794033727
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66761072,  15.91066875,   4.75238388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.129168564552721}
episode index:2338
target Thresh 69.39799873771396
target distance 64.0
model initialize at round 2338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.99584655, 15.95179562]), 'previousTarget': array([71., 15.]), 'currentState': array([50.00031787, 16.37468229,  3.18110919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5888786489002332
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02570655,  15.98907743,   0.73341106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9894114351782515}
episode index:2339
target Thresh 69.40459743907567
target distance 63.0
model initialize at round 2339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.24020546, 13.29017942]), 'previousTarget': array([71.95980905, 12.26728946]), 'currentState': array([51.25545489, 12.50931795,  3.03906882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 92
reward sum = 0.2984796143786233
running average episode reward sum: 0.5887545467487283
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0788887 ,  14.80927495,   1.04102511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2063963916332649}
episode index:2340
target Thresh 69.41118954503428
target distance 42.0
model initialize at round 2340
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.76258334, 18.92749293]), 'previousTarget': array([92.72787848, 18.71202025]), 'currentState': array([73.      , 22.      ,  4.859233], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 36
reward sum = 0.48850621804957345
running average episode reward sum: 0.5887117238829876
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26163232,  14.65317975,   2.21532636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.434437286455364}
episode index:2341
target Thresh 69.41777506218187
target distance 6.0
model initialize at round 2341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.70118045,  13.42096406,   6.06284881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.5290906207396295}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.588870513074327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09539722,  15.37992457,   0.97368136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.391718400745551}
episode index:2342
target Thresh 69.42435399710399
target distance 8.0
model initialize at round 2342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53637742,   6.65773147,   2.36131477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.355141540946207}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5890250668672531
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69994337,  14.81229557,   0.78636014]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35393069343847233}
episode index:2343
target Thresh 69.43092635637954
target distance 32.0
model initialize at round 2343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.10676054,  17.0753559 ]), 'previousTarget': array([102.84555753,  16.51930531]), 'currentState': array([83.40447675, 20.51338059,  0.2996335 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5891157688820448
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50196498,  14.88110529,   5.90112934]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.512030107491651}
episode index:2344
target Thresh 69.43749214658092
target distance 49.0
model initialize at round 2344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.81866422,  8.19193171]), 'previousTarget': array([85.33123116,  7.12869398]), 'currentState': array([66.34170562,  3.64791546,  2.37633753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.050323139005080786
running average episode reward sum: 0.5888860065665321
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56649803,  15.86042376,   0.43075767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0301694323559838}
episode index:2345
target Thresh 69.4440513742739
target distance 8.0
model initialize at round 2345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.07909543e+02, 1.04284906e+01, 1.15547180e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.436425904786045}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.58904035611612
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51361586,  14.66236928,   0.32118547]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.592084483418642}
episode index:2346
target Thresh 69.45060404601769
target distance 9.0
model initialize at round 2346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.49006936,   7.60385748,   2.88862962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.544748559572065}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5891986670040127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09469355,  14.08600814,   2.31712061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9188840978138119}
episode index:2347
target Thresh 69.45715016836499
target distance 68.0
model initialize at round 2347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.98809896, 15.95714319]), 'previousTarget': array([67., 15.]), 'currentState': array([45.9919116 , 16.34764456,  1.20304012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.08561221144467646
running average episode reward sum: 0.5889841923636553
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45672135,  15.03673158,   0.38251242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.544518959770192}
episode index:2348
target Thresh 69.4636897478619
target distance 22.0
model initialize at round 2348
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.76256907,  15.07425081]), 'previousTarget': array([112.0585156 ,  15.93592685]), 'currentState': array([93.       , 22.       ,  1.4103671], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6307431458051551
running average episode reward sum: 0.5890019696958994
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97690566,  15.40227135,   5.19305278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4029337303747614}
episode index:2349
target Thresh 69.47022279104803
target distance 49.0
model initialize at round 2349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.42275212, 17.82637836]), 'previousTarget': array([85.89668285, 17.96972624]), 'currentState': array([67.52697275, 19.86548685,  5.45801264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.48502692254462687
running average episode reward sum: 0.588957724994984
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0281465 ,  15.40284191,   1.09517313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4038240112809427}
episode index:2350
target Thresh 69.4767493044564
target distance 16.0
model initialize at round 2350
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.36184971,  13.50173352]), 'previousTarget': array([114.52228  ,  14.6118525]), 'currentState': array([99.       ,  2.       ,  2.0892532], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5501490858690777
running average episode reward sum: 0.5889412177048411
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.181854  ,  15.04259813,   5.07985044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18677654077609968}
episode index:2351
target Thresh 69.48326929461354
target distance 27.0
model initialize at round 2351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.0723969 ,  13.31880596]), 'previousTarget': array([107.17596225,  12.68176659]), 'currentState': array([88.63653566,  8.60210183,  0.25327933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5890282385662097
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13995148,  15.8103553 ,   5.39322597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1816764221466804}
episode index:2352
target Thresh 69.48978276803942
target distance 59.0
model initialize at round 2352
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.96410248, 13.19775305]), 'previousTarget': array([75.97419539, 13.01563705]), 'currentState': array([56.     , 12.     ,  2.57824], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.4771566423907612
running average episode reward sum: 0.5889806943264411
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53173917,  14.78681282,   5.39756686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5145065342995861}
episode index:2353
target Thresh 69.49628973124753
target distance 48.0
model initialize at round 2353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.7987519 , 15.37550386]), 'previousTarget': array([86.99566113, 14.41657627]), 'currentState': array([66.8005246 , 15.64178327,  0.71364295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6366187357648234
running average episode reward sum: 0.589000931387375
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05794556,  15.04374784,   6.17441143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07260551516899344}
episode index:2354
target Thresh 69.50279019074483
target distance 26.0
model initialize at round 2354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.19950718,  14.37561065]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([89.31438253, 12.23509253,  5.69812346]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5890649226578637
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34423807,  15.66221374,   6.26575132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9319606985400384}
episode index:2355
target Thresh 69.50928415303179
target distance 4.0
model initialize at round 2355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79645651,  20.54576914,   0.182634  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.549503157015972}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5892226183655641
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71088372,  15.51002594,   4.13676081]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8749183485651907}
episode index:2356
target Thresh 69.51577162460234
target distance 68.0
model initialize at round 2356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.29232195, 16.04213439]), 'previousTarget': array([66.99783772, 15.70591415]), 'currentState': array([48.29729828, 16.48826023,  5.64197702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5892167346708723
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68184167,  15.62831839,   3.79161624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7042788699116743}
episode index:2357
target Thresh 69.52225261194398
target distance 61.0
model initialize at round 2357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.65327775, 18.48467902]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([55.73125408, 20.24904092,  1.03655058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.22745593962257882
running average episode reward sum: 0.5890633161827263
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34156567,  14.85384841,   5.62374578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6744598245301031}
episode index:2358
target Thresh 69.5287271215377
target distance 39.0
model initialize at round 2358
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.08176821, 19.22441562]), 'previousTarget': array([95.68542414, 18.46671874]), 'currentState': array([76.56248668, 23.58304484,  0.24434471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7028014251051551
running average episode reward sum: 0.5891115307265681
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23406886,  15.33821205,   5.56892854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4113096449528514}
episode index:2359
target Thresh 69.535195159858
target distance 8.0
model initialize at round 2359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.63193675,  17.93378895,   0.96945017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.011372702824324}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.58926486908215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69288675,  14.86974819,   0.20766443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.33359269153867366}
episode index:2360
target Thresh 69.54165673337292
target distance 13.0
model initialize at round 2360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.78876481,   7.30536319,   5.5561136 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.28869422421127}
done in step count: 11
reward sum = 0.8260382542587164
running average episode reward sum: 0.5893651542939995
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44657036,  15.40720989,   2.80484634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.604355011001922}
episode index:2361
target Thresh 69.54811184854404
target distance 65.0
model initialize at round 2361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.2535731 , 10.93270701]), 'previousTarget': array([69.91533358, 10.83833848]), 'currentState': array([48.32884914,  9.19910521,  2.01300907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 92
reward sum = 0.3307741959639551
running average episode reward sum: 0.5892556746334026
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85884384,  15.9072768 ,   4.60693432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2493053804976935}
episode index:2362
target Thresh 69.55456051182647
target distance 46.0
model initialize at round 2362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.36430555, 19.10557361]), 'previousTarget': array([88.77237675, 18.99116006]), 'currentState': array([70.63637832, 22.39326082,  1.24585121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5893131120694264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70810609,  14.870367  ,   4.97345787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3193849821932697}
episode index:2363
target Thresh 69.56100272966887
target distance 5.0
model initialize at round 2363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06829911,  11.45872608,   1.21230555]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5419324862461004}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5894742736125442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31271706,  14.95755995,   6.07744819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6885920437892596}
episode index:2364
target Thresh 69.56743850851345
target distance 65.0
model initialize at round 2364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.14785606, 14.14671055]), 'previousTarget': array([69.9787322 , 12.92209533]), 'currentState': array([50.1514744 , 13.76628947,  0.63644433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.29453714971195133
running average episode reward sum: 0.5893495644692459
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82867978,  15.06515722,   2.08615279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18329233606310233}
episode index:2365
target Thresh 69.57386785479602
target distance 27.0
model initialize at round 2365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.16062728,  15.92965811]), 'previousTarget': array([107.6656401,  16.3582148]), 'currentState': array([89.40937134, 19.07415945,  0.42323047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5894259353827437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12624929,  15.11592823,   4.66434779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.17140081117959471}
episode index:2366
target Thresh 69.58029077494591
target distance 55.0
model initialize at round 2366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.5330045 , 17.83834807]), 'previousTarget': array([79.94731634, 17.54928608]), 'currentState': array([61.60454677, 19.52848782,  1.31745094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6341476949995691
running average episode reward sum: 0.5894448292397849
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36193971,  15.9791158 ,   4.34891111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0438716886931787}
episode index:2367
target Thresh 69.58670727538603
target distance 19.0
model initialize at round 2367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.29781178,  15.60871412]), 'previousTarget': array([114.4327075,  15.23886  ]), 'currentState': array([94.46574267, 22.34319056,  4.55608273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.589559108600174
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15445625e+02, 1.55964751e+01, 7.05287715e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.744556164483882}
episode index:2368
target Thresh 69.5931173625329
target distance 67.0
model initialize at round 2368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.40438464,  7.31017178]), 'previousTarget': array([67.73578183,  7.24020299]), 'currentState': array([49.68289479,  3.98408729,  1.00054472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5895761050217033
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84157234,  15.59006563,   5.34892422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6109638062368111}
episode index:2369
target Thresh 69.5995210427966
target distance 12.0
model initialize at round 2369
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.51400976,  13.38907729]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.        ,   3.        ,   0.90283275], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.50823483827214}
done in step count: 14
reward sum = 0.6608388127689782
running average episode reward sum: 0.5896061736747613
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33012347,  15.19414994,   1.01208789]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38298264372902063}
episode index:2370
target Thresh 69.6059183225808
target distance 44.0
model initialize at round 2370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.45397993, 18.0208805 ]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([71.61657625, 20.56595715,  0.18570018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5896632694834003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68529257,  15.55552996,   4.54542917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.882178807064828}
episode index:2371
target Thresh 69.6123092082828
target distance 23.0
model initialize at round 2371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.17929987,  12.92769889]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([93.59877262,  3.39223553,  0.5998637 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6017087618562873
running average episode reward sum: 0.589668347684232
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24990544,  15.6547852 ,   4.95455009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9956834371916049}
episode index:2372
target Thresh 69.61869370629346
target distance 16.0
model initialize at round 2372
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.48525236,  15.1265271 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 18.       ,  5.3018994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.767511043761864}
done in step count: 12
reward sum = 0.8163848717161293
running average episode reward sum: 0.5897638877280718
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0453253 ,  15.38142999,   0.59555381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3841135468937962}
episode index:2373
target Thresh 69.62507182299731
target distance 8.0
model initialize at round 2373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.49778378,   8.55688333,   2.16714686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.9119344877540145}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5899160470213203
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42494778,  14.98828908,   3.12539095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5751714523078818}
episode index:2374
target Thresh 69.63144356477245
target distance 35.0
model initialize at round 2374
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.97868535, 15.0768903 ]), 'previousTarget': array([99.99184173, 15.42880452]), 'currentState': array([80.       , 16.       ,  5.6568217], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6078720943315962
running average episode reward sum: 0.589923607462293
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30620116,  15.49255612,   5.12119585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5799747311522679}
episode index:2375
target Thresh 69.63780893799061
target distance 45.0
model initialize at round 2375
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.15986534, 12.98243036]), 'previousTarget': array([89.87767469, 12.20863052]), 'currentState': array([71.23110379, 11.29587679,  0.31686985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5899010252307041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88874964,  15.87068106,   4.29085524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8777597302321507}
episode index:2376
target Thresh 69.6441679490172
target distance 40.0
model initialize at round 2376
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.21871758,  7.53542179]), 'previousTarget': array([94.02068137,  8.18172145]), 'currentState': array([75.        ,  2.        ,  0.45803803], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.27050964343680994
running average episode reward sum: 0.5897666578004164
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32013508,  14.48255349,   0.75275505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6084713300911837}
episode index:2377
target Thresh 69.6505206042112
target distance 10.0
model initialize at round 2377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31737234,   4.96605547,   2.15176344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.057137926272176}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5899145608666908
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93686868,  14.622956  ,   3.08868217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3822927411798996}
episode index:2378
target Thresh 69.65686690992527
target distance 32.0
model initialize at round 2378
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.93336441,  10.44419989]), 'previousTarget': array([101.91373199,  10.50159537]), 'currentState': array([83.      ,  4.      ,  2.414168], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.42967804429956913
running average episode reward sum: 0.5898472062989871
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09833382,  15.36432524,   2.83952605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37736245579893773}
episode index:2379
target Thresh 69.66320687250573
target distance 66.0
model initialize at round 2379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.25370777,  6.35695483]), 'previousTarget': array([68.62296546,  5.86512956]), 'currentState': array([50.61667714,  2.56393123,  5.65883893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.38715148003532385
running average episode reward sum: 0.5897620400274477
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48922214,  14.94700407,   0.60087419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5135198075785725}
episode index:2380
target Thresh 69.66954049829252
target distance 23.0
model initialize at round 2380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.95302352,  15.56327619]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([91.14397666, 18.32038747,  1.15724432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 21
reward sum = 0.6850072494030193
running average episode reward sum: 0.5898020422153417
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55678513,  15.61073319,   5.96374774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7546088023006919}
episode index:2381
target Thresh 69.67586779361929
target distance 11.0
model initialize at round 2381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.44929707,   5.62188786,   0.29055214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.89767362091857}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5899457295812912
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82138571,  15.4830614 ,   6.13656845]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.515025613234832}
episode index:2382
target Thresh 69.68218876481332
target distance 6.0
model initialize at round 2382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.96824788,   7.67038515,   3.04246807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.393291401982981}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5901012689352226
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58956531,  14.3212496 ,   2.2887758 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7931952739676705}
episode index:2383
target Thresh 69.68850341819561
target distance 48.0
model initialize at round 2383
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.67745785, 15.07674073]), 'previousTarget': array([87., 15.]), 'currentState': array([68.67754284, 15.13504847,  5.35351581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5438886162929977
running average episode reward sum: 0.5900818844332755
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00131198,  15.14384793,   4.69382763]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1438539107667681}
episode index:2384
target Thresh 69.69481176008077
target distance 3.0
model initialize at round 2384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.05419785,  13.3652071 ,   1.82863544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4641986570408876}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5902454140414796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86369971,  14.43449034,   1.7467097 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.032365421407679}
episode index:2385
target Thresh 69.70111379677716
target distance 3.0
model initialize at round 2385
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28330374,  16.27692685,   4.96478105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3079767555530066}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5904129557790984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38988478,  15.19482714,   4.65323746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4358529053708371}
episode index:2386
target Thresh 69.70740953458684
target distance 20.0
model initialize at round 2386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.1565257 ,  15.25304229]), 'currentState': array([96.36802934, 21.70807788,  5.75080544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.80274323532312}
done in step count: 15
reward sum = 0.45451578999021847
running average episode reward sum: 0.5903560235772597
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54319378,  15.91501592,   6.21665563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0641022597241416}
episode index:2387
target Thresh 69.71369897980553
target distance 32.0
model initialize at round 2387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.92424422,  10.64180563]), 'previousTarget': array([102.2530188 ,  11.41491154]), 'currentState': array([82.95040811,  4.3177608 ,  5.69291782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6271132180495734
running average episode reward sum: 0.5903714160372564
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82483693,  15.19486957,   6.11921359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2620233738443062}
episode index:2388
target Thresh 69.71998213872266
target distance 43.0
model initialize at round 2388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.37460101,  9.13565023]), 'previousTarget': array([91.57581251, 10.09726308]), 'currentState': array([71.96365872,  4.3174224 ,  3.68079376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.4639253272614478
running average episode reward sum: 0.5903184875781623
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83100151,  15.57609107,   0.4759493 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6003677261745749}
episode index:2389
target Thresh 69.72625901762142
target distance 60.0
model initialize at round 2389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.5754452 , 18.98643121]), 'previousTarget': array([74.9007438 , 19.00992562]), 'currentState': array([56.68221838, 21.05029325,  1.03988808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.41637623922058853
running average episode reward sum: 0.5902457083947491
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82465677,  14.58437313,   0.31671507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4510997037113595}
episode index:2390
target Thresh 69.73252962277867
target distance 13.0
model initialize at round 2390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.46582607,   2.23273764,   1.98103833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.247391202329926}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5903847711241649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51834999,  14.34054557,   2.58242641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8166191741981331}
episode index:2391
target Thresh 69.73879396046502
target distance 39.0
model initialize at round 2391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.01302255, 10.90941132]), 'previousTarget': array([95.59205401, 11.01888287]), 'currentState': array([74.38243202,  7.0831971 ,  2.08020449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4980015799943175
running average episode reward sum: 0.5903461493887426
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48731505,  15.65746919,   5.86020837]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8337335315169903}
episode index:2392
target Thresh 69.7450520369448
target distance 23.0
model initialize at round 2392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.63044748,  16.74550663]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([90.61018479, 22.9285162 ,  1.54260802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5904344420925246
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9849448 ,  15.34599232,   0.44187737]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3463197173276007}
episode index:2393
target Thresh 69.75130385847612
target distance 47.0
model initialize at round 2393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.86907806,  9.97125817]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([67.18117641,  6.45180811,  1.07432652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.05274492475220727
running average episode reward sum: 0.590209843296643
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32639774,  15.62863487,   5.45924939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9213695270842631}
episode index:2394
target Thresh 69.75754943131075
target distance 46.0
model initialize at round 2394
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.05651844, 19.34583425]), 'previousTarget': array([88.83200822, 18.41321632]), 'currentState': array([69.33134932, 22.650029  ,  0.36261797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5902661149010945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47530836,  15.5807689 ,   4.41681736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7504735493141873}
episode index:2395
target Thresh 69.7637887616943
target distance 25.0
model initialize at round 2395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.83912229,  16.03623181]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([91.43190393, 20.86943732,  5.18733019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5903645717078284
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09600627,  15.32575434,   6.13243419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9608957046627604}
episode index:2396
target Thresh 69.7700218558661
target distance 6.0
model initialize at round 2396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.45040501,  10.31379835,   0.96653736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.053426624798231}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5905071252231389
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86325998,  15.64839492,   2.56950581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.079645207422636}
episode index:2397
target Thresh 69.77624872005923
target distance 53.0
model initialize at round 2397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.19997765, 17.26193176]), 'previousTarget': array([81.98577525, 16.2458198 ]), 'currentState': array([62.24736529, 18.63788959,  2.109366  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 49
reward sum = 0.3964921234598543
running average episode reward sum: 0.5904262182165654
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93865709,  15.93584685,   6.13388772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9378551519451535}
episode index:2398
target Thresh 69.78246936050057
target distance 6.0
model initialize at round 2398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.89515518,  10.54336424,   1.62918675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.842851990107051}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5905805199221859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0594762 ,  14.69091171,   2.17815626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.314758624989887}
episode index:2399
target Thresh 69.78868378341075
target distance 18.0
model initialize at round 2399
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.44178247,  14.69223766]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 10.       ,  2.0158474], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5409446540331749
running average episode reward sum: 0.5905598383113987
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97004451,  15.74771079,   0.26217985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7483105987617449}
episode index:2400
target Thresh 69.79489199500418
target distance 46.0
model initialize at round 2400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.49530276, 14.78802332]), 'previousTarget': array([88.99527578, 14.43467991]), 'currentState': array([70.49605102, 14.61502077,  1.2735483 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 30
reward sum = 0.683586232120547
running average episode reward sum: 0.5905985831651301
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59952733,  15.80514676,   5.94888437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8992439359776782}
episode index:2401
target Thresh 69.8010940014891
target distance 27.0
model initialize at round 2401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.48722137,  13.25151366]), 'previousTarget': array([107.35993796,  13.01924318]), 'currentState': array([86.89619876,  9.22760598,  1.40053272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5906606571827084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48011878,  15.61649929,   5.80083436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7813996554719548}
episode index:2402
target Thresh 69.80728980906751
target distance 28.0
model initialize at round 2402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.21619788,  12.86548102]), 'previousTarget': array([106.55604828,  13.19058177]), 'currentState': array([87.928277  ,  7.57623645,  6.27187741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.590758662994882
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49013911,  14.63373013,   0.40552896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6118741455750947}
episode index:2403
target Thresh 69.8134794239352
target distance 45.0
model initialize at round 2403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.15870326, 15.25828704]), 'previousTarget': array([89.99506356, 14.44433475]), 'currentState': array([69.15970221, 15.45817959,  1.08358073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.592988487100669
running average episode reward sum: 0.590759590542347
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43764878,  15.84757658,   5.2589857 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.017165155723807}
episode index:2404
target Thresh 69.81966285228181
target distance 34.0
model initialize at round 2404
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.9584262 ,  12.28888463]), 'previousTarget': array([100.86301209,  13.33682495]), 'currentState': array([81.       , 11.       ,  2.1766973], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.299548206045292
running average episode reward sum: 0.5906385047275873
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77108731,  15.01907106,   0.66295843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22970573999222185}
episode index:2405
target Thresh 69.82584010029075
target distance 20.0
model initialize at round 2405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.20019872,  15.29019832]), 'previousTarget': array([114.77872706,  15.03319094]), 'currentState': array([93.45521732, 18.4738612 ,  1.83395529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5907504830525722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88045677,  15.99277097,   5.92963493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9999423864579055}
episode index:2406
target Thresh 69.83201117413928
target distance 3.0
model initialize at round 2406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.01351867,  13.40276976,   1.87250907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8916565534608092}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5909163532299496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53514402,  14.56092651,   3.09381754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6394346078944654}
episode index:2407
target Thresh 69.83817607999848
target distance 46.0
model initialize at round 2407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.01278014, 12.39439046]), 'previousTarget': array([88.83200822, 11.58678368]), 'currentState': array([68.10535199, 10.4723331 ,  1.17555296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5909690169257006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90318755,  15.31700971,   5.94857987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3314631334325651}
episode index:2408
target Thresh 69.84433482403323
target distance 3.0
model initialize at round 2408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.28297335,  16.34099   ,   3.87133074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6476822852586883}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5911264805965493
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43431603,  14.55634611,   3.63590968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6208536005787022}
episode index:2409
target Thresh 69.85048741240229
target distance 69.0
model initialize at round 2409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.73132013, 15.79314631]), 'previousTarget': array([66., 15.]), 'currentState': array([44.73380916, 16.10866985,  1.41242599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.3324124667489208
running average episode reward sum: 0.5910191303833345
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72858322,  14.9662007 ,   4.29993244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2735131795233954}
episode index:2410
target Thresh 69.85663385125827
target distance 2.0
model initialize at round 2410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.05486043,  15.41073869,   1.1911844 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.973643112222355}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5911805077660042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68548905,  15.93118534,   0.68446648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9828648309230038}
episode index:2411
target Thresh 69.86277414674757
target distance 5.0
model initialize at round 2411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.46904597,  16.44533278,   5.35698199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.716680794257964}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5913296825347164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49776721,  15.58184769,   1.00296929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7686250749650813}
episode index:2412
target Thresh 69.8689083050105
target distance 64.0
model initialize at round 2412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.05006663, 11.19687329]), 'previousTarget': array([70.96105157, 12.24756572]), 'currentState': array([50.12126981,  9.51073643,  3.28643274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.3053274301016892
running average episode reward sum: 0.591211156943157
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2793679 ,  15.91435618,   5.38455827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1641983668164215}
episode index:2413
target Thresh 69.87503633218125
target distance 68.0
model initialize at round 2413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.74958366, 14.23186407]), 'previousTarget': array([66.99135509, 13.58798103]), 'currentState': array([45.75201574, 13.91997127,  1.55060971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4598720950572145
running average episode reward sum: 0.591156749709567
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12605052,  15.26042772,   0.65569597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2893291067408918}
episode index:2414
target Thresh 69.8811582343878
target distance 16.0
model initialize at round 2414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.86987695, 23.24708058,  1.2970345 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.011981848058664}
done in step count: 12
reward sum = 0.6826152210161291
running average episode reward sum: 0.5911946207121783
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18788415,  15.59983844,   5.34043461]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0096228525702964}
episode index:2415
target Thresh 69.88727401775208
target distance 7.0
model initialize at round 2415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.39974924,  10.93439293,   5.86179442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.920402402710236}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.591347518638208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68663023,  14.68808376,   0.29345286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44214517350121263}
episode index:2416
target Thresh 69.89338368838987
target distance 11.0
model initialize at round 2416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47682774,   4.19071114,   1.12050181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.819800849783007}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5914808118648713
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38339211,  15.40629382,   5.72919409]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7384307373027609}
episode index:2417
target Thresh 69.89948725241084
target distance 68.0
model initialize at round 2417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.40879962, 10.50292617]), 'previousTarget': array([66.86301209,  9.33682495]), 'currentState': array([47.49749687,  8.62143131,  0.37063515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.23862233842437014
running average episode reward sum: 0.5911375103138832
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.34419793,  22.05497448,   0.35570186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.410762233044405}
episode index:2418
target Thresh 69.90558471591856
target distance 7.0
model initialize at round 2418
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.47979647,  13.5562242 ,   0.9947201 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.657542016876292}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5912862711818394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02878016,  15.91649653,   6.04655546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3353777996659089}
episode index:2419
target Thresh 69.91167608501047
target distance 41.0
model initialize at round 2419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.37307313, 17.78826867]), 'previousTarget': array([93.85291755, 17.57891249]), 'currentState': array([75.57188896, 20.60129293,  1.37535494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5912894404939403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24419382,  15.55685574,   5.31862726]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.608045177046123}
episode index:2420
target Thresh 69.91776136577796
target distance 36.0
model initialize at round 2420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.77389346, 16.49665692]), 'previousTarget': array([98.96920706, 15.89059961]), 'currentState': array([77.84895543, 18.22779659,  3.33394122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5913027556341292
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09017486,  15.54152544,   1.16370397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5489820613150787}
episode index:2421
target Thresh 69.92384056430633
target distance 64.0
model initialize at round 2421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.31682433, 20.33808824]), 'previousTarget': array([70.88143384, 19.82546817]), 'currentState': array([52.47142084, 22.82001497,  5.78416187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = -0.3932664160674854
running average episode reward sum: 0.5908962448283068
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44878761,  15.28137015,   5.92169478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6188733810842013}
episode index:2422
target Thresh 69.92991368667475
target distance 35.0
model initialize at round 2422
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.26216893,  9.38227166]), 'previousTarget': array([99.07987434,  9.99653193]), 'currentState': array([80.     ,  4.     ,  3.51527], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.024740217232824047
running average episode reward sum: 0.590662585716629
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07822535,  15.72315364,   5.59715824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7273722522663878}
episode index:2423
target Thresh 69.93598073895635
target distance 47.0
model initialize at round 2423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.45044536, 15.35452452]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([67.45210117, 15.61187544,  0.92155814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5907091142270593
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50742212,  15.63513037,   5.94539544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8129377544101779}
episode index:2424
target Thresh 69.94204172721817
target distance 35.0
model initialize at round 2424
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.53727327, 13.27725999]), 'previousTarget': array([99.7124451, 12.3792763]), 'currentState': array([80.       ,  9.       ,  6.1901646], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.640553227272292
running average episode reward sum: 0.5907296685004799
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33145826,  14.38289284,   6.07265586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9098182785552916}
episode index:2425
target Thresh 69.94809665752123
target distance 58.0
model initialize at round 2425
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.90765026, 12.91975549]), 'previousTarget': array([76.95260657, 12.37604183]), 'currentState': array([57.      , 11.      ,  6.140066], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.0584024323248738
running average episode reward sum: 0.5905102425993358
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51010484,  15.34319185,   0.16064427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5981453912236417}
episode index:2426
target Thresh 69.95414553592043
target distance 61.0
model initialize at round 2426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.32281816,  9.08120259]), 'previousTarget': array([73.83019061,  9.60068074]), 'currentState': array([52.51242982,  6.33374669,  2.5526824 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.2171057487266157
running average episode reward sum: 0.5903563882549301
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20480217,  15.9667122 ,   0.29057252]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9881682061993957}
episode index:2427
target Thresh 69.96018836846467
target distance 27.0
model initialize at round 2427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.67745975,  15.16355695]), 'previousTarget': array([107.98629668,  14.74023321]), 'currentState': array([88.68414838, 15.68076125,  1.5523504 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7634949460374765
running average episode reward sum: 0.5904276973808702
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18048187,  15.52460569,   5.49928826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9730473226365693}
episode index:2428
target Thresh 69.96622516119677
target distance 8.0
model initialize at round 2428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02350169,   8.37070753,   1.17979908]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.629334125141692}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5905761380364977
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64832679,  15.37650134,   5.8680746 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5151963753271586}
episode index:2429
target Thresh 69.97225592015353
target distance 62.0
model initialize at round 2429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.32775437, 18.90433886]), 'previousTarget': array([72.9352791, 18.3923162]), 'currentState': array([51.40720368, 20.68525439,  1.81394804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5905771217024915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40085201,  15.71222489,   0.30260865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8172800159386557}
episode index:2430
target Thresh 69.97828065136572
target distance 31.0
model initialize at round 2430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.56950283,  15.93850525]), 'previousTarget': array([103.98960229,  15.35517412]), 'currentState': array([83.63657682, 17.57510561,  0.78755736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6658720120056872
running average episode reward sum: 0.5906080945080461
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14363802e+02, 1.49814959e+01, 4.22244072e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6364671048288284}
episode index:2431
target Thresh 69.98429936085805
target distance 55.0
model initialize at round 2431
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.99352324, 14.49105155]), 'previousTarget': array([80., 15.]), 'currentState': array([60.       , 15.       ,  3.1241062], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.3722176288422079
running average episode reward sum: 0.5905182957968348
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69812966,  15.47086889,   5.06680152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5593238893280934}
episode index:2432
target Thresh 69.99031205464924
target distance 29.0
model initialize at round 2432
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.09251978,  16.94021556]), 'previousTarget': array([105.44164417,  17.30718934]), 'currentState': array([87.66866949, 21.70614043,  5.11809266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5906017713364348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35768723,  15.92227147,   5.00319266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9892041364027336}
episode index:2433
target Thresh 69.99631873875197
target distance 2.0
model initialize at round 2433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.09412004,  16.04127175,   4.96532607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.510412365448388}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5907617952594683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93995765,  15.1644758 ,   3.872491  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9542393161179966}
episode index:2434
target Thresh 70.00231941917296
target distance 6.0
model initialize at round 2434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.77060467,  19.63254033,   4.40847588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.141073179676393}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5909097329410454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06004776,  15.84994706,   5.55057327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8520655733040342}
episode index:2435
target Thresh 70.00831410191286
target distance 5.0
model initialize at round 2435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.20160833,  16.51846066,   5.22811294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.993919309942432}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5910614924964884
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28008123,  15.5775569 ,   5.30585438]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.92295991591928}
episode index:2436
target Thresh 70.01430279296636
target distance 19.0
model initialize at round 2436
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.87367126,  15.61698822]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       ,  9.       ,  2.8911946], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.4151623111609105
running average episode reward sum: 0.5909893139239256
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48162538,  15.72299098,   6.14335117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8896225055859973}
episode index:2437
target Thresh 70.02028549832215
target distance 7.0
model initialize at round 2437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.57825858,  21.75154792,   4.27286446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.659023004179899}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5911330755463527
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52228463,  15.80455783,   5.15869296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9356950712116806}
episode index:2438
target Thresh 70.02626222396296
target distance 7.0
model initialize at round 2438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46116278,  21.37834876,   4.87093383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.40106853882669}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5912845568642919
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51443665,  15.377198  ,   5.95975878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6379054828560177}
episode index:2439
target Thresh 70.03223297586548
target distance 44.0
model initialize at round 2439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.90864872, 16.28502378]), 'previousTarget': array([90.99483671, 15.54557189]), 'currentState': array([71.93954572, 17.39629579,  6.25168676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.3996354603398296
running average episode reward sum: 0.5912060121526015
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15181251e+02, 1.58342772e+01, 3.84608398e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8537390565466558}
episode index:2440
target Thresh 70.03819776000047
target distance 14.0
model initialize at round 2440
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.97961262,  16.38577769]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  20.       ,   4.4211354], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.473416291527968}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.5912735386753816
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01611998,  15.97920726,   5.62359984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9793399319567025}
episode index:2441
target Thresh 70.04415658233275
target distance 7.0
model initialize at round 2441
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.70556163,  14.20377962,   1.06927043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.353974651248876}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5914208427340322
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94687618,  15.5582221 ,   5.83719218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5607441965375366}
episode index:2442
target Thresh 70.05010944882109
target distance 9.0
model initialize at round 2442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.91585098,  21.87083172,   0.12326234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.868814308361928}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5915641334858401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64413613,  15.74954555,   5.36345951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8297334649084825}
episode index:2443
target Thresh 70.0560563654184
target distance 9.0
model initialize at round 2443
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.44941356,  13.14869399,   1.11223143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.7742324070985696}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5917111980997576
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0591644 ,  15.8158757 ,   1.06752684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2453211565697204}
episode index:2444
target Thresh 70.06199733807158
target distance 52.0
model initialize at round 2444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.92974692,  7.88185478]), 'previousTarget': array([82.64012894,  8.77694787]), 'currentState': array([63.40490046,  3.54822223,  5.99437714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5917373527101765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72997166,  15.74078344,   5.65824643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7884639563971447}
episode index:2445
target Thresh 70.06793237272159
target distance 32.0
model initialize at round 2445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.37082534,  15.72734755]), 'previousTarget': array([102.91268452,  16.13318583]), 'currentState': array([84.4174873 , 17.09274157,  0.4406535 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5918166416668801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63618146,  15.8693998 ,   5.19961396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9424542142883898}
episode index:2446
target Thresh 70.07386147530349
target distance 67.0
model initialize at round 2446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.60638453, 12.63770671]), 'previousTarget': array([67.94453985, 11.4883985 ]), 'currentState': array([47.6311827 , 11.64206026,  0.78173614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.49558657621891733
running average episode reward sum: 0.5917773159351891
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30958166,  15.54355288,   6.15518509]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8787076960674235}
episode index:2447
target Thresh 70.07978465174637
target distance 28.0
model initialize at round 2447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.58081253,  16.07668896]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.85634507, 19.38506974,  5.91866458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5918630403116613
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79732351,  15.92810148,   5.43635561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9499737404482944}
episode index:2448
target Thresh 70.0857019079734
target distance 11.0
model initialize at round 2448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83282231,   2.20516568,   4.00273323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.79592645236726}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5919869583247062
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03186828,  15.1106999 ,   1.22327423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11519572710110276}
episode index:2449
target Thresh 70.09161324990184
target distance 53.0
model initialize at round 2449
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.9999975 , 13.98999628]), 'previousTarget': array([81.99644096, 14.37729134]), 'currentState': array([62.      , 14.      ,  6.010027], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.525097179635268
running average episode reward sum: 0.5919596563742207
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58387906,  15.75584043,   5.9752723 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9550966005402163}
episode index:2450
target Thresh 70.09751868344306
target distance 12.0
model initialize at round 2450
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.27369318,  15.19900365]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  10.       ,   0.7816084], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.51426986445371}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.592066055818551
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60463004,  15.5487892 ,   0.20850554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6763778517266288}
episode index:2451
target Thresh 70.10341821450245
target distance 39.0
model initialize at round 2451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.33902477, 18.12738336]), 'previousTarget': array([95.59205401, 18.98111713]), 'currentState': array([76.61410603, 21.43307247,  6.09590816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.405929016312825
running average episode reward sum: 0.5919901434859629
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11393379,  15.40359951,   5.9414179 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.973655940172548}
episode index:2452
target Thresh 70.10931184897959
target distance 66.0
model initialize at round 2452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.56907163, 18.79174586]), 'previousTarget': array([68.94285376, 18.48917775]), 'currentState': array([50.64150577, 20.49236901,  5.61733336]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4478514848717744
running average episode reward sum: 0.5915662374002076
{'dynamicTrap': 14, 'scaleFactor': 20, 'currentTarget': array([110.67012194,  15.005709  ]), 'previousTarget': array([111.67448457,  15.37982748]), 'currentState': array([94.41481077, 23.85159321,  1.19131871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.50634509358854}
episode index:2453
target Thresh 70.11519959276806
target distance 12.0
model initialize at round 2453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.50875164,   4.49020445,   1.9902193 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.161200442669502}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5916974317808448
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87639737,  15.24390163,   2.50662285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27343301934214304}
episode index:2454
target Thresh 70.12108145175564
target distance 44.0
model initialize at round 2454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.76535746, 14.52439735]), 'previousTarget': array([90.9793708 , 13.90815322]), 'currentState': array([69.76890869, 14.14752005,  1.38131452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.4258426544197602
running average episode reward sum: 0.5916298738267262
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30880147,  14.23669266,   0.2722465 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8234053919797362}
episode index:2455
target Thresh 70.1269574318242
target distance 22.0
model initialize at round 2455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.13345915,  13.38042123]), 'previousTarget': array([110.88854382,  12.94427191]), 'currentState': array([92.68638474,  5.65349132,  0.74823833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.643243610174679
running average episode reward sum: 0.5916508891916887
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43422648,  15.42688736,   0.6349563 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6089215485690846}
episode index:2456
target Thresh 70.13282753884968
target distance 15.0
model initialize at round 2456
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.22603257,  7.25717658,  3.80743372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.47477463647109}
done in step count: 36
reward sum = 0.4537691244251734
running average episode reward sum: 0.5915947712573109
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95293311,  15.32512083,   4.92487982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.006868943386186}
episode index:2457
target Thresh 70.13869177870221
target distance 26.0
model initialize at round 2457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.09705354,  17.3509313 ]), 'previousTarget': array([108.31231517,  16.80053053]), 'currentState': array([89.16488564, 23.79864552,  0.70059955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5916550257740444
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95800708,  15.93055402,   4.98683636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9315010428263567}
episode index:2458
target Thresh 70.14455015724604
target distance 64.0
model initialize at round 2458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.8362732 , 13.08156112]), 'previousTarget': array([70.96105157, 12.24756572]), 'currentState': array([49.85429214, 12.23277799,  1.30960345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.525513548292933
running average episode reward sum: 0.5916281280605507
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37506132,  15.7246038 ,   5.8554833 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8159176829749608}
episode index:2459
target Thresh 70.15040268033954
target distance 63.0
model initialize at round 2459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.63278884, 17.11705226]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([53.65892842, 18.13925572,  1.09374684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5916623154268107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7599987 ,  15.11581686,   5.56946315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.266484840792351}
episode index:2460
target Thresh 70.15624935383522
target distance 65.0
model initialize at round 2460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.8586842 , 10.64375547]), 'previousTarget': array([69.85022022,  9.44310403]), 'currentState': array([50.95537289,  8.6795256 ,  1.51135623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.3021491286652317
running average episode reward sum: 0.5915446749608368
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59766428,  15.27295839,   6.21162434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.657045567912897}
episode index:2461
target Thresh 70.16209018357978
target distance 66.0
model initialize at round 2461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.40536037,  9.92591069]), 'previousTarget': array([68.81660336,  8.70226409]), 'currentState': array([49.52806964,  7.71382723,  0.4637599 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.35960349914386364
running average episode reward sum: 0.5914504665222434
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93716442,  15.6525588 ,   1.16845429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6555770700365177}
episode index:2462
target Thresh 70.16792517541404
target distance 25.0
model initialize at round 2462
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.32975302,  13.28272999]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([91.55853012,  6.38047016,  0.60787421]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5915229767452572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12880985,  15.75854483,   0.86144864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7694038187777882}
episode index:2463
target Thresh 70.17375433517299
target distance 68.0
model initialize at round 2463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.40830666, 17.53599875]), 'previousTarget': array([66.99135509, 16.41201897]), 'currentState': array([47.43664093, 18.60022059,  2.26099607]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.591538520842034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84694032,  15.85991156,   5.24486374]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.206961387516023}
episode index:2464
target Thresh 70.17957766868578
target distance 15.0
model initialize at round 2464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.3215574 ,  7.12336339,  4.07822585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.444832659838813}
done in step count: 31
reward sum = 0.667711241044443
running average episode reward sum: 0.5915694225540835
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74849532,  15.98676219,   5.43773229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0183094965954393}
episode index:2465
target Thresh 70.18539518177575
target distance 46.0
model initialize at round 2465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.2325036 , 10.98333133]), 'previousTarget': array([88.7042351, 10.4268235]), 'currentState': array([70.49043409,  7.78166125,  1.49301737]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5915652039780753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41396264,  15.67889752,   0.4083175 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7951521324406293}
episode index:2466
target Thresh 70.19120688026042
target distance 9.0
model initialize at round 2466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.42990004,  21.53749631,   5.06967497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.002263316333583}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5916994477926071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75436998,  15.65137766,   5.26942736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9966779479845667}
episode index:2467
target Thresh 70.1970127699515
target distance 53.0
model initialize at round 2467
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.93453997, 13.61682286]), 'previousTarget': array([81.96803691, 13.13026624]), 'currentState': array([62.        , 12.        ,  0.16010977], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.35187221265465873
running average episode reward sum: 0.5916022730620001
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94701632,  14.84457562,   0.81489836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16420720569202618}
episode index:2468
target Thresh 70.20281285665484
target distance 5.0
model initialize at round 2468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67140918,  20.19692499,   5.39046437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.240116375973637}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5917556536723436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51906374,  15.47696293,   5.18554123]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7049260948070377}
episode index:2469
target Thresh 70.20860714617058
target distance 6.0
model initialize at round 2469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.38520904,  14.66019572,   0.36909413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.625063950305727}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5919049817518285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57487669,  15.84889067,   4.97879976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9493920116781205}
episode index:2470
target Thresh 70.21439564429298
target distance 3.0
model initialize at round 2470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.27558309,  18.1919219 ,   6.13494391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.6279441943196677}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5920620821234385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75127458,  15.18350062,   5.59287423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3090902946385449}
episode index:2471
target Thresh 70.22017835681054
target distance 59.0
model initialize at round 2471
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.46300298, 19.85802225]), 'previousTarget': array([75.86070472, 19.6436452 ]), 'currentState': array([57.62842211, 22.4250053 ,  1.26529711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5920878091211935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85163764,  15.69824358,   5.51118255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7138315498517215}
episode index:2472
target Thresh 70.22595528950596
target distance 9.0
model initialize at round 2472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.84514628,  12.62840966,   6.27462053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.49270745483246}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5922252848748473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17910479,  15.42709407,   5.66276115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9253530637468294}
episode index:2473
target Thresh 70.2317264481562
target distance 27.0
model initialize at round 2473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.98618427,  13.45065991]), 'previousTarget': array([107.35993796,  13.01924318]), 'currentState': array([89.61860383,  8.46098778,  1.28745955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5922702413866197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40197851,  15.47647355,   5.02494331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6233889375561676}
episode index:2474
target Thresh 70.2374918385324
target distance 22.0
model initialize at round 2474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.03490502,  14.2317598 ]), 'previousTarget': array([112.50265712,  14.43242207]), 'currentState': array([91.40004972, 10.4274915 ,  1.87050128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5067396712307267
running average episode reward sum: 0.592235683580496
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54038515,  15.83856245,   5.21890441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9562597958099094}
episode index:2475
target Thresh 70.24325146639997
target distance 3.0
model initialize at round 2475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.43403151,  16.3605621 ,   4.15193534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.9767588677981616}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5923923331428627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76589726,  14.48256627,   3.98550093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9243031303200756}
episode index:2476
target Thresh 70.24900533751853
target distance 24.0
model initialize at round 2476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.93632611,  12.99909161]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([91.3358445 ,  5.64911993,  2.37989292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.592431516329268
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81114265,  15.62300473,   0.59333136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6510007629215098}
episode index:2477
target Thresh 70.25475345764193
target distance 4.0
model initialize at round 2477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99437105,  20.4718404 ,   0.13460279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.471843290381539}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.592580089571266
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89454271,  15.90770987,   4.7981374 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2744190373868591}
episode index:2478
target Thresh 70.26049583251832
target distance 63.0
model initialize at round 2478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.22478385,  7.32022037]), 'previousTarget': array([71.64677133,  6.74224216]), 'currentState': array([53.55440806,  3.70410141,  5.73850221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = -0.08847262161383362
running average episode reward sum: 0.5923053607648178
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49156489,  15.92538384,   5.25693483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0558605496686522}
episode index:2479
target Thresh 70.26623246789006
target distance 57.0
model initialize at round 2479
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.95357578, 13.36191539]), 'previousTarget': array([77.97235659, 13.05117666]), 'currentState': array([58.      , 12.      ,  2.775855], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 64
reward sum = 0.2577581515917527
running average episode reward sum: 0.5921704626966028
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04822203,  15.48813132,   0.99996882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4905074433119341}
episode index:2480
target Thresh 70.27196336949379
target distance 45.0
model initialize at round 2480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.69443574, 13.53457244]), 'previousTarget': array([89.92145281, 12.77079581]), 'currentState': array([70.73068801, 12.33092106,  1.90166757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.3532938455413705
running average episode reward sum: 0.5920741803035535
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93464808,  15.98415883,   5.33003559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9863262492992777}
episode index:2481
target Thresh 70.27768854306042
target distance 6.0
model initialize at round 2481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.21406816,  19.83448441,   4.50818812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.31736187141446}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5922265674186609
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3177679 ,  15.06522054,   4.18424626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32439198329809527}
episode index:2482
target Thresh 70.28340799431511
target distance 26.0
model initialize at round 2482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.24100799,  16.00876172]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([90.67572364, 20.1560076 ,  1.10283106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5923275406872736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21695378,  15.60948897,   0.65169727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6469511179634453}
episode index:2483
target Thresh 70.28912172897732
target distance 65.0
model initialize at round 2483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.47061343,  8.84877382]), 'previousTarget': array([69.88502281, 10.14146399]), 'currentState': array([50.65874782,  6.11199086,  5.01452166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5923351049782742
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75187285,  15.78562732,   5.45067386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8238794676419398}
episode index:2484
target Thresh 70.29482975276078
target distance 52.0
model initialize at round 2484
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.93010811, 10.67056596]), 'previousTarget': array([82.86817872, 11.29248216]), 'currentState': array([63.       ,  9.       ,  2.3173146], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5856592205741435
running average episode reward sum: 0.5923324185056771
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09542054,  15.84425007,   5.25957008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2373448124715458}
episode index:2485
target Thresh 70.30053207137352
target distance 62.0
model initialize at round 2485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.44163114, 16.76859631]), 'previousTarget': array([72.99739905, 15.67746131]), 'currentState': array([53.45971749, 17.61896507,  2.30412149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.542510239532865
running average episode reward sum: 0.5923123774039181
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33343497,  15.73763537,   4.53402192]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9941905666486431}
episode index:2486
target Thresh 70.30622869051786
target distance 65.0
model initialize at round 2486
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.16839005,  7.65205776]), 'previousTarget': array([69.71961771,  7.33716607]), 'currentState': array([48.41011628,  4.55195818,  1.7974422 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = -0.037113217162215995
running average episode reward sum: 0.5920592911174017
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55351485,  15.4993174 ,   4.76673315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7454505741715588}
episode index:2487
target Thresh 70.31191961589042
target distance 28.0
model initialize at round 2487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.20048889,  17.13505051]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([87.9101879 , 22.41560802,  6.2725671 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5921500658949258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11997629,  15.66927969,   5.95189512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6799482446317868}
episode index:2488
target Thresh 70.3176048531821
target distance 64.0
model initialize at round 2488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.61912555,  6.98399023]), 'previousTarget': array([70.59974639,  5.98119849]), 'currentState': array([51.95206762,  3.34986965,  1.59111875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.592138718014073
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56918026,  15.95581814,   5.69215191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0484245161675096}
episode index:2489
target Thresh 70.32328440807818
target distance 66.0
model initialize at round 2489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.07442362, 16.65123748]), 'previousTarget': array([68.99770471, 15.69700447]), 'currentState': array([48.08679439, 17.35457083,  1.15711904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5921463399102653
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93964476,  15.50741823,   6.17225096]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5109951227544345}
episode index:2490
target Thresh 70.32895828625819
target distance 6.0
model initialize at round 2490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.34380041,  10.83833456,   1.52873254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.373243505111296}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5923020820459899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30200616,  14.17951775,   0.63380051]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.87429905358165}
episode index:2491
target Thresh 70.334626493396
target distance 38.0
model initialize at round 2491
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.98355236, 14.18905295]), 'previousTarget': array([97., 15.]), 'currentState': array([77.       , 15.       ,  2.7934916], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6264132180495734
running average episode reward sum: 0.5923157703028131
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15273322e+02, 1.47927826e+01, 3.02022696e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34299255678686896}
episode index:2492
target Thresh 70.34028903515986
target distance 42.0
model initialize at round 2492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.87497311, 13.47980097]), 'previousTarget': array([92.90990945, 12.89618185]), 'currentState': array([71.91804873, 12.16786748,  3.24161971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5087870616266481
running average episode reward sum: 0.5922822650045073
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44681698,  15.29579332,   0.13552359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5358536198411299}
episode index:2493
target Thresh 70.34594591721226
target distance 28.0
model initialize at round 2493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.13929846,  12.95777243]), 'previousTarget': array([106.68855151,  13.51581277]), 'currentState': array([86.65024654,  8.46590697,  5.39024782]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.613682108040154
running average episode reward sum: 0.5922908455349947
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90209254,  14.4978976 ,   4.18360838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0324135605872544}
episode index:2494
target Thresh 70.35159714521012
target distance 47.0
model initialize at round 2494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.61289662, 14.64174535]), 'previousTarget': array([87.98191681, 13.85029433]), 'currentState': array([68.61473968, 14.37023269,  2.15907487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5923353973784675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06277612,  15.98555437,   5.57134012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.987551651031539}
episode index:2495
target Thresh 70.35724272480464
target distance 14.0
model initialize at round 2495
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.11624193,  16.04624431]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   7.       ,   2.9470918], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.616336340679563}
done in step count: 42
reward sum = 0.26538592773117375
running average episode reward sum: 0.5922044080076152
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45324401,  15.77637975,   5.63480253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9495828687525886}
episode index:2496
target Thresh 70.36288266164142
target distance 61.0
model initialize at round 2496
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.74002521,  7.21425026]), 'previousTarget': array([73.68254035,  7.54931055]), 'currentState': array([54.       ,  4.       ,  1.8278344], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = -0.3133477040515895
running average episode reward sum: 0.591841751975553
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06601469,  15.30415862,   5.83035224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9822632160158628}
episode index:2497
target Thresh 70.3685169613604
target distance 43.0
model initialize at round 2497
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.54110498, 15.91268794]), 'previousTarget': array([92., 15.]), 'currentState': array([71.55622446, 16.69021644,  2.72414529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.28891102459439516
running average episode reward sum: 0.5917204826691554
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.87018087,  15.17422438,   6.03558905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8874507796463282}
episode index:2498
target Thresh 70.37414562959586
target distance 44.0
model initialize at round 2498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.8162583 , 17.08755002]), 'previousTarget': array([90.8721051 , 17.74180624]), 'currentState': array([7.18968473e+01, 1.88811676e+01, 1.94423994e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.48128053656942127
running average episode reward sum: 0.5916762890132531
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50306528,  15.97257998,   5.41905619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0949824144297002}
episode index:2499
target Thresh 70.37976867197649
target distance 33.0
model initialize at round 2499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.45503876,  17.43413494]), 'previousTarget': array([101.77431008,  17.00389241]), 'currentState': array([82.82121852, 21.24374159,  1.99722784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5917325398455096
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89305315,  15.08298924,   5.32389855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1353692813634115}
episode index:2500
target Thresh 70.38538609412532
target distance 22.0
model initialize at round 2500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.91001209,  15.01335289]), 'previousTarget': array([112.9793708 ,  14.90815322]), 'currentState': array([91.91019882, 15.0997789 ,  1.34163761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5918398272564634
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46828734,  15.52271738,   5.67251042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7456217676929314}
episode index:2501
target Thresh 70.39099790165977
target distance 36.0
model initialize at round 2501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.39969022, 16.76580583]), 'previousTarget': array([98.93091516, 16.3390904 ]), 'currentState': array([77.49959394, 18.7623449 ,  1.66149211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.2494031035843528
running average episode reward sum: 0.5917029620591524
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7580942 ,  15.98290008,   0.22470722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0122306997361967}
episode index:2502
target Thresh 70.39660410019167
target distance 11.0
model initialize at round 2502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.07543321,   5.06256451,   1.3146553 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.684327236989382}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5918352200425199
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47355293,  15.0451585 ,   2.51819199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5283803601957779}
episode index:2503
target Thresh 70.4022046953272
target distance 27.0
model initialize at round 2503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.87306058,  13.58945848]), 'previousTarget': array([107.35993796,  13.01924318]), 'currentState': array([88.25362685,  9.70644161,  0.50481331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5919126333495346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16549638,  15.86396935,   5.48221338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8796772680792386}
episode index:2504
target Thresh 70.40779969266694
target distance 14.0
model initialize at round 2504
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.73308416,  16.51491772]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   6.       ,   3.2716959], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.100476887461934}
done in step count: 18
reward sum = 0.49417480725708746
running average episode reward sum: 0.5918736162532902
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05271662,  14.75464153,   0.15126749]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9785430934598551}
episode index:2505
target Thresh 70.41338909780593
target distance 8.0
model initialize at round 2505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.22433115,   8.66795192,   2.44710362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.449327065754664}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5920207520847932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75578208,  15.2209944 ,   2.44353724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7874294104054568}
episode index:2506
target Thresh 70.41897291633353
target distance 43.0
model initialize at round 2506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.20285049, 15.23013873]), 'previousTarget': array([91.99459386, 14.46499055]), 'currentState': array([71.20378568, 15.42354674,  1.07076478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3918731513480101
running average episode reward sum: 0.5919409165839009
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56069124,  15.94719668,   0.30010086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.100707141481665}
episode index:2507
target Thresh 70.4245511538336
target distance 52.0
model initialize at round 2507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.34408942, 17.37721026]), 'previousTarget': array([82.96679883, 16.8480693 ]), 'currentState': array([64.40395173, 18.92346673,  1.59181279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5919585579592916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70224745,  15.93835331,   5.8677893 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1720317466405124}
episode index:2508
target Thresh 70.43012381588436
target distance 23.0
model initialize at round 2508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.59562005,  15.69214909]), 'previousTarget': array([111.54352728,  15.75140711]), 'currentState': array([93.37612398, 21.22486733,  1.43055166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5920619853060973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3608164 ,  15.14558653,   5.32022263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6555540466569925}
episode index:2509
target Thresh 70.43569090805846
target distance 66.0
model initialize at round 2509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.25266104, 11.17163834]), 'previousTarget': array([68.88845169, 10.10938124]), 'currentState': array([48.31939307,  9.53920833,  0.97712803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5920376201896229
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28386443,  15.69494415,   5.93577108]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9978965479591326}
episode index:2510
target Thresh 70.441252435923
target distance 7.0
model initialize at round 2510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.90851015,   9.24111154,   1.31525958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.861411633650427}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5921882619179425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6667846 ,  14.76652874,   1.21507999]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40686770781653137}
episode index:2511
target Thresh 70.44680840503953
target distance 27.0
model initialize at round 2511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.39207761,  14.90964935]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([88.39394688, 14.63621329,  0.32452583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5922814069664766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84696538,  15.4742652 ,   5.75122157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9707099633874964}
episode index:2512
target Thresh 70.452358820964
target distance 44.0
model initialize at round 2512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.36602247, 16.98070214]), 'previousTarget': array([90.9793708 , 16.09184678]), 'currentState': array([70.43036079, 18.58363482,  2.92608953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5923040178782959
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65216632,  15.9591834 ,   0.36969401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1598938313496912}
episode index:2513
target Thresh 70.4579036892468
target distance 2.0
model initialize at round 2513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.91385574,  15.59783742,   6.00538373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.239806003481597}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5924622103930619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6703788 ,  14.89729515,   5.05751455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3452512463173075}
episode index:2514
target Thresh 70.46344301543286
target distance 7.0
model initialize at round 2514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.53361306,  21.32159812,   4.85670686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.785695784794926}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5926047661940587
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50796911,  15.87761966,   5.21862898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0140260778169978}
episode index:2515
target Thresh 70.46897680506147
target distance 19.0
model initialize at round 2515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.27474275,  14.67051597]), 'previousTarget': array([114.76686234,  14.91410718]), 'currentState': array([96.0657267 ,  6.3981684 ,  3.74339819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 45
reward sum = 0.5728787408132546
running average episode reward sum: 0.5925969259613953
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57893018,  15.83464286,   5.98333828]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9348414235494844}
episode index:2516
target Thresh 70.47450506366641
target distance 54.0
model initialize at round 2516
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.63790032,  6.7885183 ]), 'previousTarget': array([80.5237412 ,  7.33860916]), 'currentState': array([61.       ,  3.       ,  3.1133232], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = -0.6145538435302027
running average episode reward sum: 0.5921173269270323
{'dynamicTrap': 16, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66921572,  15.92924705,   5.42761356]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9863662188126109}
episode index:2517
target Thresh 70.48002779677597
target distance 46.0
model initialize at round 2517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.11400057, 19.84146468]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([70.48206539, 23.66077365,  5.74750454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 37
reward sum = 0.5556380616760777
running average episode reward sum: 0.5921028395301892
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29994193,  15.63757583,   6.13230646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9468813234287257}
episode index:2518
target Thresh 70.48554500991285
target distance 37.0
model initialize at round 2518
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.71277441, 13.37735474]), 'previousTarget': array([97.81984861, 12.67835792]), 'currentState': array([78.        , 10.        ,  0.14331521], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.36538660100439874
running average episode reward sum: 0.592012837053601
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37455886,  15.86759894,   4.97739015]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9449985550909118}
episode index:2519
target Thresh 70.49105670859429
target distance 41.0
model initialize at round 2519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.51696108, 18.13395135]), 'previousTarget': array([93.78922128, 18.1040164 ]), 'currentState': array([75.77079029, 21.31022888,  5.65744871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5920834840015183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33519408,  15.9489653 ,   0.12201867]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1586639050967733}
episode index:2520
target Thresh 70.49656289833197
target distance 50.0
model initialize at round 2520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.24427941, 12.56341374]), 'previousTarget': array([84.9007438 , 11.99007438]), 'currentState': array([66.31569353, 10.87478562,  1.53799313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 36
reward sum = 0.6435828679453192
running average episode reward sum: 0.5921039121585765
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78234527,  15.49734528,   0.8054384 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5428866457167402}
episode index:2521
target Thresh 70.50206358463211
target distance 48.0
model initialize at round 2521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.73771663,  7.48640519]), 'previousTarget': array([86.49464637,  8.46752313]), 'currentState': array([67.40910491,  2.34784809,  5.93271458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 63
reward sum = 0.2866073762643979
running average episode reward sum: 0.591982779511513
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27437502,  15.99524076,   6.17309807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0323690370838758}
episode index:2522
target Thresh 70.50755877299537
target distance 30.0
model initialize at round 2522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.41751535,  15.33822186]), 'previousTarget': array([104.95570316,  15.66961979]), 'currentState': array([86.43302749, 16.12577824,  0.40803593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5920595513550705
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37806582,  14.98168226,   5.82484257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3785093132451557}
episode index:2523
target Thresh 70.51304846891694
target distance 19.0
model initialize at round 2523
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.94929729,  14.57678614]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.      , 16.      ,  3.848089], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.572668680265662
running average episode reward sum: 0.5920518687595516
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0799841 ,  15.80760013,   0.78596801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8115512466749524}
episode index:2524
target Thresh 70.51853267788651
target distance 24.0
model initialize at round 2524
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.27520554,  13.67999521]), 'previousTarget': array([109.72658355,  13.02246883]), 'currentState': array([92.42394627,  6.99942458,  1.52294033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5920796826891453
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86234109,  15.51539103,   4.95485188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0046193666531822}
episode index:2525
target Thresh 70.52401140538831
target distance 25.0
model initialize at round 2525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.99630567,  12.8322021 ]), 'previousTarget': array([108.81774824,  12.77438937]), 'currentState': array([91.64456481,  4.88150353,  5.87059891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5921074745966255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52151565,  15.91666351,   5.16383874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0546329035933948}
episode index:2526
target Thresh 70.52948465690106
target distance 42.0
model initialize at round 2526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.4268437 , 11.47765854]), 'previousTarget': array([92.79898987, 11.82842712]), 'currentState': array([71.64644176,  8.52203695,  4.49104071]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4852155742241817
running average episode reward sum: 0.5920651746756234
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29669586,  15.89197726,   5.22454566]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9400275889494704}
episode index:2527
target Thresh 70.53495243789801
target distance 46.0
model initialize at round 2527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.72309343,  8.82292238]), 'previousTarget': array([88.35234545,  8.04843794]), 'currentState': array([70.29480809,  4.07510934,  1.70295924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5920929503347643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6649559 ,  15.50744803,   5.2829774 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6080773363197919}
episode index:2528
target Thresh 70.54041475384695
target distance 68.0
model initialize at round 2528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.47050085, 12.54050961]), 'previousTarget': array([66.94615251, 11.46662886]), 'currentState': array([47.49722431, 11.50696029,  0.29659665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.39131646837569095
running average episode reward sum: 0.5920135606621826
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14605467,  15.78378282,   5.48163482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7972750355373122}
episode index:2529
target Thresh 70.54587161021018
target distance 67.0
model initialize at round 2529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.64947643, 11.01228296]), 'previousTarget': array([67.89172986, 10.07824043]), 'currentState': array([48.72308639,  9.29793759,  2.07251526]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5919873088546186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81514297,  15.8914912 ,   4.91087003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9104552000785769}
episode index:2530
target Thresh 70.55132301244458
target distance 26.0
model initialize at round 2530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.68019474,  12.1470165 ]), 'previousTarget': array([107.15918769,  11.38116355]), 'currentState': array([90.4515689,  3.9179704,  6.1324125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5920341543379918
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27042059,  15.55047266,   5.68562165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.613308599319727}
episode index:2531
target Thresh 70.55676896600153
target distance 44.0
model initialize at round 2531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.56544327, 16.34655817]), 'previousTarget': array([90.9793708 , 16.09184678]), 'currentState': array([72.60137216, 17.54483383,  1.32352596]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.4626099670883131
running average episode reward sum: 0.5919830389401839
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54600178,  15.53478098,   4.43201149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7642700047860967}
episode index:2532
target Thresh 70.56220947632698
target distance 11.0
model initialize at round 2532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.99418073,   5.35793695,   1.94883316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.099719270472397}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5921028001779725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70889282,  14.55933214,   2.78065115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8346958650450491}
episode index:2533
target Thresh 70.56764454886147
target distance 11.0
model initialize at round 2533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.46911317,   5.71222867,   2.68100071]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.307911186806257}
done in step count: 24
reward sum = 0.7163781408072187
running average episode reward sum: 0.592151843327392
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56998708,  15.22577502,   3.6262506 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4856804244026847}
episode index:2534
target Thresh 70.57307418904004
target distance 68.0
model initialize at round 2534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.16160851, 11.58311726]), 'previousTarget': array([66.922597  , 10.75787621]), 'currentState': array([48.21461475, 10.12797466,  6.02161557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.592131935332378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90453971,  15.99929425,   5.29905436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3478802195707467}
episode index:2535
target Thresh 70.57849840229235
target distance 1.0
model initialize at round 2535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26071364,  16.26989559,   0.9250133 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2963820517234332}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5922772287372153
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73122207,  15.28825521,   2.82633681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3941226187591993}
episode index:2536
target Thresh 70.58391719404263
target distance 28.0
model initialize at round 2536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.87368223,  17.13558961]), 'previousTarget': array([106.40285  ,  17.1492875]), 'currentState': array([88.71545132, 22.87686023,  0.94925939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5923565890268907
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95037497,  15.51686307,   1.15957146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5192399073594752}
episode index:2537
target Thresh 70.58933056970963
target distance 46.0
model initialize at round 2537
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.07392723,  9.43366026]), 'previousTarget': array([88.54352728,  9.24859289]), 'currentState': array([67.45976845,  5.52408543,  1.80361748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5923688698802652
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23533896,  15.56523021,   5.62800342]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9508899455560185}
episode index:2538
target Thresh 70.59473853470676
target distance 28.0
model initialize at round 2538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.51326859,  17.58687995]), 'previousTarget': array([106.40285  ,  17.1492875]), 'currentState': array([87.3822803 , 23.4182852 ,  0.29751611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.2295045510233189
running average episode reward sum: 0.5922259536459773
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26331035,  14.49044113,   4.00533324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5735700322989961}
episode index:2539
target Thresh 70.60014109444198
target distance 51.0
model initialize at round 2539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.16857044,  9.44211528]), 'previousTarget': array([83.69567118,  9.47570668]), 'currentState': array([65.50690062,  5.77895546,  0.68464702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.26091040244986796
running average episode reward sum: 0.5920955144525929
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85145125,  14.18869808,   3.29343202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1760867492212632}
episode index:2540
target Thresh 70.60553825431784
target distance 20.0
model initialize at round 2540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.97944907,  14.42389154]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([93.74597627,  8.93995827,  4.85335898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5921506926718774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71057728,  15.72079349,   5.58521247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0121577640735198}
episode index:2541
target Thresh 70.6109300197315
target distance 26.0
model initialize at round 2541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.92089209,  13.61737719]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([90.62312135,  8.36418342,  1.23071688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5921605823053433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03220216,  14.47989808,   2.96720492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5210978658615073}
episode index:2542
target Thresh 70.61631639607472
target distance 31.0
model initialize at round 2542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.79616107,  11.6646123 ]), 'previousTarget': array([103.50882004,  12.40521743]), 'currentState': array([83.50372378,  6.39186505,  5.42305779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5573046174468912
running average episode reward sum: 0.5921468756734681
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21170023,  15.77078727,   3.19305053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7993309746176818}
episode index:2543
target Thresh 70.6216973887339
target distance 2.0
model initialize at round 2543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46854977,  12.76540078,   2.33878934]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2969268642913523}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5923032644802002
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8315205 ,  14.37340325,   0.33878934]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6488519287956126}
episode index:2544
target Thresh 70.62707300309003
target distance 39.0
model initialize at round 2544
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.8442146 , 19.49571669]), 'previousTarget': array([95.59205401, 18.98111713]), 'currentState': array([75.32389484, 23.84969375,  0.98122478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5923670821708578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65650582,  15.81381482,   6.08616788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0456072190732473}
episode index:2545
target Thresh 70.6324432445187
target distance 35.0
model initialize at round 2545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.7844787 , 10.45616072]), 'previousTarget': array([99.36985865, 10.9808208 ]), 'currentState': array([78.44670835,  5.35217817,  4.54672217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5923673175849311
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85462014,  15.0107764 ,   3.65538325]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.854688077265893}
episode index:2546
target Thresh 70.63780811839017
target distance 10.0
model initialize at round 2546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.33181034,  21.77453181,   4.18104529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.492254487570733}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5924934070744869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57420804,  14.95911225,   4.77219972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5756619498437062}
episode index:2547
target Thresh 70.64316763006931
target distance 60.0
model initialize at round 2547
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.57154347,  7.11760687]), 'previousTarget': array([74.61161351,  6.9223227 ]), 'currentState': array([55.       ,  3.       ,  5.4433355], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.060947687392649885
running average episode reward sum: 0.5922847941546746
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72618573,  15.06117804,   1.30196929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7287581660033627}
episode index:2548
target Thresh 70.64852178491564
target distance 36.0
model initialize at round 2548
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.27626653,  17.76507179]), 'previousTarget': array([98.72787848, 17.71202025]), 'currentState': array([80.61988075, 21.4564805 ,  5.54786128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6061205908521812
running average episode reward sum: 0.5922902220859017
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31972914,  14.73028958,   4.69621481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4182946738122357}
episode index:2549
target Thresh 70.65387058828331
target distance 31.0
model initialize at round 2549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.77728122,  15.08138246]), 'previousTarget': array([104.,  15.]), 'currentState': array([85.77805983, 15.25785815,  5.62579436]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5923310546333383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15736819e+02, 1.53990662e+01, 5.87172210e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8379476576057622}
episode index:2550
target Thresh 70.65921404552113
target distance 62.0
model initialize at round 2550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.98979896,  7.27373036]), 'previousTarget': array([72.69246532,  7.49382449]), 'currentState': array([51.29105335,  3.81549076,  4.12029612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.4048307216067349
running average episode reward sum: 0.5922575539147862
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97766007,  15.94111254,   4.99826647]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9413776510650509}
episode index:2551
target Thresh 70.66455216197254
target distance 10.0
model initialize at round 2551
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.43519293,   6.59022513,   0.90977597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.568791819451995}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5923943966246162
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19148078,  15.20584431,   0.16611779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2811347861735001}
episode index:2552
target Thresh 70.66988494297566
target distance 26.0
model initialize at round 2552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.13377056,  13.98797096]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([90.55274228,  9.91570728,  0.32703525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5924550224364873
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11887688,  15.19495498,   4.43559443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22834000441804883}
episode index:2553
target Thresh 70.67521239386329
target distance 44.0
model initialize at round 2553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.00146789, 17.48549262]), 'previousTarget': array([90.95367385, 16.63952224]), 'currentState': array([70.09959534, 19.46424709,  2.8865366 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.3591564005939659
running average episode reward sum: 0.592363676069282
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59774159,  14.41763314,   2.71819047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7077873890625336}
episode index:2554
target Thresh 70.68053451996286
target distance 42.0
model initialize at round 2554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.09850498, 18.59983714]), 'previousTarget': array([92.79898987, 18.17157288]), 'currentState': array([71.32155523, 21.57847083,  1.96586061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3355495489582358
running average episode reward sum: 0.5922631617338178
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53328535,  14.89548475,   2.79618618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4782739816204554}
episode index:2555
target Thresh 70.68585132659652
target distance 65.0
model initialize at round 2555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.03001904, 20.85333446]), 'previousTarget': array([69.88502281, 19.85853601]), 'currentState': array([51.20491038, 23.49247574,  1.39524272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5922370793104186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23129377,  14.60811414,   0.97998195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.455050919730105}
episode index:2556
target Thresh 70.69116281908106
target distance 8.0
model initialize at round 2556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.31131084,  10.52942664,   5.88864052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.89392866639375}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5923736624430314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46347997,  15.79754614,   5.91555381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9612146474955641}
episode index:2557
target Thresh 70.69646900272797
target distance 25.0
model initialize at round 2557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.4642356 ,  15.08486654]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([91.46999422, 15.56477469,  0.72215002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5924401085149251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0093925 ,  15.60706066,   0.8158465 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.607133311646057}
episode index:2558
target Thresh 70.70176988284344
target distance 64.0
model initialize at round 2558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.66443634, 20.74230099]), 'previousTarget': array([70.84555753, 20.51930531]), 'currentState': array([52.84591212, 23.4304412 ,  5.9420349 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5924597144131548
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78106554,  15.22375041,   5.32480282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8124823843888749}
episode index:2559
target Thresh 70.70706546472836
target distance 64.0
model initialize at round 2559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.90516884, 17.42561064]), 'previousTarget': array([70.93924283, 18.44224665]), 'currentState': array([51.93677425, 18.5495401 ,  6.22027612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.4778566423907612
running average episode reward sum: 0.5924149475881462
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69714727,  14.26412039,   3.08076326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.013673080519377}
episode index:2560
target Thresh 70.7123557536783
target distance 18.0
model initialize at round 2560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.48314552,  14.71285862]), 'currentState': array([98.20226707,  4.14942222,  4.80798249]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.997471584907608}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5924667107230035
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5710935 ,  15.63633298,   5.3329155 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7673854608678605}
episode index:2561
target Thresh 70.71764075498355
target distance 20.0
model initialize at round 2561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.45332982,  14.22577554]), 'previousTarget': array([113.87716713,  14.60700849]), 'currentState': array([93.31807602,  8.40838246,  1.97635043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5925018738316247
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19545378,  15.09709725,   3.99193195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2182431165632019}
episode index:2562
target Thresh 70.72292047392911
target distance 49.0
model initialize at round 2562
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.63984317, 16.77263077]), 'previousTarget': array([85.96262067, 16.77779873]), 'currentState': array([67.68168732, 18.06569505,  5.31223041]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.592547933665195
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57034608,  14.97184936,   4.2673451 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.571040379224704}
episode index:2563
target Thresh 70.72819491579472
target distance 21.0
model initialize at round 2563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.79074722,  11.90175982]), 'previousTarget': array([111.00530293,  12.52709229]), 'currentState': array([92.60125684,  1.67818803,  2.45586312]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6305096075915034
running average episode reward sum: 0.592562739310252
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48074453,  14.8658027 ,   5.58429971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.499123452508398}
episode index:2564
target Thresh 70.7334640858548
target distance 58.0
model initialize at round 2564
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.95246751, 19.62194332]), 'previousTarget': array([76.89383588, 18.94201698]), 'currentState': array([57.       , 21.       ,  4.4886703], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.10387633453532541
running average episode reward sum: 0.5923722182947452
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23091059,  15.45696359,   1.89250685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5119916230961132}
episode index:2565
target Thresh 70.73872798937852
target distance 49.0
model initialize at round 2565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.06562284, 14.95065593]), 'previousTarget': array([85.99583637, 15.59192171]), 'currentState': array([6.70656540e+01, 1.49153274e+01, 5.90217670e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.21005682464736902
running average episode reward sum: 0.5922232255458569
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92448591,  14.25484605,   3.68771177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.187404143663006}
episode index:2566
target Thresh 70.7439866316298
target distance 6.0
model initialize at round 2566
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.05197   ,  22.67657598,   2.56384873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.748319794709131}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5923378190971503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41182885,  14.51003983,   2.34459236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7655104621295291}
episode index:2567
target Thresh 70.74924001786727
target distance 9.0
model initialize at round 2567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7164453 ,   7.94602808,   1.63245988]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.090261896118298}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5924774811807962
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11226325,  15.8021387 ,   0.5996635 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1964543576319233}
episode index:2568
target Thresh 70.75448815334431
target distance 66.0
model initialize at round 2568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.18262914, 13.74810075]), 'previousTarget': array([68.9793708 , 12.90815322]), 'currentState': array([50.1904273 , 13.1896516 ,  1.79507225]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.45949571119362864
running average episode reward sum: 0.5924257171597813
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77840872,  15.9837679 ,   5.82085941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2544797457586478}
episode index:2569
target Thresh 70.75973104330907
target distance 63.0
model initialize at round 2569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.87104528, 11.19945218]), 'previousTarget': array([71.95980905, 12.26728946]), 'currentState': array([51.94824828,  9.44384407,  3.66914511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5921952013165285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.14983414, 19.85271557]), 'previousTarget': array([89.73580052, 19.97983981]), 'currentState': array([71.55139535, 23.84034607,  0.53933597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2570
target Thresh 70.76496869300445
target distance 51.0
model initialize at round 2570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.96360991, 17.19884473]), 'previousTarget': array([83.9045705 , 18.04857152]), 'currentState': array([65.01698669, 18.65905774,  0.08806675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.592988487100669
running average episode reward sum: 0.592195509867981
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64461881,  15.88485625,   6.20362066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9535545982994151}
episode index:2571
target Thresh 70.77020110766809
target distance 22.0
model initialize at round 2571
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.45693642,  12.7033433 ]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([93.       ,  5.       ,  1.7976016], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.25437079576538724
running average episode reward sum: 0.5920641627785166
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0409399 ,  15.24174083,   2.683347  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2451830002097133}
episode index:2572
target Thresh 70.77542829253238
target distance 19.0
model initialize at round 2572
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.11094385,  14.35289527]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.      , 21.      ,  0.426798], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.356699064434988}
done in step count: 38
reward sum = 0.3449190303593169
running average episode reward sum: 0.5919681094818127
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59012948,  15.03962781,   1.46901443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4117817502410767}
episode index:2573
target Thresh 70.78065025282454
target distance 4.0
model initialize at round 2573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.76345408,  17.71285903,   0.77797151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.030698308638065}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5921113215643761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99281139,  15.22527477,   4.74070904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0180487154656954}
episode index:2574
target Thresh 70.78586699376652
target distance 8.0
model initialize at round 2574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.44963344,   8.74335076,   1.14171338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.422390271428764}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5922120386321549
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73688922,  14.72236752,   2.49606846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3825010756875623}
episode index:2575
target Thresh 70.79107852057506
target distance 5.0
model initialize at round 2575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.16605479e+02, 9.03005032e+00, 7.91392883e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.182059745076561}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.59235504483222
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69281927,  14.66118667,   1.55085412]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4573340906377864}
episode index:2576
target Thresh 70.79628483846167
target distance 2.0
model initialize at round 2576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52605715,  16.29039612,   0.57474917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.374679591155043}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5924761263340347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46241799,  14.63046169,   3.94384551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6523442244896482}
episode index:2577
target Thresh 70.8014859526327
target distance 24.0
model initialize at round 2577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.55232867,  16.15950738]), 'previousTarget': array([110.2,  16.4]), 'currentState': array([92.59569394, 22.53492891,  1.33345908]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.41762075671708243
running average episode reward sum: 0.5924083003566813
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88109238,  15.99620836,   5.37090585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3299454427788495}
episode index:2578
target Thresh 70.80668186828925
target distance 63.0
model initialize at round 2578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.11244051, 15.32584015]), 'previousTarget': array([71.99748095, 14.31742033]), 'currentState': array([51.11299171, 15.47432468,  1.16717243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3075478068908087
running average episode reward sum: 0.5922978465011304
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03583854,  15.53084014,   3.39587494]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5320485431285946}
episode index:2579
target Thresh 70.81187259062723
target distance 13.0
model initialize at round 2579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.67338236,  22.88498566,   1.54329062]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.35313339701594}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.592408398119928
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68626749,  15.86609514,   5.46031331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9211671278373437}
episode index:2580
target Thresh 70.81705812483737
target distance 56.0
model initialize at round 2580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([79.86627419,  7.2716455 ]), 'previousTarget': array([78.481944  ,  6.52259414]), 'currentState': array([60.3332606 ,  2.97497471,  1.56787115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.39545564168032593
running average episode reward sum: 0.592332089419254
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20969844,  14.42950258,   3.54583556]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.974701933726578}
episode index:2581
target Thresh 70.8222384761052
target distance 29.0
model initialize at round 2581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.02031904,  13.8207333 ]), 'previousTarget': array([105.70920231,  13.39813833]), 'currentState': array([87.23520572, 10.89681625,  1.63799351]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6634394623034625
running average episode reward sum: 0.592359629067931
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42529885,  15.64022517,   6.10933952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7686139343933135}
episode index:2582
target Thresh 70.82741364961107
target distance 62.0
model initialize at round 2582
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.31624067, 14.25624375]), 'previousTarget': array([72.99739905, 14.32253869]), 'currentState': array([51.31913886, 13.91577466,  2.18165922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5923464298846093
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05099861,  14.16158626,   3.22408943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8399633635239413}
episode index:2583
target Thresh 70.83258365053018
target distance 65.0
model initialize at round 2583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.59365294,  7.73944304]), 'previousTarget': array([69.66764361,  6.63094959]), 'currentState': array([50.85574095,  4.51224143,  0.14105988]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5923125829259802
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79527708,  14.20610072,   2.77524661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8198704371915635}
episode index:2584
target Thresh 70.83774848403249
target distance 13.0
model initialize at round 2584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.59144288,   1.44188694,   0.66146558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.450215156842928}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5924030494795236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34460128,  15.86047448,   4.771959  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0816486527354685}
episode index:2585
target Thresh 70.84290815528286
target distance 65.0
model initialize at round 2585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.8193367 , 15.89360926]), 'previousTarget': array([70., 15.]), 'currentState': array([48.82307998, 16.28054264,  3.22582197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5923942320599558
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95206976,  15.19841506,   2.82242687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9725252506359444}
episode index:2586
target Thresh 70.84806266944095
target distance 10.0
model initialize at round 2586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.52275562,  11.2833387 ,   0.57011956]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.25620027729048}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5925255312929851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.83728026,  15.10911679,   4.50712299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8443605350256556}
episode index:2587
target Thresh 70.85321203166129
target distance 32.0
model initialize at round 2587
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.87212771,  14.25799479]), 'previousTarget': array([102.91268452,  13.86681417]), 'currentState': array([83.       , 12.       ,  0.7181937], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7236142836436554
running average episode reward sum: 0.5925761838248054
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98920569,  15.42008198,   3.98379946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.074707757884794}
episode index:2588
target Thresh 70.85835624709323
target distance 61.0
model initialize at round 2588
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.70364756,  9.43020013]), 'previousTarget': array([73.78580727,  8.91921747]), 'currentState': array([54.      ,  6.      ,  6.167346], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.17488595288315156
running average episode reward sum: 0.59241485117477
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.87257549,  15.1194121 ,   4.94878166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8807083673110679}
episode index:2589
target Thresh 70.86349532088099
target distance 31.0
model initialize at round 2589
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.86214843,  15.34415441]), 'previousTarget': array([103.95850618,  14.28764556]), 'currentState': array([84.       , 13.       ,  3.6754358], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.5558172432269912
running average episode reward sum: 0.592400720824211
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54200537,  15.28524967,   3.6926491 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6124844391011232}
episode index:2590
target Thresh 70.86862925816364
target distance 31.0
model initialize at round 2590
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.977501  ,  12.94839537]), 'previousTarget': array([103.90700027,  13.9264839 ]), 'currentState': array([84.       , 12.       ,  2.6106346], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.42821454325176367
running average episode reward sum: 0.5923373529440209
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00236421,  14.82823958,   3.1609866 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1717766883493149}
episode index:2591
target Thresh 70.87375806407512
target distance 17.0
model initialize at round 2591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.5932024 , 23.54230451,  1.33808487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.616480309777707}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5924473775080853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82895688,  15.71930981,   3.67778909]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0975318338853899}
episode index:2592
target Thresh 70.87888174374423
target distance 68.0
model initialize at round 2592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.04695691, 10.9673386 ]), 'previousTarget': array([66.89486598, 10.04800091]), 'currentState': array([48.12031726,  9.25589702,  6.11557872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2874998640202829
running average episode reward sum: 0.5921080226135507
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.94459134,  22.84821295,   5.11026027]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.232798223239216}
episode index:2593
target Thresh 70.88400030229467
target distance 70.0
model initialize at round 2593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.29116712,  8.92972736]), 'previousTarget': array([64.9007438 ,  9.99007438]), 'currentState': array([44.43294613,  6.55253174,  5.26345778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5918797620034452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.98006268,  17.44267272,   6.26004171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.884156466887556}
episode index:2594
target Thresh 70.88911374484498
target distance 59.0
model initialize at round 2594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.21509667, 10.02784664]), 'previousTarget': array([75.81864176,  9.68727346]), 'currentState': array([54.36208465,  7.60753392,  3.68400013]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.34783181729288903
running average episode reward sum: 0.5917857165526897
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57545583,  15.43756608,   4.85603315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6096735385154403}
episode index:2595
target Thresh 70.89422207650861
target distance 68.0
model initialize at round 2595
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.52705772, 17.40187196]), 'previousTarget': array([66.96548746, 17.82555956]), 'currentState': array([48.55371589, 18.43415887,  0.66074389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 67
reward sum = 0.36889725715835725
running average episode reward sum: 0.5916998581322759
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92946345,  14.70519531,   0.66849066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3031257339720919}
episode index:2596
target Thresh 70.89932530239389
target distance 46.0
model initialize at round 2596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.34577267, 19.87871441]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([70.72622178, 23.76113929,  1.00950402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4148975842131024
running average episode reward sum: 0.5916317787045057
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77084708,  15.39377457,   5.31228888]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4555979295916861}
episode index:2597
target Thresh 70.90442342760404
target distance 14.0
model initialize at round 2597
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.50070613,  15.54815171]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  22.      ,   5.001335], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.067480233619401}
done in step count: 9
reward sum = 0.7742172474836408
running average episode reward sum: 0.5917020579457601
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89446714,  15.28963487,   4.32362728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30826213144749826}
episode index:2598
target Thresh 70.9095164572372
target distance 17.0
model initialize at round 2598
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.64256919,  15.90673032]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.        , 13.        ,  0.40109193], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.910344181904634}
done in step count: 26
reward sum = 0.4942152158051551
running average episode reward sum: 0.591664548579796
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95488923,  15.30810973,   0.12508654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3113945852125723}
episode index:2599
target Thresh 70.9146043963864
target distance 63.0
model initialize at round 2599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.48546942, 11.84815153]), 'previousTarget': array([71.95980905, 12.26728946]), 'currentState': array([50.53541509, 10.43558922,  2.49942589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.41559599251256585
running average episode reward sum: 0.5915968299043856
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92157208,  14.52376775,   5.40016655]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4826469707701488}
episode index:2600
target Thresh 70.91968725013956
target distance 36.0
model initialize at round 2600
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.9825323, 14.1642951]), 'previousTarget': array([99., 15.]), 'currentState': array([79.        , 15.        ,  0.35930127], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.4519309934667487
running average episode reward sum: 0.5915431329276698
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37977362,  14.382253  ,   5.73126722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7251478190351113}
episode index:2601
target Thresh 70.92476502357955
target distance 42.0
model initialize at round 2601
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.5524977 , 18.79288296]), 'previousTarget': array([92.64677133, 19.25775784]), 'currentState': array([73.       , 23.       ,  1.4125559], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.3667836720847181
running average episode reward sum: 0.5914567534269616
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39950144,  14.87925073,   4.01565301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41735091847372363}
episode index:2602
target Thresh 70.92983772178415
target distance 41.0
model initialize at round 2602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.41334869, 18.52299351]), 'previousTarget': array([93.78922128, 18.1040164 ]), 'currentState': array([72.65228526, 21.60526088,  1.78036654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5882904214745737
running average episode reward sum: 0.5914555370105372
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03742419,  15.22345122,   4.52159024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22656350011567505}
episode index:2603
target Thresh 70.93490534982604
target distance 27.0
model initialize at round 2603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.36492502,  16.55775213]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([87.76862944, 20.55590162,  0.83690751]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5915393589503263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90045173,  15.33401683,   5.02230512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3485356488555245}
episode index:2604
target Thresh 70.93996791277286
target distance 12.0
model initialize at round 2604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.31280744,   3.474945  ,   1.35712879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.853512997039577}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5916559804072584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16809913,  15.97298518,   5.54860884]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2801403146584265}
episode index:2605
target Thresh 70.94502541568718
target distance 9.0
model initialize at round 2605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.07512658,   5.11467152,   3.00987291]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.070990860000927}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5917866056442115
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61699185,  14.07665667,   0.3409234 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9996290046643646}
episode index:2606
target Thresh 70.95007786362648
target distance 46.0
model initialize at round 2606
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.79486569, 14.63143769]), 'previousTarget': array([88.98112317, 13.86874449]), 'currentState': array([69.79700352, 14.33901877,  1.98434723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.591796388358173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28136152,  15.8761775 ,   6.28013698]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1331938378895687}
episode index:2607
target Thresh 70.95512526164322
target distance 12.0
model initialize at round 2607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.27287347,   1.48221376,   3.25562477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.707534378314275}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5919232857147948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04677121,  14.38943463,   2.62635064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6123541609920669}
episode index:2608
target Thresh 70.96016761478482
target distance 22.0
model initialize at round 2608
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.18297877,  16.34196805]), 'previousTarget': array([112.0585156 ,  15.93592685]), 'currentState': array([93.      , 22.      ,  4.710808], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.6704278682212584
running average episode reward sum: 0.5919533756275991
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98272502,  14.85496732,   5.48661724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1460578810385638}
episode index:2609
target Thresh 70.9652049280936
target distance 45.0
model initialize at round 2609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.28713713, 18.03869605]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([71.4493546 , 20.58081808,  1.39114636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5919678853040649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22248496,  15.27935067,   3.50235532]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.826175790068397}
episode index:2610
target Thresh 70.9702372066069
target distance 58.0
model initialize at round 2610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.89926128, 20.71294545]), 'previousTarget': array([76.85591226, 19.6035968 ]), 'currentState': array([57.12036935, 23.67865516,  0.48921323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5919922787683582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53268671,  15.37500116,   3.72176859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5991724134683937}
episode index:2611
target Thresh 70.97526445535698
target distance 70.0
model initialize at round 2611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.56322519, 20.17797372]), 'previousTarget': array([64.9007438 , 20.00992562]), 'currentState': array([46.67653468, 22.3038948 ,  1.15379434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.33293362799165216
running average episode reward sum: 0.5918930985804651
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93510162,  15.18152531,   0.63270405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19277769004848525}
episode index:2612
target Thresh 70.98028667937109
target distance 39.0
model initialize at round 2612
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.76913001, 17.96990118]), 'previousTarget': array([95.76743395, 17.95885631]), 'currentState': array([76.       , 21.       ,  5.1990447], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.13209971347409266
running average episode reward sum: 0.5917171347897624
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04565894,  15.4086442 ,   4.83433956]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41118709135608733}
episode index:2613
target Thresh 70.98530388367148
target distance 24.0
model initialize at round 2613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.75270466,  15.44852817]), 'previousTarget': array([110.98266146,  15.16738911]), 'currentState': array([90.86330094, 17.54891376,  0.65886843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5918068254894739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.594454  ,  14.05723365,   6.15331687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0262923359272786}
episode index:2614
target Thresh 70.99031607327531
target distance 47.0
model initialize at round 2614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.91620965, 14.53561635]), 'previousTarget': array([87.9954746 , 15.57456437]), 'currentState': array([67.91914892, 14.19274307,  5.44349313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5918189549615205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84422257,  14.58627877,   5.88068919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44207675900736043}
episode index:2615
target Thresh 70.99532325319481
target distance 63.0
model initialize at round 2615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.25398051, 18.39285001]), 'previousTarget': array([71.95980905, 17.73271054]), 'currentState': array([53.31970908, 20.01298036,  5.92784208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5915927244741499
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.44075733,  17.21980445]), 'previousTarget': array([106.92948089,  17.63821116]), 'currentState': array([89.49622138, 23.63108807,  5.71033232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2616
target Thresh 71.00032542843715
target distance 24.0
model initialize at round 2616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.35679219,  15.71510554]), 'previousTarget': array([110.84555753,  15.51930531]), 'currentState': array([91.73128153, 19.56729079,  2.06359811]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7370384609290914
running average episode reward sum: 0.5916483017521228
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11708193,  15.25747999,   4.20721861]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9196957431809685}
episode index:2617
target Thresh 71.0053226040045
target distance 28.0
model initialize at round 2617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.48224875,  15.42415268]), 'previousTarget': array([106.98725709,  15.28616939]), 'currentState': array([88.52446426, 16.72293616,  1.43657976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5916250997816122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20241364,  14.8424233 ,   4.8386139 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8130033331007497}
episode index:2618
target Thresh 71.01031478489404
target distance 47.0
model initialize at round 2618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.50086802, 17.19346564]), 'previousTarget': array([87.95938166, 16.72599692]), 'currentState': array([69.57445632, 18.90755926,  0.88672639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.4921805710229587
running average episode reward sum: 0.5915871293620786
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18275111,  15.23234117,   6.20565154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.849634135560273}
episode index:2619
target Thresh 71.01530197609794
target distance 52.0
model initialize at round 2619
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.42541244,  6.44516811]), 'previousTarget': array([82.40285  ,  6.8507125]), 'currentState': array([64.12140668,  1.21493276,  0.16847437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.10045124275822503
running average episode reward sum: 0.59139967291681
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98512149,  15.26066031,   0.2229816 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26108459875718154}
episode index:2620
target Thresh 71.0202841826034
target distance 69.0
model initialize at round 2620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.57553212, 21.31213173]), 'previousTarget': array([65.86691472, 20.6965896 ]), 'currentState': array([45.73666892, 23.8458063 ,  0.72502518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.525513548292933
running average episode reward sum: 0.5913745351355724
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75326374,  15.98830711,   2.74986106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2426412235031445}
episode index:2621
target Thresh 71.02526140939264
target distance 24.0
model initialize at round 2621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.17751068,  15.50675996]), 'previousTarget': array([110.84555753,  15.51930531]), 'currentState': array([92.49227978, 19.04111765,  1.36270219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5914547243248948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37105519,  15.53819458,   3.12857322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6537089316145438}
episode index:2622
target Thresh 71.03023366144286
target distance 70.0
model initialize at round 2622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.46706623, 16.17540749]), 'previousTarget': array([64.9979595 , 15.71431486]), 'currentState': array([46.47292913, 16.6596407 ,  5.69424385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.3640740185043505
running average episode reward sum: 0.591368037056187
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90732949,  15.50329886,   4.07659039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5117592902012169}
episode index:2623
target Thresh 71.03520094372634
target distance 5.0
model initialize at round 2623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.84466309,  16.32421406,   5.62995815]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.322691187085738}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.591512446721943
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41972404,  14.05170402,   0.28280313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1117488266342128}
episode index:2624
target Thresh 71.04016326121034
target distance 23.0
model initialize at round 2624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.51064701,  15.35089387]), 'previousTarget': array([111.98112317,  15.13125551]), 'currentState': array([91.61101218, 17.352027  ,  2.80880505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5915632916321282
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10925852,  15.06911888,   0.76850631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8934191611066508}
episode index:2625
target Thresh 71.0451206188572
target distance 8.0
model initialize at round 2625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.75325669,  19.11134906,   6.02776068]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.478301479269559}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5916929573047385
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71209523,  15.34204332,   3.52326864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7899830679288605}
episode index:2626
target Thresh 71.05007302162426
target distance 64.0
model initialize at round 2626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.27662988, 12.03083546]), 'previousTarget': array([70.97806349, 12.93647173]), 'currentState': array([50.32056044, 10.70596093,  3.35775864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3256117028073551
running average episode reward sum: 0.5915916701884472
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23832127,  15.13407179,   3.21873513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7733884730960277}
episode index:2627
target Thresh 71.05502047446393
target distance 17.0
model initialize at round 2627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.79140314,  14.86502556]), 'currentState': array([99.65485224,  3.65288967,  0.60467499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.084823097785794}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.59169382646107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4069091 ,  14.86731018,   4.5329821 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42799720127355984}
episode index:2628
target Thresh 71.05996298232365
target distance 48.0
model initialize at round 2628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.67974921, 14.14252017]), 'previousTarget': array([86.99566113, 14.41657627]), 'currentState': array([68.69035449, 13.49129147,  0.57095044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5917206762954264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41151818,  14.64925485,   5.46559788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6850788391738105}
episode index:2629
target Thresh 71.06490055014595
target distance 65.0
model initialize at round 2629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.04575243, 19.6280235 ]), 'previousTarget': array([69.94108971, 18.46607002]), 'currentState': array([50.1509036 , 21.67619098,  0.47101068]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.2311046876951818
running average episode reward sum: 0.591583559949951
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66877198,  14.71532662,   5.69019628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.436750428035345}
episode index:2630
target Thresh 71.0698331828684
target distance 44.0
model initialize at round 2630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.44823538, 15.41036498]), 'previousTarget': array([91., 15.]), 'currentState': array([69.45081416, 15.73152642,  3.65864372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.591618136550126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53840427,  14.47738218,   5.72269005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6972804354523011}
episode index:2631
target Thresh 71.0747608854236
target distance 45.0
model initialize at round 2631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.49848179, 18.02245339]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([71.66185487, 20.57357711,  1.20924717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5916449845381327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0886171 ,  15.16778993,   5.90466883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18975366084875786}
episode index:2632
target Thresh 71.0796836627393
target distance 23.0
model initialize at round 2632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.17638443,  11.54081549]), 'previousTarget': array([109.73169692,  12.25132013]), 'currentState': array([90.33765712,  2.4975966 ,  2.51854062]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5916743528533744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36813408,  14.60348142,   0.17966729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7459768906431319}
episode index:2633
target Thresh 71.08460151973824
target distance 7.0
model initialize at round 2633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.94602698,  15.85326768,   6.27377534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.1138085538130875}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5918144142266268
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09714728,  14.95041147,   0.49982893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10907160986771397}
episode index:2634
target Thresh 71.08951446133828
target distance 9.0
model initialize at round 2634
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.06100518,  21.03541291,   4.88049841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.627976054706819}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5919435417156896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01277184,  14.89718202,   0.34428076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1036081888457252}
episode index:2635
target Thresh 71.0944224924524
target distance 44.0
model initialize at round 2635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.50256667, 17.3774005 ]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([72.61331059, 19.4791837 ,  1.29871958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5919627632863911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39457404,  15.01559325,   5.28279018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6056267392747686}
episode index:2636
target Thresh 71.09932561798858
target distance 8.0
model initialize at round 2636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.87256381,   6.0535088 ,   2.82996368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.017251082062582}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5920953068533666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24161834,  14.42689207,   1.09745347]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9505763690828194}
episode index:2637
target Thresh 71.10422384284998
target distance 23.0
model initialize at round 2637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.10080466,  16.13487968]), 'previousTarget': array([110.88993593,  16.4295875 ]), 'currentState': array([93.47684718, 23.42516142,  5.5669573 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6557072836436554
running average episode reward sum: 0.5921194205670854
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76094109,  14.41787228,   5.97516631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6293026655059567}
episode index:2638
target Thresh 71.1091171719348
target distance 59.0
model initialize at round 2638
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.84868074, 10.45557993]), 'previousTarget': array([75.86070472, 10.3563548 ]), 'currentState': array([56.        ,  8.        ,  0.93573356], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.446283153360603
running average episode reward sum: 0.5917259372120541
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.51229896,  23.41051504,   5.35764179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.895120091654386}
episode index:2639
target Thresh 71.11400561013639
target distance 62.0
model initialize at round 2639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.03873821,  7.05480798]), 'previousTarget': array([72.69246532,  7.49382449]), 'currentState': array([54.40468142,  3.24642063,  0.51759833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5917111484014294
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25626303,  14.69556345,   0.26705853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3979351120055025}
episode index:2640
target Thresh 71.1188891623432
target distance 53.0
model initialize at round 2640
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.63833568,  6.78626094]), 'previousTarget': array([81.50626598,  7.41651305]), 'currentState': array([62.     ,  3.     ,  0.86506], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.0705565175603785
running average episode reward sum: 0.5915138160913798
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62316592,  14.90760184,   4.75653568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38799657476837696}
episode index:2641
target Thresh 71.12376783343875
target distance 71.0
model initialize at round 2641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.4656987 , 11.23234689]), 'previousTarget': array([63.92896584, 10.68413796]), 'currentState': array([45.52330239,  9.71549837,  1.44911688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.3837695918321282
running average episode reward sum: 0.5914351846666034
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99159775,  15.09986985,   3.54240806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10022267868636844}
episode index:2642
target Thresh 71.12864162830172
target distance 61.0
model initialize at round 2642
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.86374738, 11.3305665 ]), 'previousTarget': array([73.90394822, 10.9577654 ]), 'currentState': array([54.      ,  9.      ,  4.955804], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = -0.4896587393730025
running average episode reward sum: 0.5910261442110454
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([67.51106781, 20.43159989]), 'previousTarget': array([68.87397236, 21.17314778]), 'currentState': array([47.64061709, 22.70430497,  4.64891285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2643
target Thresh 71.13351055180593
target distance 18.0
model initialize at round 2643
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.14681544,  14.33721419]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 22.       ,  0.6502869], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.8728267271219}
done in step count: 12
reward sum = 0.5460459175231291
running average episode reward sum: 0.5910091320224342
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23935482,  14.80966058,   5.05355807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30581010668786157}
episode index:2644
target Thresh 71.13837460882029
target distance 50.0
model initialize at round 2644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.11941631,  9.2596074 ]), 'previousTarget': array([84.53288933,  8.29723565]), 'currentState': array([64.45626225,  5.60442311,  0.90627599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3289067174813253
running average episode reward sum: 0.590910038481965
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95060848,  14.8734197 ,   5.6615306 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.13587528900118953}
episode index:2645
target Thresh 71.14323380420883
target distance 68.0
model initialize at round 2645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.12113894,  7.67652582]), 'previousTarget': array([66.69567118,  6.47570668]), 'currentState': array([47.35107472,  4.65252834,  0.362306  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.1910620204661148
running average episode reward sum: 0.5907589243406135
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12563188,  15.95068166,   5.54781709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.291632773418911}
episode index:2646
target Thresh 71.14808814283077
target distance 2.0
model initialize at round 2646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30705606,  14.79442978,   0.64980972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36951662435800803}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.590913529960432
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30705606,  14.79442978,   0.64980972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36951662435800803}
episode index:2647
target Thresh 71.15293762954047
target distance 1.0
model initialize at round 2647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.59245939,  17.35832292,   2.74188101]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7464263294584663}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5910531381477582
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99430657,  15.9171998 ,   3.95307017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9172174670790358}
episode index:2648
target Thresh 71.15778226918738
target distance 58.0
model initialize at round 2648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.45045509,  6.83103299]), 'previousTarget': array([76.51579154,  6.37422914]), 'currentState': array([58.93202616,  2.46858737,  0.4294459 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.38777798212900194
running average episode reward sum: 0.5906836284760796
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([104.55980976,  18.11880651]), 'previousTarget': array([103.33259045,  18.22242485]), 'currentState': array([85.396603  , 23.84344682,  1.16925057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2649
target Thresh 71.16262206661617
target distance 68.0
model initialize at round 2649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.91168064, 15.66725527]), 'previousTarget': array([67., 15.]), 'currentState': array([47.91368831, 15.95063268,  6.10867757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.3372623804516624
running average episode reward sum: 0.5905879978164477
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60716483,  15.50432209,   2.86566981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6392653911508315}
episode index:2650
target Thresh 71.16745702666661
target distance 63.0
model initialize at round 2650
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.6834798 , 18.07987616]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([52.7362423 , 19.53167373,  0.14064324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.2450263625872712
running average episode reward sum: 0.5904576463885981
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17193173,  14.9599395 ,   5.42411319]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8290367288799696}
episode index:2651
target Thresh 71.17228715417369
target distance 59.0
model initialize at round 2651
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.24595974,  6.83713932]), 'previousTarget': array([75.53149897,  6.30355062]), 'currentState': array([54.63546233,  2.90924022,  1.54381919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.571077172516225
running average episode reward sum: 0.5900196619169146
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([91.86338631, 19.79459108]), 'previousTarget': array([90.35489401, 19.78554084]), 'currentState': array([72.27947296, 23.85295751,  1.39261503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2652
target Thresh 71.17711245396751
target distance 13.0
model initialize at round 2652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.59698247,   1.25554255,   2.83209407]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.237542875766472}
done in step count: 16
reward sum = 0.7216210681314318
running average episode reward sum: 0.5900692666685974
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04468743,  15.37089364,   6.11576122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0247849549723653}
episode index:2653
target Thresh 71.18193293087337
target distance 13.0
model initialize at round 2653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.80482573,   3.92885735,   2.106776  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.6866222344554}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5901946153602926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26934476,  14.90908531,   3.08887678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28427465387337814}
episode index:2654
target Thresh 71.18674858971178
target distance 61.0
model initialize at round 2654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.19227494, 18.40531279]), 'previousTarget': array([73.95713898, 17.69133515]), 'currentState': array([55.26505354, 20.1099674 ,  1.73019188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.310933466663314
running average episode reward sum: 0.5900894322534388
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60544425,  14.86238318,   6.12147182]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41786675661200856}
episode index:2655
target Thresh 71.19155943529837
target distance 39.0
model initialize at round 2655
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.98763306, 14.29677545]), 'previousTarget': array([96., 15.]), 'currentState': array([76.      , 15.      ,  3.144036], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5648406949995691
running average episode reward sum: 0.5900799259517618
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15552475,  14.08460733,   6.00365305]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9285104714245502}
episode index:2656
target Thresh 71.196365472444
target distance 39.0
model initialize at round 2656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.39525302, 16.45310611]), 'previousTarget': array([95.97375327, 15.97570496]), 'currentState': array([74.44480304, 17.86006931,  3.4766655 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5901334537815333
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9900769 ,  15.22209294,   3.12589449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22231450790169727}
episode index:2657
target Thresh 71.2011667059547
target distance 7.0
model initialize at round 2657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.33761334,  17.45407293,   4.88568557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.171312393662341}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5902728302135192
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33779311,  15.34346868,   5.2533429 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7459816998403294}
episode index:2658
target Thresh 71.20596314063171
target distance 46.0
model initialize at round 2658
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.6045846 , 12.38055811]), 'previousTarget': array([88.83200822, 11.58678368]), 'currentState': array([68.70234594, 10.40548946,  0.76949239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.543747148450699
running average episode reward sum: 0.5902553327777302
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25330132,  15.15218201,   6.26927305]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7620487450551126}
episode index:2659
target Thresh 71.21075478127145
target distance 32.0
model initialize at round 2659
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.10105183,   9.92872828]), 'previousTarget': array([101.91373199,  10.50159537]), 'currentState': array([83.       ,  4.       ,  1.9417825], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.43643801873323407
running average episode reward sum: 0.5901975067198187
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23441282,  15.05840384,   5.49740408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7678116588502958}
episode index:2660
target Thresh 71.21554163266558
target distance 43.0
model initialize at round 2660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.56560305, 19.81934276]), 'previousTarget': array([91.66260099, 19.34184168]), 'currentState': array([71.97557338, 23.84808171,  2.28606522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.15398539338475045
running average episode reward sum: 0.5899178438486784
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([111.60839904,  16.18896691]), 'previousTarget': array([110.15798616,  16.48920386]), 'currentState': array([92.73454248, 22.80542657,  5.64062372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2661
target Thresh 71.22032369960094
target distance 7.0
model initialize at round 2661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.58658388,  21.17981008,   4.67739272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.339384653324809}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5900570918449787
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79608617,  15.13938584,   5.14148331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2470005300860388}
episode index:2662
target Thresh 71.2251009868596
target distance 6.0
model initialize at round 2662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.41878985,  11.19820466,   1.1488015 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.953245691398591}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5901855215318212
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98285434,  15.71603112,   4.36177635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.21601941454679}
episode index:2663
target Thresh 71.22987349921884
target distance 66.0
model initialize at round 2663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.50697577, 15.70600082]), 'previousTarget': array([68.99770471, 15.69700447]), 'currentState': array([50.50949313, 16.02331436,  0.96693831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.4430479816261725
running average episode reward sum: 0.5901302897225473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17730877,  14.65849117,   5.78450633]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.890757623445637}
episode index:2664
target Thresh 71.2346412414512
target distance 55.0
model initialize at round 2664
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.99979597, 15.09033878]), 'previousTarget': array([80., 15.]), 'currentState': array([60.       , 15.       ,  5.2230926], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.11649116398986914
running average episode reward sum: 0.5899525639718034
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82904136,  15.3171289 ,   2.34398716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3602743335411874}
episode index:2665
target Thresh 71.2394042183244
target distance 27.0
model initialize at round 2665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.73969667,  13.17382033]), 'previousTarget': array([107.17596225,  12.68176659]), 'currentState': array([89.53990927,  7.57309148,  1.35949736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.5900032120483172
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52551615,  15.38168106,   2.23060733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6089461054309039}
episode index:2666
target Thresh 71.24416243460142
target distance 64.0
model initialize at round 2666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.19021024,  8.42618049]), 'previousTarget': array([70.71097782,  7.38782431]), 'currentState': array([51.41163892,  5.45833337,  0.28568697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.13291156520142025
running average episode reward sum: 0.5897321528892435
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([105.06784546,  18.00395371]), 'previousTarget': array([103.95028723,  18.23449419]), 'currentState': array([85.9242627 , 23.79387938,  6.17299635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2667
target Thresh 71.24891589504047
target distance 25.0
model initialize at round 2667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.04281099,  13.13726132]), 'previousTarget': array([108.56953382,  12.42781353]), 'currentState': array([91.32094101,  6.10223576,  6.03301293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5897695280515297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75300532,  15.02881928,   3.23412589]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2486703115896587}
episode index:2668
target Thresh 71.25366460439504
target distance 67.0
model initialize at round 2668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.56333253, 15.61604322]), 'previousTarget': array([68., 15.]), 'currentState': array([49.56517054, 15.88718397,  1.3120498 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5895485578274564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.17478659, 19.46194576]), 'previousTarget': array([93.82480227, 19.55268846]), 'currentState': array([75.66285977, 23.85338193,  1.54767572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
episode index:2669
target Thresh 71.2584085674138
target distance 66.0
model initialize at round 2669
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.17075167,  8.39585367]), 'previousTarget': array([68.72787848,  7.28797975]), 'currentState': array([48.36671701,  5.60296786,  1.0070169 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.3326890216928771
running average episode reward sum: 0.5894523557539978
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06002955,  14.16898469,   0.3019696 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8331806530587731}
episode index:2670
target Thresh 71.26314778884073
target distance 5.0
model initialize at round 2670
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.09324089,   8.58239891,   6.18380284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.694873718534812}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5895913088255987
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67509278,  14.73374963,   1.39902776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7256993277043142}
episode index:2671
target Thresh 71.26788227341507
target distance 67.0
model initialize at round 2671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.59208535, 21.28006179]), 'previousTarget': array([67.85893586, 20.62878378]), 'currentState': array([46.75829449, 23.85314168,  1.39061069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5302927686033893
running average episode reward sum: 0.5895691162581503
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61141615,  15.41720655,   5.97079708]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5701392100354363}
episode index:2672
target Thresh 71.27261202587128
target distance 10.0
model initialize at round 2672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57913475,   6.52827018,   1.07227683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.491501826292922}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5896972480320556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03962321,  14.67769287,   0.45465779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32473355685242433}
episode index:2673
target Thresh 71.27733705093912
target distance 7.0
model initialize at round 2673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.04292528,  13.08821621,   0.27776807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.2563293186911135}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5898288048388504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36708668,  14.87760165,   4.32488488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38695475768413884}
episode index:2674
target Thresh 71.28205735334362
target distance 13.0
model initialize at round 2674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.51076556,   6.30641026,   0.36282211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.407671934195205}
done in step count: 10
reward sum = 0.8357750750088044
running average episode reward sum: 0.5899207473697551
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32805926,  14.55533772,   5.83720124]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8057474126071609}
episode index:2675
target Thresh 71.28677293780508
target distance 5.0
model initialize at round 2675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.65943416,  15.32177163,   5.47713739]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.3560269884617573}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5900665542653568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05326442,  14.14592261,   0.12849069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8557366905364062}
episode index:2676
target Thresh 71.2914838090391
target distance 22.0
model initialize at round 2676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.29433295,  14.89223399]), 'previousTarget': array([112.9793708 ,  14.90815322]), 'currentState': array([91.30278487, 14.31085168,  3.89706075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5901578681268378
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.400172  ,  15.19884309,   6.07198638]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6319273710364225}
episode index:2677
target Thresh 71.2961899717565
target distance 69.0
model initialize at round 2677
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([65.68352104,  5.54386787]), 'previousTarget': array([65.65421157,  5.7029674 ]), 'currentState': array([46.      ,  2.      ,  3.938128], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3132914592675557
running average episode reward sum: 0.5898205084078706
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.98667649,  23.41199723,   5.36926743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.32889687198914}
episode index:2678
target Thresh 71.30089143066351
target distance 22.0
model initialize at round 2678
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.78885464,  14.31868716]), 'previousTarget': array([111.79586847,  13.83486126]), 'currentState': array([93.67560947,  8.42938677,  2.13926151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5898906841641197
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47077994,  14.77494485,   2.06077941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5218079857209816}
episode index:2679
target Thresh 71.30558819046155
target distance 46.0
model initialize at round 2679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.71708895,  7.77446927]), 'previousTarget': array([88.24618806,  7.4391401 ]), 'currentState': array([70.4869801 ,  2.27875153,  1.19531077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.5898852603080796
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70181849,  15.09984277,   5.72995512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3144531646432365}
episode index:2680
target Thresh 71.31028025584739
target distance 55.0
model initialize at round 2680
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.89097195, 11.08548192]), 'previousTarget': array([79.88204353, 11.1689502 ]), 'currentState': array([60.       ,  9.       ,  2.1050866], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 78
reward sum = 0.2487027477439145
running average episode reward sum: 0.5897580008852658
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16824656,  14.92013215,   5.88082885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8355792355333173}
episode index:2681
target Thresh 71.3149676315131
target distance 39.0
model initialize at round 2681
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.11335627,  7.88893981]), 'previousTarget': array([94.97366596,  8.32455532]), 'currentState': array([76.       ,  2.       ,  1.9603565], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 96
reward sum = 0.05420175265159982
running average episode reward sum: 0.5895583154832398
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52650835,  15.73807819,   6.13538757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8769000832482343}
episode index:2682
target Thresh 71.31965032214605
target distance 7.0
model initialize at round 2682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06174046,  20.59587753,   4.48725939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.596218112778238}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5897002240499625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19260815,  15.24058817,   4.87358578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8424750796917292}
episode index:2683
target Thresh 71.32432833242892
target distance 58.0
model initialize at round 2683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.46043187, 18.66135608]), 'previousTarget': array([76.92609538, 18.28223316]), 'currentState': array([58.56008746, 20.65542034,  1.39220053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5569560913981999
running average episode reward sum: 0.5896880242986019
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33492208,  14.58086773,   0.13768366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7861300793830008}
episode index:2684
target Thresh 71.32900166703975
target distance 3.0
model initialize at round 2684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.29563306,  11.20570588,   0.26199668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.159511337239205}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5898297788519358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59180903,  15.17715099,   2.28004032]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4449745423193947}
episode index:2685
target Thresh 71.33367033065184
target distance 50.0
model initialize at round 2685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.28854673, 10.05858428]), 'previousTarget': array([84.80683493, 10.77295689]), 'currentState': array([65.55953953,  6.77738408,  6.15159273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.589842323757386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98558104,  15.68927613,   5.59838753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6894269298238322}
episode index:2686
target Thresh 71.33833432793388
target distance 10.0
model initialize at round 2686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.24242429,   6.10588338,   1.73731917]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.655293139376141}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5899731900862448
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11885884,  14.76752993,   2.47851389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2610933820274342}
episode index:2687
target Thresh 71.34299366354986
target distance 46.0
model initialize at round 2687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.40830455, 15.39492739]), 'previousTarget': array([89., 15.]), 'currentState': array([70.41088309, 15.71607359,  1.41883677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5899833526423668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15078378e+02, 1.47309204e+01, 9.31181014e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28026229740497394}
episode index:2688
target Thresh 71.34764834215912
target distance 11.0
model initialize at round 2688
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.0708453 ,  13.81397064]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  10.       ,   1.8962495], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.76885774742011}
done in step count: 9
reward sum = 0.8435172474836408
running average episode reward sum: 0.5900776382112924
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19822559,  14.84395576,   0.27456784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8168182203398935}
episode index:2689
target Thresh 71.35229836841633
target distance 64.0
model initialize at round 2689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.72235847, 15.81223589]), 'previousTarget': array([70.99755904, 15.68753814]), 'currentState': array([52.72604843, 16.19640398,  1.02879256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.5546164631441778
running average episode reward sum: 0.5900644556183307
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66282229,  15.61147022,   5.03458874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9017922238131576}
episode index:2690
target Thresh 71.35694374697152
target distance 18.0
model initialize at round 2690
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.96504705,  16.67186502]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 14.      ,  5.669668], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.162647885309923}
done in step count: 26
reward sum = 0.4537191149901328
running average episode reward sum: 0.5900137884534744
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07925744,  15.55488677,   5.2986038 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.075019157157977}
episode index:2691
target Thresh 71.36158448247006
target distance 3.0
model initialize at round 2691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03722026,  16.40816198,   4.18480322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.408653792946235}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5901623717415675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8317777 ,  15.10005741,   4.93592373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19572998621463975}
episode index:2692
target Thresh 71.36622057955272
target distance 58.0
model initialize at round 2692
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.99331305, 14.48286059]), 'previousTarget': array([77., 15.]), 'currentState': array([57.       , 15.       ,  2.5921075], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.1393
running average episode reward sum: 0.5898914982281098
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([85.87934897, 20.16086272]), 'previousTarget': array([87.33293116, 20.1756747 ]), 'currentState': array([66.18622047, 23.65094721,  3.69864652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:2693
target Thresh 71.37085204285556
target distance 7.0
model initialize at round 2693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.31374822,  10.68588123,   0.43140852]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.95723467278762}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5900291019815515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04378045,  14.80066357,   0.8751663 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9767757418204772}
episode index:2694
target Thresh 71.37547887701005
target distance 5.0
model initialize at round 2694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38044718,  10.81147076,   1.90635467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.205771900662283}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5901738407192206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99003986,  14.67772891,   1.9758606 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3224249694799951}
episode index:2695
target Thresh 71.38010108664304
target distance 27.0
model initialize at round 2695
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.30636741,  14.5513009 ]), 'previousTarget': array([107.87767469,  14.20863052]), 'currentState': array([89.36818532, 12.98002909,  1.631581  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5902348738967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51376998,  15.138945  ,   5.69196758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5322267473072672}
episode index:2696
target Thresh 71.38471867637674
target distance 36.0
model initialize at round 2696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([100.09824555,  18.15840265]), 'previousTarget': array([98.5237412 , 18.66139084]), 'currentState': array([80.5328769 , 22.30525076,  0.58446139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.4685614965131268
running average episode reward sum: 0.5901897595558089
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5921303 ,  15.5433119 ,   5.51663309]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6793714089725121}
episode index:2697
target Thresh 71.38933165082872
target distance 43.0
model initialize at round 2697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.539181  , 15.62464669]), 'previousTarget': array([91.97840172, 16.07077201]), 'currentState': array([70.545699  , 16.13521269,  2.73337591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5902068076382804
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38766365,  14.59363172,   5.32037422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7349088294739387}
episode index:2698
target Thresh 71.39394001461196
target distance 33.0
model initialize at round 2698
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.41622806,  10.79688314]), 'previousTarget': array([101.29527642,  11.26234812]), 'currentState': array([82.      ,  6.      ,  1.738094], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.1271641749795659
running average episode reward sum: 0.5900352468258837
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66562188,  14.61341782,   6.03099103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5111306172270623}
episode index:2699
target Thresh 71.39854377233485
target distance 28.0
model initialize at round 2699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.62488052,  16.24378559]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([87.9033692 , 19.56974278,  6.19722152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.59012891643572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49802082,  15.91099446,   5.63211279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0401413398931727}
episode index:2700
target Thresh 71.40314292860111
target distance 33.0
model initialize at round 2700
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.94954885,  16.58031666]), 'previousTarget': array([101.91786413,  16.18928508]), 'currentState': array([82.       , 18.       ,  4.3779845], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7156781408072188
running average episode reward sum: 0.5901753989327105
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62397956,  15.17152054,   5.81463948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.413292473558922}
episode index:2701
target Thresh 71.40773748800993
target distance 45.0
model initialize at round 2701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.64767796, 18.69879227]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([69.85719435, 21.58613677,  0.70028913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5901972076778755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44882644,  15.62766233,   5.58554275]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8353156825435701}
episode index:2702
target Thresh 71.41232745515585
target distance 47.0
model initialize at round 2702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.306676  , 15.47860782]), 'previousTarget': array([87.98191681, 16.14970567]), 'currentState': array([69.31014502, 15.85109742,  0.18579739]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5902443898180607
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15946526,  15.71358956,   5.65801659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.102591813172971}
episode index:2703
target Thresh 71.41691283462885
target distance 10.0
model initialize at round 2703
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.46119819,  16.58225981]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.       ,   7.       ,   3.4389994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.186485490243596}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.5903414683330791
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49724351,  15.81108305,   2.25324793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9542640122085823}
episode index:2704
target Thresh 71.42149363101429
target distance 22.0
model initialize at round 2704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.78720071,  12.79838198]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([91.36304337,  5.01696655,  2.28370464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5903859089093968
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.432387  ,  14.87399359,   1.55053559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5814311034747512}
episode index:2705
target Thresh 71.42606984889298
target distance 60.0
model initialize at round 2705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.36116   , 20.73413355]), 'previousTarget': array([74.82455801, 20.3567256 ]), 'currentState': array([56.57782329, 23.67004713,  1.35807246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5904076076231658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66724507,  15.83580062,   5.08573039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0694758835027693}
episode index:2706
target Thresh 71.43064149284115
target distance 27.0
model initialize at round 2706
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.88776283,  14.36488717]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([87.96703146, 12.58599374,  0.58158064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7334101988187652
running average episode reward sum: 0.5904604345870356
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46586426,  14.57421576,   6.11813359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6830762790999659}
episode index:2707
target Thresh 71.43520856743042
target distance 47.0
model initialize at round 2707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.46356598, 17.44304513]), 'previousTarget': array([87.92796014, 17.30400339]), 'currentState': array([69.55446804, 19.34772864,  1.23007315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.26181876184970254
running average episode reward sum: 0.590339075032849
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38433245,  15.61971102,   6.1683056 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8735492420538775}
episode index:2708
target Thresh 71.43977107722787
target distance 48.0
model initialize at round 2708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.10491241,  7.57154093]), 'previousTarget': array([86.30452744,  7.22830951]), 'currentState': array([65.69515983,  2.74852269,  1.64189386]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5170285391600109
running average episode reward sum: 0.590312013188673
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84934836,  15.11461516,   5.57915542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.18929488162926186}
episode index:2709
target Thresh 71.44432902679604
target distance 34.0
model initialize at round 2709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.35780793, 18.69218257]), 'previousTarget': array([100.58914087,  17.96694159]), 'currentState': array([79.89270626, 23.28672463,  1.29611754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5903174353488018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09140716,  14.97291201,   5.69008487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9089965367771113}
episode index:2710
target Thresh 71.44888242069284
target distance 2.0
model initialize at round 2710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.83925442,  12.42060364,   0.1656416 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1679871203320826}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5904612134988022
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89942458,  14.3030956 ,   3.83475118]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1378226211435816}
episode index:2711
target Thresh 71.4534312634717
target distance 67.0
model initialize at round 2711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.48143132, 19.34369791]), 'previousTarget': array([67.89172986, 19.92175957]), 'currentState': array([46.56110267, 21.12709533,  4.6968863 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5904599508027988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71387894,  15.09631492,   4.8293143 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7203469369608215}
episode index:2712
target Thresh 71.45797555968143
target distance 24.0
model initialize at round 2712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.57493431,  13.23154796]), 'previousTarget': array([110.2,  13.6]), 'currentState': array([89.29202315,  7.9240686 ,  2.22161007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5905319073785468
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72886046,  14.06060179,   6.08744362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9777451834032272}
episode index:2713
target Thresh 71.46251531386635
target distance 24.0
model initialize at round 2713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.17723212,  13.07601185]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([91.60090699,  5.66519915,  1.30026853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5627484662088554
running average episode reward sum: 0.5905216702963176
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75111253,  15.15844744,   5.00150261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7676429038054378}
episode index:2714
target Thresh 71.46705053056618
target distance 1.0
model initialize at round 2714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15620248,  15.26005733,   0.56747115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8829632331523455}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5906724910439065
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15620248,  15.26005733,   0.56747115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8829632331523455}
episode index:2715
target Thresh 71.47158121431619
target distance 44.0
model initialize at round 2715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.55067423, 13.14603525]), 'previousTarget': array([90.91786413, 12.81071492]), 'currentState': array([69.60353408, 11.69290069,  3.67919576]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5566791560151133
running average episode reward sum: 0.5906599750884467
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08355522,  15.23508143,   5.90081748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9461153823217963}
episode index:2716
target Thresh 71.47610736964702
target distance 8.0
model initialize at round 2716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.51341642,   8.29542669,   0.29708278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.067263199719347}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5907925956533386
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46040228,  14.8855023 ,   1.84667194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5516116619551662}
episode index:2717
target Thresh 71.48062900108486
target distance 34.0
model initialize at round 2717
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.63128709,  13.82263875]), 'previousTarget': array([100.78718271,  12.90987981]), 'currentState': array([81.      , 10.      ,  5.882795], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5523505101854967
running average episode reward sum: 0.5907784521340348
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47689255,  14.44724218,   5.74547913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7610404771545443}
episode index:2718
target Thresh 71.48514611315132
target distance 36.0
model initialize at round 2718
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.60669802, 10.94682058]), 'previousTarget': array([98.5237412 , 11.33860916]), 'currentState': array([79.       ,  7.       ,  2.0322602], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.11320015480952739
running average episode reward sum: 0.5906028073023597
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37299706,  14.44516777,   6.14050866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8372403992872561}
episode index:2719
target Thresh 71.48965871036351
target distance 11.0
model initialize at round 2719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.16293419,   5.44044954,   1.08721352]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.75289215125451}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5907318063251901
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09360573,  14.26106525,   2.1068491 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1694336770291245}
episode index:2720
target Thresh 71.49416679723404
target distance 6.0
model initialize at round 2720
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.29054056,  11.3422985 ,   5.89589381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.780612611448091}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5908677358377498
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26429463,  14.04563891,   0.34552598]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2050176313372456}
episode index:2721
target Thresh 71.49867037827099
target distance 47.0
model initialize at round 2721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.17012518, 11.80895486]), 'previousTarget': array([87.78180356, 10.94622606]), 'currentState': array([67.30031786,  9.53063157,  0.98955536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5908663284336718
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88117973,  14.13314289,   2.43907961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8749625709504049}
episode index:2722
target Thresh 71.50316945797793
target distance 65.0
model initialize at round 2722
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.05687079, 15.45709725]), 'previousTarget': array([69.99763356, 14.3076559 ]), 'currentState': array([50.05790512, 15.66049814,  0.55516744]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 47
reward sum = 0.3512604198110404
running average episode reward sum: 0.5907783350775856
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14813515e+02, 1.48664884e+01, 1.19172931e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22935126009802825}
episode index:2723
target Thresh 71.50766404085398
target distance 6.0
model initialize at round 2723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.81383048,  10.19139193,   1.34399796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.835266744428458}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5909105713899285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48109244,  14.93671378,   0.74429478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48523713851686034}
episode index:2724
target Thresh 71.51215413139369
target distance 39.0
model initialize at round 2724
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.99727297, 13.33026331]), 'previousTarget': array([95.97375327, 14.02429504]), 'currentState': array([76.       , 13.       ,  3.2115061], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.2881577900112236
running average episode reward sum: 0.590799469451808
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60682438,  14.69084411,   0.18799658]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5001644039600747}
episode index:2725
target Thresh 71.51663973408714
target distance 12.0
model initialize at round 2725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.52872955,   3.81716055,   1.62460423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.279208082473072}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5909281124011657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74789087,  14.56918638,   0.82182553]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49915868160954363}
episode index:2726
target Thresh 71.52112085341996
target distance 65.0
model initialize at round 2726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.62420943, 13.39610249]), 'previousTarget': array([69.9622374 , 12.22844538]), 'currentState': array([50.63726019, 12.67370369,  0.39397418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5909423755177048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03044779,  15.32721748,   5.88927962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0232803994051896}
episode index:2727
target Thresh 71.52559749387326
target distance 26.0
model initialize at round 2727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.74667511,  14.00351952]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([90.09705436, 10.27626792,  1.87076681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 55
reward sum = 0.47070310625351786
running average episode reward sum: 0.5908982995392356
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5270804 ,  15.13692093,   6.18395832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4923416374729696}
episode index:2728
target Thresh 71.53006965992368
target distance 48.0
model initialize at round 2728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.30563464, 19.73907573]), 'previousTarget': array([86.72787848, 19.71202025]), 'currentState': array([68.61354738, 23.23503046,  5.44440288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5909555636633808
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50451554,  14.60218524,   6.27362595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6354222440434916}
episode index:2729
target Thresh 71.53453735604339
target distance 19.0
model initialize at round 2729
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.34245184,  14.08621244]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       ,  9.       ,  1.8260862], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6476172745742564
running average episode reward sum: 0.5909763188688426
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44865251,  15.32501331,   6.27920212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6400138322226703}
episode index:2730
target Thresh 71.53900058670007
target distance 61.0
model initialize at round 2730
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.67454249,  6.59337973]), 'previousTarget': array([73.62388913,  6.86043721]), 'currentState': array([54.       ,  3.       ,  6.0990953], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.386296487525562
running average episode reward sum: 0.5909013720247037
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16091333,  15.79718516,   5.59619503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8132633466620862}
episode index:2731
target Thresh 71.54345935635698
target distance 35.0
model initialize at round 2731
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.80125478,  9.82003069]), 'previousTarget': array([98.91891892,  9.48648649]), 'currentState': array([80.      ,  3.      ,  2.898879], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 98
reward sum = 0.16624335045426913
running average episode reward sum: 0.5907459335102198
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79151375,  14.93632914,   6.21958951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21799195934478593}
episode index:2732
target Thresh 71.54791366947288
target distance 22.0
model initialize at round 2732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.76003674,  13.38654466]), 'previousTarget': array([110.55791146,  12.57704261]), 'currentState': array([9.38570914e+01, 4.47113402e+00, 6.96280003e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5677450257396457
running average episode reward sum: 0.5907375175176216
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71704312,  15.89295176,   4.70185593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.145213378797663}
episode index:2733
target Thresh 71.55236353050206
target distance 12.0
model initialize at round 2733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.65663555,   8.01411676,   0.88638573]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.481496419828991}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.590852237545965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23505166,  15.1952233 ,   1.26137203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7894669737826825}
episode index:2734
target Thresh 71.55680894389442
target distance 47.0
model initialize at round 2734
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.83946272, 15.58764917]), 'previousTarget': array([88., 15.]), 'currentState': array([66.84381596, 16.00491483,  1.47103572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5908783544759241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80129414,  15.16309711,   5.81874029]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25706941990824417}
episode index:2735
target Thresh 71.56124991409534
target distance 66.0
model initialize at round 2735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.43797849,  7.89556981]), 'previousTarget': array([68.77431008,  7.99610759]), 'currentState': array([50.68740621,  4.74677724,  0.82129448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.39574694819195266
running average episode reward sum: 0.590807034517487
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38812024,  15.43075337,   6.03724558]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7482949291020585}
episode index:2736
target Thresh 71.56568644554581
target distance 30.0
model initialize at round 2736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.25227354,  12.8298735 ]), 'previousTarget': array([104.72787848,  13.28797975]), 'currentState': array([85.73021541,  8.48369741,  6.17118931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.590869707400143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14743027e+02, 1.53103602e+01, 5.30069990e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4029377526905341}
episode index:2737
target Thresh 71.57011854268237
target distance 67.0
model initialize at round 2737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.49396917, 10.59202591]), 'previousTarget': array([67.89172986, 10.07824043]), 'currentState': array([49.58714365,  8.66373708,  5.73359818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.2062430818919887
running average episode reward sum: 0.5907292301811846
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70144743,  15.91228421,   6.03497199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9598938008514118}
episode index:2738
target Thresh 71.57454620993707
target distance 6.0
model initialize at round 2738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.6123915 ,  19.22190813,   4.9374963 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.266090906098416}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5908642673406658
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35814463,  15.89574001,   4.78738299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9646853097839908}
episode index:2739
target Thresh 71.57896945173763
target distance 69.0
model initialize at round 2739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.71454572,  9.72086713]), 'previousTarget': array([65.92481176, 10.73259233]), 'currentState': array([46.83301895,  7.54718516,  6.2120347 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.465898092065339
running average episode reward sum: 0.5908186592474997
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15280441e+02, 1.43696323e+01, 2.61562690e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6899350192111801}
episode index:2740
target Thresh 71.58338827250728
target distance 50.0
model initialize at round 2740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.06592586,  8.89749684]), 'previousTarget': array([84.53288933,  8.29723565]), 'currentState': array([66.49644754,  4.77008627,  1.48526686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5908328894452215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15024634,  14.43013232,   6.02705992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5893412765930863}
episode index:2741
target Thresh 71.58780267666482
target distance 41.0
model initialize at round 2741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.19392865, 12.00273856]), 'previousTarget': array([93.78922128, 11.8959836 ]), 'currentState': array([72.36444516,  9.39667172,  1.89365649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.586497460816985
running average episode reward sum: 0.590831308326101
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77951837,  14.92488835,   5.95817358]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23292468374220415}
episode index:2742
target Thresh 71.59221266862468
target distance 63.0
model initialize at round 2742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.15919787, 20.95572306]), 'previousTarget': array([71.84067458, 20.48054926]), 'currentState': array([53.35878381, 23.77416288,  6.08300704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5908647473660879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23467895,  14.73315414,   6.02639318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8105078797715605}
episode index:2743
target Thresh 71.59661825279684
target distance 64.0
model initialize at round 2743
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.71662541, 19.29689237]), 'previousTarget': array([70.91268452, 19.13318583]), 'currentState': array([52.81910132, 21.31890504,  5.76397324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.469018774659337
running average episode reward sum: 0.590820342857084
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81077218,  15.10300868,   6.18161301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2154482655796988}
episode index:2744
target Thresh 71.60101943358688
target distance 5.0
model initialize at round 2744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.5905692 ,  11.74456032,   2.02628443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5474473466392937}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5909621569398319
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81180401,  14.52577814,   2.20118567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.510200066520034}
episode index:2745
target Thresh 71.60541621539599
target distance 23.0
model initialize at round 2745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.89507517,  16.25684408]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([90.47499586, 21.03811167,  3.00177932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.591033065892442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45193394,  14.47042697,   6.08855522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7621181001758796}
episode index:2746
target Thresh 71.60980860262094
target distance 68.0
model initialize at round 2746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.37555961, 20.40178675]), 'previousTarget': array([66.89486598, 19.95199909]), 'currentState': array([45.49300743, 22.56606923,  1.77227116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 61
reward sum = 0.49485705286697595
running average episode reward sum: 0.5909980546026621
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95487679,  15.91865186,   3.9946733 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3250324206672675}
episode index:2747
target Thresh 71.61419659965415
target distance 38.0
model initialize at round 2747
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.52471015, 12.3342466 ]), 'previousTarget': array([96.66906377, 11.62324859]), 'currentState': array([77.      ,  8.      ,  5.891561], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.09440711524535167
running average episode reward sum: 0.5908173446538421
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39988522,  14.77129548,   5.92763995]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6422176465274049}
episode index:2748
target Thresh 71.61858021088358
target distance 21.0
model initialize at round 2748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.37162172,  15.25263852]), 'previousTarget': array([112.97366596,  15.67544468]), 'currentState': array([95.81522443, 22.71320952,  0.76581949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5909248628521186
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24156689,  15.17894244,   5.45063946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7792568149573708}
episode index:2749
target Thresh 71.62295944069285
target distance 8.0
model initialize at round 2749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.23817738,  13.50067131,   1.03054881]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.889180064335129}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5910523375017728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30654493,  15.63030223,   0.27943098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9371023659364286}
episode index:2750
target Thresh 71.6273342934612
target distance 4.0
model initialize at round 2750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.11623273,  20.04260864,   6.02449316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.3829806790052706}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5911866681715285
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32053715,  15.7290528 ,   5.12574965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9965880509420597}
episode index:2751
target Thresh 71.63170477356348
target distance 8.0
model initialize at round 2751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.0793804 ,   8.16405243,   1.83312243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.43372033829846}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5913139550469753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11032657,  14.7445794 ,   0.77478485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9256124935757428}
episode index:2752
target Thresh 71.63607088537017
target distance 45.0
model initialize at round 2752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.31074596, 17.6708934 ]), 'previousTarget': array([89.92145281, 17.22920419]), 'currentState': array([71.43666568, 19.91163723,  5.9307502 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5913470973063154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88494538,  14.73346341,   6.06147538]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29030900705799084}
episode index:2753
target Thresh 71.64043263324739
target distance 28.0
model initialize at round 2753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.49267269,  14.68838501]), 'previousTarget': array([106.949174,  14.424941]), 'currentState': array([88.51556477, 13.73174552,  5.72299499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5914263931563208
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24340092,  15.8074735 ,   5.67084238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1065512275668368}
episode index:2754
target Thresh 71.64479002155686
target distance 13.0
model initialize at round 2754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.22816377,  20.80701462,   0.18913322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.12621601679482}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5915466538827352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94014847,  14.60094173,   6.19887838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40352162960978427}
episode index:2755
target Thresh 71.64914305465601
target distance 7.0
model initialize at round 2755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.22807523,  20.77932023,   6.10624564]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.167965348337423}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5916770760148169
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42981502,  15.31201993,   5.99310201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6499748817316341}
episode index:2756
target Thresh 71.65349173689785
target distance 26.0
model initialize at round 2756
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.34729608,  17.29942902]), 'previousTarget': array([108.11558017,  17.11828302]), 'currentState': array([89.44456399, 23.83293649,  0.33340025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5917532289032914
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40665656,  15.26897607,   6.02482015]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6514634040344611}
episode index:2757
target Thresh 71.65783607263106
target distance 8.0
model initialize at round 2757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.3659384 ,  18.05800483,   0.26321214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.304941264046577}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5918869645019487
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45800842,  14.63568563,   5.97910105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.653054238207822}
episode index:2758
target Thresh 71.66217606619998
target distance 63.0
model initialize at round 2758
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([71.66698539,  5.63451312]), 'previousTarget': array([71.58733278,  6.04183057]), 'currentState': array([52.        ,  2.        ,  0.12656386], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.42483865960020695
running average episode reward sum: 0.5918264178165911
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55514706,  15.31939893,   5.52812623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6404716483934981}
episode index:2759
target Thresh 71.66651172194459
target distance 26.0
model initialize at round 2759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.9421239 ,  11.26005128]), 'previousTarget': array([107.15918769,  11.38116355]), 'currentState': array([90.2699142 ,  1.89559699,  0.29419821]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5918854369721661
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68829436,  15.18582141,   0.83894463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36289117342406507}
episode index:2760
target Thresh 71.67084304420058
target distance 53.0
model initialize at round 2760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.69407592, 19.42104542]), 'previousTarget': array([81.8278102 , 19.38123261]), 'currentState': array([63.89057458, 22.21771032,  5.84912843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.47695109156103377
running average episode reward sum: 0.5918438091759288
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53799312,  15.98167046,   0.12261662]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1194255151469663}
episode index:2761
target Thresh 71.67517003729924
target distance 61.0
model initialize at round 2761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.42475849,  9.28647045]), 'previousTarget': array([73.78580727,  8.91921747]), 'currentState': array([55.62998657,  6.42867326,  5.69097132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3243610553345438
running average episode reward sum: 0.5917469653113954
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24962104,  15.39348164,   5.48260362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8472876585897876}
episode index:2762
target Thresh 71.67949270556758
target distance 7.0
model initialize at round 2762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.2778616 ,   7.29686205,   5.81035447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.223770288796961}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5918701351928993
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14554966e+02, 1.50426290e+01, 3.06458135e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4470707399005612}
episode index:2763
target Thresh 71.68381105332828
target distance 62.0
model initialize at round 2763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.64072174, 14.7583421 ]), 'previousTarget': array([72.98960229, 13.64482588]), 'currentState': array([52.6410472 , 14.6442448 ,  0.77573276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4770481029440434
running average episode reward sum: 0.5918285932130698
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64347775,  15.12135712,   6.16421512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3766107618739206}
episode index:2764
target Thresh 71.68812508489965
target distance 62.0
model initialize at round 2764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.19018664,  6.55824213]), 'previousTarget': array([72.57433899,  6.10429689]), 'currentState': array([54.60482098,  2.50689023,  1.31593817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = -0.2233008753673556
running average episode reward sum: 0.5915337905119556
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45291723,  14.72693913,   6.14575867]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6114423892923139}
episode index:2765
target Thresh 71.69243480459578
target distance 55.0
model initialize at round 2765
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.76796877, 19.96233465]), 'previousTarget': array([79.79172879, 20.12120309]), 'currentState': array([60.      , 23.      ,  5.509287], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5361678997162502
running average episode reward sum: 0.5915137739209233
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59054371,  15.1225262 ,   5.9551091 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42739574352427945}
episode index:2766
target Thresh 71.69674021672633
target distance 27.0
model initialize at round 2766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.3248245 ,  13.94867989]), 'previousTarget': array([107.6656401,  13.6417852]), 'currentState': array([89.65940884, 10.30568293,  1.16775602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.5484500251673063
running average episode reward sum: 0.5914982105856311
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19865622,  15.12833618,   6.28141417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8115553104557898}
episode index:2767
target Thresh 71.70104132559673
target distance 12.0
model initialize at round 2767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.57628074,   4.58123009,   0.2113061 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.048389627179771}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.591614546942892
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38219987,  15.10151606,   1.01574813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.626085068869777}
episode index:2768
target Thresh 71.70533813550811
target distance 63.0
model initialize at round 2768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.60785316, 18.53519946]), 'previousTarget': array([71.93730785, 18.41767398]), 'currentState': array([53.6804009 , 20.23715318,  1.12636214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.576993281098978
running average episode reward sum: 0.5916092666013087
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11299539,  15.45994104,   5.92673388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47361769754513355}
episode index:2769
target Thresh 71.70963065075728
target distance 61.0
model initialize at round 2769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.42642583, 19.42873196]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([53.53895026, 21.54729646,  0.89368892]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5916185376750781
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79767405,  14.76511944,   1.30992977]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3100075263179634}
episode index:2770
target Thresh 71.71391887563672
target distance 51.0
model initialize at round 2770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.77909673, 17.63112399]), 'previousTarget': array([83.93876756, 17.43617509]), 'currentState': array([65.8596836 , 19.4247183 ,  5.97207624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 42
reward sum = 0.46310949403241713
running average episode reward sum: 0.5915721612609162
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38958531,  15.64971079,   4.96263422]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7575624249668909}
episode index:2771
target Thresh 71.71820281443469
target distance 6.0
model initialize at round 2771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.1357835 ,  19.23251536,   3.94122785]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.382258601001939}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5917123228189028
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34939943,  15.86751857,   4.57533634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0843751965360309}
episode index:2772
target Thresh 71.72248247143511
target distance 44.0
model initialize at round 2772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.07239592, 19.35800764]), 'previousTarget': array([90.6773982 , 19.42229124]), 'currentState': array([69.3490684 , 22.67317752,  2.32702827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.3675964757411226
running average episode reward sum: 0.5916315021023223
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19221266,  15.32306443,   5.84242493]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8699948393687004}
episode index:2773
target Thresh 71.72675785091766
target distance 51.0
model initialize at round 2773
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.59301143,  7.01421262]), 'previousTarget': array([83.46834337,  7.58078667]), 'currentState': array([64.       ,  3.       ,  2.2901547], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.28802139685529327
running average episode reward sum: 0.59152205361449
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13653824,  15.20058663,   5.84932583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8864542873598031}
episode index:2774
target Thresh 71.7310289571577
target distance 15.0
model initialize at round 2774
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.72890195,  14.3999243 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  13.       ,   1.4310733], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.800091915761007}
done in step count: 8
reward sum = 0.71483769442792
running average episode reward sum: 0.5915664916832515
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13364182,  14.24363444,   5.89806044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7680813704909947}
episode index:2775
target Thresh 71.73529579442634
target distance 43.0
model initialize at round 2775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.43048395, 19.18356369]), 'previousTarget': array([91.66260099, 19.34184168]), 'currentState': array([73.79638605, 22.99173903,  0.79732936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 24
reward sum = 0.5839461866142188
running average episode reward sum: 0.591563746616584
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11393673,  15.65196881,   5.82884324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.100077926872989}
episode index:2776
target Thresh 71.73955836699042
target distance 69.0
model initialize at round 2776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.60625869, 14.59652748]), 'previousTarget': array([66., 15.]), 'currentState': array([47.6069834 , 14.4262696 ,  0.62728029]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5915844664155583
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01376065,  14.66201288,   6.01068317]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0425465675331897}
episode index:2777
target Thresh 71.74381667911251
target distance 47.0
model initialize at round 2777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.66743294, 14.95224254]), 'previousTarget': array([88., 15.]), 'currentState': array([69.66746848, 14.9145382 ,  0.95879238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.591629875366668
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90355507,  14.98152199,   6.11020831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.09819909323646975}
episode index:2778
target Thresh 71.74807073505092
target distance 26.0
model initialize at round 2778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.16781978,  13.84731594]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([88.4465277 , 10.52005902,  0.90998769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5916968748211597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27610168,  15.61318119,   6.06673678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9486938124257794}
episode index:2779
target Thresh 71.75232053905971
target distance 2.0
model initialize at round 2779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.53803532,  14.77248055,   3.21981251]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5547725709102658}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5918401493266198
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64308355,  14.5356165 ,   3.58448833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7932266291770417}
episode index:2780
target Thresh 71.75656609538869
target distance 29.0
model initialize at round 2780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.94946753,  16.95782578]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([87.51588717, 21.68391873,  0.95745653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5919304416833466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07048266,  15.15746431,   5.67800608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9427605747186508}
episode index:2781
target Thresh 71.76080740828341
target distance 69.0
model initialize at round 2781
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.66457993, 18.57480558]), 'previousTarget': array([65.94769592, 18.55451479]), 'currentState': array([47.72137093, 20.08093133,  5.38968206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.591937338447491
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55038617,  15.588069  ,   5.48250604]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7402551883294883}
episode index:2782
target Thresh 71.76504448198519
target distance 28.0
model initialize at round 2782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.44031867,  16.56495699]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([8.89862867e+01, 2.12061464e+01, 6.86182082e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.59202450209212
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57856221,  15.24474026,   5.96263601]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48734752027803513}
episode index:2783
target Thresh 71.76927732073109
target distance 24.0
model initialize at round 2783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.18546149,  13.57579023]), 'previousTarget': array([109.72658355,  13.02246883]), 'currentState': array([92.44881692,  6.58020948,  5.67192501]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.4368295931902678
running average episode reward sum: 0.5919687567943822
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35060713,  15.11987039,   5.99487064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.660363549136901}
episode index:2784
target Thresh 71.77350592875396
target distance 41.0
model initialize at round 2784
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.0639748 ,  8.36801882]), 'previousTarget': array([93.31685674,  9.18257132]), 'currentState': array([74.99771746,  2.32833083,  5.65573299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 46
reward sum = 0.09082473890606957
running average episode reward sum: 0.5917888128023218
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16723532,  15.80185406,   0.25509268]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1560566379920412}
episode index:2785
target Thresh 71.77773031028242
target distance 53.0
model initialize at round 2785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.78240853, 17.74922625]), 'previousTarget': array([81.96803691, 16.86973376]), 'currentState': array([6.28548306e+01, 1.94497078e+01, 2.80165672e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5918263664294744
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15512723,  15.43091522,   5.63709025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9484186428637342}
episode index:2786
target Thresh 71.78195046954083
target distance 33.0
model initialize at round 2786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.25509074,  10.21640689]), 'previousTarget': array([100.60816786,   9.33049037]), 'currentState': array([82.36632991,  3.64261696,  0.34137106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 37
reward sum = 0.46610352411429523
running average episode reward sum: 0.5917812559729565
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48778216,  15.80083783,   5.38917661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9506357588301323}
episode index:2787
target Thresh 71.78616641074935
target distance 49.0
model initialize at round 2787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.48540785, 17.66366246]), 'previousTarget': array([85.96262067, 16.77779873]), 'currentState': array([66.57210265, 19.5238442 ,  0.20164537]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.4656442727135368
running average episode reward sum: 0.5917360131525622
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32483285,  15.31619914,   3.2410453 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45331917285499557}
episode index:2788
target Thresh 71.79037813812394
target distance 25.0
model initialize at round 2788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.20951843,  15.18273141]), 'previousTarget': array([110.,  15.]), 'currentState': array([91.23271808, 16.14577232,  6.02204472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5918171070659521
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29557893,  14.99188603,   5.99081362]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.704467802469559}
episode index:2789
target Thresh 71.79458565587632
target distance 4.0
model initialize at round 2789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.12158752e+02, 1.71588098e+01, 7.77935982e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5683541777872434}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5919527636584017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50362985,  14.4127604 ,   5.38525808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7689172101646747}
episode index:2790
target Thresh 71.79878896821398
target distance 17.0
model initialize at round 2790
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.41843931,  14.70149833]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.       , 16.       ,  3.9241755], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.46415481730355}
done in step count: 16
reward sum = 0.384550320349507
running average episode reward sum: 0.5918784524999248
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81193471,  15.07087552,   0.35052912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20097734560276667}
episode index:2791
target Thresh 71.8029880793403
target distance 55.0
model initialize at round 2791
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([80.64516322,  8.21197862]), 'previousTarget': array([79.54031523,  7.2633415 ]), 'currentState': array([61.02449261,  4.33521646,  6.1895091 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.4452594972781685
running average episode reward sum: 0.5918259385474814
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42795945,  14.36908136,   5.8127715 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8516388461385133}
episode index:2792
target Thresh 71.80718299345432
target distance 52.0
model initialize at round 2792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.6406232 ,  8.05955977]), 'previousTarget': array([82.64012894,  8.77694787]), 'currentState': array([64.11315608,  3.73775136,  4.42510425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3846588396108139
running average episode reward sum: 0.5917517648636517
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15499736,  15.74729524,   5.49833991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.128042390827719}
episode index:2793
target Thresh 71.81137371475099
target distance 20.0
model initialize at round 2793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.23504596,  15.33813862]), 'previousTarget': array([113.87716713,  15.39299151]), 'currentState': array([95.94251351, 23.42413   ,  1.74964172]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.5285609388390459
running average episode reward sum: 0.5917291482473221
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81965577,  14.88256394,   5.88327107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2152098308608396}
episode index:2794
target Thresh 71.81556024742102
target distance 26.0
model initialize at round 2794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.84462611,  16.83165229]), 'previousTarget': array([108.48782391,  16.50280987]), 'currentState': array([87.33073792, 21.21436465,  2.00387287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5918013790649952
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76795448,  15.18818552,   5.61212331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29876230754683036}
episode index:2795
target Thresh 71.81974259565096
target distance 22.0
model initialize at round 2795
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.99520833,  16.43777159]), 'previousTarget': array([112.9793708 ,  15.09184678]), 'currentState': array([93.     , 16.     ,  5.51104], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7397278682212585
running average episode reward sum: 0.5918542855346505
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2957666 ,  15.71356033,   5.73353912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0025532534904469}
episode index:2796
target Thresh 71.82392076362314
target distance 44.0
model initialize at round 2796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.96021312, 18.92786467]), 'previousTarget': array([90.81660336, 18.29773591]), 'currentState': array([72.24466786, 22.2890066 ,  6.27494187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6968874835351415
running average episode reward sum: 0.5918918376254623
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68493516,  15.87437646,   4.81080296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.110707140991419}
episode index:2797
target Thresh 71.82809475551574
target distance 22.0
model initialize at round 2797
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.73260577,  15.80968002]), 'previousTarget': array([112.6773982 ,  15.42229124]), 'currentState': array([92.31977576, 20.6202965 ,  1.08992243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5919815629134388
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14778512e+02, 1.58678038e+01, 7.25867450e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8956229175701785}
episode index:2798
target Thresh 71.83226457550275
target distance 49.0
model initialize at round 2798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.91647328, 15.42331212]), 'previousTarget': array([85.99583637, 14.40807829]), 'currentState': array([66.91874494, 15.72474438,  1.45710766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5038682298206187
running average episode reward sum: 0.5919500826229448
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43370569,  15.20922415,   5.72562396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6037085302465428}
episode index:2799
target Thresh 71.83643022775398
target distance 3.0
model initialize at round 2799
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.87862703,  13.49047891,   0.96990994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6036277016145455}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5920887075934366
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0676477 ,  15.05216867,   0.35012801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08542705278644507}
episode index:2800
target Thresh 71.84059171643509
target distance 11.0
model initialize at round 2800
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.41634881,  16.58899332]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  11.       ,   4.0906525], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.821047705852022}
done in step count: 9
reward sum = 0.8435172474836408
running average episode reward sum: 0.5921784714420228
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39070087,  14.57278333,   1.41692262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7441501963529265}
episode index:2801
target Thresh 71.84474904570757
target distance 26.0
model initialize at round 2801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.13062361,  11.83851956]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([89.96238866,  3.47698626,  6.06723487]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5639403040985035
running average episode reward sum: 0.5921683935807297
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10985315,  15.09366874,   6.22325923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.14436601480853636}
episode index:2802
target Thresh 71.84890221972876
target distance 26.0
model initialize at round 2802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.70654598,  13.99585587]), 'previousTarget': array([108.64012894,  13.77694787]), 'currentState': array([87.89344393, 11.26803854,  1.2782948 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5922263853372844
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.81902022,  14.28568251,   2.80869656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0867582967154523}
episode index:2803
target Thresh 71.85305124265182
target distance 3.0
model initialize at round 2803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.09113467,  15.47455891,   3.58522534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.1443065001815245}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5923682446863082
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98758569,  14.89714328,   3.6628613 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9929274870510335}
episode index:2804
target Thresh 71.85719611862577
target distance 5.0
model initialize at round 2804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.47157736,  11.88823033,   2.19117266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.442186862359121}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5925064734760813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34250225,  14.63731507,   2.0024244 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7508952354939536}
episode index:2805
target Thresh 71.8613368517955
target distance 69.0
model initialize at round 2805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.52896295, 16.18882429]), 'previousTarget': array([65.99789993, 15.71017536]), 'currentState': array([47.5352316 , 16.6895303 ,  5.69532186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3580160101324961
running average episode reward sum: 0.5924229059552889
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00906472,  14.04726706,   5.89665268]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9527760635144162}
episode index:2806
target Thresh 71.86547344630173
target distance 15.0
model initialize at round 2806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.62057313,  19.02080597,   1.07461285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.970538433615538}
done in step count: 8
reward sum = 0.85344469442792
running average episode reward sum: 0.5925158955486173
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21333929,  14.83760746,   6.04961714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8032474171319154}
episode index:2807
target Thresh 71.86960590628108
target distance 60.0
model initialize at round 2807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.38407224, 17.02044105]), 'previousTarget': array([74.98889814, 16.3337034 ]), 'currentState': array([56.4113915 , 18.06544052,  1.50388127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5344815250766803
running average episode reward sum: 0.5924952280377654
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56730303,  15.82425615,   5.0042439 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9309268815824235}
episode index:2808
target Thresh 71.87373423586597
target distance 66.0
model initialize at round 2808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.3157833 , 11.36157125]), 'previousTarget': array([68.91786413, 10.81071492]), 'currentState': array([50.38175633,  9.73843579,  5.76500267]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.19271273160146998
running average episode reward sum: 0.5922156951222656
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.699827  ,  22.60830827,   0.14827873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.544139634884807}
episode index:2809
target Thresh 71.87785843918476
target distance 22.0
model initialize at round 2809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.99332765,  16.23344501]), 'previousTarget': array([112.29527642,  15.73765188]), 'currentState': array([91.87858275, 22.11787593,  1.34777594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5923079520888038
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77398726,  14.82696428,   6.04947292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28464560498982333}
episode index:2810
target Thresh 71.88197852036164
target distance 48.0
model initialize at round 2810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.8066854 , 18.46598492]), 'previousTarget': array([86.84555753, 18.51930531]), 'currentState': array([68.97951355, 21.08958104,  6.09038014]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5482120338655323
running average episode reward sum: 0.5922922651737476
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6046512 ,  15.32731593,   5.90168   ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.513260548301322}
episode index:2811
target Thresh 71.88609448351671
target distance 28.0
model initialize at round 2811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.07573712,  15.83760135]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.22047863, 18.23941534,  0.45270651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5923814013502092
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29634269,  14.58438175,   5.94041675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8172344403064992}
episode index:2812
target Thresh 71.89020633276591
target distance 65.0
model initialize at round 2812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.21337417, 11.01279141]), 'previousTarget': array([69.88502281, 10.14146399]), 'currentState': array([51.29578121,  9.1990961 ,  1.6095094 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.2716547731108835
running average episode reward sum: 0.5922673854852112
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49493501,  15.66928524,   6.02836512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8384708563124359}
episode index:2813
target Thresh 71.8943140722211
target distance 34.0
model initialize at round 2813
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.90889484,  12.90680527]), 'previousTarget': array([100.86301209,  13.33682495]), 'currentState': array([81.       , 11.       ,  1.4653412], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6623305895390459
running average episode reward sum: 0.5922922835676753
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06924638,  15.59960083,   5.44644948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1071691179149468}
episode index:2814
target Thresh 71.89841770599004
target distance 56.0
model initialize at round 2814
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.99379387, 14.5017965 ]), 'previousTarget': array([79., 15.]), 'currentState': array([59.      , 15.      ,  2.123401], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.42260270120323223
running average episode reward sum: 0.5922320030766045
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16575233,  15.46173621,   0.27227831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9535038038096973}
episode index:2815
target Thresh 71.90251723817633
target distance 53.0
model initialize at round 2815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.23993581, 17.19643729]), 'previousTarget': array([81.96803691, 16.86973376]), 'currentState': array([63.28759212, 18.57628563,  1.33005756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6117182729378741
running average episode reward sum: 0.5922389229167542
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56625   ,  15.5305089 ,   5.82986189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6852581706493467}
episode index:2816
target Thresh 71.9066126728795
target distance 61.0
model initialize at round 2816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.32542005, 15.17567623]), 'previousTarget': array([74., 15.]), 'currentState': array([52.32558952, 15.2580085 ,  1.99763238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.48256451378080684
running average episode reward sum: 0.5921999898641677
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74709014,  15.22752595,   0.23229063]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34019326648296655}
episode index:2817
target Thresh 71.91070401419503
target distance 39.0
model initialize at round 2817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.25644947, 17.2195938 ]), 'previousTarget': array([95.94108971, 16.46607002]), 'currentState': array([75.38164924, 19.45394284,  2.98291457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5922155986279007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33534965,  14.59822037,   1.71501742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7766511202303322}
episode index:2818
target Thresh 71.91479126621422
target distance 3.0
model initialize at round 2818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.20257545,  13.58206053,   1.18672943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.053515209395146}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5923199509773467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42021363,  15.11093527,   0.84080777]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5903040519281951}
episode index:2819
target Thresh 71.91887443302434
target distance 45.0
model initialize at round 2819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.24952756, 11.27220356]), 'previousTarget': array([89.76232938, 11.07414013]), 'currentState': array([71.49142141,  8.17103587,  1.12418097]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5923355061316327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92820372,  14.52890311,   0.15709546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4765364463188887}
episode index:2820
target Thresh 71.92295351870854
target distance 40.0
model initialize at round 2820
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.91846814, 16.19594155]), 'previousTarget': array([94.9439862 , 16.50420104]), 'currentState': array([75.      , 18.      ,  5.702507], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.43676299377929206
running average episode reward sum: 0.5922803581300897
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01497781,  15.89976772,   6.08390179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3341104402166717}
episode index:2821
target Thresh 71.92702852734594
target distance 7.0
model initialize at round 2821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.51303179,  21.88438473,   5.36496938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.728916190106869}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.592404100083056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69213806,  14.89394106,   4.48632084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32561860049683666}
episode index:2822
target Thresh 71.93109946301152
target distance 10.0
model initialize at round 2822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.77445304,   6.16614137,   1.78627485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.606432954221296}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5925277543690348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31764437,  15.29882375,   3.17074482]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4361118922774321}
episode index:2823
target Thresh 71.93516632977621
target distance 8.0
model initialize at round 2823
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.07800603e+02, 1.13363014e+01, 2.10254192e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.077995515682808}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5926075628616794
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66609456,  14.27015124,   1.47383116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9881098998171491}
episode index:2824
target Thresh 71.93922913170691
target distance 39.0
model initialize at round 2824
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.99473156, 15.4590314 ]), 'previousTarget': array([96., 15.]), 'currentState': array([76.      , 15.      ,  4.795322], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.3942986004660377
running average episode reward sum: 0.5925373649988844
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10462965,  15.52012525,   4.7980178 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5305446593448516}
episode index:2825
target Thresh 71.94328787286639
target distance 46.0
model initialize at round 2825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.07723962, 16.78249951]), 'previousTarget': array([88.98112317, 16.13125551]), 'currentState': array([70.12819668, 18.20927402,  1.83007019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6206362285316084
running average episode reward sum: 0.5925473079796109
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41981495,  14.73809257,   6.2693285 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6365612211305992}
episode index:2826
target Thresh 71.9473425573134
target distance 30.0
model initialize at round 2826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.09141226,  13.55052704]), 'previousTarget': array([104.61161351,  12.9223227 ]), 'currentState': array([85.30202955, 10.65564677,  0.38035655]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6603993725607656
running average episode reward sum: 0.5925713094173827
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94601686,  15.86015364,   5.65092766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8618459619676584}
episode index:2827
target Thresh 71.95139318910265
target distance 46.0
model initialize at round 2827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.23771017, 19.82861736]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([70.60744414, 23.65649612,  1.30477041]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.5926031281180875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79013192,  15.78181711,   5.87345371]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.809495273172053}
episode index:2828
target Thresh 71.95543977228475
target distance 44.0
model initialize at round 2828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.63867877, 12.80724689]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([69.71301647, 11.08446369,  3.45138705]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5243694115078581
running average episode reward sum: 0.5925790087414137
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04763146,  15.96601809,   6.26469897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.356538531600369}
episode index:2829
target Thresh 71.95948231090627
target distance 25.0
model initialize at round 2829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.53106405,  16.31498461]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([91.34444182, 21.96064907,  0.55523193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5926417169170546
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03389187,  14.71847007,   3.21332767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28356262590799014}
episode index:2830
target Thresh 71.96352080900976
target distance 36.0
model initialize at round 2830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.41273547,  9.86491187]), 'previousTarget': array([97.81107799,  8.79288928]), 'currentState': array([79.30732182,  3.95025471,  1.69539326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6944217843471036
running average episode reward sum: 0.592677668901311
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71692025,  14.84899452,   5.99243939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3208376516035464}
episode index:2831
target Thresh 71.96755527063374
target distance 30.0
model initialize at round 2831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.44573852,  12.23211034]), 'previousTarget': array([104.47682419,  12.54459231]), 'currentState': array([86.23562566,  6.6669002 ,  6.21746564]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.46200792624006415
running average episode reward sum: 0.5926315284554561
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47210467,  14.96799426,   6.01149505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5288646746369946}
episode index:2832
target Thresh 71.97158569981265
target distance 2.0
model initialize at round 2832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.06791784,  15.7149229 ,   2.48407471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.18801255637135}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5927717926529656
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25935316,  15.99586582,   3.50106335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0290834743913928}
episode index:2833
target Thresh 71.97561210057692
target distance 2.0
model initialize at round 2833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.29846765,  16.7502144 ,   3.88588893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4409962732430093}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5929050062053112
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77344232,  15.44496346,   2.14722401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4993204021788254}
episode index:2834
target Thresh 71.97963447695295
target distance 20.0
model initialize at round 2834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.92219445,  13.56292288]), 'previousTarget': array([112.14985851,  13.28991511]), 'currentState': array([96.47315192,  2.18623621,  0.50531882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.30524317616218466
running average episode reward sum: 0.5928035381876592
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14978068e+02, 1.46750196e+01, 5.67879230e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32571968211798547}
episode index:2835
target Thresh 71.98365283296313
target distance 11.0
model initialize at round 2835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.63711029,  20.42031634,   5.9640882 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.69484127084213}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5929264848065638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50343415,  15.51396961,   6.08822075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.714662442067451}
episode index:2836
target Thresh 71.9876671726258
target distance 61.0
model initialize at round 2836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.22647886,  9.82560968]), 'previousTarget': array([73.83019061,  9.60068074]), 'currentState': array([52.3712338 ,  7.42368473,  3.89710641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.3739937798958046
running average episode reward sum: 0.5928493143078291
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06357248,  15.50976298,   5.63697628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0661870347749944}
episode index:2837
target Thresh 71.99167749995533
target distance 28.0
model initialize at round 2837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.04059853,  14.5843714 ]), 'previousTarget': array([106.88618308,  14.13066247]), 'currentState': array([87.06781068, 13.54142089,  0.51683235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5929117504711474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6028153 ,  15.11253194,   6.1758041 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4128185080834573}
episode index:2838
target Thresh 71.99568381896201
target distance 42.0
model initialize at round 2838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.30741382, 11.65168883]), 'previousTarget': array([92.72787848, 11.28797975]), 'currentState': array([71.50419344,  8.85303514,  1.68047559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.40281519204952
running average episode reward sum: 0.592844791486145
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3703238 ,  15.02659999,   6.01568303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6302377914350173}
episode index:2839
target Thresh 71.99968613365218
target distance 65.0
model initialize at round 2839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.26421854,  9.02486552]), 'previousTarget': array([69.81099734,  8.74306117]), 'currentState': array([48.42569595,  6.48852534,  1.85047853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5520455665887987
running average episode reward sum: 0.5928304255618854
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88041843,  15.28135453,   5.88929335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30571247937345764}
episode index:2840
target Thresh 72.00368444802814
target distance 29.0
model initialize at round 2840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.89380254,  11.65066651]), 'previousTarget': array([105.10128274,  11.9279843 ]), 'currentState': array([87.12321088,  4.74669022,  0.16991299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5928847520908433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0694681 ,  14.05471836,   6.24388599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9478307829347928}
episode index:2841
target Thresh 72.00767876608822
target distance 14.0
model initialize at round 2841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.02232591e+02, 4.33816520e+00, 7.23941326e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.63374418942838}
done in step count: 12
reward sum = 0.8191431510161292
running average episode reward sum: 0.5929643644761091
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02490315,  15.04175169,   6.23453838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9759903007184225}
episode index:2842
target Thresh 72.01166909182675
target distance 25.0
model initialize at round 2842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.21385278,  11.92668052]), 'previousTarget': array([107.74433602,  11.22705473]), 'currentState': array([89.99511942,  3.67575604,  0.56370878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5208375591114673
running average episode reward sum: 0.5929389945129137
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45099583,  15.08205828,   0.19202903]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5551028238378607}
episode index:2843
target Thresh 72.01565542923402
target distance 31.0
model initialize at round 2843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.77579795,  17.35107536]), 'previousTarget': array([103.63559701,  17.19956187]), 'currentState': array([85.28448972, 21.83314001,  5.85606581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7001882007868602
running average episode reward sum: 0.5929767052042899
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70466562,  15.51838144,   5.71236875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5966085113524092}
episode index:2844
target Thresh 72.01963778229639
target distance 26.0
model initialize at round 2844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.5330765 ,  16.88889828]), 'previousTarget': array([108.11558017,  17.11828302]), 'currentState': array([90.62961893, 23.4203064 ,  5.52565104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5930675597089966
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58442398,  14.5603531 ,   0.41959959]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7313280994468048}
episode index:2845
target Thresh 72.0236161549962
target distance 63.0
model initialize at round 2845
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.5090081 , 16.08943521]), 'previousTarget': array([72., 15.]), 'currentState': array([52.51557855, 16.60205083,  2.26943961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = -0.19267211285910724
running average episode reward sum: 0.5927914740896825
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27722347,  14.87810617,   5.74031667]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7329829631876904}
episode index:2846
target Thresh 72.02759055131186
target distance 3.0
model initialize at round 2846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.62800251,  19.58001616,   2.72479796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.280430399055581}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5929206643025067
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18172329,  14.45550955,   4.97494736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9828767103765866}
episode index:2847
target Thresh 72.03156097521772
target distance 6.0
model initialize at round 2847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.37462006,  15.3289162 ,   5.18057203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.6335394222588056}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5930497637918667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39301057,  15.33424507,   0.79876643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5159235147684662}
episode index:2848
target Thresh 72.03552743068424
target distance 72.0
model initialize at round 2848
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([62.99399627, 12.49001351]), 'previousTarget': array([62.98266146, 12.83261089]), 'currentState': array([43.       , 12.       ,  1.8384782], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.44186641411810956
running average episode reward sum: 0.5929966983830658
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83273053,  14.67674307,   5.3923404 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3639699444893956}
episode index:2849
target Thresh 72.03948992167786
target distance 66.0
model initialize at round 2849
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.3005387 ,  7.32868749]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([50.58872168,  3.94574943,  6.1974116 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.3356747573204943
running average episode reward sum: 0.592906409982693
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85811333,  15.9801688 ,   0.11728152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9903851268636646}
episode index:2850
target Thresh 72.04344845216106
target distance 12.0
model initialize at round 2850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.81889056,   4.93473017,   4.8854692 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.584670716764037}
done in step count: 13
reward sum = 0.8082210229989678
running average episode reward sum: 0.5929819324705977
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60270687,  14.00020577,   0.58364683]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0758393623473284}
episode index:2851
target Thresh 72.04740302609237
target distance 21.0
model initialize at round 2851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.00236297,  14.42216955]), 'previousTarget': array([113.64677133,  14.74224216]), 'currentState': array([92.36389041, 10.63661865,  4.33980179]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 28
reward sum = 0.6920456094055224
running average episode reward sum: 0.5930166672801822
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7408105 ,  15.76001182,   5.58233446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8029926305462921}
episode index:2852
target Thresh 72.0513536474264
target distance 18.0
model initialize at round 2852
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.3859521 ,  14.19265625]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 11.       ,  1.6241258], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.647091208706367}
done in step count: 21
reward sum = 0.6704278682212584
running average episode reward sum: 0.5930438005437437
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00713695,  15.20956052,   5.2191767 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20968201838025055}
episode index:2853
target Thresh 72.05530032011374
target distance 22.0
model initialize at round 2853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.62039847,  16.68207136]), 'previousTarget': array([111.79586847,  16.16513874]), 'currentState': array([91.95007365, 23.85277365,  1.32504857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7117950085723674
running average episode reward sum: 0.5930854092361153
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26507185,  15.98171841,   6.03819764]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.226332106358315}
episode index:2854
target Thresh 72.05924304810108
target distance 7.0
model initialize at round 2854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.28497394,   9.41004979,   0.86762485]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.847123891043979}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5932141344903232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1557048 ,  15.49232689,   0.21150813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9773536471896516}
episode index:2855
target Thresh 72.06318183533112
target distance 57.0
model initialize at round 2855
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.6117913 , 16.77735839]), 'previousTarget': array([77.98769988, 16.2986772 ]), 'currentState': array([56.63319342, 17.70235931,  3.45812631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.5931999430836958
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33434802,  15.43957654,   5.73735099]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7976967453016576}
episode index:2856
target Thresh 72.06711668574268
target distance 42.0
model initialize at round 2856
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.95913135, 15.72208142]), 'previousTarget': array([92.97736275, 16.04869701]), 'currentState': array([73.      , 17.      ,  5.650201], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.612554595010387
running average episode reward sum: 0.5932067175505935
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34296148,  15.52619419,   5.66095972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8417719074930281}
episode index:2857
target Thresh 72.0710476032706
target distance 16.0
model initialize at round 2857
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.27212407,  14.13119892]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       , 17.       ,  3.5323994], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.508748944068945}
done in step count: 12
reward sum = 0.6146529175231292
running average episode reward sum: 0.5932142214694083
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27835157,  15.94071978,   5.50230213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.185634919628572}
episode index:2858
target Thresh 72.07497459184579
target distance 69.0
model initialize at round 2858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.57424308, 17.97079327]), 'previousTarget': array([65.96647808, 17.84252301]), 'currentState': array([47.61336684, 19.22116101,  1.14623135]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 57
reward sum = 0.19147489343704444
running average episode reward sum: 0.5930737040409254
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40811694,  15.7207368 ,   6.07535005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9326237700915228}
episode index:2859
target Thresh 72.07889765539524
target distance 9.0
model initialize at round 2859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11005753,   7.43021416,   2.5039959 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.570585861125857}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5931889736179838
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39260444,  15.60970709,   0.56564337]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8606230910683619}
episode index:2860
target Thresh 72.08281679784204
target distance 14.0
model initialize at round 2860
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.55019269,  14.14533817]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,   7.       ,   1.5226634], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.441717144409314}
done in step count: 14
reward sum = 0.7987458127689782
running average episode reward sum: 0.593260821517023
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12961208,  15.29703374,   0.66977975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9196761213626133}
episode index:2861
target Thresh 72.08673202310528
target distance 67.0
model initialize at round 2861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.90500444,  6.67539027]), 'previousTarget': array([67.73578183,  7.24020299]), 'currentState': array([49.22338925,  3.1209531 ,  0.38694304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.03123034527462476
running average episode reward sum: 0.5930644446909425
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83382882,  15.33624082,   5.58812854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3750609988847162}
episode index:2862
target Thresh 72.09064333510025
target distance 72.0
model initialize at round 2862
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.40061523, 20.43526711]), 'previousTarget': array([62.90614324, 20.06468052]), 'currentState': array([44.51501172, 22.57133195,  1.3163864 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.13396501695847496
running average episode reward sum: 0.5929040886211792
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25824214,  15.26211871,   5.7674581 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7867089275453019}
episode index:2863
target Thresh 72.0945507377382
target distance 2.0
model initialize at round 2863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.92939096,  14.29853162,   1.25028491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1497139214494565}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5930160345565362
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97363264,  14.64097115,   3.37581801]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3599957654049537}
episode index:2864
target Thresh 72.09845423492658
target distance 28.0
model initialize at round 2864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.07263888,  15.32902744]), 'previousTarget': array([107.,  15.]), 'currentState': array([88.09516024, 16.27789363,  6.13543529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.59309741416885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34933181,  15.79258534,   0.48796513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0254562955447588}
episode index:2865
target Thresh 72.10235383056887
target distance 46.0
model initialize at round 2865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.22682397, 17.86457773]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.35920604, 20.16191482,  1.5258637 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5931383966577207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32630341,  15.44997965,   5.84507439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8101535507163088}
episode index:2866
target Thresh 72.10624952856466
target distance 69.0
model initialize at round 2866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.53987055, 20.68437423]), 'previousTarget': array([65.86691472, 20.6965896 ]), 'currentState': array([47.68179767, 23.0628068 ,  1.04732769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 69
reward sum = 0.34292560705455327
running average episode reward sum: 0.5930511232745316
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14582797e+02, 1.50398014e+01, 1.95661147e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41909724066010773}
episode index:2867
target Thresh 72.11014133280966
target distance 42.0
model initialize at round 2867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.83647513, 13.79652168]), 'previousTarget': array([92.99433347, 14.47605556]), 'currentState': array([71.8634146 , 12.75880633,  2.96095657]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5930752623671778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72948514,  15.99305114,   5.3134134 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0292370269756452}
episode index:2868
target Thresh 72.11402924719566
target distance 22.0
model initialize at round 2868
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.27040773,  14.89162927]), 'previousTarget': array([112.91786413,  14.81071492]), 'currentState': array([94.48745227, 11.95314782,  5.09075069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5931289733577544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68691108,  14.40135752,   5.36934072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6755719756439107}
episode index:2869
target Thresh 72.11791327561059
target distance 25.0
model initialize at round 2869
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.15012234,  14.00925004]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([91.78121226,  9.02474181,  0.8458758 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.5703383830049353
running average episode reward sum: 0.5931210323855061
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02257907,  15.29903828,   5.95832883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29988949460854036}
episode index:2870
target Thresh 72.12179342193848
target distance 18.0
model initialize at round 2870
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.84100182,  14.70944487]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      ,  8.      ,  4.072615], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.700043145805155
running average episode reward sum: 0.5931582745009432
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02482576,  15.8169798 ,   5.34619305]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2721716827960152}
episode index:2871
target Thresh 72.12566969005945
target distance 26.0
model initialize at round 2871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.56206697,  13.09511864]), 'previousTarget': array([107.89972147,  12.54221128]), 'currentState': array([89.38395233,  7.4206214 ,  0.40791416]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.5328150301652426
running average episode reward sum: 0.5931372636219961
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22408975,  15.14981093,   6.00598213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26955431581596756}
episode index:2872
target Thresh 72.1295420838498
target distance 8.0
model initialize at round 2872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.64515549,  22.32803024,   5.05502174]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.45222321056364}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5932651643342753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81200267,  14.83723216,   5.28456168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24866919069273843}
episode index:2873
target Thresh 72.1334106071819
target distance 37.0
model initialize at round 2873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.06154464,  9.66379724]), 'previousTarget': array([97.30726786, 10.2181805 ]), 'currentState': array([78.98576192,  3.65426201,  4.36477033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4839854358643739
running average episode reward sum: 0.5932271407683498
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71073381,  15.8206519 ,   0.12156114]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0856390271623246}
episode index:2874
target Thresh 72.1372752639243
target distance 61.0
model initialize at round 2874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.80549426,  8.06828405]), 'previousTarget': array([73.62388913,  6.86043721]), 'currentState': array([55.09642568,  4.66937014,  0.89764413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.4645564164070324
running average episode reward sum: 0.5931823857337893
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65476847,  14.8528248 ,   5.65831062]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37529368249942546}
episode index:2875
target Thresh 72.14113605794164
target distance 62.0
model initialize at round 2875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.97703203,  6.7564677 ]), 'previousTarget': array([72.63559701,  6.80043813]), 'currentState': array([51.31871738,  3.07534489,  2.08680868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.17724203954463102
running average episode reward sum: 0.5930377611349753
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11828102,  15.65721681,   5.16388551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0997100980670975}
episode index:2876
target Thresh 72.1449929930947
target distance 51.0
model initialize at round 2876
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.8336913 , 18.42615278]), 'previousTarget': array([83.86301209, 18.66317505]), 'currentState': array([64.       , 21.       ,  3.5650065], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.30104500656740446
running average episode reward sum: 0.5929362690409302
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17027729,  15.506481  ,   0.16439834]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5343382482939899}
episode index:2877
target Thresh 72.14884607324045
target distance 26.0
model initialize at round 2877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.82966357,  12.10267863]), 'previousTarget': array([107.15918769,  11.38116355]), 'currentState': array([89.2862716 ,  4.6098411 ,  2.16838306]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6518269221402049
running average episode reward sum: 0.5929567313943351
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15182603e+02, 1.53871989e+01, 3.71504799e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42809689764545394}
episode index:2878
target Thresh 72.15269530223193
target distance 45.0
model initialize at round 2878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.72841408, 16.22894087]), 'previousTarget': array([89.99506356, 15.55566525]), 'currentState': array([68.75026035, 17.16348551,  1.42374706]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5930051324496529
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06206341,  15.24233551,   0.25786223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25015667899813504}
episode index:2879
target Thresh 72.15654068391842
target distance 1.0
model initialize at round 2879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41962333,  14.06824438,   3.91407406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0977274745125332}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5931464501119967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41962333,  14.06824438,   3.91407406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0977274745125332}
episode index:2880
target Thresh 72.16038222214526
target distance 28.0
model initialize at round 2880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.60434691,  16.35515299]), 'previousTarget': array([106.88618308,  15.86933753]), 'currentState': array([85.80918492, 19.21024702,  1.47856379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5932132781892946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67551568,  15.31418852,   5.16835428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4516685712378568}
episode index:2881
target Thresh 72.164219920754
target distance 26.0
model initialize at round 2881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.34370383,  12.87461545]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([90.62177477,  5.83974713,  6.02198702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.2861126414697506
running average episode reward sum: 0.593106720022494
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20754561,  15.61207444,   5.91343208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0013086873354464}
episode index:2882
target Thresh 72.16805378358234
target distance 65.0
model initialize at round 2882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.36728767, 17.52548639]), 'previousTarget': array([69.99053926, 16.38490648]), 'currentState': array([50.39922821, 18.65535415,  0.35787201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5231610844081905
running average episode reward sum: 0.5930824586157599
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42735895,  15.54927673,   5.09070695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6959458305509373}
episode index:2883
target Thresh 72.17188381446415
target distance 32.0
model initialize at round 2883
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.99459282,  13.5349645 ]), 'previousTarget': array([102.99024152,  14.62469505]), 'currentState': array([83.      , 14.      ,  2.357649], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7316305895390458
running average episode reward sum: 0.5931304988830703
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62468913,  15.1408758 ,   6.13611557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4008793349277621}
episode index:2884
target Thresh 72.17571001722945
target distance 62.0
model initialize at round 2884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.00194912, 15.39432487]), 'previousTarget': array([72.99739905, 14.32253869]), 'currentState': array([53.00283061, 15.58209905,  0.64622498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5211269836850869
running average episode reward sum: 0.5931055409921872
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38371396,  14.93989287,   5.50381932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.619210262043297}
episode index:2885
target Thresh 72.17953239570444
target distance 28.0
model initialize at round 2885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.59197797,  17.94872677]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([85.34934025, 23.40041402,  1.91998184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5931563361524076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52082848,  15.48894723,   6.24845682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6845982333384001}
episode index:2886
target Thresh 72.18335095371154
target distance 24.0
model initialize at round 2886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.26100962,  15.4170892 ]), 'previousTarget': array([110.98266146,  15.16738911]), 'currentState': array([91.3842971 , 17.63436215,  0.49422491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5932399376159676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25778954,  15.45430125,   5.80989643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5223457435649634}
episode index:2887
target Thresh 72.18716569506925
target distance 62.0
model initialize at round 2887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.32269582, 13.30111015]), 'previousTarget': array([72.97662792, 12.96661103]), 'currentState': array([51.33780797, 12.52376998,  1.93101525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.32621986002400516
running average episode reward sum: 0.5931474791403472
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06641938,  15.72537042,   5.78005018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7284049610706813}
episode index:2888
target Thresh 72.19097662359233
target distance 8.0
model initialize at round 2888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.327662  ,   6.08349447,   0.40580338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.567851687061687}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5932552100492666
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39422132,  14.89579868,   3.84892967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6146753016656095}
episode index:2889
target Thresh 72.19478374309173
target distance 25.0
model initialize at round 2889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.11934542,  15.87818743]), 'previousTarget': array([109.44774604,  16.33254095]), 'currentState': array([91.61259093, 20.29254233,  0.4580118 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.5049624238355738
running average episode reward sum: 0.5932246589121685
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31882196,  14.49022484,   6.28239365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6012637985280752}
episode index:2890
target Thresh 72.19858705737457
target distance 49.0
model initialize at round 2890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.23529515, 10.4317159 ]), 'previousTarget': array([85.85172777, 11.43082381]), 'currentState': array([65.46677434,  7.39764518,  5.3366214 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.10517451820489487
running average episode reward sum: 0.5930558418451649
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58021106,  15.43107582,   5.98245327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6017051753065681}
episode index:2891
target Thresh 72.20238657024412
target distance 62.0
model initialize at round 2891
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.50925166, 12.68550589]), 'previousTarget': array([72.9352791, 11.6076838]), 'currentState': array([52.5388562 , 11.59770762,  2.86178809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5930642216166369
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28821063,  15.32501924,   0.15469575]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7824842634511712}
episode index:2892
target Thresh 72.20618228549995
target distance 36.0
model initialize at round 2892
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.33346549,  9.12026487]), 'previousTarget': array([98.12703142,  9.84437071]), 'currentState': array([79.       ,  4.       ,  2.0243511], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.0920756495616204
running average episode reward sum: 0.5928910489335898
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33978654,  15.33051618,   0.87403254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4740209233437061}
episode index:2893
target Thresh 72.20997420693773
target distance 60.0
model initialize at round 2893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.46764606,  9.15130287]), 'previousTarget': array([74.86526278, 10.31761399]), 'currentState': array([54.67266616,  6.29494663,  3.63634598]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5928890260355262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42284649,  15.89265296,   5.36081439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0629842307586772}
episode index:2894
target Thresh 72.2137623383494
target distance 34.0
model initialize at round 2894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.57587197, 18.15777443]), 'previousTarget': array([100.69567118,  17.52429332]), 'currentState': array([79.98228173, 22.16916089,  1.27705431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5929529059434239
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3153266 ,  15.71038037,   0.7981874 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9866194488880539}
episode index:2895
target Thresh 72.21754668352311
target distance 25.0
model initialize at round 2895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.52117647,  13.52062511]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([91.53032424,  7.24786402,  1.0573104 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6149168421664546
running average episode reward sum: 0.5929604901755451
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24202399,  15.25758303,   6.1257075 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8005477190612503}
episode index:2896
target Thresh 72.22132724624318
target distance 36.0
model initialize at round 2896
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.40279332, 10.85093922]), 'previousTarget': array([98.40285  , 10.8507125]), 'currentState': array([79.       ,  6.       ,  2.5123498], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.27513212339238236
running average episode reward sum: 0.5928507806944325
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.86982326,  15.59035281,   4.68698002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0512416164681238}
episode index:2897
target Thresh 72.22510403029018
target distance 41.0
model initialize at round 2897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.55129927, 18.62478933]), 'previousTarget': array([93.62981184, 19.16979281]), 'currentState': array([72.80703452, 21.81289367,  4.93452907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3096693950194669
running average episode reward sum: 0.5927530645503073
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75808498,  14.82020134,   0.19337183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30141405773658925}
episode index:2898
target Thresh 72.2288770394409
target distance 24.0
model initialize at round 2898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.69627379,  15.65811943]), 'previousTarget': array([110.93091516,  15.3390904 ]), 'currentState': array([90.92609173, 18.68134638,  0.60472584]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5928393667679112
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63375086,  15.85763456,   5.20471859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0663851046460968}
episode index:2899
target Thresh 72.23264627746833
target distance 7.0
model initialize at round 2899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.0104391 ,   9.4182829 ,   3.10957015]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.925699822361819}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5929085994978683
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40213628,  15.70959386,   0.84110169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8156206411453344}
episode index:2900
target Thresh 72.23641174814175
target distance 53.0
model initialize at round 2900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.1783208 ,  8.79872766]), 'previousTarget': array([81.71773129,  9.34829399]), 'currentState': array([63.54760091,  4.97317718,  5.02915791]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.4487717475193364
running average episode reward sum: 0.592858914267955
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04150735,  15.58271446,   0.42236296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1217238075593026}
episode index:2901
target Thresh 72.24017345522658
target distance 11.0
model initialize at round 2901
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.79801092,  16.58271384]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,   6.       ,   3.4128973], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.848876739308615}
done in step count: 19
reward sum = 0.7561686238355867
running average episode reward sum: 0.5929151891506453
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94155619,  15.2007592 ,   4.66791376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2090931231050451}
episode index:2902
target Thresh 72.24393140248456
target distance 2.0
model initialize at round 2902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40145501,  14.81106952,   3.79065611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44369004095571984}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5930554181588608
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40145501,  14.81106952,   3.79065611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44369004095571984}
episode index:2903
target Thresh 72.24768559367364
target distance 69.0
model initialize at round 2903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.68644497, 21.11114628]), 'previousTarget': array([65.86691472, 20.6965896 ]), 'currentState': array([4.78512162e+01, 2.36731179e+01, 4.40548658e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = -0.1590723424736501
running average episode reward sum: 0.5927964209961085
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14069376e+02, 1.57070911e+01, 5.93832433e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1687769531835355}
episode index:2904
target Thresh 72.25143603254799
target distance 7.0
model initialize at round 2904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.29808178,  16.19995165,   6.17081851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.826813478692377}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5929132089227559
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65582494,  14.75793364,   3.63509833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6990725777573802}
episode index:2905
target Thresh 72.25518272285807
target distance 20.0
model initialize at round 2905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.27902412,  13.78527604]), 'previousTarget': array([113.56953382,  14.42781353]), 'currentState': array([94.01627294,  5.63224474,  3.10418975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.592961175254735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40638653,  15.6039028 ,   5.47744483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8468031325145886}
episode index:2906
target Thresh 72.25892566835056
target distance 6.0
model initialize at round 2906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.13973304,  22.49703085,   3.02691746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.7964048746317}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5930876406261644
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75876705,  15.78609179,   4.81102124]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8222734562437168}
episode index:2907
target Thresh 72.2626648727684
target distance 54.0
model initialize at round 2907
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.73760828, 10.22905861]), 'previousTarget': array([80.78406925,  9.93097322]), 'currentState': array([61.       ,  7.       ,  2.8879235], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.4503405226503064
running average episode reward sum: 0.5930385528964616
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92413553,  15.68589893,   5.27294399]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.15086220241138}
episode index:2908
target Thresh 72.26640033985079
target distance 15.0
model initialize at round 2908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.1038287 ,   7.15690496,   0.35773056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.956745181534474}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5931075029585954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14315831e+02, 1.57656808e+01, 4.97740944e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0268173400725773}
episode index:2909
target Thresh 72.27013207333322
target distance 38.0
model initialize at round 2909
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.29130406, 11.27689184]), 'previousTarget': array([96.46160575, 10.60932768]), 'currentState': array([77.       ,  6.       ,  3.3445399], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.19073091540760256
running average episode reward sum: 0.592969229217169
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29899506,  15.98328346,   0.33055788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2075820030647868}
episode index:2910
target Thresh 72.27386007694741
target distance 5.0
model initialize at round 2910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11091724e+02, 1.66818394e+01, 1.21634802e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.25478573068681}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5930580263803011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04783938,  15.80035327,   0.42110912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8017817457009282}
episode index:2911
target Thresh 72.27758435442136
target distance 57.0
model initialize at round 2911
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.61578032,  5.90143081]), 'previousTarget': array([77.49929107,  6.44720674]), 'currentState': array([58.       ,  2.       ,  3.0058534], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = -0.2963830069935651
running average episode reward sum: 0.5927525864649942
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27636391,  15.50887913,   5.57821982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8846508675531637}
episode index:2912
target Thresh 72.28130490947937
target distance 23.0
model initialize at round 2912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.57538973,  14.4627268 ]), 'previousTarget': array([111.92481176,  14.73259233]), 'currentState': array([90.72122741, 12.05186829,  2.77231467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5928161184845391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87371189,  15.06114316,   5.70452419]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.14031098370811987}
episode index:2913
target Thresh 72.28502174584197
target distance 20.0
model initialize at round 2913
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.46469134,  16.59628013]), 'previousTarget': array([114.77872706,  14.96680906]), 'currentState': array([95.       , 12.       ,  6.0214696], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.29777795149762387
running average episode reward sum: 0.592714869971503
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06138291,  15.79264313,   5.11462409]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.228529685077192}
episode index:2914
target Thresh 72.28873486722603
target distance 59.0
model initialize at round 2914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.21668906, 19.428204  ]), 'previousTarget': array([75.9285661 , 18.31113847]), 'currentState': array([55.33944446, 21.64070195,  1.09685516]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 76
reward sum = 0.32935284516979335
running average episode reward sum: 0.5926245227931833
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00703343,  15.84430399,   0.62543965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3033924316337737}
episode index:2915
target Thresh 72.29244427734461
target distance 45.0
model initialize at round 2915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.19082348, 14.62702564]), 'previousTarget': array([89.98027613, 13.88801227]), 'currentState': array([70.19308323, 14.3263851 ,  0.41624069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5926530222877879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65513901,  15.70069904,   4.6667263 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7809662279229854}
episode index:2916
target Thresh 72.29614997990718
target distance 66.0
model initialize at round 2916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.6061539 , 13.56282053]), 'previousTarget': array([68.9793708 , 12.90815322]), 'currentState': array([47.61534312, 12.9566156 ,  1.52702165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.19615229901650114
running average episode reward sum: 0.5925170947172459
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08045596,  15.04054643,   5.91101221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9204375388243865}
episode index:2917
target Thresh 72.29985197861942
target distance 23.0
model initialize at round 2917
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.52155773,  14.65026138]), 'previousTarget': array([111.98112317,  14.86874449]), 'currentState': array([90.58226662, 13.09312617,  2.70066786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5925675344974621
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47425839,  15.31724344,   0.53744569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6140420487214128}
episode index:2918
target Thresh 72.30355027718332
target distance 7.0
model initialize at round 2918
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.26953429,   9.69947118,   2.35853052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.962470021488752}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5926743568820154
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06032016,  14.77959028,   4.91522677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9651832192350072}
episode index:2919
target Thresh 72.3072448792972
target distance 13.0
model initialize at round 2919
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.03641057,  13.25087522]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.       ,   5.       ,   0.9730564], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.592879130015257}
done in step count: 9
reward sum = 0.7742172474836408
running average episode reward sum: 0.5927365291048242
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04638831,  15.10200219,   1.84442723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9590514646208503}
episode index:2920
target Thresh 72.31093578865566
target distance 63.0
model initialize at round 2920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.31356708, 12.05614054]), 'previousTarget': array([71.90990945, 10.89618185]), 'currentState': array([52.36095947, 10.68011381,  2.30818823]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.592724728217951
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95851097,  15.01872579,   5.96059847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.04551916930006008}
episode index:2921
target Thresh 72.31462300894958
target distance 49.0
model initialize at round 2921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.04132537, 15.4751934 ]), 'previousTarget': array([86., 15.]), 'currentState': array([67.04421348, 15.81506987,  1.67277354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.23327235104009658
running average episode reward sum: 0.5926017123462269
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20438446,  15.22901076,   0.78168329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8279190866209161}
episode index:2922
target Thresh 72.31830654386621
target distance 20.0
model initialize at round 2922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.79778545,  15.55648026]), 'previousTarget': array([114.1565257 ,  15.25304229]), 'currentState': array([93.40727618, 20.45629151,  2.46055889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.592625550980725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32478096,  14.92405684,   0.63520024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3335416521369533}
episode index:2923
target Thresh 72.32198639708908
target distance 8.0
model initialize at round 2923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.74720465,  14.33612422,   5.57161045]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.279454216898921}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5927352950629763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26988186,  15.76887464,   1.41609895]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8148646717161219}
episode index:2924
target Thresh 72.32566257229804
target distance 22.0
model initialize at round 2924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.08566088,  15.71395159]), 'previousTarget': array([112.6773982 ,  15.42229124]), 'currentState': array([91.41025938, 19.30262486,  1.94348216]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5928208362247954
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14751053,  14.89370033,   4.81512304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.859091335318222}
episode index:2925
target Thresh 72.32933507316926
target distance 58.0
model initialize at round 2925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.27488047,  6.9380588 ]), 'previousTarget': array([76.64973049,  7.72667302]), 'currentState': array([55.67443909,  2.96028333,  4.77737308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5927856583152874
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94906838,  15.32439185,   1.35129572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.328365800897696}
episode index:2926
target Thresh 72.33300390337524
target distance 25.0
model initialize at round 2926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.03227951,  16.15783216]), 'previousTarget': array([109.74881264,  15.84018998]), 'currentState': array([90.55432571, 20.69758057,  0.43191123]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.592871123820948
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89966283,  14.51741328,   0.48835041]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4929071801437864}
episode index:2927
target Thresh 72.33666906658485
target distance 27.0
model initialize at round 2927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.68416157,  15.53119401]), 'previousTarget': array([107.98629668,  15.25976679]), 'currentState': array([88.75452512, 17.20737741,  1.99379159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5929187441234867
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95086604,  14.70860463,   1.75119609]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.994513694830096}
episode index:2928
target Thresh 72.3403305664632
target distance 3.0
model initialize at round 2928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.40595058,  17.30002099,   3.00602508]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.6956990885600414}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5930129151950625
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45231741,  15.09538838,   5.14698788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5559272979536294}
episode index:2929
target Thresh 72.3439884066718
target distance 23.0
model initialize at round 2929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.76006659,  12.63016765]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([93.30199972,  2.87230411,  0.13999647]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5930896708341076
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05330668,  14.96955416,   5.22450104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9471827619568965}
episode index:2930
target Thresh 72.34764259086852
target distance 23.0
model initialize at round 2930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.50218857,  12.80778122]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([90.92461901,  5.40008863,  1.36282206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5931249228120044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82361543,  15.06149799,   2.02717444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8259082123473708}
episode index:2931
target Thresh 72.35129312270752
target distance 71.0
model initialize at round 2931
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([63.99836345, 15.74414942]), 'previousTarget': array([63.99801656, 15.71833779]), 'currentState': array([44.      , 16.      ,  4.096151], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3687245183324737
running average episode reward sum: 0.5930483878855107
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30640296,  15.71514608,   0.95061469]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9962483482168236}
episode index:2932
target Thresh 72.35494000583934
target distance 20.0
model initialize at round 2932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.86093021,  15.43230866]), 'previousTarget': array([114.1565257 ,  15.25304229]), 'currentState': array([95.16232841, 22.52894804,  0.45502257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5931423863256347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10010654,  14.58427493,   0.56550694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9912797668210126}
episode index:2933
target Thresh 72.35858324391086
target distance 3.0
model initialize at round 2933
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.80545023,  15.8934317 ,   3.12563169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.0144157325990024}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5932709332287275
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69608987,  14.08693645,   6.00888932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9623130565312812}
episode index:2934
target Thresh 72.36222284056532
target distance 70.0
model initialize at round 2934
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.80067124,  7.60084512]), 'previousTarget': array([64.7124451,  6.3792763]), 'currentState': array([45.01444849,  4.68444503,  0.55479896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.37546316553068065
running average episode reward sum: 0.5931967227456958
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0882764 ,  14.91547864,   0.9915923 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.12221531595665337}
episode index:2935
target Thresh 72.36585879944232
target distance 4.0
model initialize at round 2935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.17437590e+02, 1.00703445e+01, 4.65720495e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.499395169924745}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.593321858742717
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06801892,  14.14053967,   3.44906741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2677778946103246}
episode index:2936
target Thresh 72.36949112417781
target distance 42.0
model initialize at round 2936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.93234623, 10.9744075 ]), 'previousTarget': array([92.55604828, 10.19058177]), 'currentState': array([72.23010799,  7.53612187,  2.98475814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5933342869934697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84588007,  14.72797755,   0.61509302]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31264862787759223}
episode index:2937
target Thresh 72.37311981840413
target distance 12.0
model initialize at round 2937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.5010777 ,   2.23474186,   2.60363579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.007552726540458}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5934340319167926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60410658,  15.39791198,   1.40632533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.561306998318402}
episode index:2938
target Thresh 72.37674488574997
target distance 1.0
model initialize at round 2938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38073171,  14.22938009,   3.75348675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9886092546877534}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5935723667136905
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38073171,  14.22938009,   3.75348675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9886092546877534}
episode index:2939
target Thresh 72.3803663298404
target distance 10.0
model initialize at round 2939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28276373,   4.94680542,   2.22285283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.07874739795581}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.593657186722762
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2761474 ,  15.54105748,   3.8087878 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.903717757282456}
episode index:2940
target Thresh 72.38398415429684
target distance 7.0
model initialize at round 2940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.63416848,   8.98330683,   1.5076139 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.169772475502882}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5937537062182657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4920926 ,  15.33672464,   1.74331528]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6093877377368917}
episode index:2941
target Thresh 72.38759836273714
target distance 30.0
model initialize at round 2941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.04212491,  12.46239656]), 'previousTarget': array([104.32469879,  12.15325301]), 'currentState': array([86.79933058,  7.01126216,  5.49355722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.38866956040768663
running average episode reward sum: 0.5936839971272355
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38616155,  15.01394667,   1.547839  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6139968697456714}
episode index:2942
target Thresh 72.3912089587755
target distance 65.0
model initialize at round 2942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.01004121, 21.01918007]), 'previousTarget': array([69.85022022, 20.55689597]), 'currentState': array([51.19467914, 23.73053139,  5.89087242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5936663284486218
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07347301,  15.57971357,   0.234041  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0929410211550725}
episode index:2943
target Thresh 72.3948159460225
target distance 62.0
model initialize at round 2943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.27618844, 20.12157833]), 'previousTarget': array([72.87373448, 19.75619127]), 'currentState': array([54.43250177, 22.61719433,  6.18515945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5936620825538084
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05498246,  15.24440631,   0.23864335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2505145028922285}
episode index:2944
target Thresh 72.39841932808517
target distance 7.0
model initialize at round 2944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43398421,  21.3835727 ,   4.52659369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.408617189924052}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5937866781149106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7801226 ,  15.44583424,   3.66517344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8985318307146399}
episode index:2945
target Thresh 72.40201910856686
target distance 3.0
model initialize at round 2945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.3867778 ,  15.88969144,   0.16643685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8422911676213838}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5939111890897528
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14419584e+02, 1.56648384e+01, 5.70838968e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.88254929131486}
episode index:2946
target Thresh 72.40561529106738
target distance 17.0
model initialize at round 2946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.70181848,  7.34365587,  5.5442729 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.91683613052652}
done in step count: 31
reward sum = 0.671491162760569
running average episode reward sum: 0.5939375141571673
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11405282,  14.47040948,   6.18442732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.03216690789764}
episode index:2947
target Thresh 72.40920787918286
target distance 10.0
model initialize at round 2947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.46007361,   5.07724513,   0.91336315]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.535280873831503}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5940191207539424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03707055,  15.09904958,   0.46713983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10575937083896132}
episode index:2948
target Thresh 72.41279687650594
target distance 25.0
model initialize at round 2948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.05681909,  14.93116532]), 'previousTarget': array([109.93630557,  14.59490445]), 'currentState': array([90.05875792, 14.65268872,  2.40396464]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5940895213876436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51769878,  15.24905586,   2.18957846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5428105445507995}
episode index:2949
target Thresh 72.41638228662558
target distance 24.0
model initialize at round 2949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.4708919 ,  11.65002779]), 'previousTarget': array([108.58583933,  11.52566297]), 'currentState': array([92.36556086,  1.28626158,  0.41925162]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5941290006099774
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29141262,  14.71684514,   5.46986385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7630679858276503}
episode index:2950
target Thresh 72.41996411312722
target distance 43.0
model initialize at round 2950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.34974633, 14.75764935]), 'previousTarget': array([91.97840172, 13.92922799]), 'currentState': array([71.35079631, 14.55271461,  2.97681564]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5941520955067494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09641939,  15.98912931,   0.21874279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3397144092563726}
episode index:2951
target Thresh 72.42354235959269
target distance 8.0
model initialize at round 2951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.54528125,   7.22542992,   1.97785091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.909497235939054}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5942278932174847
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99056767,  14.6674744 ,   1.1098093 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3326593530917294}
episode index:2952
target Thresh 72.4271170296002
target distance 63.0
model initialize at round 2952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.00050242,  6.2381171 ]), 'previousTarget': array([71.58733278,  6.04183057]), 'currentState': array([50.36917453,  2.41568784,  1.88201308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.2793830336221671
running average episode reward sum: 0.5941212745721764
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67009101,  15.37574465,   3.85907229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5000239805972873}
episode index:2953
target Thresh 72.43068812672445
target distance 10.0
model initialize at round 2953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.00229667,   9.50861484,   0.55927396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.406549997246065}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5942202128244255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44372172,  15.60852275,   4.03324253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.824466774224019}
episode index:2954
target Thresh 72.43425565453651
target distance 23.0
model initialize at round 2954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.34296798,  15.55975965]), 'previousTarget': array([111.92481176,  15.26740767]), 'currentState': array([91.57321564, 18.5857954 ,  2.84369665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5943043830378127
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79990043,  14.04038875,   5.23036693]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9802517962012376}
episode index:2955
target Thresh 72.43781961660395
target distance 62.0
model initialize at round 2955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.72440524, 11.44726529]), 'previousTarget': array([72.90700027, 10.9264839 ]), 'currentState': array([51.7914633 ,  9.81085902,  1.62933397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5942865821896832
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32269463,  14.58336715,   0.45601286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7951889706646428}
episode index:2956
target Thresh 72.4413800164907
target distance 23.0
model initialize at round 2956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.66540597,  14.49486807]), 'previousTarget': array([111.7042351,  14.4268235]), 'currentState': array([90.7998423 , 12.17983455,  3.37473345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5943539909490522
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0967259 ,  14.80613112,   2.060386  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21665881693564937}
episode index:2957
target Thresh 72.44493685775717
target distance 9.0
model initialize at round 2957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.57277279,   7.62784059,   2.83745813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.12985981585496}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5944557435735652
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17075947,  14.89340142,   1.13308209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20130090663645817}
episode index:2958
target Thresh 72.44849014396021
target distance 15.0
model initialize at round 2958
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.70339749,  16.64631689]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.      ,  12.      ,   3.900638], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.420057017646956}
done in step count: 24
reward sum = 0.5118879073142187
running average episode reward sum: 0.5944278396072729
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88349268,  15.97850059,   1.80042508]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9854122800958489}
episode index:2959
target Thresh 72.4520398786531
target distance 30.0
model initialize at round 2959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.93625521,  12.62352653]), 'previousTarget': array([104.32469879,  12.15325301]), 'currentState': array([86.59019022,  7.55107455,  1.34359473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5944744191782346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42232978,  14.87864442,   1.97943994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5902796410387161}
episode index:2960
target Thresh 72.45558606538556
target distance 40.0
model initialize at round 2960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.01035465, 18.88734899]), 'previousTarget': array([94.77872706, 18.03319094]), 'currentState': array([74.3447779 , 22.52947627,  1.01320243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.4545932513855644
running average episode reward sum: 0.5944271779868153
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2139148 ,  15.23012162,   2.72473648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31419022671731434}
episode index:2961
target Thresh 72.45912870770378
target distance 39.0
model initialize at round 2961
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.32334454,  8.15832879]), 'previousTarget': array([95.11558017,  8.88171698]), 'currentState': array([76.       ,  3.       ,  3.2437487], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.23463947483116498
running average episode reward sum: 0.5943057101599566
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29551401,  15.32658616,   1.55475432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4404396094094596}
episode index:2962
target Thresh 72.46266780915043
target distance 44.0
model initialize at round 2962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.57979124, 11.47759877]), 'previousTarget': array([90.6773982 , 10.57770876]), 'currentState': array([70.78465536,  8.62232379,  2.70856035]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5943425451194029
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53491146,  14.67675758,   0.91967262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6249927427478118}
episode index:2963
target Thresh 72.46620337326459
target distance 8.0
model initialize at round 2963
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.65423389,   8.52086067,   2.17455189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.0017286474159635}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5944321928284183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23390404,  15.54626894,   0.25931972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9409106122332412}
episode index:2964
target Thresh 72.46973540358184
target distance 4.0
model initialize at round 2964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.36774778,  18.58995028,   4.39771652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.8416763284523596}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5945622662878354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22844276,  15.89442386,   5.27178335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.181225893164515}
episode index:2965
target Thresh 72.47326390363419
target distance 41.0
model initialize at round 2965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.19831178, 11.51120239]), 'previousTarget': array([93.85291755, 12.42108751]), 'currentState': array([73.44957369,  8.35092648,  3.47714961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.5945538505549255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8128941 ,  15.43318067,   1.956413  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47186238266826075}
episode index:2966
target Thresh 72.47678887695015
target distance 53.0
model initialize at round 2966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.38923098,  9.93147127]), 'previousTarget': array([81.77598173,  9.98505385]), 'currentState': array([63.64147256,  6.76507943,  0.86784952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5945700479770791
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75417353,  14.34444518,   0.88107605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7001305416819729}
episode index:2967
target Thresh 72.4803103270547
target distance 40.0
model initialize at round 2967
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.58112838, 17.92821764]), 'previousTarget': array([94.70060934, 18.55239336]), 'currentState': array([75.       , 22.       ,  6.1146555], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.31489847328318665
running average episode reward sum: 0.5944758190098642
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09128224,  15.05828188,   5.93128651]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9105848350139986}
episode index:2968
target Thresh 72.48382825746927
target distance 67.0
model initialize at round 2968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.94884215, 21.2197577 ]), 'previousTarget': array([67.85893586, 20.62878378]), 'currentState': array([48.12133042, 23.84078393,  0.52643287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5944544076673063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02047348,  15.70202837,   5.01672595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2051207574758325}
episode index:2969
target Thresh 72.48734267171182
target distance 27.0
model initialize at round 2969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.44051246,  17.23880042]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([87.09142343, 22.29970486,  1.17089367]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5945135284545581
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27091033,  15.13133696,   1.02993788]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7408246416858244}
episode index:2970
target Thresh 72.49085357329673
target distance 36.0
model initialize at round 2970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.92430745,  9.03496067]), 'previousTarget': array([97.97366596,  9.32455532]), 'currentState': array([80.17354453,  2.07731092,  4.71925928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.47058185305812095
running average episode reward sum: 0.594471814662772
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58809329,  15.35745077,   2.16981448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5453789447586088}
episode index:2971
target Thresh 72.49436096573494
target distance 28.0
model initialize at round 2971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.59916632,  13.09941256]), 'previousTarget': array([106.23047895,  12.49442256]), 'currentState': array([88.22774039,  8.12469073,  6.01480586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2843841687194819
running average episode reward sum: 0.5943674783081477
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26797063,  14.52927686,   1.95883188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8703144650948589}
episode index:2972
target Thresh 72.49786485253381
target distance 25.0
model initialize at round 2972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.56150698,  13.77072749]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([88.91635992, 10.01996794,  1.46140528]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7182193154225054
running average episode reward sum: 0.5944091371837327
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27189233,  14.71634593,   1.68378167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7814092483078517}
episode index:2973
target Thresh 72.50136523719723
target distance 45.0
model initialize at round 2973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.61255135, 15.84617923]), 'previousTarget': array([90., 15.]), 'currentState': array([70.62457953, 16.53970837,  0.17543054]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.594436480799024
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18833539,  14.08139807,   5.10650388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.225817658958617}
episode index:2974
target Thresh 72.50486212322562
target distance 71.0
model initialize at round 2974
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([63.99152043, 14.58233246]), 'previousTarget': array([63.99801656, 14.28166221]), 'currentState': array([44.     , 14.     ,  3.70838], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.32299854295511315
running average episode reward sum: 0.5943452411560513
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54009846,  14.47151329,   4.31992961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7005766392356699}
episode index:2975
target Thresh 72.50835551411583
target distance 26.0
model initialize at round 2975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.66097209,  15.54095983]), 'previousTarget': array([108.98522349,  15.23133756]), 'currentState': array([89.76284939, 17.55707311,  0.19379866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6235711023920733
running average episode reward sum: 0.5943550616739397
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77170056,  14.55625949,   1.68686156]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49902532218410567}
episode index:2976
target Thresh 72.51184541336127
target distance 63.0
model initialize at round 2976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.08217633,  6.58674295]), 'previousTarget': array([71.58733278,  6.04183057]), 'currentState': array([53.4732385 ,  2.65106547,  1.27485817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 67
reward sum = 0.396039801313341
running average episode reward sum: 0.5942884458659583
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23317299,  15.34702318,   2.8136641 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8416939769210341}
episode index:2977
target Thresh 72.51533182445183
target distance 5.0
model initialize at round 2977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.99063115,  15.21539312,   6.16011864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.015150407430991}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5944114504207381
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32749615,  14.85278034,   5.4370659 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6884294117016727}
episode index:2978
target Thresh 72.51881475087393
target distance 5.0
model initialize at round 2978
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75056349,  20.33262583,   5.63995761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.385187439329965}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5944704070153619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21892088,  15.81581997,   1.23552479]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1294453537875426}
episode index:2979
target Thresh 72.52229419611048
target distance 22.0
model initialize at round 2979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.99508876,  15.78021135]), 'previousTarget': array([112.50265712,  15.56757793]), 'currentState': array([91.36414015, 19.604588  ,  2.36875868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5945509584765817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35761715,  14.61555101,   0.42300838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7486365959039153}
episode index:2980
target Thresh 72.52577016364094
target distance 67.0
model initialize at round 2980
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.5575854 ,  9.47828443]), 'previousTarget': array([67.85893586,  9.37121622]), 'currentState': array([49.7036175 ,  7.0658254 ,  5.34911293]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.2236731492170703
running average episode reward sum: 0.5944265445855185
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74775105,  15.30948517,   4.87562299]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.39926257500253215}
episode index:2981
target Thresh 72.52924265694128
target distance 38.0
model initialize at round 2981
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.68015223,  8.92927989]), 'previousTarget': array([96.21128529,  9.56116153]), 'currentState': array([75.51708915,  3.20415863,  4.64414811]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 52
reward sum = 0.5345504830999933
running average episode reward sum: 0.5944064654233838
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99957901,  14.84542598,   4.1277314 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.154574588633763}
episode index:2982
target Thresh 72.53271167948398
target distance 42.0
model initialize at round 2982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.08478342,  8.85507294]), 'previousTarget': array([92.10571781,  7.91367456]), 'currentState': array([72.76728167,  3.67490635,  2.70532447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.34080842470925826
running average episode reward sum: 0.5943214509947166
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42776538,  15.24406622,   2.43217475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4924952219954206}
episode index:2983
target Thresh 72.53617723473808
target distance 58.0
model initialize at round 2983
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.90764179, 12.91984328]), 'previousTarget': array([76.95260657, 12.37604183]), 'currentState': array([57.       , 11.       ,  5.9844475], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.16544916222169836
running average episode reward sum: 0.5941777270373529
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76441192,  15.84463194,   5.1538288 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1391789566901245}
episode index:2984
target Thresh 72.53963932616911
target distance 4.0
model initialize at round 2984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.09802447,  12.68011288,   0.50251842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9998978604884647}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.594200542552913
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14818948,  15.39891648,   4.42362734]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4255519735858054}
episode index:2985
target Thresh 72.54309795723918
target distance 38.0
model initialize at round 2985
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.24061491, 11.45882203]), 'previousTarget': array([96.46160575, 10.60932768]), 'currentState': array([77.       ,  6.       ,  3.9531405], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = -0.0983095716464954
running average episode reward sum: 0.593968623559544
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26484187,  15.14739913,   1.90960533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7497892894088766}
episode index:2986
target Thresh 72.54655313140694
target distance 16.0
model initialize at round 2986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.67857631,  20.92014375,   0.86494463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.496815108762712}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5940725450364269
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53322351,  14.56703261,   1.59538902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6366640043625621}
episode index:2987
target Thresh 72.55000485212751
target distance 66.0
model initialize at round 2987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.51053787,  9.32904759]), 'previousTarget': array([68.85467564,  9.40662735]), 'currentState': array([50.67106351,  6.800163  ,  0.89097851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 75
reward sum = 0.14010229802230784
running average episode reward sum: 0.5939206138961947
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28370801,  15.95736864,   3.41371439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1956709084075594}
episode index:2988
target Thresh 72.55345312285267
target distance 47.0
model initialize at round 2988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.64075846, 15.85012909]), 'previousTarget': array([87.9954746 , 15.57456437]), 'currentState': array([69.65198721, 16.52022152,  5.66331929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.4595841175687474
running average episode reward sum: 0.5938756702707924
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03488292,  14.82277002,   1.19960008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9812550346541448}
episode index:2989
target Thresh 72.55689794703066
target distance 50.0
model initialize at round 2989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.19657684, 16.91062103]), 'previousTarget': array([84.96409691, 16.80215419]), 'currentState': array([63.23257065, 18.10997777,  2.22692513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5938582152225301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63288157,  15.44596404,   3.53187131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5776329834633624}
episode index:2990
target Thresh 72.56033932810632
target distance 48.0
model initialize at round 2990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.55522485, 17.48921825]), 'previousTarget': array([86.93091516, 17.3390904 ]), 'currentState': array([68.64324293, 19.36351169,  5.49089522]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.5938599195994086
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2630917 ,  14.35310081,   3.5657675 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6983522093971585}
episode index:2991
target Thresh 72.56377726952101
target distance 62.0
model initialize at round 2991
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([72.90103391, 21.01282885]), 'previousTarget': array([72.83555733, 20.44057325]), 'currentState': array([53.      , 23.      ,  4.426556], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5598236312032323
running average episode reward sum: 0.5938485438345703
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17290226,  14.38732439,   5.62684924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0293017410843548}
episode index:2992
target Thresh 72.56721177471267
target distance 65.0
model initialize at round 2992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.54884127,  8.85369683]), 'previousTarget': array([69.81099734,  8.74306117]), 'currentState': array([51.74597771,  6.05251944,  0.77506214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 78
reward sum = 0.19768846550641128
running average episode reward sum: 0.5937161816299836
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02286783,  14.76215133,   0.70343463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.005663598096979}
episode index:2993
target Thresh 72.57064284711583
target distance 52.0
model initialize at round 2993
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([82.52653065,  6.32603754]), 'previousTarget': array([82.40285  ,  6.8507125]), 'currentState': array([63.      ,  2.      ,  3.315954], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3850594464014994
running average episode reward sum: 0.5936464900016508
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21606902,  15.63395781,   5.74573294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0081915920418618}
episode index:2994
target Thresh 72.57407049016157
target distance 63.0
model initialize at round 2994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.35941152, 19.97303038]), 'previousTarget': array([71.84067458, 20.48054926]), 'currentState': array([50.48237125, 22.18736331,  4.57682967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.5936255414383632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0061602 ,  15.98814963,   6.16803217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.988168834189884}
episode index:2995
target Thresh 72.5774947072775
target distance 5.0
model initialize at round 2995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.84275464,  21.48833771,   0.13205552]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.837560513309449}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5937173706344014
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04446392,  14.84422232,   2.39874842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1619991493903298}
episode index:2996
target Thresh 72.58091550188784
target distance 51.0
model initialize at round 2996
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.5866779 , 17.02753527]), 'previousTarget': array([83.96548746, 16.82555956]), 'currentState': array([65.6340262 , 18.40292242,  1.2517597 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2625087285249083
running average episode reward sum: 0.5936068572403042
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27909978,  14.48762552,   1.29360988]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5834589035895491}
episode index:2997
target Thresh 72.58433287741342
target distance 68.0
model initialize at round 2997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.67010789, 18.07212765]), 'previousTarget': array([66.922597  , 19.24212379]), 'currentState': array([47.71210678, 19.36757804,  5.95650816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 55
reward sum = 0.1913369600747371
running average episode reward sum: 0.5934726778216365
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13980704,  15.33617526,   6.00957822]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.923550608617543}
episode index:2998
target Thresh 72.58774683727158
target distance 67.0
model initialize at round 2998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.88857316, 18.08629753]), 'previousTarget': array([67.97998109, 17.10537398]), 'currentState': array([46.92959747, 19.36664487,  1.31747198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5209623292932194
running average episode reward sum: 0.5934484996460685
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31880752,  14.62530708,   3.57396789]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.777443231036222}
episode index:2999
target Thresh 72.5911573848763
target distance 57.0
model initialize at round 2999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.6417166 , 17.93622509]), 'previousTarget': array([77.97235659, 16.94882334]), 'currentState': array([57.70320574, 19.50331947,  0.75571561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.48338347716238506
running average episode reward sum: 0.5934118113052406
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43943501,  15.324167  ,   1.26734003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6475471799672992}
episode index:3000
target Thresh 72.5945645236381
target distance 27.0
model initialize at round 3000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.67009532,  16.74126429]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([89.38640851, 22.04592478,  0.4072625 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5934866180784134
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32101674,  14.52408893,   1.22492694]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5740584370688302}
episode index:3001
target Thresh 72.59796825696415
target distance 67.0
model initialize at round 3001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.69849553,  9.69711708]), 'previousTarget': array([67.92028312, 10.78390595]), 'currentState': array([48.82838887,  7.42140594,  6.02494836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5934748857401287
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44988993,  14.65525733,   2.31702637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.649206126337574}
episode index:3002
target Thresh 72.60136858825818
target distance 25.0
model initialize at round 3002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.68759113,  15.48696566]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([90.81389816, 17.731142  ,  0.94122034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6514350782916389
running average episode reward sum: 0.5934941865035491
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18478873,  14.64864165,   1.70627445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8877060914015196}
episode index:3003
target Thresh 72.60476552092052
target distance 56.0
model initialize at round 3003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.90125468,  8.38079169]), 'previousTarget': array([78.68855151,  8.51581277]), 'currentState': array([57.19643836,  4.95731192,  3.95146954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.5934596978505866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57990747,  14.87756908,   0.3545874 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43756949204526}
episode index:3004
target Thresh 72.6081590583481
target distance 7.0
model initialize at round 3004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.60387287,   9.70017669,   0.30985968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.563485647772959}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5935631661957308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01472012,  14.33389466,   4.53283625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.666267965766038}
episode index:3005
target Thresh 72.61154920393444
target distance 42.0
model initialize at round 3005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.61053271,  8.71693718]), 'previousTarget': array([92.10571781,  7.91367456]), 'currentState': array([72.29529727,  3.52832658,  0.99283791]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5935590421930437
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42173217,  14.51047876,   1.84645456]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.757644197945837}
episode index:3006
target Thresh 72.6149359610697
target distance 38.0
model initialize at round 3006
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.79291928, 10.87060035]), 'previousTarget': array([96.66906377, 11.62324859]), 'currentState': array([77.       ,  8.       ,  1.3232739], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4939051904523875
running average episode reward sum: 0.5935259015705825
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16172516,  15.13152844,   5.65603404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8485307508237308}
episode index:3007
target Thresh 72.61831933314065
target distance 11.0
model initialize at round 3007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.46812172,   3.29214345,   2.5720973 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.406690078268188}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5936322816722823
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05189321,  14.5700965 ,   6.06977682]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0410204153628633}
episode index:3008
target Thresh 72.62169932353065
target distance 63.0
model initialize at round 3008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.35302668, 17.4417159 ]), 'previousTarget': array([71.95980905, 17.73271054]), 'currentState': array([50.38286905, 18.53387171,  2.41222954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4036841382770392
running average episode reward sum: 0.5935691550044873
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43736988,  14.58416578,   2.37915767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6996218617588427}
episode index:3009
target Thresh 72.62507593561969
target distance 21.0
model initialize at round 3009
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.73369411,  15.20782601]), 'previousTarget': array([113.79898987,  15.17157288]), 'currentState': array([92.81726098, 17.03421356,  2.88241935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5936356151801149
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.87166614,  14.67878865,   4.4398461 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.928966411684371}
episode index:3010
target Thresh 72.62844917278439
target distance 8.0
model initialize at round 3010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.21575375,   9.83624276,   0.24645012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.525865705521745}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5937511397680328
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05868559,  14.94855065,   5.87170176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9427193946470259}
episode index:3011
target Thresh 72.63181903839798
target distance 46.0
model initialize at round 3011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.12167042, 18.74343314]), 'previousTarget': array([88.88288926, 17.83881638]), 'currentState': array([68.3128639 , 21.50227061,  0.99596918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5937995956888895
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73899724,  14.82359172,   0.13597372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3150274911969577}
episode index:3012
target Thresh 72.63518553583032
target distance 20.0
model initialize at round 3012
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.66554244,  13.24855305]), 'previousTarget': array([114.9007438 ,  14.99007438]), 'currentState': array([95.        , 13.        ,  0.56990546], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.6671131057819}
done in step count: 23
reward sum = 0.7236142836436554
running average episode reward sum: 0.5938426805504743
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13418015,  15.40651027,   5.65537765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9565012318550746}
episode index:3013
target Thresh 72.63854866844792
target distance 64.0
model initialize at round 3013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.21856599,  9.18984063]), 'previousTarget': array([70.76024068,  8.08753761]), 'currentState': array([50.38480727,  6.61651305,  2.86359572]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.5938271908231486
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80815163,  15.85641588,   5.88852212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8776411292257484}
episode index:3014
target Thresh 72.64190843961391
target distance 64.0
model initialize at round 3014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.65450066,  8.20311045]), 'previousTarget': array([70.71097782,  7.38782431]), 'currentState': array([49.87545791,  5.23840677,  1.30794144]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.5938229915605598
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24443214,  14.28900324,   0.78536638]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0374965974517054}
episode index:3015
target Thresh 72.64526485268806
target distance 49.0
model initialize at round 3015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.88658409, 16.3943533 ]), 'previousTarget': array([85.99583637, 15.59192171]), 'currentState': array([64.90798972, 17.31942999,  1.30307019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5179147003445472
running average episode reward sum: 0.5937978230289895
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79887499,  14.26992925,   1.75101775]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.082222046933751}
episode index:3016
target Thresh 72.64861791102678
target distance 34.0
model initialize at round 3016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.05138292,  8.92434725]), 'previousTarget': array([99.6810367 ,  9.14274933]), 'currentState': array([79.22450077,  2.17538374,  2.21659052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5937860458713888
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14167942e+02, 1.57548713e+01, 7.57352635e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1234551510272963}
episode index:3017
target Thresh 72.65196761798313
target distance 33.0
model initialize at round 3017
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.2344532 ,  11.48049361]), 'previousTarget': array([101.29527642,  11.26234812]), 'currentState': array([82.       ,  6.       ,  0.9927945], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.5580332322654911
running average episode reward sum: 0.5937741993460057
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52984332,  15.92694871,   5.28016279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0393657760897517}
episode index:3018
target Thresh 72.65531397690683
target distance 13.0
model initialize at round 3018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.63124765,   1.66899338,   5.07298726]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.29384110854944}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.593871122390845
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11577159,  14.84133341,   0.76022813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8983512529196092}
episode index:3019
target Thresh 72.65865699114421
target distance 27.0
model initialize at round 3019
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.48654901,  15.19909132]), 'previousTarget': array([108.,  15.]), 'currentState': array([89.49957567, 15.9208231 ,  1.42443853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5939480420933102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14378043e+02, 1.52496841e+01, 2.81981190e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6702032860045709}
episode index:3020
target Thresh 72.66199666403831
target distance 13.0
model initialize at round 3020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54270131,   2.67491214,   1.34385234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.33703025416765}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5940538246836413
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06680674,  15.49952708,   6.09784481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0584786053503585}
episode index:3021
target Thresh 72.6653329989288
target distance 26.0
model initialize at round 3021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.9333611 ,  16.82652363]), 'previousTarget': array([108.31231517,  16.80053053]), 'currentState': array([87.42715492, 21.2433008 ,  4.49047029]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5941306330222091
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25784144,  14.46550008,   5.98019998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9145979924898137}
episode index:3022
target Thresh 72.66866599915201
target distance 66.0
model initialize at round 3022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.75309858,  7.35894322]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([50.0323268 ,  4.02860385,  1.75656312]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.4631577231496877
running average episode reward sum: 0.5940873075475572
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71431226,  14.32120164,   0.9908598 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9853980004057935}
episode index:3023
target Thresh 72.67199566804095
target distance 61.0
model initialize at round 3023
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.99344327, 17.48791956]), 'previousTarget': array([73.97585674, 17.01758082]), 'currentState': array([54.      , 18.      ,  5.129514], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.46090554295511316
running average episode reward sum: 0.594043265958737
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38167136,  15.62167013,   4.53315473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.876814722441349}
episode index:3024
target Thresh 72.67532200892526
target distance 28.0
model initialize at round 3024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.33424181,  16.32637943]), 'previousTarget': array([106.68855151,  16.48418723]), 'currentState': array([88.71880521, 20.22953807,  0.56690889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5941283616629142
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.739009  ,  15.05881669,   5.77611667]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26753636341936365}
episode index:3025
target Thresh 72.67864502513132
target distance 10.0
model initialize at round 3025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.94236516,   6.57809522,   2.53390628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.95977739419815}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5942431507533762
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41471705,  14.66694174,   0.50314885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6734121610389348}
episode index:3026
target Thresh 72.68196471998212
target distance 27.0
model initialize at round 3026
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.92263471,  11.74444611]), 'previousTarget': array([106.97366596,  12.32455532]), 'currentState': array([88.37265194,  4.2679403 ,  5.77519691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.594288760340063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20307209,  14.63364526,   0.48298139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4188723796345933}
episode index:3027
target Thresh 72.68528109679737
target distance 15.0
model initialize at round 3027
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.74851086,  15.07085546]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  11.       ,   0.8897622], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.33852904244918}
done in step count: 16
reward sum = 0.6435507710948756
running average episode reward sum: 0.5943050291679212
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34696238,  15.68850489,   6.15753478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9489452659969024}
episode index:3028
target Thresh 72.68859415889342
target distance 20.0
model initialize at round 3028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.40285  ,  15.1492875]), 'currentState': array([96.62241354, 19.59240402,  0.63332206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.942699349026757}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5943956335864095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03452211,  15.21511268,   0.54830066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9891516706842999}
episode index:3029
target Thresh 72.69190390958337
target distance 55.0
model initialize at round 3029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([79.94630544,  7.51102807]), 'previousTarget': array([79.6773982 ,  8.57770876]), 'currentState': array([60.38768489,  3.33246824,  5.90114975]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.5943644261264467
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03612216,  14.02372823,   0.26348248]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3719209350281556}
episode index:3030
target Thresh 72.69521035217696
target distance 6.0
model initialize at round 3030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09736730e+02, 1.43746786e+01, 8.60427618e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.300286303734812}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5944820855206312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65577519,  15.33428003,   4.8079944 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4798269075637055}
episode index:3031
target Thresh 72.69851348998063
target distance 13.0
model initialize at round 3031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.44537016,   1.95438241,   2.1609273 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.600315062478934}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5945813124892123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15169989,  14.81908947,   5.93925161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8673763249906296}
episode index:3032
target Thresh 72.7018133262975
target distance 68.0
model initialize at round 3032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.36005312, 19.21437558]), 'previousTarget': array([66.89486598, 19.95199909]), 'currentState': array([48.44120541, 21.01423838,  0.29931873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5945992885247807
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17783272,  15.6143836 ,   5.66381992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0263655532956277}
episode index:3033
target Thresh 72.70510986442743
target distance 25.0
model initialize at round 3033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.02555406,  15.30135995]), 'previousTarget': array([110.,  15.]), 'currentState': array([89.05094905, 16.30890876,  1.24867094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7001882007868602
running average episode reward sum: 0.5946340904075303
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72821452,  14.42333294,   6.22413618]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.63750470305989}
episode index:3034
target Thresh 72.70840310766694
target distance 69.0
model initialize at round 3034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.01355   ,  8.70858139]), 'previousTarget': array([65.79321198,  7.86858145]), 'currentState': array([47.1832589 ,  6.10866699,  6.02604944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.31085987905350626
running average episode reward sum: 0.5945405898436575
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25723837,  14.4686343 ,   2.5556527 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5903567414003548}
episode index:3035
target Thresh 72.71169305930928
target distance 47.0
model initialize at round 3035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.92261287,  9.61695709]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([69.33557669,  5.57368942,  1.33627623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.45995320172221527
running average episode reward sum: 0.5944962593469113
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57619747,  15.16549771,   5.80232223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4549704114837855}
episode index:3036
target Thresh 72.71497972264443
target distance 37.0
model initialize at round 3036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.63437211,  8.34948173]), 'previousTarget': array([96.86920279,  8.6297199 ]), 'currentState': array([7.89571840e+01, 1.19667495e+00, 5.44549783e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5945185793276939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4009573 ,  15.20165694,   0.32283208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6320741071503314}
episode index:3037
target Thresh 72.718263100959
target distance 37.0
model initialize at round 3037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.43728152,  8.6142413 ]), 'previousTarget': array([97.02445638,  9.17009396]), 'currentState': array([76.4245795 ,  2.40802167,  4.51102853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5347347353342727
running average episode reward sum: 0.5944989006430351
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03333139,  15.9870859 ,   5.77034194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3815884942048575}
episode index:3038
target Thresh 72.72154319753642
target distance 58.0
model initialize at round 3038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.27352204,  7.2523469 ]), 'previousTarget': array([76.51579154,  6.37422914]), 'currentState': array([56.66213814,  3.32887626,  0.80976748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.3180372674013066
running average episode reward sum: 0.5944079293915571
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8044587 ,  15.09362288,   0.18580417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21679862818247048}
episode index:3039
target Thresh 72.72482001565675
target distance 8.0
model initialize at round 3039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.66054584,  22.81706723,   5.46650458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.159631480028299}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5945220978849812
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76187362,  15.80898245,   5.48627259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8433011136214666}
episode index:3040
target Thresh 72.72809355859684
target distance 22.0
model initialize at round 3040
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([109.70992658,  12.98992055]), 'previousTarget': array([110.21853056,  12.17458624]), 'currentState': array([93.      ,  2.      ,  4.476118], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6004003733882802
running average episode reward sum: 0.5945240308923812
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10525964,  15.26745725,   6.04791585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9338595651297503}
episode index:3041
target Thresh 72.7313638296302
target distance 25.0
model initialize at round 3041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.12819292,  15.6751676 ]), 'previousTarget': array([109.85753677,  15.61709559]), 'currentState': array([91.42551454, 19.11093024,  1.42877394]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5945817294837398
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14001608e+02, 1.53469579e+01, 2.39272965e-05]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0569605397408408}
episode index:3042
target Thresh 72.73463083202712
target distance 8.0
model initialize at round 3042
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.63016247,  23.33742963,   5.47729797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.413193553652482}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.594695728307242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14603022e+02, 1.40475753e+01, 3.55373435e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0318449569310122}
episode index:3043
target Thresh 72.7378945690546
target distance 48.0
model initialize at round 3043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.39429124, 11.88136087]), 'previousTarget': array([86.84555753, 11.48069469]), 'currentState': array([68.53028945,  9.55296384,  1.34732383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5471710322090102
running average episode reward sum: 0.5946801157263951
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49094632,  14.85417628,   5.09013024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5295282872341128}
episode index:3044
target Thresh 72.74115504397638
target distance 50.0
model initialize at round 3044
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.53646281, 14.59923805]), 'previousTarget': array([85., 15.]), 'currentState': array([66.53844493, 14.31766923,  0.59254807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5946835068434431
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07274943,  15.07450442,   5.68760543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9302389581587274}
episode index:3045
target Thresh 72.74441226005293
target distance 45.0
model initialize at round 3045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.18130497, 19.71703671]), 'previousTarget': array([89.69125016, 19.49933331]), 'currentState': array([71.56232869, 23.60236423,  5.65412301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.21416014551536416
running average episode reward sum: 0.5945585812487851
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35220403,  15.65222365,   4.85157906]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9192580234392919}
episode index:3046
target Thresh 72.74766622054146
target distance 72.0
model initialize at round 3046
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([62.99105781, 13.59800303]), 'previousTarget': array([62.99228841, 13.55534134]), 'currentState': array([43.        , 13.        ,  0.92485553], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 63
reward sum = -0.27170386514874845
running average episode reward sum: 0.5942742811351003
{'dynamicTrap': 15, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58488415,  14.9357142 ,   4.18364612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5884064376772362}
episode index:3047
target Thresh 72.75091692869596
target distance 59.0
model initialize at round 3047
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.81444012, 19.2819193 ]), 'previousTarget': array([75.86070472, 19.6436452 ]), 'currentState': array([56.       , 22.       ,  1.1338427], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.3124335226503064
running average episode reward sum: 0.5941818136946525
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14364368e+02, 1.48023425e+01, 9.09319744e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6656552377998749}
episode index:3048
target Thresh 72.7541643877671
target distance 22.0
model initialize at round 3048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.18535415,  15.52550195]), 'previousTarget': array([112.81660336,  15.29773591]), 'currentState': array([91.3724706 , 18.25490577,  1.97623348]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5942578998901726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14211929e+02, 1.52007649e+01, 5.92411757e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8132416853840744}
episode index:3049
target Thresh 72.75740860100237
target distance 23.0
model initialize at round 3049
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.83684329,  16.46642568]), 'previousTarget': array([111.35234545,  15.95156206]), 'currentState': array([90.59776377, 21.93065656,  1.54565048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5943312274435192
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09235367,  15.3336143 ,   5.0383759 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34616138258116735}
episode index:3050
target Thresh 72.76064957164596
target distance 33.0
model initialize at round 3050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.74496339,  10.92367491]), 'previousTarget': array([100.97366596,  10.32455532]), 'currentState': array([83.76726553,  4.61122816,  0.97011476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5943491793940026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63189059,  14.52584172,   1.27870345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7900074613823364}
episode index:3051
target Thresh 72.76388730293885
target distance 3.0
model initialize at round 3051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.30797218,  11.76550713,   4.12583041]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4889447330218006}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5944691816320778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04711919,  14.42388705,   5.19257427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1135025663614433}
episode index:3052
target Thresh 72.7671217981188
target distance 43.0
model initialize at round 3052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.65791886, 13.95882515]), 'previousTarget': array([91.95150202, 13.39196526]), 'currentState': array([70.67618878, 13.10415399,  3.38589001]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 59
reward sum = 0.49656933589465185
running average episode reward sum: 0.5944371148630843
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07452943,  14.50317353,   6.26664132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5023854829598423}
episode index:3053
target Thresh 72.77035306042025
target distance 8.0
model initialize at round 3053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58739909,  23.40844587,   3.87012184]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.428938224272919}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5945446157077354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36738257,  14.27980598,   1.34449887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9585844992566784}
episode index:3054
target Thresh 72.7735810930745
target distance 20.0
model initialize at round 3054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.80043565,  13.48457434]), 'previousTarget': array([112.88854382,  13.94427191]), 'currentState': array([93.72533477,  4.92357392,  2.19147873]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.41014222095569824
running average episode reward sum: 0.5944842548583895
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09623314,  15.00202222,   5.75013719]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9037691227639986}
episode index:3055
target Thresh 72.77680589930957
target distance 46.0
model initialize at round 3055
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.43273414, 12.15344905]), 'previousTarget': array([88.83200822, 11.58678368]), 'currentState': array([70.56565038,  9.85149708,  1.32845634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4102998852096291
running average episode reward sum: 0.5944239851039234
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11343473,  14.72301359,   3.32573483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9288269167576547}
episode index:3056
target Thresh 72.78002748235028
target distance 9.0
model initialize at round 3056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.57122794,   6.67436135,   1.36953765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.713637129827182}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.594528366282327
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05867562,  14.2653053 ,   2.38412928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7370340138702296}
episode index:3057
target Thresh 72.7832458454182
target distance 38.0
model initialize at round 3057
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.06348159, 10.00860464]), 'previousTarget': array([96.46160575, 10.60932768]), 'currentState': array([77.79562675,  4.64672343,  4.23388338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5367830840202747
running average episode reward sum: 0.5945094829329933
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07208951,  14.05014934,   1.13793985]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9525823736766185}
episode index:3058
target Thresh 72.78646099173169
target distance 4.0
model initialize at round 3058
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15189846e+02, 9.82962102e+00, 8.25720629e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.173863203470314}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5946198313687481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13598173,  15.24438946,   5.4033435 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8979163565088195}
episode index:3059
target Thresh 72.78967292450591
target distance 49.0
model initialize at round 3059
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.97264246, 13.98786153]), 'previousTarget': array([85.99583637, 14.40807829]), 'currentState': array([66.98567086, 13.26608165,  0.36715453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.5946463376490394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48840198,  14.33872688,   0.18770952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8360709712860076}
episode index:3060
target Thresh 72.79288164695278
target distance 70.0
model initialize at round 3060
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.45882126,  6.59674335]), 'previousTarget': array([64.7124451,  6.3792763]), 'currentState': array([46.7519407 ,  3.18516622,  5.51846832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 80
reward sum = 0.32523845958175024
running average episode reward sum: 0.5945583246212487
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01450236,  15.37187029,   0.29358515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0533247876585528}
episode index:3061
target Thresh 72.79608716228104
target distance 46.0
model initialize at round 3061
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.33968833,  9.07242175]), 'previousTarget': array([88.35234545,  8.04843794]), 'currentState': array([69.85285788,  4.57092878,  1.48627996]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4829129008577011
running average episode reward sum: 0.5945218630197583
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14361709e+02, 1.57773586e+01, 2.85509045e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0058337888445203}
episode index:3062
target Thresh 72.79928947369619
target distance 20.0
model initialize at round 3062
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([112.48508548,  11.70936588]), 'previousTarget': array([111.76887233,  12.89976701]), 'currentState': array([95.       ,  2.       ,  2.8882277], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.4419026025982749
running average episode reward sum: 0.5944720362941881
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94346594,  15.19617374,   0.26959059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20415737966968403}
episode index:3063
target Thresh 72.80248858440056
target distance 43.0
model initialize at round 3063
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.59796537, 17.20765798]), 'previousTarget': array([91.91402432, 17.14753262]), 'currentState': array([73.70352661, 19.25980475,  1.09191376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5940027299851236
running average episode reward sum: 0.5944718831263327
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7066631 ,  15.1437505 ,   3.22089002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3266661049375635}
episode index:3064
target Thresh 72.80568449759323
target distance 46.0
model initialize at round 3064
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.73766179, 12.79970905]), 'previousTarget': array([88.88288926, 12.16118362]), 'currentState': array([67.80248322, 11.19077907,  1.35880792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5945120980201245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23102703,  15.08771966,   0.30366121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7739600579869615}
episode index:3065
target Thresh 72.80887721647014
target distance 70.0
model initialize at round 3065
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.64906414, 13.32727885]), 'previousTarget': array([64.96742669, 12.14099581]), 'currentState': array([44.66009152, 12.66322015,  0.78716087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.17777500274398989
running average episode reward sum: 0.5943761759407781
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66581194,  15.8011998 ,   5.89124161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8681029742391857}
episode index:3066
target Thresh 72.812066744224
target distance 3.0
model initialize at round 3066
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.44513597,  16.07581668,   4.66259611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.8907627260392246}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5945051696884335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77817564,  15.10267434,   0.3794108 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24443417870496942}
episode index:3067
target Thresh 72.81525308404434
target distance 11.0
model initialize at round 3067
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.57821856,  11.64002845,   5.145939  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.00296827630711}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.594618264531886
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49743369,  15.46001915,   5.38338954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6813152788328133}
episode index:3068
target Thresh 72.81843623911749
target distance 72.0
model initialize at round 3068
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.28437929, 21.2672211 ]), 'previousTarget': array([62.87767469, 20.79136948]), 'currentState': array([44.43536191, 23.7200784 ,  5.8954317 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.005081967889522199
running average episode reward sum: 0.5944261705935862
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74944084,  15.15643473,   5.83932956]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29538401633898576}
episode index:3069
target Thresh 72.82161621262662
target distance 45.0
model initialize at round 3069
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.72257805, 18.5301294 ]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([70.93071784, 21.4080226 ,  6.26807779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6117182729378741
running average episode reward sum: 0.5944318032002129
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90631049,  14.83245084,   6.07871005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1919646988360695}
episode index:3070
target Thresh 72.82479300775171
target distance 7.0
model initialize at round 3070
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.08843165e+02, 1.57077280e+01, 1.04331334e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.197378134774196}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5945510360907371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14702936e+02, 1.50182049e+01, 8.83031393e-04]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29762169442880804}
episode index:3071
target Thresh 72.82796662766953
target distance 64.0
model initialize at round 3071
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.50745393, 12.45183904]), 'previousTarget': array([70.99024152, 13.62469505]), 'currentState': array([50.54017386, 11.30828022,  3.56079555]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5945485900444633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23263632,  14.63801227,   5.06495091]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8484586839774111}
episode index:3072
target Thresh 72.83113707555371
target distance 42.0
model initialize at round 3072
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.56655826, 18.85876854]), 'previousTarget': array([92.64677133, 19.25775784]), 'currentState': array([73.       , 23.       ,  1.4175035], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.3797071322282128
running average episode reward sum: 0.5944786774320923
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14807930e+02, 1.48171654e+01, 8.18815718e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2651783557399258}
episode index:3073
target Thresh 72.8343043545747
target distance 31.0
model initialize at round 3073
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.49328301,  14.94601194]), 'previousTarget': array([103.98960229,  14.64482588]), 'currentState': array([85.49360551, 14.832435  ,  5.82766563]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6775304401310079
running average episode reward sum: 0.5945056949215844
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30788505,  15.3853356 ,   6.25292476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.792153158924369}
episode index:3074
target Thresh 72.8374684678998
target distance 49.0
model initialize at round 3074
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.21259988,  8.98620559]), 'previousTarget': array([85.59608118,  8.99920024]), 'currentState': array([67.66514635,  4.75571385,  5.1275189 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.36966828458364553
running average episode reward sum: 0.594432577064564
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09584958,  14.17014353,   0.44877112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8353735102573437}
episode index:3075
target Thresh 72.84062941869308
target distance 68.0
model initialize at round 3075
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.60017965, 15.42632903]), 'previousTarget': array([66.99783772, 14.29408585]), 'currentState': array([47.60098858, 15.60620811,  2.16791198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.24922248276028947
running average episode reward sum: 0.5943203501158305
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33780111,  14.39718072,   5.26160285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8954878305276127}
episode index:3076
target Thresh 72.84378721011552
target distance 9.0
model initialize at round 3076
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.40808027,  10.44612014,   1.23374885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.852969386704569}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5944301144960031
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23578449,  15.74674298,   5.25823343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.068480426737401}
episode index:3077
target Thresh 72.84694184532492
target distance 13.0
model initialize at round 3077
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.01286254,   3.70291961,   5.07185602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.979545036356974}
done in step count: 15
reward sum = 0.7934890511482884
running average episode reward sum: 0.5944947860153833
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20496789,  15.57184006,   6.22147343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9793248171180265}
episode index:3078
target Thresh 72.8500933274759
target distance 29.0
model initialize at round 3078
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.90155639,  17.23153152]), 'previousTarget': array([105.27985236,  17.68142004]), 'currentState': array([87.6201656 , 22.54453155,  0.73594874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5945646895821927
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40031904,  15.62778709,   5.8017283 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8681784905250555}
episode index:3079
target Thresh 72.85324165971994
target distance 7.0
model initialize at round 3079
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.724119  ,  20.76894347,   4.26935232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.814211643351227}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5946866812414193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77374099,  15.73359432,   4.48459685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.767693800605792}
episode index:3080
target Thresh 72.85638684520538
target distance 26.0
model initialize at round 3080
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.22900786,  14.99060317]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([90.22904665, 14.95121172,  5.99871344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5947461212538042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81516519,  14.59822758,   4.80754798]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44224991087932936}
episode index:3081
target Thresh 72.8595288870774
target distance 2.0
model initialize at round 3081
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31135036,  13.41471198,   0.51654053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6155733202218057}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5948743671586537
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54868622,  14.86066416,   2.3176887 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5661016166136799}
episode index:3082
target Thresh 72.86266778847805
target distance 2.0
model initialize at round 3082
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48145723,  13.38831882,   2.69796002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6820574564137873}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5950025298679763
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04581385,  15.06749258,   0.93367505]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08157301529190658}
episode index:3083
target Thresh 72.86580355254621
target distance 8.0
model initialize at round 3083
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.55236485,  21.05617051,   4.35870361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.968743738738292}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5951179603219425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33209004,  15.02665472,   4.29979074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.33315801929485916}
episode index:3084
target Thresh 72.86893618241768
target distance 71.0
model initialize at round 3084
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.46367329, 19.06843328]), 'previousTarget': array([63.95059037, 18.59502885]), 'currentState': array([45.5307878 , 20.70552711,  1.4195289 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.5951153408151728
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27070068,  15.87873154,   0.58870914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1419486094953588}
episode index:3085
target Thresh 72.87206568122507
target distance 71.0
model initialize at round 3085
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.93589594, 21.33932962]), 'previousTarget': array([63.87423731, 20.76064932]), 'currentState': array([45.09433025, 23.85175299,  1.82301539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.4738174613883824
running average episode reward sum: 0.5950760349566417
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01427454,  15.71182864,   6.22621973]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2158760997342393}
episode index:3086
target Thresh 72.87519205209789
target distance 42.0
model initialize at round 3086
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.8445078 ,  9.03962719]), 'previousTarget': array([92.23047895,  8.49442256]), 'currentState': array([74.59395528,  3.61595429,  1.33461886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5950641107919482
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72732088,  15.75945756,   0.37275847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8069260740051101}
episode index:3087
target Thresh 72.87831529816248
target distance 10.0
model initialize at round 3087
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.76110482,   6.23836645,   5.70197159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.026870593320451}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5951672368077162
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4466349 ,  14.87751979,   1.1944142 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46312431965322143}
episode index:3088
target Thresh 72.88143542254211
target distance 31.0
model initialize at round 3088
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.48902141,  12.82126402]), 'previousTarget': array([103.74482241,  13.18464878]), 'currentState': array([8.49053118e+01, 8.76191479e+00, 7.05763658e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.595218888491237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22348096,  14.77915372,   4.9942578 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3141923289123443}
episode index:3089
target Thresh 72.88455242835691
target distance 2.0
model initialize at round 3089
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92168508,  15.29025393,   3.27695239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9663077794921759}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5953498856147026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92168508,  15.29025393,   3.27695239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9663077794921759}
episode index:3090
target Thresh 72.88766631872389
target distance 43.0
model initialize at round 3090
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.93706604, 16.2086651 ]), 'previousTarget': array([91.99459386, 15.53500945]), 'currentState': array([72.96701   , 17.30267689,  1.94222515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.44815812199501537
running average episode reward sum: 0.5953022661505746
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45218515,  15.52836824,   4.81885023]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6954454711566178}
episode index:3091
target Thresh 72.89077709675692
target distance 44.0
model initialize at round 3091
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.58185433, 13.19696005]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([70.63615608, 11.72416637,  2.64795518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.45850252638605515
running average episode reward sum: 0.5952580230264592
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82807319,  14.46993122,   3.82523866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5572537459225634}
episode index:3092
target Thresh 72.89388476556678
target distance 7.0
model initialize at round 3092
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91470907,   9.2201531 ,   3.30776381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.851779466025244}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5953579661405822
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.24561416,  15.10834594,   5.07154639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2684495441951335}
episode index:3093
target Thresh 72.89698932826116
target distance 8.0
model initialize at round 3093
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.802638  ,  20.13552442,   5.78670058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.048658699003669}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5954760133428638
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56586965,  15.88637563,   6.1070486 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9869807118572401}
episode index:3094
target Thresh 72.90009078794459
target distance 55.0
model initialize at round 3094
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.23985501, 20.75308148]), 'previousTarget': array([79.79172879, 20.12120309]), 'currentState': array([58.48037814, 23.84550347,  1.72658253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.20033698213104245
running average episode reward sum: 0.5953483432196937
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59757303,  15.82849388,   5.27546935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9210589420055425}
episode index:3095
target Thresh 72.90318914771856
target distance 7.0
model initialize at round 3095
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.43629759,  20.12282423,   4.72492066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.562943385807796}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5954540913951485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13150612,  15.71620505,   5.65039317]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1257136768504405}
episode index:3096
target Thresh 72.90628441068144
target distance 9.0
model initialize at round 3096
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.33689062,   5.7422108 ,   2.28537321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.36586947839575}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5955423354123826
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51231574,  15.63796025,   5.927554  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8030125889263772}
episode index:3097
target Thresh 72.90937657992845
target distance 9.0
model initialize at round 3097
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.59277494,  18.58629349,   0.22084439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.1401823912972}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5956570699877497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75397139,  14.99546096,   0.37841976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24607047332835907}
episode index:3098
target Thresh 72.91246565855178
target distance 56.0
model initialize at round 3098
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.99999991, 16.00187147]), 'previousTarget': array([78.99681199, 15.64291407]), 'currentState': array([59.      , 16.      ,  4.771818], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = -0.2754739529339175
running average episode reward sum: 0.5953759693027153
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89977712,  15.68191321,   5.03423824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1289838265364944}
episode index:3099
target Thresh 72.91555164964052
target distance 28.0
model initialize at round 3099
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.98315777,  10.7510697 ]), 'previousTarget': array([105.3829006 ,  10.87838597]), 'currentState': array([85.32288397,  3.55425221,  1.9612087 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5954373570999748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03634161,  15.06732269,   4.74582425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.07650527251680336}
episode index:3100
target Thresh 72.91863455628065
target distance 13.0
model initialize at round 3100
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.25273472,   3.50797304,   1.02087188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.560105016648462}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5955459117567974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96442924,  14.2728081 ,   2.72671211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7280613588854308}
episode index:3101
target Thresh 72.92171438155508
target distance 35.0
model initialize at round 3101
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.91204887, 15.12641792]), 'previousTarget': array([99.96742669, 15.85900419]), 'currentState': array([80.     , 17.     ,  3.16665], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.4739151712137938
running average episode reward sum: 0.5955067013310904
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06294137,  15.19866092,   5.83132393]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9578857082425979}
episode index:3102
target Thresh 72.92479112854363
target distance 27.0
model initialize at round 3102
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.49347508,  12.02515319]), 'previousTarget': array([106.97366596,  12.32455532]), 'currentState': array([88.90032297,  4.6566598 ,  6.23609543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5955437772337463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15720304,  14.44532491,   4.16031188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0089455704052792}
episode index:3103
target Thresh 72.92786480032305
target distance 51.0
model initialize at round 3103
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.77136277, 14.77824201]), 'previousTarget': array([83.99615643, 14.3920815 ]), 'currentState': array([65.77193837, 14.62650616,  6.00995982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4828058654051064
running average episode reward sum: 0.5955074570301931
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99918776,  15.01956768,   6.22451087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.01958453296669075}
episode index:3104
target Thresh 72.93093539996703
target distance 26.0
model initialize at round 3104
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.31141319,  14.40359846]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([89.42043324, 12.31819279,  5.87941229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.59555630232401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87449131,  14.30816644,   5.7632959 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7031259526497788}
episode index:3105
target Thresh 72.93400293054614
target distance 2.0
model initialize at round 3105
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.41044641,  15.08349409,   0.31700081]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5917449115450197}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5956832964314396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07744018,  14.82158582,   5.20387889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9396532573285779}
episode index:3106
target Thresh 72.93706739512794
target distance 65.0
model initialize at round 3106
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.26473405, 15.2621094 ]), 'previousTarget': array([69.99763356, 15.6923441 ]), 'currentState': array([51.26509321, 15.38196905,  0.55557364]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.37587782638353195
running average episode reward sum: 0.5956125511884245
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30737673,  14.96687069,   4.20954674]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6934151366991091}
episode index:3107
target Thresh 72.94012879677688
target distance 8.0
model initialize at round 3107
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.58704572,  21.75786806,   6.15324664]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.175731950304575}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5957299847337306
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31525834,  15.55081861,   5.03068307]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.878790238354521}
episode index:3108
target Thresh 72.94318713855434
target distance 17.0
model initialize at round 3108
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.458385  , 21.63603962,  0.95016551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.754873461041875}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5958234729572696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10266435,  14.36676789,   5.07915774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0982687214828823}
episode index:3109
target Thresh 72.9462424235187
target distance 44.0
model initialize at round 3109
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.8190466 , 16.43779725]), 'previousTarget': array([90.9793708 , 16.09184678]), 'currentState': array([72.8609326 , 17.73150727,  0.41192442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5958770161217036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51210875,  14.67923074,   5.76460971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6042750063718226}
episode index:3110
target Thresh 72.94929465472522
target distance 68.0
model initialize at round 3110
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.42274392, 18.22244071]), 'previousTarget': array([66.9805647, 17.1185045]), 'currentState': array([47.46846116, 19.57395819,  0.27296913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5958983613563106
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16670113,  15.68120516,   4.47378187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.076302689253554}
episode index:3111
target Thresh 72.95234383522615
target distance 2.0
model initialize at round 3111
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78687227,  11.4178646 ,   5.91671741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.5884700717645956}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5960063841668988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26774527,  14.29072891,   4.73197319]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0194422329218915}
episode index:3112
target Thresh 72.95538996807065
target distance 22.0
model initialize at round 3112
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.71855992,  15.62543813]), 'previousTarget': array([112.29527642,  15.73765188]), 'currentState': array([92.07223104, 19.3700024 ,  3.57184303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5960830007352519
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11100538,  14.78522333,   4.53608381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9145711865673201}
episode index:3113
target Thresh 72.95843305630488
target distance 44.0
model initialize at round 3113
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.52850255,  9.32836771]), 'previousTarget': array([90.29527642,  8.26234812]), 'currentState': array([71.0449374 ,  4.81276267,  2.31287003]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6056010902084954
running average episode reward sum: 0.5960860572829312
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6235171 ,  15.72122972,   5.99588009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8135795456503551}
episode index:3114
target Thresh 72.96147310297191
target distance 33.0
model initialize at round 3114
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.04389079,  17.29906013]), 'previousTarget': array([101.5646825 ,  17.84991583]), 'currentState': array([83.40370347, 21.07570442,  0.42769402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5961625990820217
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13472536,  15.9151696 ,   5.47001465]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2594584562478273}
episode index:3115
target Thresh 72.96451011111178
target distance 12.0
model initialize at round 3115
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.72872525,   2.89737498,   5.34670967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.630936114400791}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5962557384506463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14065987e+02, 1.52010846e+01, 4.41248119e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.955413691529702}
episode index:3116
target Thresh 72.96754408376151
target distance 12.0
model initialize at round 3116
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.68985209,   3.29470865,   1.12712115]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.931078280162357}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5963575227012183
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9956143 ,  14.14994294,   1.41727602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8500683690089323}
episode index:3117
target Thresh 72.97057502395508
target distance 33.0
model initialize at round 3117
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.09890582,  10.20370852]), 'previousTarget': array([100.60816786,   9.33049037]), 'currentState': array([83.3525149 ,  3.23428765,  0.21725094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5510659216189805
running average episode reward sum: 0.5963429968509675
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63702754,  15.76962202,   5.00689382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8509213005746626}
episode index:3118
target Thresh 72.9736029347234
target distance 69.0
model initialize at round 3118
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.14396082,  5.431559  ]), 'previousTarget': array([65.65421157,  5.7029674 ]), 'currentState': array([47.53212929,  1.51032656,  4.96384234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.31885233010048025
running average episode reward sum: 0.596049570968649
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17327708,  16.32546899,   0.43307453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.5621583844872646}
episode index:3119
target Thresh 72.97662781909442
target distance 70.0
model initialize at round 3119
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([63.97636384, 11.133452  ]), 'previousTarget': array([64.96742669, 12.14099581]), 'currentState': array([44.03354309,  9.62219409,  5.11069298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.4815088333413357
running average episode reward sum: 0.5960128591937685
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31561568,  15.96637616,   5.02263857]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0166101223911244}
episode index:3120
target Thresh 72.979649680093
target distance 8.0
model initialize at round 3120
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.5765907 ,  22.10206122,   4.71437812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.575983530685743}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5961235504113934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77249292,  15.70260766,   4.51734889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7385235195174089}
episode index:3121
target Thresh 72.98266852074103
target distance 60.0
model initialize at round 3121
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.94024845, 17.90627602]), 'previousTarget': array([74.97504678, 17.00124766]), 'currentState': array([53.99016137, 19.31837565,  1.2144413 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.30422089345822495
running average episode reward sum: 0.5960300518025039
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54662071,  14.40520176,   5.08014598]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7478888529849369}
episode index:3122
target Thresh 72.9856843440573
target distance 35.0
model initialize at round 3122
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.83015406, 10.14558811]), 'previousTarget': array([99.23047895, 10.49442256]), 'currentState': array([80.78169342,  4.05000387,  0.12771624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.4705803283602831
running average episode reward sum: 0.5959898821824455
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46319215,  15.91126754,   0.13781428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0576252615933324}
episode index:3123
target Thresh 72.98869715305769
target distance 40.0
model initialize at round 3123
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([94.80585039, 13.77998026]), 'previousTarget': array([94.9007438 , 12.99007438]), 'currentState': array([75.       , 11.       ,  5.5525885], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5016479763198918
running average episode reward sum: 0.5959596831088659
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14250637e+02, 1.55698397e+01, 6.65103048e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9414148267638001}
episode index:3124
target Thresh 72.99170695075496
target distance 29.0
model initialize at round 3124
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.17060161,  12.08870325]), 'previousTarget': array([105.44164417,  12.69281066]), 'currentState': array([85.99404557,  6.40894108,  5.68579137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5696429570675462
running average episode reward sum: 0.5959512617565327
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4139481 ,  14.82690691,   0.28313358]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6110794145312903}
episode index:3125
target Thresh 72.99471374015893
target distance 46.0
model initialize at round 3125
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.79511506, 15.57784224]), 'previousTarget': array([88.99527578, 15.56532009]), 'currentState': array([70.8008118 , 16.05516544,  5.52262372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5794050424607328
running average episode reward sum: 0.5959459686601488
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28111534,  15.08104591,   6.16829677]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29256499441627126}
episode index:3126
target Thresh 72.9977175242764
target distance 26.0
model initialize at round 3126
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.6181565 ,  15.75324724]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([89.81121503, 18.52544303,  6.22534084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5960249572193825
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28703891,  15.09584368,   4.5854169 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3026174946246342}
episode index:3127
target Thresh 73.00071830611114
target distance 12.0
model initialize at round 3127
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08088684,   4.75713435,   2.04982372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.243185027334539}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5961294072632471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36485867,  14.05185522,   5.6099488 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0159234095029586}
episode index:3128
target Thresh 73.00371608866394
target distance 7.0
model initialize at round 3128
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65532787,  21.69622749,   5.09169269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.728217979003561}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5962458874814436
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63070034,  15.01609883,   4.17615074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3696503950549213}
episode index:3129
target Thresh 73.00671087493257
target distance 3.0
model initialize at round 3129
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.12867036,  13.70859603,   1.41818279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2736751824782773}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.596371687517392
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38038118,  14.29619565,   5.72301865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9376929399975251}
episode index:3130
target Thresh 73.00970266791184
target distance 6.0
model initialize at round 3130
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.96748084,  18.90642918,   4.63746166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.3707486293708335}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5964849479333558
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01704342,  14.03518805,   5.25253326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9649624733239842}
episode index:3131
target Thresh 73.01269147059352
target distance 70.0
model initialize at round 3131
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.69970703,  8.38525712]), 'previousTarget': array([64.87065345,  9.27093182]), 'currentState': array([45.87733488,  5.72564016,  6.18219864]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.24992686593475
running average episode reward sum: 0.5963742972047483
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47453652,  14.46994359,   0.27283599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7463723382952125}
episode index:3132
target Thresh 73.01567728596642
target distance 37.0
model initialize at round 3132
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.30566018,  9.22412527]), 'previousTarget': array([97.17072713,  9.69940536]), 'currentState': array([78.       ,  4.       ,  1.8995929], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.444034313828538
running average episode reward sum: 0.5963256728883181
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23383959,  15.32963372,   5.87321812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8340624467426272}
episode index:3133
target Thresh 73.01866011701635
target distance 28.0
model initialize at round 3133
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.52174265,  16.62353004]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([85.80884259, 20.00015132,  1.43314004]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.3357081456426784
running average episode reward sum: 0.5962425147749659
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29530995,  15.33354334,   6.26709426]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4454875110860257}
episode index:3134
target Thresh 73.02163996672616
target distance 3.0
model initialize at round 3134
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.76809418,  19.64130889,   0.71605098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.655701853288852}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5963618310381956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23030971,  15.89012088,   6.22033209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1767490500244693}
episode index:3135
target Thresh 73.02461683807569
target distance 68.0
model initialize at round 3135
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.80913352, 15.90405332]), 'previousTarget': array([66.99783772, 15.70591415]), 'currentState': array([48.8129631 , 16.29542082,  5.72920549]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5963685046064049
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50545326,  14.90474246,   4.8994957 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5143510438916671}
episode index:3136
target Thresh 73.02759073404181
target distance 24.0
model initialize at round 3136
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.05287361,  14.91103946]), 'previousTarget': array([110.93091516,  14.6609096 ]), 'currentState': array([92.06197905, 14.30760389,  1.89607495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7411208682212584
running average episode reward sum: 0.5964146481714718
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21821139,  14.73276984,   3.83265872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.34500459118726584}
episode index:3137
target Thresh 73.03056165759841
target distance 52.0
model initialize at round 3137
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.99663718,  8.34979751]), 'previousTarget': array([82.48782391,  7.49719013]), 'currentState': array([62.39070247,  4.39918689,  1.01819968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5964273221159881
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84430251,  15.37659417,   5.69957821]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4075105853783022}
episode index:3138
target Thresh 73.03352961171643
target distance 6.0
model initialize at round 3138
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.43974296,  10.6245043 ,   2.31643811]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.645359477000663}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5965464274609655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78007791,  15.41950005,   0.23645893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4736517919591179}
episode index:3139
target Thresh 73.03649459936379
target distance 21.0
model initialize at round 3139
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.05775887,  14.49117655]), 'previousTarget': array([113.64677133,  14.74224216]), 'currentState': array([92.35028678, 11.0830181 ,  3.98795033]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5966041583310095
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94081543,  15.07624229,   0.25615671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.09651787837948791}
episode index:3140
target Thresh 73.03945662350553
target distance 29.0
model initialize at round 3140
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.07416915,  15.40247748]), 'previousTarget': array([106.,  15.]), 'currentState': array([87.09990596, 16.41678013,  0.10306633]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5966772447574676
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14341891,  15.95535338,   4.95830383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9660585207133936}
episode index:3141
target Thresh 73.04241568710364
target distance 7.0
model initialize at round 3141
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19385445,  20.20194305,   5.59682822]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.20555386909989}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5967930686802055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56843605,  15.37468939,   3.54277772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5715239185110157}
episode index:3142
target Thresh 73.04537179311718
target distance 8.0
model initialize at round 3142
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.05570335,   7.68713132,   0.1135872 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.084706451405522}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5969027368573359
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35786026,  14.28919694,   2.62333637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9579062732588975}
episode index:3143
target Thresh 73.04832494450228
target distance 70.0
model initialize at round 3143
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([64.95995487, 13.26499082]), 'previousTarget': array([64.98165792, 12.85635677]), 'currentState': array([45.      , 12.      ,  3.275599], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.16173835451518032
running average episode reward sum: 0.5967643257942499
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2523067 ,  15.64801566,   6.22169247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9894289082797003}
episode index:3144
target Thresh 73.05127514421207
target distance 38.0
model initialize at round 3144
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.40847261, 16.68194177]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([77.49926504, 18.58547903,  2.12380612]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5968269171957918
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55289437,  15.95170783,   6.16591005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.051499517206196}
episode index:3145
target Thresh 73.05422239519676
target distance 58.0
model initialize at round 3145
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.79850393, 10.83182662]), 'previousTarget': array([76.85591226, 10.3964032 ]), 'currentState': array([57.        ,  8.        ,  0.25260165], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5289560064661609
running average episode reward sum: 0.5968053434797302
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34765984,  14.80191085,   0.47965835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4001333227582133}
episode index:3146
target Thresh 73.0571667004036
target distance 51.0
model initialize at round 3146
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.91365388,  7.85390564]), 'previousTarget': array([83.38029071,  6.9400741 ]), 'currentState': array([63.39194988,  3.5061392 ,  0.94450068]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5968240450612664
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14032564e+02, 1.50192126e+01, 2.39305347e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9676272159445258}
episode index:3147
target Thresh 73.0601080627769
target distance 70.0
model initialize at round 3147
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.85432058,  6.63360598]), 'previousTarget': array([64.79898987,  7.82842712]), 'currentState': array([45.12700256,  3.34226528,  5.79885173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.5968135879918227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09310346,  15.20942949,   5.8252468 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9307642286732598}
episode index:3148
target Thresh 73.06304648525801
target distance 51.0
model initialize at round 3148
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.2470347 , 20.38320721]), 'previousTarget': array([83.75839053, 19.90064462]), 'currentState': array([64.54658183, 23.83170783,  0.30562532]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5968497072802573
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07948608,  15.38279023,   6.1275042 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3909557435861621}
episode index:3149
target Thresh 73.06598197078537
target distance 26.0
model initialize at round 3149
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.61704739,  14.51571735]), 'previousTarget': array([108.86817872,  14.29248216]), 'currentState': array([90.73802633, 12.31923798,  5.60511595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.665263889936749
running average episode reward sum: 0.5968714260684023
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63026195,  15.44782972,   5.40189769]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5807389116898553}
episode index:3150
target Thresh 73.06891452229445
target distance 3.0
model initialize at round 3150
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.61543098,  15.98260937,   6.24394365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6978081542677046}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5969961891829474
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24722862,  15.89734897,   6.05223278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1712813181650332}
episode index:3151
target Thresh 73.07184414271781
target distance 21.0
model initialize at round 3151
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.03190013,  14.41112281]), 'previousTarget': array([112.6897547 ,  14.11990655]), 'currentState': array([93.87121875,  8.6780356 ,  2.65739143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.5970148005507745
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09779717,  14.36992683,   1.51275858]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6376178233991208}
episode index:3152
target Thresh 73.07477083498506
target distance 70.0
model initialize at round 3152
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.24716114, 13.59956567]), 'previousTarget': array([64.98165792, 12.85635677]), 'currentState': array([46.2554074 , 13.02529884,  6.1129604 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.597006106101655
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20384137,  14.91338785,   5.69893149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8008559377783262}
episode index:3153
target Thresh 73.07769460202292
target distance 46.0
model initialize at round 3153
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.57956848, 19.04133252]), 'previousTarget': array([88.83200822, 18.41321632]), 'currentState': array([67.79331288, 21.95750975,  1.46082592]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5970443827111974
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56859602,  15.6981149 ,   4.81085573]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8206544984214362}
episode index:3154
target Thresh 73.08061544675512
target distance 27.0
model initialize at round 3154
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.8232949 ,  11.80184565]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([89.55511017,  3.6609964 ,  0.79536455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.5970992159166155
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08557995,  14.65760742,   5.43806469]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9764203508095016}
episode index:3155
target Thresh 73.08353337210254
target distance 8.0
model initialize at round 3155
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.501936  ,  18.23192394,   5.4263919 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.257421590297858}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5972143923405963
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26006756,  14.97017712,   5.1515283 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7405332015192214}
episode index:3156
target Thresh 73.08644838098307
target distance 41.0
model initialize at round 3156
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.60617313,  8.3606873 ]), 'previousTarget': array([93.06461275,  8.04487721]), 'currentState': array([75.58859685,  2.16941989,  1.0990774 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.48783404969403343
running average episode reward sum: 0.5971797454154628
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14455273e+02, 1.45970692e+01, 8.96374171e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6775548995241849}
episode index:3157
target Thresh 73.08936047631175
target distance 52.0
model initialize at round 3157
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.13908721,  8.01081247]), 'previousTarget': array([82.56699406,  8.13917182]), 'currentState': array([64.6330678 ,  3.59321055,  5.02905834]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5381437995723325
running average episode reward sum: 0.5971610513224156
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80478861,  15.2396375 ,   0.25203019]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3090851295345485}
episode index:3158
target Thresh 73.09226966100066
target distance 10.0
model initialize at round 3158
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.87076119,   4.13230024,   2.80694795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.02754032330192}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5972611957339893
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95901722,  14.42142034,   0.7502321 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5800293201094583}
episode index:3159
target Thresh 73.095175937959
target distance 38.0
model initialize at round 3159
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.24959189, 10.42708134]), 'previousTarget': array([96.34149075, 10.08986599]), 'currentState': array([77.      ,  5.      ,  3.026715], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2786023195538422
running average episode reward sum: 0.5971603543174766
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.21217495,  15.60085055,   0.37065938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6372123602310581}
episode index:3160
target Thresh 73.09807931009301
target distance 3.0
model initialize at round 3160
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.28035223,  19.65945494,   0.39343429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.966659727076966}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5972783988115236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0385512 ,  15.02258803,   4.08922643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9617141018420391}
episode index:3161
target Thresh 73.10097978030612
target distance 63.0
model initialize at round 3161
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.40224348, 16.63297273]), 'previousTarget': array([71.97736275, 17.04869701]), 'currentState': array([50.41563702, 17.36479392,  2.5537771 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.597278929364229
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94784655,  14.71946202,   4.51268176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.285344596695617}
episode index:3162
target Thresh 73.10387735149875
target distance 47.0
model initialize at round 3162
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.01950056, 10.65898902]), 'previousTarget': array([87.64310384,  9.76144542]), 'currentState': array([68.2734506 ,  7.48196037,  2.35595465]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.5973080694706169
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77675307,  15.41049535,   5.01619447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.467274681553067}
episode index:3163
target Thresh 73.10677202656849
target distance 27.0
model initialize at round 3163
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.50606081,  16.16697559]), 'previousTarget': array([107.6656401,  16.3582148]), 'currentState': array([89.94253165, 20.32249319,  0.33183039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.597391113176423
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82842137,  15.49270935,   0.12631018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5217295569079857}
episode index:3164
target Thresh 73.10966380841002
target distance 49.0
model initialize at round 3164
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.41658623, 14.46408578]), 'previousTarget': array([86., 15.]), 'currentState': array([67.42035996, 14.07558194,  0.41323191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.41507337992663623
running average episode reward sum: 0.5973335088373235
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9100981 ,  15.84115797,   4.7464754 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8459486255925082}
episode index:3165
target Thresh 73.11255269991511
target distance 65.0
model initialize at round 3165
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.39194387, 19.34665636]), 'previousTarget': array([69.94108971, 18.46607002]), 'currentState': array([51.49056175, 21.33033728,  0.80899945]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5929855427760334
running average episode reward sum: 0.597332135506287
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42415523,  14.40451729,   4.75856164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8283700016213035}
episode index:3166
target Thresh 73.11543870397267
target distance 43.0
model initialize at round 3166
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.62480942, 10.3807094 ]), 'previousTarget': array([91.48016054,  9.53026989]), 'currentState': array([73.03785577,  6.3370419 ,  1.90449816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.1641158126732981
running average episode reward sum: 0.5971953447507351
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4033948 ,  15.34410514,   0.31400638]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6887278940787115}
episode index:3167
target Thresh 73.11832182346869
target distance 18.0
model initialize at round 3167
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.56026848,  15.63083306]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.       , 16.       ,  2.5358095], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.5637518729976}
done in step count: 22
reward sum = 0.7316305895390458
running average episode reward sum: 0.5972377801184082
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.70314735,  15.9300758 ,   5.03725967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1659576292842846}
episode index:3168
target Thresh 73.1212020612863
target distance 24.0
model initialize at round 3168
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.20600731,  15.47995093]), 'previousTarget': array([110.84555753,  15.51930531]), 'currentState': array([92.49471564, 18.86594865,  0.34435627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.5973022776915924
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.11455702,  15.65818228,   0.40062347]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6680772600764893}
episode index:3169
target Thresh 73.12407942030572
target distance 4.0
model initialize at round 3169
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.73503229,  13.79527844,   5.2683428 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.565430324599008}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5974230340708695
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26918621,  15.60016478,   2.33014929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.945667253847788}
episode index:3170
target Thresh 73.12695390340431
target distance 25.0
model initialize at round 3170
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.36594458,  13.77596467]), 'previousTarget': array([109.61161351,  13.9223227 ]), 'currentState': array([91.0291317 ,  8.66835589,  0.09715527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.711627182503649
running average episode reward sum: 0.5974590492548596
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07068727,  14.08948645,   4.94228125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.30102155487488}
episode index:3171
target Thresh 73.1298255134566
target distance 22.0
model initialize at round 3171
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.54235352,  15.60834617]), 'previousTarget': array([112.81660336,  15.29773591]), 'currentState': array([91.84490394, 19.07396013,  1.38257766]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.720962032026851
running average episode reward sum: 0.5974979846214333
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61466612,  14.97919624,   5.20571787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3858950532024691}
episode index:3172
target Thresh 73.13269425333414
target distance 27.0
model initialize at round 3172
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.91303184,  17.29386593]), 'previousTarget': array([107.5237412 ,  16.66139084]), 'currentState': array([87.67209458, 22.75155163,  0.92505229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7705675541262327
running average episode reward sum: 0.5975525290807793
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06312965,  15.11937563,   5.95628363]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.13504033869409493}
episode index:3173
target Thresh 73.13556012590568
target distance 70.0
model initialize at round 3173
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.75553929, 20.65341439]), 'previousTarget': array([64.87065345, 20.72906818]), 'currentState': array([46.89145883, 22.98114024,  5.6923098 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.42621146611620897
running average episode reward sum: 0.597498546389234
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26820982,  14.91254934,   4.99745227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28210658605367156}
episode index:3174
target Thresh 73.13842313403714
target distance 11.0
model initialize at round 3174
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.44123427,  16.5531399 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.     ,  13.     ,   3.44559], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.98026063272241}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.5975818745156963
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14811883,  14.81096268,   0.51940125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8726033653845937}
episode index:3175
target Thresh 73.14128328059148
target distance 5.0
model initialize at round 3175
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.86742097,  18.41363995,   5.57061028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.025025590621166}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5977023147315289
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3062041 ,  15.08424061,   4.77732557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6988914277023627}
episode index:3176
target Thresh 73.14414056842887
target distance 7.0
model initialize at round 3176
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.35019789,  20.56289917,   5.35011919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.802386937021515}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.597819594141434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0450916 ,  15.07805583,   5.72057192]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.09014413533840844}
episode index:3177
target Thresh 73.14699500040658
target distance 25.0
model initialize at round 3177
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.35272955,  17.04028121]), 'previousTarget': array([109.04848294,  16.90448546]), 'currentState': array([90.54270423, 23.83608488,  0.42870891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5978994047698021
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48160068,  15.62606368,   0.8045716 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8128305999819028}
episode index:3178
target Thresh 73.14984657937904
target distance 2.0
model initialize at round 3178
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.74890163,  12.33572136,   2.61969757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7675321706589266}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5980196314433568
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46054117,  15.33736432,   2.53321838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5708877806967085}
episode index:3179
target Thresh 73.15269530819788
target distance 38.0
model initialize at round 3179
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.64576292,  8.67730554]), 'previousTarget': array([96.0716533 ,  9.02262736]), 'currentState': array([75.54603806,  2.74430378,  4.3256743 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.5980019161743391
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56415371,  15.37357392,   0.11494228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5740378550157349}
episode index:3180
target Thresh 73.15554118971176
target distance 18.0
model initialize at round 3180
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.09457508,  13.11003653]), 'previousTarget': array([113.21358457,  13.70981108]), 'currentState': array([95.32949338,  2.20444   ,  2.00981617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5980488102888178
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15233960e+02, 1.47859978e+01, 4.55565155e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3170711963778308}
episode index:3181
target Thresh 73.1583842267666
target distance 37.0
model initialize at round 3181
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.99644674, 10.53188319]), 'previousTarget': array([97.54828331, 11.22665585]), 'currentState': array([78.65313667,  5.44893566,  4.10094261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5980841667994978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19646958,  15.37619573,   4.82892432]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.424409617800076}
episode index:3182
target Thresh 73.16122442220544
target distance 73.0
model initialize at round 3182
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([61.99967275, 14.11441037]), 'previousTarget': array([61.99812374, 14.2739469 ]), 'currentState': array([42.      , 14.      ,  5.671173], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.20831157618493923
running average episode reward sum: 0.5979617123255379
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21665443,  15.74730112,   4.84069368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0826306998746102}
episode index:3183
target Thresh 73.16406177886844
target distance 69.0
model initialize at round 3183
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.80934033,  6.67773856]), 'previousTarget': array([65.65421157,  5.7029674 ]), 'currentState': array([47.10106502,  3.27422799,  6.13666815]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.42225131442295505
running average episode reward sum: 0.597906526899061
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77025238,  14.46058825,   5.17688814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9403476806711613}
episode index:3184
target Thresh 73.166896299593
target distance 27.0
model initialize at round 3184
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.7080135 ,  16.27967015]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([89.10924624, 20.26568593,  6.12435967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5979834614882242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15506375e+02, 1.53000053e+01, 4.24510837e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.588573191175975}
episode index:3185
target Thresh 73.16972798721362
target distance 14.0
model initialize at round 3185
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.96832459,  16.1816751 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.      ,  11.      ,   4.954948], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.83983894452538}
done in step count: 33
reward sum = 0.3773642714402049
running average episode reward sum: 0.597914215038115
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95111487,  14.18004935,   3.94860772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8214066092061655}
episode index:3186
target Thresh 73.172556844562
target distance 37.0
model initialize at round 3186
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.19551967, 18.46337504]), 'previousTarget': array([97.54828331, 18.77334415]), 'currentState': array([76.52634257, 22.08600837,  2.77563369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.46972627088831986
running average episode reward sum: 0.5978739929031449
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85373889,  14.99551211,   5.05939512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1463299446204882}
episode index:3187
target Thresh 73.17538287446698
target distance 41.0
model initialize at round 3187
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.897283  , 15.80516089]), 'previousTarget': array([94., 15.]), 'currentState': array([73.91182468, 16.56769349,  2.63698238]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6894747128501433
running average episode reward sum: 0.5979027258767795
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20439143,  15.70693126,   5.54525938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0643048452210804}
episode index:3188
target Thresh 73.1782060797546
target distance 63.0
model initialize at round 3188
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.31010647, 20.88435611]), 'previousTarget': array([71.87767469, 19.79136948]), 'currentState': array([51.48907469, 23.55394397,  0.87845469]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.592988487100669
running average episode reward sum: 0.5979011848799854
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20036483,  14.97086   ,   4.93947283]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2024727265666149}
episode index:3189
target Thresh 73.18102646324807
target distance 45.0
model initialize at round 3189
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.31886551, 17.21204753]), 'previousTarget': array([89.92145281, 17.22920419]), 'currentState': array([68.38724856, 18.86451415,  2.21540499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5979364989998577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71060958,  14.30869479,   4.1006445 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9913974303124565}
episode index:3190
target Thresh 73.18384402776776
target distance 9.0
model initialize at round 3190
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.50737625,   5.14676764,   0.40961057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.163629321584187}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5980412087613454
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5361644 ,  14.56381525,   5.27461398]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6911797142942813}
episode index:3191
target Thresh 73.18665877613125
target distance 62.0
model initialize at round 3191
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.40582243, 19.20565744]), 'previousTarget': array([72.87373448, 19.75619127]), 'currentState': array([51.49824801, 21.1261992 ,  2.75357795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3723540724888475
running average episode reward sum: 0.597970504771285
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48580628,  15.12804796,   4.91966478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5298975959427344}
episode index:3192
target Thresh 73.18947071115329
target distance 25.0
model initialize at round 3192
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.78590032,  13.87712943]), 'previousTarget': array([109.25928039,  13.39259851]), 'currentState': array([90.23413636,  9.66660381,  2.44122267]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5980393855833194
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71701727,  15.9217094 ,   5.89575065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9641718951996547}
episode index:3193
target Thresh 73.19227983564579
target distance 13.0
model initialize at round 3193
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.32313949,   1.79871627,   2.26103723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.978493526433045}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5981296628175501
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05093313,  14.48323695,   0.88057506]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5192670147722512}
episode index:3194
target Thresh 73.1950861524179
target distance 21.0
model initialize at round 3194
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.96317803,  15.03026294]), 'previousTarget': array([113.97736275,  14.95130299]), 'currentState': array([92.96538524, 15.32738856,  1.22287202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.5981518356174725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3271307 ,  15.42357075,   5.62056382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5351884433406972}
episode index:3195
target Thresh 73.19788966427592
target distance 28.0
model initialize at round 3195
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.20387188,  10.59876812]), 'previousTarget': array([105.14017935,  10.42222613]), 'currentState': array([85.68370904,  3.04869738,  1.45886326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.5981938104403878
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64776962,  15.07881647,   0.77959868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6525469441011157}
episode index:3196
target Thresh 73.20069037402338
target distance 59.0
model initialize at round 3196
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.7654019,  6.1319474]), 'previousTarget': array([75.53149897,  6.30355062]), 'currentState': array([54.21247438,  1.92682845,  3.99932504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.21661864915727463
running average episode reward sum: 0.5980744563079877
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96103971,  14.49227395,   5.45407354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5092186616531713}
episode index:3197
target Thresh 73.20348828446099
target distance 21.0
model initialize at round 3197
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.52394639,  15.03356544]), 'previousTarget': array([113.97736275,  14.95130299]), 'currentState': array([93.52911545, 15.48824768,  0.86723208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.5063421591713458
running average episode reward sum: 0.5980457720374633
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28263338,  14.94009609,   4.61770271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7198634187216028}
episode index:3198
target Thresh 73.20628339838663
target distance 66.0
model initialize at round 3198
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.52376679, 15.45976978]), 'previousTarget': array([68.99770471, 14.30299553]), 'currentState': array([48.52474535, 15.65761166,  0.91153121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.3955853141641025
running average episode reward sum: 0.5979824833666683
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10364544,  15.81209898,   5.5592203 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2095272843479457}
episode index:3199
target Thresh 73.20907571859546
target distance 47.0
model initialize at round 3199
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.38466196, 10.67527084]), 'previousTarget': array([87.7164234 , 10.35598696]), 'currentState': array([69.66375705,  7.34571991,  1.18719691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.5980025769784236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26439429,  15.29397435,   5.99258161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3953798919743281}
episode index:3200
target Thresh 73.21186524787976
target distance 7.0
model initialize at round 3200
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.64198133,  21.25358328,   5.65600359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.235026895865216}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5981158520277899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73098492,  15.76025276,   6.23280507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8064449006779072}
episode index:3201
target Thresh 73.21465198902906
target distance 8.0
model initialize at round 3201
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.94493705,   8.34571445,   1.27189279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.698114729312351}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5982230863492681
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31892278,  14.56351485,   5.88041938]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8089409514969843}
episode index:3202
target Thresh 73.21743594483014
target distance 23.0
model initialize at round 3202
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.78615763,  12.44686525]), 'previousTarget': array([109.41125677,  11.84114513]), 'currentState': array([93.68092524,  2.08293622,  1.05929964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6819275199781069
running average episode reward sum: 0.5982492194849625
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76428764,  15.57638457,   6.17826015]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6227194285894304}
episode index:3203
target Thresh 73.22021711806693
target distance 9.0
model initialize at round 3203
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.14388659,   7.53235796,   2.40263534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.489981134756523}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5983563452433632
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61937733,  15.03531819,   6.16275865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3822577595154671}
episode index:3204
target Thresh 73.22299551152061
target distance 9.0
model initialize at round 3204
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.39419457,   8.40104678,   2.29528046]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.844633267822948}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5984546793782276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78485098,  14.94726081,   0.24804211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22151867498983524}
episode index:3205
target Thresh 73.22577112796957
target distance 63.0
model initialize at round 3205
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.93217546,  9.20878206]), 'previousTarget': array([71.75271057,  8.13535088]), 'currentState': array([53.11903697,  6.48122949,  0.34244466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3110437628323389
running average episode reward sum: 0.5983650315564728
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33266349,  14.94031848,   1.10568217]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6699999222787935}
episode index:3206
target Thresh 73.2285439701894
target distance 12.0
model initialize at round 3206
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.72407038,  20.47284419,   6.15309143]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.533898525179263}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5984720209914103
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63995248,  15.76459134,   5.99790007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8451237370206529}
episode index:3207
target Thresh 73.231314040953
target distance 24.0
model initialize at round 3207
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.01768175,  12.13938978]), 'previousTarget': array([108.58583933,  11.52566297]), 'currentState': array([92.67320678,  2.18101703,  1.11776656]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5985183738821023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69954192,  14.52420669,   4.43577187]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8460131078628436}
episode index:3208
target Thresh 73.2340813430304
target distance 4.0
model initialize at round 3208
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40125395,  11.52573676,   3.83388865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.525478955490367}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5986342294838841
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37038943,  14.07699307,   0.78244704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9945502126970324}
episode index:3209
target Thresh 73.2368458791889
target distance 22.0
model initialize at round 3209
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.644487  ,  16.18914508]), 'previousTarget': array([112.29527642,  15.73765188]), 'currentState': array([91.35064826, 21.45676187,  1.91009653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.5986999907420577
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51977924,  14.61423666,   4.59297778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6159751058424564}
episode index:3210
target Thresh 73.23960765219307
target distance 70.0
model initialize at round 3210
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.36670547, 18.60071602]), 'previousTarget': array([64.96742669, 17.85900419]), 'currentState': array([46.42129752, 20.07743585,  1.47640639]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.2241747581102177
running average episode reward sum: 0.5985833525506432
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20916254,  14.81613373,   0.26941011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2784883750981666}
episode index:3211
target Thresh 73.24236666480465
target distance 44.0
model initialize at round 3211
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.54275521, 16.79142149]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([69.59209115, 18.19534593,  2.69753802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5136640531705082
running average episode reward sum: 0.5985569144126046
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20382979,  14.38157383,   4.66223973]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0081358731404777}
episode index:3212
target Thresh 73.24512291978267
target distance 41.0
model initialize at round 3212
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.21571911, 18.53879389]), 'previousTarget': array([93.62981184, 19.16979281]), 'currentState': array([75.52818213, 22.06028331,  0.25550049]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7105796386991466
running average episode reward sum: 0.5985917798730112
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18176083,  15.58250376,   5.79046048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0044032900702697}
episode index:3213
target Thresh 73.24787641988338
target distance 27.0
model initialize at round 3213
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.54428014,  12.89895972]), 'previousTarget': array([106.97366596,  12.32455532]), 'currentState': array([89.52613879,  6.70942828,  1.44513577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5986244046132496
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01851303,  15.39106037,   6.17215502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.39149833213669033}
episode index:3214
target Thresh 73.25062716786029
target distance 3.0
model initialize at round 3214
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.37133909,  13.28214551,   0.13335741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.367184064363309}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5987461388575379
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54905427,  14.51245353,   1.49472702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6641186747858675}
episode index:3215
target Thresh 73.25337516646411
target distance 3.0
model initialize at round 3215
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.07867119,  14.36607508,   5.77050042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9893181165515625}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5988647190382415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3953641 ,  14.98955369,   5.94810963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3955020816115747}
episode index:3216
target Thresh 73.25612041844289
target distance 48.0
model initialize at round 3216
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.68970257, 15.8023119 ]), 'previousTarget': array([86.99566113, 15.58342373]), 'currentState': array([68.69899506, 16.41191285,  5.62576849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.598897228511652
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90208593,  15.97698912,   0.29203837]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3297619221802246}
episode index:3217
target Thresh 73.25886292654187
target distance 67.0
model initialize at round 3217
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.60615927, 12.59484858]), 'previousTarget': array([67.94453985, 11.4883985 ]), 'currentState': array([48.63298119, 11.55939825,  2.19520231]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.5056229998194989
running average episode reward sum: 0.5988682433566823
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10477547,  15.5722901 ,   5.06191183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.581802253677974}
episode index:3218
target Thresh 73.26160269350352
target distance 44.0
model initialize at round 3218
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.51326671, 13.25276901]), 'previousTarget': array([90.91786413, 12.81071492]), 'currentState': array([69.56009894, 11.88488899,  1.60327387]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5989029389093122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4645511 ,  15.2759528 ,   4.79790503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5403310722997479}
episode index:3219
target Thresh 73.26433972206765
target distance 11.0
model initialize at round 3219
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.56922008,   9.608224  ,   1.37976807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.863280277971946}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5990064054959574
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52385064,  15.66856929,   0.43601495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8207941924780728}
episode index:3220
target Thresh 73.26707401497127
target distance 43.0
model initialize at round 3220
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.33310536, 18.43372469]), 'previousTarget': array([91.7401454 , 18.78648796]), 'currentState': array([70.52411076, 21.19121138,  4.65289974]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6544390860045589
running average episode reward sum: 0.5990236152694776
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42780307,  15.10550405,   1.19372649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44062066165663194}
episode index:3221
target Thresh 73.26980557494868
target distance 21.0
model initialize at round 3221
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.06220267,  14.30513834]), 'previousTarget': array([113.23047895,  14.49442256]), 'currentState': array([92.59921242,  9.70166025,  1.66722202]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6173353913681524
running average episode reward sum: 0.5990292986264294
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67380357,  14.52175524,   4.74754697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5788973675957682}
episode index:3222
target Thresh 73.27253440473145
target distance 65.0
model initialize at round 3222
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.22963424, 19.06096721]), 'previousTarget': array([69.94108971, 18.46607002]), 'currentState': array([51.31516175, 20.90861043,  1.24297732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3907833477457075
running average episode reward sum: 0.5989646861688183
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15097358e+02, 1.47852886e+01, 1.08220406e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23575332254326178}
episode index:3223
target Thresh 73.27526050704837
target distance 41.0
model initialize at round 3223
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.50572948, 18.00109405]), 'previousTarget': array([93.78922128, 18.1040164 ]), 'currentState': array([75.73859667, 21.04419456,  0.93731874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6592132918856305
running average episode reward sum: 0.5989833737016088
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89868733,  14.40858268,   5.980178  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6000322492454351}
episode index:3224
target Thresh 73.2779838846256
target distance 44.0
model initialize at round 3224
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.64782297, 19.98832541]), 'previousTarget': array([90.6773982 , 19.42229124]), 'currentState': array([70.0240822 , 23.84951668,  1.3159852 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.513629449953481
running average episode reward sum: 0.5989569073686637
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89125761,  15.83659531,   5.61835203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8436329923863105}
episode index:3225
target Thresh 73.28070454018646
target distance 38.0
model initialize at round 3225
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.5424696 ,  9.83326837]), 'previousTarget': array([96.34149075, 10.08986599]), 'currentState': array([78.36475183,  4.15743   ,  4.7804156 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.3463825139340798
running average episode reward sum: 0.5988786140043009
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72185396,  14.92597954,   6.27660959]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.725639142863179}
episode index:3226
target Thresh 73.28342247645165
target distance 30.0
model initialize at round 3226
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.34447417,  16.99810275]), 'previousTarget': array([104.82455801,  16.3567256 ]), 'currentState': array([84.68709448, 20.684215  ,  0.92702723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.5988901748571237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13008323,  14.78609124,   5.61230321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2503569511220302}
episode index:3227
target Thresh 73.28613769613908
target distance 8.0
model initialize at round 3227
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.02367888,  21.66203844,   6.28127849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.646336741998796}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5989963055803406
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95768018,  14.881536  ,   5.60561627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.12579621222952825}
episode index:3228
target Thresh 73.288850201964
target distance 66.0
model initialize at round 3228
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.35372538, 13.37211905]), 'previousTarget': array([68.96336993, 12.20990121]), 'currentState': array([49.36643173, 12.65931313,  2.07639503]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.599013853711339
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53663243,  15.16410731,   0.46828048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49156964367638}
episode index:3229
target Thresh 73.29155999663887
target distance 7.0
model initialize at round 3229
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.48522753,   9.52730167,   0.26509428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.508389237995509}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5991169656290466
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30118141,  15.33888401,   1.26773149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4533791084404348}
episode index:3230
target Thresh 73.29426708287353
target distance 28.0
model initialize at round 3230
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.19378201,  14.77538022]), 'previousTarget': array([106.98725709,  15.28616939]), 'currentState': array([87.20205658, 14.20012894,  5.59556007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5761947424250979
running average episode reward sum: 0.5991098711619454
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19914716,  15.97505569,   4.8661457 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2617840025224247}
episode index:3231
target Thresh 73.29697146337503
target distance 38.0
model initialize at round 3231
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.97913561, 14.91331278]), 'previousTarget': array([96.99307839, 14.52613364]), 'currentState': array([77.       , 14.       ,  2.7416906], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4290971595933045
running average episode reward sum: 0.5990572682190095
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35923346,  15.95006164,   5.16838957]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.145948900653899}
episode index:3232
target Thresh 73.29967314084779
target distance 68.0
model initialize at round 3232
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.98258359, 11.76267392]), 'previousTarget': array([66.922597  , 10.75787621]), 'currentState': array([4.80298241e+01, 1.03888513e+01, 2.50041485e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.3982577545314745
running average episode reward sum: 0.5989951588736067
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57676802,  14.25551057,   3.83362324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9417674121487714}
episode index:3233
target Thresh 73.30237211799346
target distance 31.0
model initialize at round 3233
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.51936521,  17.14947549]), 'previousTarget': array([103.74482241,  16.81535122]), 'currentState': array([84.92716465, 21.16764384,  6.17288083]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5990528839762455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45648089,  14.19874447,   5.48297689]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9221633411221316}
episode index:3234
target Thresh 73.30506839751101
target distance 5.0
model initialize at round 3234
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.87130824,  12.81624844,   4.69119883]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.50612277947356}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5991587347538111
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76987993,  15.97385298,   6.03623907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0006722137935529}
episode index:3235
target Thresh 73.30776198209674
target distance 35.0
model initialize at round 3235
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.8544073 , 14.40884012]), 'previousTarget': array([99.92693298, 13.70802283]), 'currentState': array([80.      , 12.      ,  3.407504], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.4688433697993416
running average episode reward sum: 0.5991184642454815
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19323353,  15.10034553,   5.77148652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.812983005371744}
episode index:3236
target Thresh 73.31045287444422
target distance 50.0
model initialize at round 3236
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.38939742, 19.4046786 ]), 'previousTarget': array([84.80683493, 19.22704311]), 'currentState': array([66.62228001, 22.44787907,  1.27937001]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4678020111015011
running average episode reward sum: 0.598788862615779
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([106.84794249,  17.40998351]), 'previousTarget': array([105.10895317,  17.78865677]), 'currentState': array([8.76684971e+01, 2.30799812e+01, 3.00296950e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
episode index:3237
target Thresh 73.31314107724437
target distance 49.0
model initialize at round 3237
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.02892271,  7.73430859]), 'previousTarget': array([85.42594623,  7.75737459]), 'currentState': array([67.6713289 ,  2.70603237,  0.81216806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5192406087091953
running average episode reward sum: 0.5987642955206874
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57277036,  14.8526542 ,   4.67857582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5914191984738666}
episode index:3238
target Thresh 73.31582659318536
target distance 47.0
model initialize at round 3238
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.22152549, 10.62215701]), 'previousTarget': array([87.7164234 , 10.35598696]), 'currentState': array([66.4489965 ,  7.61431768,  1.78545725]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4526849717121368
running average episode reward sum: 0.598719195389842
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88799394,  14.30495942,   6.2220735 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7040076471960377}
episode index:3239
target Thresh 73.31850942495272
target distance 8.0
model initialize at round 3239
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.66031846,  23.84899839,   0.92399204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.24024171257326}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5988220800048164
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.98322627,  15.61458847,   4.35373164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1595054485170966}
episode index:3240
target Thresh 73.32118957522928
target distance 48.0
model initialize at round 3240
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.45942223, 17.81371012]), 'previousTarget': array([86.89236818, 17.92787831]), 'currentState': array([68.57087627, 19.9222024 ,  5.22370077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5988587688207971
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30187777,  15.82829708,   5.17306304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.083259295582417}
episode index:3241
target Thresh 73.3238670466952
target distance 11.0
model initialize at round 3241
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.77273125,   5.15160736,   1.37798929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.495678056943076}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5989530079652104
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61895356,  15.64019965,   4.83793426]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8904825118480243}
episode index:3242
target Thresh 73.32654184202794
target distance 34.0
model initialize at round 3242
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.41351179,  16.95239442]), 'previousTarget': array([100.86301209,  16.66317505]), 'currentState': array([82.64987164, 20.01809632,  0.11775637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.5990205238238697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75686086,  15.18810437,   6.086884  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7798856401129242}
episode index:3243
target Thresh 73.32921396390229
target distance 65.0
model initialize at round 3243
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.93288599, 17.95080076]), 'previousTarget': array([69.9787322 , 17.07790467]), 'currentState': array([50.97757421, 19.28703869,  6.19439453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3264691749299245
running average episode reward sum: 0.5989365067619419
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29638042,  14.69003116,   5.20391691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7688700780851676}
episode index:3244
target Thresh 73.33188341499039
target distance 8.0
model initialize at round 3244
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.89857039,   8.47302317,   0.62954593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.284081757241582}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5990362935686185
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90412259,  14.3841447 ,   3.11122066]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6232737993687458}
episode index:3245
target Thresh 73.33455019796169
target distance 9.0
model initialize at round 3245
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.14269875,  14.10188312,   6.0404014 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.9084636179899555}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5991447204806122
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87210344,  14.91483345,   6.20183321]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.15365829588256533}
episode index:3246
target Thresh 73.33721431548295
target distance 51.0
model initialize at round 3246
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.40748744, 17.14243228]), 'previousTarget': array([83.96548746, 16.82555956]), 'currentState': array([65.45969668, 18.58660809,  5.65530259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.1380742773276601
running average episode reward sum: 0.5990027215760378
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4142302 ,  15.79826856,   5.25952911]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8993438447680573}
episode index:3247
target Thresh 73.33987577021831
target distance 37.0
model initialize at round 3247
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.578694  , 13.08347173]), 'previousTarget': array([97.74210955, 12.20142317]), 'currentState': array([78.      ,  9.      ,  5.958626], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5350060671375363
running average episode reward sum: 0.5989830181725777
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08355799,  15.56078955,   5.32514403]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0744072239136633}
episode index:3248
target Thresh 73.34253456482922
target distance 43.0
model initialize at round 3248
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.61631351,  8.95831387]), 'previousTarget': array([91.26392603,  8.37597936]), 'currentState': array([73.30731746,  3.7465319 ,  5.79203922]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5064865876769922
running average episode reward sum: 0.5989545489726714
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36743789,  14.89579119,   5.46002946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6410883747013074}
episode index:3249
target Thresh 73.34519070197447
target distance 47.0
model initialize at round 3249
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.66401385,  8.58601865]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([69.23200643,  3.8534625 ,  0.26051968]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5989564109782606
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55050046,  14.8831872 ,   5.79895955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46442983368011526}
episode index:3250
target Thresh 73.34784418431022
target distance 27.0
model initialize at round 3250
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.39766799,  14.41618867]), 'previousTarget': array([107.78406925,  13.93097322]), 'currentState': array([88.47540228, 12.65456195,  2.2301853 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5990288678685933
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97456436,  15.33455432,   5.48386556]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.335519842328769}
episode index:3251
target Thresh 73.35049501448991
target distance 2.0
model initialize at round 3251
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.49407715,  12.19264057,   2.66762102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.185760606127764}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5991460484135291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68141604,  14.97605757,   0.69689205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31948236091563414}
episode index:3252
target Thresh 73.35314319516439
target distance 38.0
model initialize at round 3252
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.87723331, 13.21259941]), 'previousTarget': array([96.89010906, 13.09369569]), 'currentState': array([77.       , 11.       ,  4.3791795], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.16986107320434268
running average episode reward sum: 0.5990140825434987
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14357439e+02, 1.51462527e+01, 9.66861397e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6589945932103468}
episode index:3253
target Thresh 73.35578872898188
target distance 64.0
model initialize at round 3253
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.14165151, 16.83290912]), 'previousTarget': array([70.99024152, 16.37530495]), 'currentState': array([52.15991635, 17.68746153,  1.54502123]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5990294754586263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96423625,  15.480771  ,   5.49737551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4820993640744131}
episode index:3254
target Thresh 73.35843161858784
target distance 45.0
model initialize at round 3254
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.29910384, 17.58015555]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([68.39183198, 19.50383098,  2.43085718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.5990726923243497
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01347711,  14.44562065,   5.49586093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5545431396838111}
episode index:3255
target Thresh 73.36107186662521
target distance 23.0
model initialize at round 3255
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.09316713,  13.7593932 ]), 'previousTarget': array([111.13347761,  13.82323232]), 'currentState': array([90.70330874,  8.8570069 ,  3.44877255]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.5991300035800262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15072498e+02, 1.54175252e+01, 1.03150010e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42377262553069983}
episode index:3256
target Thresh 73.36370947573424
target distance 2.0
model initialize at round 3256
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.5841726 ,  17.90984662,   3.58041918]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.236012137603624}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.599246973182857
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93453127,  15.21632627,   6.03677875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22601594571993802}
episode index:3257
target Thresh 73.36634444855251
target distance 28.0
model initialize at round 3257
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.15981039,  13.63289663]), 'previousTarget': array([106.55604828,  13.19058177]), 'currentState': array([88.54768174,  9.71315045,  5.74089104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.599297033263018
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65351386,  14.60480169,   0.22990174]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7637159557121181}
episode index:3258
target Thresh 73.36897678771501
target distance 46.0
model initialize at round 3258
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.66658143, 17.37048982]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.76081208, 19.30965112,  0.99327772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.5993424076297159
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05176331,  15.65476975,   4.71711935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6568126592897939}
episode index:3259
target Thresh 73.3716064958541
target distance 58.0
model initialize at round 3259
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.61286452,  8.85319923]), 'previousTarget': array([76.81242258,  9.73274794]), 'currentState': array([55.85205491,  5.76930501,  2.95176554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.4303678142513094
running average episode reward sum: 0.5992905749323605
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3218595 ,  15.38435184,   5.55062916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5013181382966848}
episode index:3260
target Thresh 73.37423357559943
target distance 65.0
model initialize at round 3260
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.57152134, 15.38844266]), 'previousTarget': array([69.99763356, 14.3076559 ]), 'currentState': array([50.57228572, 15.56329799,  2.14198413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.5992960945784843
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86792781,  15.84125039,   5.2363717 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8515546228872183}
episode index:3261
target Thresh 73.37685802957814
target distance 37.0
model initialize at round 3261
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.00071798, 17.994213  ]), 'previousTarget': array([97.74210955, 17.79857683]), 'currentState': array([76.24455065, 21.10770818,  2.20472038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.489015724978559
running average episode reward sum: 0.5992622869851061
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23939286,  15.38774638,   5.26133715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8537391146177946}
episode index:3262
target Thresh 73.37947986041466
target distance 68.0
model initialize at round 3262
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.89636205, 10.36561248]), 'previousTarget': array([66.86301209,  9.33682495]), 'currentState': array([47.992465  ,  8.40732696,  1.87974926]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5992640472609726
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04141826,  14.18380976,   4.93186856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8172404646354102}
episode index:3263
target Thresh 73.38209907073082
target distance 52.0
model initialize at round 3263
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.83045221, 17.88479995]), 'previousTarget': array([82.90818058, 18.08575187]), 'currentState': array([64.92126104, 19.78850875,  5.71794814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4076660841549218
running average episode reward sum: 0.5992053469046288
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28849589,  14.15422052,   6.00788702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8936290127374159}
episode index:3264
target Thresh 73.38471566314584
target distance 5.0
model initialize at round 3264
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67212373,  11.43329277,   2.14230663]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.6294835413684328}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5993160331720393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06611643,  15.7735349 ,   0.91163933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.776355347941389}
episode index:3265
target Thresh 73.38732964027629
target distance 60.0
model initialize at round 3265
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.29670852, 16.74286348]), 'previousTarget': array([74.9972228 , 15.66671295]), 'currentState': array([55.31595037, 17.6199629 ,  2.3881788 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5843166891616117
running average episode reward sum: 0.5993114405988579
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82638735,  15.5995645 ,   5.59478167]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6241946331791762}
episode index:3266
target Thresh 73.38994100473617
target distance 62.0
model initialize at round 3266
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.67481229, 10.05959243]), 'previousTarget': array([72.79255473,  8.87311278]), 'currentState': array([52.80968292,  7.74083519,  0.885975  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.08566984777346531
running average episode reward sum: 0.5991542194195418
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56948259,  15.34423356,   5.44257687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5512186346641537}
episode index:3267
target Thresh 73.39254975913684
target distance 8.0
model initialize at round 3267
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.57216818,  23.42896162,   0.24142647]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.234771004900301}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.599256089409899
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98312564,  15.90128353,   4.85028343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9014414849363913}
episode index:3268
target Thresh 73.39515590608704
target distance 43.0
model initialize at round 3268
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.96251368, 16.66942468]), 'previousTarget': array([91.97840172, 16.07077201]), 'currentState': array([73.01965434, 18.18017329,  0.73385185]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5992923312095897
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.38267661,  14.93776393,   5.61072625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38770441497755065}
episode index:3269
target Thresh 73.39775944819294
target distance 9.0
model initialize at round 3269
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.28225365,   5.98035306,   2.20296836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.755801804045207}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5993912463053751
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20989314,  14.96783398,   0.76978731]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7907613410507451}
episode index:3270
target Thresh 73.40036038805808
target distance 3.0
model initialize at round 3270
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10912792,  16.33587754,   5.28010181]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6056842337079555}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.5995106620050678
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42889882,  14.8442533 ,   5.59653389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4563016900369692}
episode index:3271
target Thresh 73.40295872828338
target distance 7.0
model initialize at round 3271
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97915455,   9.70667499,   1.92359543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.293366059475415}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5996151759070837
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74460122,  14.37181015,   0.15425229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6781231610105786}
episode index:3272
target Thresh 73.4055544714672
target distance 68.0
model initialize at round 3272
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.30668399, 15.19271361]), 'previousTarget': array([66.99135509, 16.41201897]), 'currentState': array([47.30684726, 15.27352662,  5.77367449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 63
reward sum = 0.47964430707930533
running average episode reward sum: 0.5995785211961678
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.78687179,  14.63447297,   5.08168627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.867627349903859}
episode index:3273
target Thresh 73.40814762020526
target distance 20.0
model initialize at round 3273
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.1565257 ,  15.25304229]), 'currentState': array([96.66636047, 21.18375715,  1.11983078]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.34841571875078}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5996634150574394
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23611072,  14.7974705 ,   4.97571307]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7902816192361961}
episode index:3274
target Thresh 73.41073817709074
target distance 9.0
model initialize at round 3274
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.75766719,   7.68938211,   2.68287086]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.646776464680518}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5997592482887145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59799712,  15.21226487,   0.2022945 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6345525400062259}
episode index:3275
target Thresh 73.41332614471416
target distance 69.0
model initialize at round 3275
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.2485656 ,  7.38357574]), 'previousTarget': array([65.79321198,  7.86858145]), 'currentState': array([47.49821925,  4.23336635,  0.53703898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.37404108539166175
running average episode reward sum: 0.5996903477505896
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91507255,  15.00219377,   2.48961257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08495577599034797}
episode index:3276
target Thresh 73.41591152566353
target distance 69.0
model initialize at round 3276
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.60906075, 15.39310452]), 'previousTarget': array([65.99789993, 14.28982464]), 'currentState': array([46.60972063, 15.55556946,  2.12859043]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 82
reward sum = 0.32485980576037476
running average episode reward sum: 0.5996064812440317
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.57809791,  15.61954981,   0.19570895]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8473719138163134}
episode index:3277
target Thresh 73.4184943225242
target distance 64.0
model initialize at round 3277
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.5325877,  7.592402 ]), 'previousTarget': array([70.65744374,  6.6857707 ]), 'currentState': array([51.81683168,  4.23249656,  6.25869853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.5995775853952041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41387841,  15.63871827,   6.11832484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8668907375019591}
episode index:3278
target Thresh 73.42107453787898
target distance 4.0
model initialize at round 3278
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.32397451,  12.3805247 ,   0.33029246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1097768907523866}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.599693633707069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11747107,  15.23261006,   0.28901243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9126690227245573}
episode index:3279
target Thresh 73.42365217430809
target distance 13.0
model initialize at round 3279
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.73568101,   1.8958514 ,   0.82595104]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.504609913559483}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5997652252094297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29598955,  15.75948672,   0.87873778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0355919945684147}
episode index:3280
target Thresh 73.42622723438915
target distance 9.0
model initialize at round 3280
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.6489323 ,  17.15763373,   0.9848997 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.661173516654501}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.599872273312048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20036031,  14.97797371,   1.10148835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.20156738442057806}
episode index:3281
target Thresh 73.42879972069723
target distance 6.0
model initialize at round 3281
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.07654865,  19.44120114,   5.74987459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.630583744003396}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5999851394688694
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.329905  ,  15.76979305,   5.76682696]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0205923042142797}
episode index:3282
target Thresh 73.43136963580481
target distance 73.0
model initialize at round 3282
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([63.43443214, 16.41427508]), 'previousTarget': array([61.99812374, 15.7260531 ]), 'currentState': array([43.44195016, 16.96260355,  0.35755295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.5999724318840626
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2229029 ,  14.41654719,   0.36218391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9717494904210725}
episode index:3283
target Thresh 73.43393698228184
target distance 64.0
model initialize at round 3283
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.38796613,  9.93562766]), 'previousTarget': array([70.80513158,  8.78509663]), 'currentState': array([51.52146367,  7.62866427,  2.14184195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.40305485341908853
running average episode reward sum: 0.5999124691622401
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33632478,  15.06684337,   5.3771336 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3429028913939457}
episode index:3284
target Thresh 73.43650176269561
target distance 66.0
model initialize at round 3284
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.60000278, 11.17572685]), 'previousTarget': array([68.88845169, 10.10938124]), 'currentState': array([49.67058291,  9.49697025,  0.13964009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5633213100663098
running average episode reward sum: 0.5999013303010237
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7738614 ,  15.77548509,   5.71974856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.807784501008872}
episode index:3285
target Thresh 73.43906397961094
target distance 24.0
model initialize at round 3285
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.61718999,  16.85609007]), 'previousTarget': array([109.97366596,  16.67544468]), 'currentState': array([89.41269717, 22.44066307,  2.47038794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.599977884300048
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56043112,  14.81958988,   5.96051018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5887536413205716}
episode index:3286
target Thresh 73.44162363559003
target distance 66.0
model initialize at round 3286
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.55011656, 18.70979398]), 'previousTarget': array([68.91786413, 19.18928508]), 'currentState': array([50.61941073, 20.3732137 ,  0.54749077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.377429686655513
running average episode reward sum: 0.5999101787333778
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22681524,  14.53913171,   3.05294414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5136581850917372}
episode index:3287
target Thresh 73.44418073319255
target distance 22.0
model initialize at round 3287
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.44736818,  13.81169879]), 'previousTarget': array([111.51093912,  13.57265691]), 'currentState': array([92.48026418,  7.46749152,  2.9211778 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6514350782916389
running average episode reward sum: 0.5999258493232679
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07586289,  15.08703757,   5.64667991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11545872497173963}
episode index:3288
target Thresh 73.4467352749756
target distance 41.0
model initialize at round 3288
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.65198903,  9.17117215]), 'previousTarget': array([93.31685674,  9.18257132]), 'currentState': array([75.42528609,  3.6635507 ,  0.76346343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5188641069267779
running average episode reward sum: 0.5999012030045094
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0921822 ,  15.79420544,   5.65575128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2061904656894578}
episode index:3289
target Thresh 73.44928726349369
target distance 50.0
model initialize at round 3289
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.38684124, 11.06362994]), 'previousTarget': array([84.85753677, 11.38290441]), 'currentState': array([66.57345706,  8.33786264,  0.57054036]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5058554540625074
running average episode reward sum: 0.5998726176704845
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74598051,  15.73551357,   5.42165492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7781427315619281}
episode index:3290
target Thresh 73.45183670129884
target distance 18.0
model initialize at round 3290
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.11461515,  16.76460255]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 14.      ,  5.487935], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.324363812508132}
done in step count: 21
reward sum = 0.5469544555912016
running average episode reward sum: 0.5998565380101748
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.99792493,  15.80740877,   5.65469329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2836522502951528}
episode index:3291
target Thresh 73.45438359094048
target distance 45.0
model initialize at round 3291
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.14139829,  8.51794864]), 'previousTarget': array([89.21428366,  7.55079306]), 'currentState': array([70.78852029,  3.4715526 ,  2.0888876 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5490702985127359
running average episode reward sum: 0.5998411108414332
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82365004,  14.53660623,   3.36109013]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4958155832085561}
episode index:3292
target Thresh 73.45692793496549
target distance 62.0
model initialize at round 3292
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.49645797, 12.37870543]), 'previousTarget': array([72.97662792, 12.96661103]), 'currentState': array([51.53266574, 11.17579234,  2.6433773 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.5998445351137355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39796583,  15.16412641,   4.93414121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43048145136526206}
episode index:3293
target Thresh 73.45946973591823
target distance 63.0
model initialize at round 3293
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.11380681,  5.65789604]), 'previousTarget': array([71.58733278,  6.04183057]), 'currentState': array([50.53339867,  1.58265162,  2.40883923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.30635850185584856
running average episode reward sum: 0.5997554379573123
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75344403,  15.7737646 ,   5.0790432 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.079995168394146}
episode index:3294
target Thresh 73.4620089963405
target distance 7.0
model initialize at round 3294
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.10835392e+02, 9.36778664e+00, 3.09324265e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.0046974327320894}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5998562907372667
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77340486,  15.71806927,   5.98761243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7529733317229673}
episode index:3295
target Thresh 73.46454571877155
target distance 35.0
model initialize at round 3295
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.99361493, 14.4946665 ]), 'previousTarget': array([100.,  15.]), 'currentState': array([80.       , 15.       ,  5.8211718], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6623305895390459
running average episode reward sum: 0.5998752453182138
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14269616,  15.2624248 ,   5.49405836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29871218378741354}
episode index:3296
target Thresh 73.46707990574812
target distance 56.0
model initialize at round 3296
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.84123289,  9.51505017]), 'previousTarget': array([78.79898987,  9.82842712]), 'currentState': array([59.       ,  7.       ,  1.6925696], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 62
reward sum = 0.3605594941734369
running average episode reward sum: 0.599802659406432
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40322834,  15.78267876,   0.420539  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8804425765704744}
episode index:3297
target Thresh 73.46961155980436
target distance 38.0
model initialize at round 3297
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.48719969, 17.50010575]), 'previousTarget': array([96.66906377, 18.37675141]), 'currentState': array([77.       , 22.       ,  2.1030872], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.45437504098398346
running average episode reward sum: 0.5997585637064857
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82729891,  15.55418993,   5.67938489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9957660220889933}
episode index:3298
target Thresh 73.47214068347196
target distance 1.0
model initialize at round 3298
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17447873,  15.1834303 ,   3.12118578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25315904028577463}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.599879885754468
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17447873,  15.1834303 ,   3.12118578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.25315904028577463}
episode index:3299
target Thresh 73.47466727928004
target distance 46.0
model initialize at round 3299
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.4258435 , 11.82335933]), 'previousTarget': array([88.77237675, 11.00883994]), 'currentState': array([69.57836928,  9.35804659,  2.14337844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.5999155980716935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17008822,  14.82167678,   5.16626474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24643289954333128}
episode index:3300
target Thresh 73.47719134975517
target distance 51.0
model initialize at round 3300
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.58038  , 20.3479596]), 'previousTarget': array([83.75839053, 19.90064462]), 'currentState': array([6.48824734e+01, 2.38109749e+01, 8.83913040e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.30375270296657325
running average episode reward sum: 0.5998258789274629
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41771727,  15.80394455,   4.86971128]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9059881717608268}
episode index:3301
target Thresh 73.47971289742146
target distance 44.0
model initialize at round 3301
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.01235886, 10.33876269]), 'previousTarget': array([90.6773982 , 10.57770876]), 'currentState': array([72.41126172,  6.36421982,  0.58672636]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.44019506023053506
running average episode reward sum: 0.5997775352512978
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59840581,  14.34126483,   5.93450273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7714984898435706}
episode index:3302
target Thresh 73.48223192480044
target distance 25.0
model initialize at round 3302
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.19335973,  15.80399424]), 'previousTarget': array([109.61161351,  16.0776773 ]), 'currentState': array([91.62505873, 19.93698299,  5.24218762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.02180811670824989
running average episode reward sum: 0.5996025520788658
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45048743,  15.8586426 ,   5.00106292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0194268879380413}
episode index:3303
target Thresh 73.48474843441112
target distance 69.0
model initialize at round 3303
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.77027207,  8.27227676]), 'previousTarget': array([65.86691472,  9.3034104 ]), 'currentState': array([46.9620614 ,  5.50916437,  6.13129926]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.32987726187170074
running average episode reward sum: 0.5995209160951469
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89901549,  14.56748281,   6.22247073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4441497393536735}
episode index:3304
target Thresh 73.48726242877002
target distance 4.0
model initialize at round 3304
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.42007844,  18.09680272,   4.70670122]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4765412782793224}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.599636068616752
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97566993,  15.48893026,   6.16772741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0913224105460875}
episode index:3305
target Thresh 73.48977391039116
target distance 32.0
model initialize at round 3305
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.07419191,  10.1121417 ]), 'previousTarget': array([101.52933154,   9.52754094]), 'currentState': array([82.20287392,  3.48844505,  1.06170297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5248280631108624
running average episode reward sum: 0.5996134406659033
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20984641,  15.43735592,   5.36313435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9031184262529571}
episode index:3306
target Thresh 73.492282881786
target distance 24.0
model initialize at round 3306
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.1639703,  15.6222242]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([92.62862352, 19.90827437,  1.01632488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.5996948233003463
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3409993 ,  15.22717319,   5.23524107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.697057806890346}
episode index:3307
target Thresh 73.4947893454635
target distance 47.0
model initialize at round 3307
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.44159106, 13.07881376]), 'previousTarget': array([87.92796014, 12.69599661]), 'currentState': array([66.48669349, 11.73640443,  1.67870665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.5997240610254821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61365479,  15.96863727,   5.09579271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1466605243103623}
episode index:3308
target Thresh 73.49729330393014
target distance 7.0
model initialize at round 3308
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67555515,  20.3734729 ,   5.5255022 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.383258816473339}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.599833118731428
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3678242 ,  14.68011057,   3.12434186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4874668085760017}
episode index:3309
target Thresh 73.49979475968986
target distance 8.0
model initialize at round 3309
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.08431448,  22.35001275,   5.0475095 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.59556044119189}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5999421105414788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01520209,  14.84409661,   5.46930608]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1566428120610395}
episode index:3310
target Thresh 73.50229371524415
target distance 28.0
model initialize at round 3310
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.794422  ,  12.80520577]), 'previousTarget': array([106.04057123,  12.12018361]), 'currentState': array([87.47362118,  7.63735748,  0.31582642]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.599993485061341
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89491142,  15.42506512,   5.23463947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9907304434454396}
episode index:3311
target Thresh 73.50479017309195
target distance 8.0
model initialize at round 3311
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.1724648 ,  19.79264157,   0.20994061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.341741432955892}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6000909340979854
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64534432,  14.75287316,   1.79713957]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43226418457742477}
episode index:3312
target Thresh 73.5072841357297
target distance 7.0
model initialize at round 3312
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99202396,  20.31704888,   5.7176497 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.317054863787585}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6001591736662732
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42010795,  15.77135702,   0.95653533]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9650214734137534}
episode index:3313
target Thresh 73.5097756056514
target distance 57.0
model initialize at round 3313
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([77.77150188, 19.98541655]), 'previousTarget': array([77.80587954, 20.22022743]), 'currentState': array([58.       , 23.       ,  1.2566656], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = -0.061665440432857044
running average episode reward sum: 0.599959467989116
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05930799,  15.49626347,   4.9105924 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0635689445851835}
episode index:3314
target Thresh 73.51226458534848
target distance 42.0
model initialize at round 3314
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.84943427,  9.09564512]), 'previousTarget': array([92.34744447,  9.06718784]), 'currentState': array([71.42161805,  4.34591284,  1.94902384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.3773296651109875
running average episode reward sum: 0.5998923096775389
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55179216,  14.94799884,   6.2173016 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5542370489718353}
episode index:3315
target Thresh 73.51475107730994
target distance 8.0
model initialize at round 3315
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.96510838,   8.32717271,   3.24308062]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.976202972452525}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.5999953216919308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68081167,  15.56366891,   0.66148764]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6477683496228737}
episode index:3316
target Thresh 73.51723508402227
target distance 10.0
model initialize at round 3316
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.80592021,  21.97130081,   0.76600498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.75834460046094}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6000043142483105
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53035881,  14.62140247,   5.06839253]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.651626089253428}
episode index:3317
target Thresh 73.51971660796949
target distance 45.0
model initialize at round 3317
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.2559036, 18.2604482]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([71.44183557, 20.98123999,  1.5004993 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5449941209901132
running average episode reward sum: 0.5999877349254479
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20363348,  15.81023664,   5.08850792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1360823218003973}
episode index:3318
target Thresh 73.5221956516331
target distance 45.0
model initialize at round 3318
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.17436747, 15.65779058]), 'previousTarget': array([90., 15.]), 'currentState': array([70.18138439, 16.18753322,  2.42922729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6689966244037812
running average episode reward sum: 0.6000085269982042
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40150582,  15.72455216,   1.04939431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8283614904744032}
episode index:3319
target Thresh 73.52467221749215
target distance 66.0
model initialize at round 3319
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.52061091,  9.29447642]), 'previousTarget': array([68.85467564,  9.40662735]), 'currentState': array([50.68314855,  6.74985718,  0.81144017]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.2910343167039194
running average episode reward sum: 0.5999154624770312
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76155883,  15.82417496,   4.67405452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8579735142116351}
episode index:3320
target Thresh 73.52714630802322
target distance 44.0
model initialize at round 3320
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.86951754, 19.74190855]), 'previousTarget': array([90.6773982 , 19.42229124]), 'currentState': array([72.27699584, 23.75851073,  6.00418157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.5999690324550264
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75938547,  14.65068319,   5.06783197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4241669353343607}
episode index:3321
target Thresh 73.52961792570039
target distance 10.0
model initialize at round 3321
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.65080122,   9.58465795,   0.22267997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.804325398869508}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6000552503476396
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82575052,  15.12619094,   5.33732645]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21514422168430358}
episode index:3322
target Thresh 73.53208707299528
target distance 64.0
model initialize at round 3322
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.95967491, 17.28115097]), 'previousTarget': array([70.93924283, 18.44224665]), 'currentState': array([51.98770621, 18.33967168,  5.70792627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6000531170933675
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71929715,  15.53024918,   5.82524428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8936176903250264}
episode index:3323
target Thresh 73.53455375237704
target distance 59.0
model initialize at round 3323
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.28539773,  6.94587098]), 'previousTarget': array([75.66120598,  7.66564857]), 'currentState': array([54.66559903,  3.0646986 ,  2.74293232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.5416850759668536
running average episode reward sum: 0.6000355575142079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.01639352,  14.07674486,   3.78815331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3490299402541592}
episode index:3324
target Thresh 73.53701796631235
target distance 68.0
model initialize at round 3324
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.1970895 ,  8.27645719]), 'previousTarget': array([66.78718271,  7.90987981]), 'currentState': array([45.37689352,  5.60067107,  1.85991263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6000316481080193
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6070904 ,  14.93273819,   0.14827698]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.398625266029149}
episode index:3325
target Thresh 73.53947971726541
target distance 26.0
model initialize at round 3325
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.78139238,  13.71597967]), 'previousTarget': array([108.31231517,  13.19946947]), 'currentState': array([89.19456819,  9.67168517,  0.44492722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6000606263310926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76565803,  15.73329784,   5.93267198]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0601688241927052}
episode index:3326
target Thresh 73.54193900769799
target distance 68.0
model initialize at round 3326
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.65832568, 19.59985851]), 'previousTarget': array([66.94615251, 18.53337114]), 'currentState': array([46.74825652, 21.49436263,  0.72769141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5993639024593417
running average episode reward sum: 0.6000604169160425
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42172581,  14.53471137,   5.44272795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7422227102872033}
episode index:3327
target Thresh 73.54439584006937
target distance 62.0
model initialize at round 3327
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.49841215, 20.03302719]), 'previousTarget': array([72.83555733, 20.44057325]), 'currentState': array([54.65107023, 22.49940485,  0.6425082 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5225665762266005
running average episode reward sum: 0.6000371315071815
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89376837,  15.121959  ,   5.0701282 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16173792485523833}
episode index:3328
target Thresh 73.54685021683639
target distance 41.0
model initialize at round 3328
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.28819328,  7.28825112]), 'previousTarget': array([93.06461275,  8.04487721]), 'currentState': array([74.       ,  2.       ,  2.8466222], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6057290490602831
running average episode reward sum: 0.6000388413051848
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31365984,  15.14000647,   4.05633149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3434884369566632}
episode index:3329
target Thresh 73.5493021404534
target distance 8.0
model initialize at round 3329
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.47311742,  21.29170889,   2.44892948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.416965114971182}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6001357499697861
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60152452,  14.87590455,   5.03591983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4173516399503128}
episode index:3330
target Thresh 73.55175161337237
target distance 16.0
model initialize at round 3330
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.1876047 ,  13.71078091]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.       ,  6.       ,  1.5856632], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.837990808500734}
done in step count: 34
reward sum = 0.503332297272292
running average episode reward sum: 0.6001066885910117
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72458302,  14.21750278,   3.77992309]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8295519361953558}
episode index:3331
target Thresh 73.55419863804275
target distance 33.0
model initialize at round 3331
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.99311718,  14.47534261]), 'previousTarget': array([102.,  15.]), 'currentState': array([82.       , 15.       ,  5.8444943], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6549803359578534
running average episode reward sum: 0.6001231572726944
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15189056,  15.79299781,   5.99206127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1610922243152686}
episode index:3332
target Thresh 73.55664321691155
target distance 46.0
model initialize at round 3332
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.06731627, 16.97348148]), 'previousTarget': array([88.98112317, 16.13125551]), 'currentState': array([68.12079268, 18.4350549 ,  1.11047602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.40358850990114314
running average episode reward sum: 0.6000641909818538
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10057431,  15.37350283,   4.02628512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9738947237913623}
episode index:3333
target Thresh 73.55908535242338
target distance 48.0
model initialize at round 3333
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.74881523,  9.17679548]), 'previousTarget': array([86.49464637,  8.46752313]), 'currentState': array([68.1903682 ,  4.99742354,  1.66647964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6000693577334917
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14465782,  14.29802977,   4.46225892]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1065136511675704}
episode index:3334
target Thresh 73.56152504702035
target distance 26.0
model initialize at round 3334
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.54570116,  12.77436457]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([89.6382552 ,  6.25451127,  0.09015894]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6001180154116367
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43825095,  15.57378887,   5.73591096]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8029916992402899}
episode index:3335
target Thresh 73.56396230314216
target distance 43.0
model initialize at round 3335
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.98348577, 10.65613347]), 'previousTarget': array([91.66260099, 10.65815832]), 'currentState': array([70.27834299,  7.23453327,  2.09138417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5642892164329874
running average episode reward sum: 0.6001072753639812
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33092371,  14.260115  ,   0.24810349]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8105185460606326}
episode index:3336
target Thresh 73.56639712322608
target distance 27.0
model initialize at round 3336
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.5363752,  14.4662886]), 'previousTarget': array([107.87767469,  14.20863052]), 'currentState': array([89.63112027, 12.52185394,  1.26899224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.600155892516808
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14425211,  15.6573214 ,   5.1939483 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0790624992064712}
episode index:3337
target Thresh 73.56882950970692
target distance 29.0
model initialize at round 3337
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.12408656,  14.8146007 ]), 'previousTarget': array([105.95260657,  14.37604183]), 'currentState': array([85.12760984, 14.43920933,  1.10573244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.5496570181463355
running average episode reward sum: 0.6001407640343722
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5576717 ,  15.73201188,   0.26542474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8552752265562926}
episode index:3338
target Thresh 73.57125946501705
target distance 28.0
model initialize at round 3338
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.97236784,  15.48816659]), 'previousTarget': array([107.,  15.]), 'currentState': array([87.00924511, 16.7021397 ,  0.58797455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6001939777496657
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71128129,  14.96357296,   4.9241978 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2910075986336808}
episode index:3339
target Thresh 73.57368699158646
target distance 4.0
model initialize at round 3339
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.60105697,  10.30179583,   0.48689359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.9020571229922005}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6002425252755931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08421488,  15.95367938,   5.74049914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9573904702048776}
episode index:3340
target Thresh 73.57611209184266
target distance 44.0
model initialize at round 3340
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.11800892, 17.87594705]), 'previousTarget': array([90.8721051 , 17.74180624]), 'currentState': array([72.27413147, 20.37004561,  5.55891794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.3113531509261926
running average episode reward sum: 0.6001560573395411
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47151172,  14.92682525,   3.48519539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5335301344714668}
episode index:3341
target Thresh 73.57853476821074
target distance 70.0
model initialize at round 3341
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.56659533, 12.4537866 ]), 'previousTarget': array([64.99184173, 13.57119548]), 'currentState': array([45.59307335, 11.4249914 ,  6.08252001]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.1220785737483987
running average episode reward sum: 0.6000130060278742
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49547218,  14.58451793,   3.44090271]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6466204701866362}
episode index:3342
target Thresh 73.58095502311339
target distance 50.0
model initialize at round 3342
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.45339531, 10.67711064]), 'previousTarget': array([84.80683493, 10.77295689]), 'currentState': array([66.67884379,  7.68259696,  0.77943211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6000219233551777
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80818679,  15.08751341,   1.73355741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.812911114829813}
episode index:3343
target Thresh 73.58337285897086
target distance 4.0
model initialize at round 3343
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.1279626 ,  13.16474025,   1.72810191]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.4083393625749294}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6001212186376393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27531489,  15.43356955,   5.5950945 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.844482714021148}
episode index:3344
target Thresh 73.58578827820098
target distance 28.0
model initialize at round 3344
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.47510948,  16.52169881]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([88.99777305, 21.06409528,  6.25467406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6001790640980298
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16382655,  14.15830566,   5.77071061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1864381150271839}
episode index:3345
target Thresh 73.58820128321919
target distance 31.0
model initialize at round 3345
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.45159087,  15.08222421]), 'previousTarget': array([103.98960229,  15.35517412]), 'currentState': array([85.45233237, 15.25444381,  0.527888  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6002368749825323
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98297837,  15.97043735,   5.34409508]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9705866231287348}
episode index:3346
target Thresh 73.59061187643847
target distance 70.0
model initialize at round 3346
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.48969265, 10.35197648]), 'previousTarget': array([64.9007438 ,  9.99007438]), 'currentState': array([46.5808708 ,  8.44440926,  1.28404396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.39196773897141785
running average episode reward sum: 0.6001746493667537
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82085876,  14.62238114,   5.19811473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41795643895944806}
episode index:3347
target Thresh 73.59302006026944
target distance 27.0
model initialize at round 3347
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.16570715,  17.80301973]), 'previousTarget': array([107.17596225,  17.31823341]), 'currentState': array([87.10227754, 23.85162714,  1.13469052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6002185554136368
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5001563 ,  14.71597994,   4.46685151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.57490096424296}
episode index:3348
target Thresh 73.59542583712027
target distance 44.0
model initialize at round 3348
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.38387141, 10.67845303]), 'previousTarget': array([90.6773982 , 10.57770876]), 'currentState': array([72.73929281,  6.92471733,  0.81647223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5184425610894113
running average episode reward sum: 0.6001941373800972
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54593717,  15.65015524,   6.17934079]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7930163267046025}
episode index:3349
target Thresh 73.59782920939674
target distance 66.0
model initialize at round 3349
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.16338457, 13.23505061]), 'previousTarget': array([68.96336993, 12.20990121]), 'currentState': array([49.17819463, 12.46551628,  2.24300265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3626355469960265
running average episode reward sum: 0.6001232243680422
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91537692,  14.73030919,   4.41240282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.282655620041008}
episode index:3350
target Thresh 73.6002301795022
target distance 61.0
model initialize at round 3350
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.83878491, 11.53428752]), 'previousTarget': array([73.90394822, 10.9577654 ]), 'currentState': array([54.       ,  9.       ,  5.5937567], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.021870265798054234
running average episode reward sum: 0.5999506630554281
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31490813,  15.60011578,   3.72729813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9107633165340776}
episode index:3351
target Thresh 73.60262874983765
target distance 13.0
model initialize at round 3351
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.26370387,   2.40772364,   1.32209748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.134889895871071}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6000256950089005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12069659,  15.81150221,   0.3240905 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.196540986786756}
episode index:3352
target Thresh 73.60502492280165
target distance 56.0
model initialize at round 3352
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.8739568 , 10.24183878]), 'previousTarget': array([78.84555753, 10.48069469]), 'currentState': array([59.       ,  8.       ,  1.8710847], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.386296487525562
running average episode reward sum: 0.59996195232847
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53065667,  15.98072559,   5.23650569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1150870724223392}
episode index:3353
target Thresh 73.60741870079038
target distance 23.0
model initialize at round 3353
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.57105213,  15.03894208]), 'previousTarget': array([112.,  15.]), 'currentState': array([93.57847486, 15.58378523,  1.36424034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.14642039814869712
running average episode reward sum: 0.5998267282514934
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29069588,  15.86494771,   5.99923375]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.118591468638661}
episode index:3354
target Thresh 73.6098100861976
target distance 48.0
model initialize at round 3354
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.52728754, 17.10923505]), 'previousTarget': array([86.89236818, 17.92787831]), 'currentState': array([67.58597321, 18.6402422 ,  6.08662486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.5998394808219355
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64127146,  15.74953513,   4.99236611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8309567295224458}
episode index:3355
target Thresh 73.6121990814147
target distance 47.0
model initialize at round 3355
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.9136358 ,  9.55230113]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([69.33598363,  5.46383741,  1.33442837]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.41159299896789503
running average episode reward sum: 0.5997833883064844
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08021893,  14.69112506,   0.30970728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31912192627435615}
episode index:3356
target Thresh 73.6145856888307
target distance 62.0
model initialize at round 3356
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.4910432 , 20.61833674]), 'previousTarget': array([72.83555733, 20.44057325]), 'currentState': array([54.68067119, 23.36591036,  0.11336827]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5284710167909834
running average episode reward sum: 0.5997621454195271
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54913748,  14.67948126,   4.08572415]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6358334946150274}
episode index:3357
target Thresh 73.61696991083218
target distance 25.0
model initialize at round 3357
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.50465852,  13.52314093]), 'previousTarget': array([109.04848294,  13.09551454]), 'currentState': array([91.5037958 ,  7.28076832,  1.19458121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.5997951385945876
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76235461,  15.35411043,   0.27613633]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4264616386865702}
episode index:3358
target Thresh 73.61935174980337
target distance 15.0
model initialize at round 3358
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.27261907,  16.35354727]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,  11.       ,   4.4292536], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.243625669519624}
done in step count: 18
reward sum = 0.6952137614500875
running average episode reward sum: 0.5998235454486678
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91482584,  15.64920231,   0.57489478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.654765815933574}
episode index:3359
target Thresh 73.62173120812612
target distance 7.0
model initialize at round 3359
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48300386,  21.19493717,   4.5169903 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.216472597852585}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5999338060006176
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36688065,  15.2400829 ,   4.5453185 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.677111444732869}
episode index:3360
target Thresh 73.62410828817988
target distance 72.0
model initialize at round 3360
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.5902298 , 19.20386817]), 'previousTarget': array([62.93091516, 19.3390904 ]), 'currentState': array([44.6594146 , 20.86597691,  5.19259495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.3073078405074248
running average episode reward sum: 0.5998467408517056
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23264396,  15.6910632 ,   1.15795489]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0326682152390894}
episode index:3361
target Thresh 73.62648299234172
target distance 41.0
model initialize at round 3361
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.52111954, 16.7404774 ]), 'previousTarget': array([93.94667447, 16.54048723]), 'currentState': array([75.60048248, 18.52042665,  1.32438439]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7260760968305775
running average episode reward sum: 0.5998842867636566
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.10664231,  15.0560413 ,   5.0562665 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.12047078211227402}
episode index:3362
target Thresh 73.62885532298637
target distance 10.0
model initialize at round 3362
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.61882122,   5.63304909,   1.71840298]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.664873596086277}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.599974830262986
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15327089,  14.90413079,   4.65476269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8521391305001027}
episode index:3363
target Thresh 73.63122528248614
target distance 33.0
model initialize at round 3363
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.23237468,  10.87038997]), 'previousTarget': array([100.97366596,  10.32455532]), 'currentState': array([83.20300758,  4.715458  ,  1.54653138]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6000014278419414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29357154,  14.50789832,   3.66217542]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8609327675725753}
episode index:3364
target Thresh 73.63359287321099
target distance 66.0
model initialize at round 3364
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.1313619 ,  6.55601712]), 'previousTarget': array([68.6773982 ,  6.57770876]), 'currentState': array([47.43544968,  3.08167667,  2.18654752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5056489911426563
running average episode reward sum: 0.5999733884848242
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16105336,  15.06563881,   5.69249394]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8415105004875815}
episode index:3365
target Thresh 73.63595809752853
target distance 25.0
model initialize at round 3365
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.15272195,  12.99318882]), 'previousTarget': array([108.56953382,  12.42781353]), 'currentState': array([91.67378763,  5.34276489,  1.16838437]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6000239142594291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60484872,  15.47592647,   5.35197109]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6185875361879012}
episode index:3366
target Thresh 73.63832095780396
target distance 47.0
model initialize at round 3366
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.30532576, 17.36466178]), 'previousTarget': array([87.95938166, 16.72599692]), 'currentState': array([69.38948539, 19.19750178,  1.43258878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.24171512053504585
running average episode reward sum: 0.5999174964412751
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51989653,  15.6302866 ,   4.32042204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7923134074346738}
episode index:3367
target Thresh 73.64068145640013
target distance 64.0
model initialize at round 3367
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.39147255, 11.48347994]), 'previousTarget': array([70.91268452, 10.86681417]), 'currentState': array([52.45924004,  9.83845487,  1.53152102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.5998954296334023
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07934756,  14.22418175,   4.90626561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.203949700997016}
episode index:3368
target Thresh 73.64303959567758
target distance 23.0
model initialize at round 3368
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.88736396,  15.36505974]), 'previousTarget': array([111.98112317,  15.13125551]), 'currentState': array([92.02351431, 17.69475444,  0.56711876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.15816941371966386
running average episode reward sum: 0.5997643147577971
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91647396,  14.76351221,   5.08019992]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2508048475281402}
episode index:3369
target Thresh 73.64539537799443
target distance 18.0
model initialize at round 3369
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.86281309,  13.75386139]), 'previousTarget': array([113.64100589,  14.09400392]), 'currentState': array([97.       ,  3.       ,  2.5040247], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 36
reward sum = 0.6264132180495734
running average episode reward sum: 0.5997722224442339
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72980358,  15.03567137,   4.64239273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27254092115322553}
episode index:3370
target Thresh 73.64774880570647
target distance 8.0
model initialize at round 3370
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.37016962,  21.56195554,   5.48274684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.498995201510585}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5998792600554934
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0507975 ,  15.00848109,   5.539471  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.05150062736472611}
episode index:3371
target Thresh 73.65009988116711
target distance 46.0
model initialize at round 3371
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.78856487, 17.27106648]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.87597549, 19.13889522,  5.74836529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.5999251793992503
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78964728,  15.66968755,   5.93159637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7019470619290279}
episode index:3372
target Thresh 73.65244860672743
target distance 13.0
model initialize at round 3372
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.86440897,  13.82423463]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,  15.      ,   6.095963], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.922525920992516}
done in step count: 8
reward sum = 0.7834446944279201
running average episode reward sum: 0.5999795877938631
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.3286165 ,  15.99814108,   4.96855095]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0508446163293532}
episode index:3373
target Thresh 73.65479498473617
target distance 46.0
model initialize at round 3373
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.63144943,  9.74181671]), 'previousTarget': array([88.45157783,  8.65146426]), 'currentState': array([70.04770005,  5.68265916,  0.78000709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.5999865663970336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11642525,  15.41041636,   0.89816163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9742412084741408}
episode index:3374
target Thresh 73.65713901753969
target distance 8.0
model initialize at round 3374
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.89423367,  19.42026558,   6.22791407]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.537846530618921}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6000905674291827
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09259817,  15.87233206,   1.78037634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2587061982062897}
episode index:3375
target Thresh 73.65948070748205
target distance 31.0
model initialize at round 3375
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([102.15879092,  10.38202317]), 'previousTarget': array([102.44388764,   9.73453353]), 'currentState': array([84.      ,  2.      ,  4.778302], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.30603297766840215
running average episode reward sum: 0.6000034650625473
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.63716982,  14.7763496 ,   0.58708459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6752813333954529}
episode index:3376
target Thresh 73.6618200569049
target distance 4.0
model initialize at round 3376
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.00563397,  17.64219873,   5.09028935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.789172591436739}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6001131172789931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36124903,  15.07400631,   4.33299702]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6430239034368801}
episode index:3377
target Thresh 73.66415706814763
target distance 13.0
model initialize at round 3377
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61089278,   2.21894165,   1.14508551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.786979977746146}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.600200513707939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00100923,  15.33724692,   1.1070112 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0543804068860556}
episode index:3378
target Thresh 73.66649174354723
target distance 40.0
model initialize at round 3378
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.69870015, 19.36831578]), 'previousTarget': array([94.61161351, 19.0776773 ]), 'currentState': array([76.19204466, 23.78310814,  6.11136812]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6002310692513814
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.8036519 ,  15.69910115,   4.95106431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.726150801624817}
episode index:3379
target Thresh 73.66882408543837
target distance 11.0
model initialize at round 3379
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.78971269,   4.96899161,   1.71394056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.723015045488113}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6002859352488831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07908661,  15.44603365,   0.8448873 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.023243611098358}
episode index:3380
target Thresh 73.6711540961534
target distance 63.0
model initialize at round 3380
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.45788462, 19.55059439]), 'previousTarget': array([71.90990945, 19.10381815]), 'currentState': array([5.35768093e+01, 2.17284014e+01, 8.75842571e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5542253948911999
running average episode reward sum: 0.6002723119006553
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49753286,  15.79742374,   4.02938132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.942527369237494}
episode index:3381
target Thresh 73.67348177802235
target distance 46.0
model initialize at round 3381
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.56658679, 17.68789775]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.68651933, 19.87488604,  6.20409567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 52
reward sum = 0.42294894624701607
running average episode reward sum: 0.6002198803910002
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30849866,  14.00753114,   1.94406099]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.039310277827974}
episode index:3382
target Thresh 73.67580713337287
target distance 28.0
model initialize at round 3382
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.78636802,  17.07752641]), 'previousTarget': array([106.40285  ,  17.1492875]), 'currentState': array([88.56753605, 22.61255107,  5.64571232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6002842277327697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86344836,  14.33569335,   5.44041874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6781959012952017}
episode index:3383
target Thresh 73.67813016453033
target distance 19.0
model initialize at round 3383
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.75279854,  14.13479664]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 11.       ,  1.7099286], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6697003733882803
running average episode reward sum: 0.6003047407781761
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77362642,  14.48456627,   3.79883707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5629537498852768}
episode index:3384
target Thresh 73.68045087381776
target distance 64.0
model initialize at round 3384
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.41706321, 11.25015486]), 'previousTarget': array([70.91268452, 10.86681417]), 'currentState': array([49.48439568,  9.61040934,  1.78898799]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.39577205431773943
running average episode reward sum: 0.6002443175325453
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93247949,  15.83318699,   4.70659358]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8359184007284681}
episode index:3385
target Thresh 73.68276926355587
target distance 12.0
model initialize at round 3385
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.23779243,  16.41075943]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  15.       ,   3.5554023], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.318839461908677}
done in step count: 9
reward sum = 0.7742172474836408
running average episode reward sum: 0.6002956976063643
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1823831 ,  15.68076207,   0.30627659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.063924047855491}
episode index:3386
target Thresh 73.68508533606305
target distance 32.0
model initialize at round 3386
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.63638422,  11.29683557]), 'previousTarget': array([102.40285  ,  11.8507125]), 'currentState': array([83.47733533,  5.5582952 ,  6.04211998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7163781408072187
running average episode reward sum: 0.6003299705450122
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08034642,  14.64955728,   5.78119488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.359535322793583}
episode index:3387
target Thresh 73.68739909365539
target distance 3.0
model initialize at round 3387
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.91135066,  17.2536038 ,   5.08392443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.6816698312065443}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6004091960002141
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61164049,  15.11741905,   2.53014431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6228092217121729}
episode index:3388
target Thresh 73.68971053864662
target distance 70.0
model initialize at round 3388
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.95188073,  7.068489  ]), 'previousTarget': array([64.7124451,  6.3792763]), 'currentState': array([46.20835869,  3.87578835,  5.9003461 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = -0.12390095156184083
running average episode reward sum: 0.6001954721443387
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37501452,  15.28849061,   5.48536875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6883557824051905}
episode index:3389
target Thresh 73.6920196733482
target distance 30.0
model initialize at round 3389
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.00727823,  16.89132726]), 'previousTarget': array([104.61161351,  17.0776773 ]), 'currentState': array([86.43545926, 21.00762439,  1.01531142]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6002670791417545
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26137555,  15.82664134,   5.76043148]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1085585127508673}
episode index:3390
target Thresh 73.69432650006925
target distance 66.0
model initialize at round 3390
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.82966439,  6.79977687]), 'previousTarget': array([68.62296546,  5.86512956]), 'currentState': array([50.15130348,  3.22736705,  1.82736462]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6002580358280817
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15844358,  14.48910244,   5.29994127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9844965817162304}
episode index:3391
target Thresh 73.69663102111662
target distance 59.0
model initialize at round 3391
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.78171378, 19.19142897]), 'previousTarget': array([75.9285661 , 18.31113847]), 'currentState': array([54.88944879, 21.26454106,  1.28144979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6002763212069602
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44853653,  15.82485535,   5.52643921]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9922188848468746}
episode index:3392
target Thresh 73.69893323879481
target distance 58.0
model initialize at round 3392
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.24093607,  9.14472559]), 'previousTarget': array([76.76347807,  9.0667466 ]), 'currentState': array([55.45435218,  6.23077674,  1.98338008]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = -0.07224707438470607
running average episode reward sum: 0.6000781121307469
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39694964,  15.06361378,   5.45162924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6063962838348542}
episode index:3393
target Thresh 73.70123315540604
target distance 25.0
model initialize at round 3393
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.91674705,  13.2339127 ]), 'previousTarget': array([108.81774824,  12.77438937]), 'currentState': array([89.70980697,  7.65776237,  0.73409152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6001259213830205
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55493222,  14.92703595,   5.50751833]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5597084276416544}
episode index:3394
target Thresh 73.70353077325025
target distance 14.0
model initialize at round 3394
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.98320052,  13.5934446 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.010000e+02, 3.000000e+00, 4.466678e-02], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.756627474743855}
done in step count: 46
reward sum = -0.09371818361099182
running average episode reward sum: 0.5999215490398705
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50235266,  14.53910054,   4.45259119]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6782928488156823}
episode index:3395
target Thresh 73.70582609462504
target distance 50.0
model initialize at round 3395
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.70328154, 14.48798939]), 'previousTarget': array([84.9960012 , 14.39992002]), 'currentState': array([66.70655479, 14.12616171,  5.39526564]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.1704503378183877
running average episode reward sum: 0.5997950851967546
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09991699,  14.63878603,   4.74926379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37477852103013104}
episode index:3396
target Thresh 73.70811912182572
target distance 58.0
model initialize at round 3396
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.93569166, 16.58641681]), 'previousTarget': array([76.99702801, 15.65522365]), 'currentState': array([55.95216333, 17.3979557 ,  3.11140478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4465013640761777
running average episode reward sum: 0.5997499589909493
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43460975,  14.37220853,   5.21482148]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7635494500080454}
episode index:3397
target Thresh 73.71040985714534
target distance 4.0
model initialize at round 3397
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.50665712,  16.4835913 ,   0.67979735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.901344832485021}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.5998618924933063
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06473616,  14.68912788,   5.65496445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3175409376782468}
episode index:3398
target Thresh 73.71269830287463
target distance 44.0
model initialize at round 3398
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.04407607, 14.06959506]), 'previousTarget': array([90.95367385, 13.36047776]), 'currentState': array([70.05796102, 13.32447418,  3.13746524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.47438193628993347
running average episode reward sum: 0.5998249757659738
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44615331,  14.88950721,   5.35304235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5647608426049686}
episode index:3399
target Thresh 73.71498446130202
target distance 62.0
model initialize at round 3399
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.68873039, 20.7787078 ]), 'previousTarget': array([72.83555733, 20.44057325]), 'currentState': array([54.89111489, 23.61673886,  0.1327984 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.471049463239577
running average episode reward sum: 0.5997871006152308
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69144321,  15.40780391,   0.43176953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.511381780597249}
episode index:3400
target Thresh 73.7172683347137
target distance 60.0
model initialize at round 3400
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.25540876, 11.12889991]), 'previousTarget': array([74.9007438 , 10.99007438]), 'currentState': array([53.34085213,  9.28216384,  1.96313882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.5997959322913813
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12533865,  15.66337292,   1.18180904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6751099272243836}
episode index:3401
target Thresh 73.71954992539351
target distance 46.0
model initialize at round 3401
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.72284024, 12.57212583]), 'previousTarget': array([88.95760212, 13.30158275]), 'currentState': array([69.81446302, 10.65992385,  6.23855209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.599777258656142
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82038411,  15.11806075,   5.17635909]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2149423389646326}
episode index:3402
target Thresh 73.72182923562306
target distance 4.0
model initialize at round 3402
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([118.16616572,  18.52449724,   4.29530644]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.737793387311693}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.5998861395674978
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67581935,  15.24497046,   3.48894405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7188479106166379}
episode index:3403
target Thresh 73.72410626768166
target distance 63.0
model initialize at round 3403
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.40396895, 20.23589671]), 'previousTarget': array([71.90990945, 19.10381815]), 'currentState': array([51.5466679 , 22.62076466,  0.83352923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.553886469269383
running average episode reward sum: 0.5998726261508415
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32955859,  15.03555471,   5.38026756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6713835108680072}
episode index:3404
target Thresh 73.72638102384633
target distance 36.0
model initialize at round 3404
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([98.59876893, 12.98600759]), 'previousTarget': array([98.72787848, 12.28797975]), 'currentState': array([79.       ,  9.       ,  5.1432405], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5164974608169849
running average episode reward sum: 0.5998481400523586
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82630071,  14.7150303 ,   4.18500276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3337351805757729}
episode index:3405
target Thresh 73.72865350639184
target distance 11.0
model initialize at round 3405
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.87315978,   2.01240325,   4.80871972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.320678881888124}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5999245376491258
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22487405,  14.35235872,   1.00634152]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6855709770437967}
episode index:3406
target Thresh 73.73092371759066
target distance 62.0
model initialize at round 3406
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.37221098,  9.31388798]), 'previousTarget': array([72.83555733,  9.55942675]), 'currentState': array([54.56525702,  6.54178145,  0.59819775]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.47238507596685353
running average episode reward sum: 0.5998871031138507
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38776331,  15.54169387,   5.43044356]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8174753908589797}
episode index:3407
target Thresh 73.73319165971301
target distance 68.0
model initialize at round 3407
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.76810706, 15.20711888]), 'previousTarget': array([66.99783772, 14.29408585]), 'currentState': array([45.76828404, 15.29125825,  1.50678754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.15231529229629215
running average episode reward sum: 0.5997557733571555
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56168557,  15.26274361,   6.21430569]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6201007070897185}
episode index:3408
target Thresh 73.73545733502684
target distance 32.0
model initialize at round 3408
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.01895769,  12.09769252]), 'previousTarget': array([102.40285  ,  11.8507125]), 'currentState': array([84.68292093,  6.98714632,  1.00236243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.5870889238402315
running average episode reward sum: 0.5997520576488783
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12223217,  15.89314691,   5.13942949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9014721884137458}
episode index:3409
target Thresh 73.7377207457978
target distance 49.0
model initialize at round 3409
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.2073338 , 20.16656273]), 'previousTarget': array([85.73865762, 19.77736202]), 'currentState': array([67.54420302, 23.8218723 ,  0.16837132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5063487831004272
running average episode reward sum: 0.5997246666592746
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34605709,  15.20570608,   5.06834418]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40257980189877557}
episode index:3410
target Thresh 73.73998189428933
target distance 64.0
model initialize at round 3410
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.32689641,  7.24259307]), 'previousTarget': array([70.71097782,  7.38782431]), 'currentState': array([52.64938954,  3.66548207,  0.80990094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4986640945712224
running average episode reward sum: 0.5996950388163874
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31598085,  15.12329562,   5.14460259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6950424542697868}
episode index:3411
target Thresh 73.74224078276256
target distance 50.0
model initialize at round 3411
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.67149303, 17.00796967]), 'previousTarget': array([84.93630557, 17.40509555]), 'currentState': array([66.7215465 , 18.42205364,  5.19805828]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4702223532694625
running average episode reward sum: 0.5996570925427804
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45112344,  15.9965931 ,   4.68161792]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1377448245793689}
episode index:3412
target Thresh 73.7444974134764
target distance 7.0
model initialize at round 3412
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.35132872,   6.99106469,   0.26721471]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.176867477466695}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5997333894259034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95521166,  14.93567524,   5.31038969]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.0783815728827304}
episode index:3413
target Thresh 73.74675178868745
target distance 28.0
model initialize at round 3413
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.32831925,  16.24269189]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([87.58565517, 19.44069364,  5.95364451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.7229194351610663
running average episode reward sum: 0.5997694720403541
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36485741,  15.68633346,   4.7996335 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9351255118063132}
episode index:3414
target Thresh 73.7490039106501
target distance 2.0
model initialize at round 3414
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41646562,  15.22099119,   4.44929711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.471466563900505}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.5998866698523483
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41646562,  15.22099119,   4.44929711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.471466563900505}
episode index:3415
target Thresh 73.75125378161648
target distance 10.0
model initialize at round 3415
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.64436841,   5.40122614,   1.23305314]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.093046580880484}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.599978482082334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58080054,  15.96479178,   0.929614  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1261227517493}
episode index:3416
target Thresh 73.75350140383645
target distance 40.0
model initialize at round 3416
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.46377694, 19.33902801]), 'previousTarget': array([94.70060934, 18.55239336]), 'currentState': array([73.85774755, 23.28916869,  1.35382879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6000328278999297
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69151828,  15.48214895,   0.26931712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5723884905186672}
episode index:3417
target Thresh 73.75574677955764
target distance 65.0
model initialize at round 3417
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.6989411 ,  9.09022059]), 'previousTarget': array([69.76743395,  8.04114369]), 'currentState': array([5.08745573e+01, 6.44563967e+00, 2.78915167e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.3139203553624497
running average episode reward sum: 0.5999491203304337
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37768484,  14.46367337,   5.97726841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8215366142569619}
episode index:3418
target Thresh 73.75798991102542
target distance 69.0
model initialize at round 3418
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([65.78063511,  5.9540607 ]), 'previousTarget': array([65.7042351,  6.4268235]), 'currentState': array([46.       ,  3.       ,  2.7941976], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.23655695460578258
running average episode reward sum: 0.5998428342334099
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46486902,  14.03886727,   3.91832748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1000642227263375}
episode index:3419
target Thresh 73.76023080048293
target distance 53.0
model initialize at round 3419
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.02212496,  9.46545879]), 'previousTarget': array([81.8278102 , 10.61876739]), 'currentState': array([62.29796681,  6.15523441,  4.23213661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.5998443439506332
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86932703,  15.66956107,   4.4116836 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6821931175636227}
episode index:3420
target Thresh 73.76246945017104
target distance 4.0
model initialize at round 3420
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.76101272,  19.45912873,   0.19650567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.511339913754717}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.5999469881207442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51401443,  15.15862368,   4.75793611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5379333621340622}
episode index:3421
target Thresh 73.76470586232843
target distance 20.0
model initialize at round 3421
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.07971149,  14.43581015]), 'previousTarget': array([114.40285  ,  14.8507125]), 'currentState': array([93.89077682,  8.7980104 ,  2.98798919]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.5999944444989518
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52969013,  15.85452331,   5.33703871]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9753981049128231}
episode index:3422
target Thresh 73.76694003919148
target distance 45.0
model initialize at round 3422
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.65322182, 11.56434726]), 'previousTarget': array([89.87767469, 12.20863052]), 'currentState': array([7.08494258e+01, 8.76976945e+00, 4.34986909e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6000145956278068
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60296544,  14.78772284,   5.86767384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.450219986459462}
episode index:3423
target Thresh 73.7691719829944
target distance 69.0
model initialize at round 3423
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.67461708, 16.02599211]), 'previousTarget': array([66., 15.]), 'currentState': array([46.67912307, 16.45051474,  0.12291527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.5999959781130811
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79154951,  15.25142652,   5.49152011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3265989891680338}
episode index:3424
target Thresh 73.7714016959691
target distance 35.0
model initialize at round 3424
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.38664821, 11.91506574]), 'previousTarget': array([99.49717013, 11.45649603]), 'currentState': array([80.      ,  7.      ,  3.025119], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 44
reward sum = 0.3073211762513822
running average episode reward sum: 0.5999105256161871
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83774401,  15.82954207,   5.2789152 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8452615333394072}
episode index:3425
target Thresh 73.77362918034532
target distance 65.0
model initialize at round 3425
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.4409883 , 17.17556145]), 'previousTarget': array([69.9622374 , 17.77155462]), 'currentState': array([51.46588691, 18.17322092,  0.49646204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.5999084987395921
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54318042,  14.82666157,   0.23142252]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48860038809097384}
episode index:3426
target Thresh 73.77585443835054
target distance 55.0
model initialize at round 3426
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([80.72928201,  6.14860699]), 'previousTarget': array([79.46369226,  6.60050908]), 'currentState': array([61.36473896,  1.14715994,  4.687174  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6059204946650831
running average episode reward sum: 0.5999102530424592
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79131887,  15.47170059,   5.09591045]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5157996380823671}
episode index:3427
target Thresh 73.77807747221
target distance 6.0
model initialize at round 3427
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.55467267,   8.00856544,   2.85987961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.139266647417409}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6000017369964967
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06906575,  15.01038266,   5.24279967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.06984180456272801}
episode index:3428
target Thresh 73.78029828414677
target distance 8.0
model initialize at round 3428
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.38979103,  12.66719613,   0.7749626 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.009767221768397}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6001040957929108
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65229249,  15.33386308,   0.27356463]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4820426019894484}
episode index:3429
target Thresh 73.78251687638162
target distance 5.0
model initialize at round 3429
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.10828431e+02, 1.38576628e+01, 6.66477044e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.32514973871784}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.600209195476353
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57970037,  14.52654131,   5.41319507]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6330994416505656}
episode index:3430
target Thresh 73.78473325113316
target distance 22.0
model initialize at round 3430
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.29716736,  14.73680495]), 'previousTarget': array([112.91786413,  14.81071492]), 'currentState': array([91.34749951, 13.31879493,  1.94101715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7028014251051551
running average episode reward sum: 0.6002390970297278
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36697113,  15.48748097,   5.5085831 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6101684253917661}
episode index:3431
target Thresh 73.78694741061777
target distance 28.0
model initialize at round 3431
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.02014329,  15.54272774]), 'previousTarget': array([106.88618308,  15.86933753]), 'currentState': array([88.08033091, 17.09317351,  0.31169384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 19
reward sum = 0.7628618785849703
running average episode reward sum: 0.6002864812900875
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45778353,  15.41224363,   4.64864713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6160442928946084}
episode index:3432
target Thresh 73.7891593570496
target distance 59.0
model initialize at round 3432
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.9825465 , 16.16463459]), 'previousTarget': array([75.98851894, 16.32242309]), 'currentState': array([56.       , 17.       ,  3.8092093], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.4545183449410818
running average episode reward sum: 0.6002440204289314
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82542027,  15.95517938,   4.99439311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9710024334655114}
episode index:3433
target Thresh 73.79136909264061
target distance 12.0
model initialize at round 3433
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.30449503,   4.74186116,   1.32770467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.3973145328009}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6003352473442064
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52718435,  15.20724856,   6.27499512]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.51624277631091}
episode index:3434
target Thresh 73.79357661960054
target distance 45.0
model initialize at round 3434
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.96497402,  8.08097915]), 'previousTarget': array([89.21428366,  7.55079306]), 'currentState': array([68.58945072,  3.12223971,  1.68012619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5053706181393378
running average episode reward sum: 0.600307601163943
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08895471,  15.35311892,   4.59621845]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9770857133253553}
episode index:3435
target Thresh 73.79578194013689
target distance 17.0
model initialize at round 3435
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.44987899, 22.5284643 ,  0.90627885]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.096714959216413}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6003908599737661
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97704872,  15.8552425 ,   5.21942235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8555503999867715}
episode index:3436
target Thresh 73.797985056455
target distance 33.0
model initialize at round 3436
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.98267167,  14.16763422]), 'previousTarget': array([102.,  15.]), 'currentState': array([82.       , 15.       ,  6.2240434], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7479069375972307
running average episode reward sum: 0.6004337799847127
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39821711,  15.94622612,   5.4586695 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.121377063972578}
episode index:3437
target Thresh 73.80018597075798
target distance 49.0
model initialize at round 3437
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.43940688, 16.44509665]), 'previousTarget': array([85.98336106, 16.18435261]), 'currentState': array([67.46684298, 17.49232684,  5.65509445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6366187357648234
running average episode reward sum: 0.6004443049863939
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42152615,  14.43125571,   4.39629539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8112349060041127}
episode index:3438
target Thresh 73.80238468524675
target distance 59.0
model initialize at round 3438
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.81137395, 19.86092099]), 'previousTarget': array([75.89737675, 18.97653796]), 'currentState': array([56.97145153, 22.38628797,  6.24931056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.465144982273161
running average episode reward sum: 0.6004049623511182
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6429269 ,  15.96587388,   5.35782817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0297638275278638}
episode index:3439
target Thresh 73.80458120212003
target distance 7.0
model initialize at round 3439
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.36459061,  20.55133666,   5.30904078]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.232175254872984}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6005096690510161
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93357293,  15.86933421,   5.06088832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2756568409922908}
episode index:3440
target Thresh 73.80677552357432
target distance 63.0
model initialize at round 3440
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.662828  , 13.52780918]), 'previousTarget': array([71.99748095, 14.31742033]), 'currentState': array([50.67384424, 12.86408624,  2.9507606 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.07038670461815116
running average episode reward sum: 0.6003556083231949
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38582997,  14.67757815,   4.80988701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6936574591683704}
episode index:3441
target Thresh 73.80896765180395
target distance 15.0
model initialize at round 3441
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.70633261,  14.78047738]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   6.       ,   2.0867848], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.87321732453199}
done in step count: 34
reward sum = 0.502646227272292
running average episode reward sum: 0.6003272209376483
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54166824,  15.40017047,   5.59615442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6084442530536323}
episode index:3442
target Thresh 73.81115758900107
target distance 18.0
model initialize at round 3442
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.39153783,  14.72757371]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.13762865,  6.55476516,  3.16439962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6003655526683241
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22978764,  15.93860913,   5.55695925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2141722224203506}
episode index:3443
target Thresh 73.81334533735557
target distance 39.0
model initialize at round 3443
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([95.86353745, 11.33235503]), 'previousTarget': array([95.76743395, 12.04114369]), 'currentState': array([76.       ,  9.       ,  0.3635956], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4842253948912
running average episode reward sum: 0.6003318302067164
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.51780205,  15.17147489,   5.2396915 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5117797419946245}
episode index:3444
target Thresh 73.81553089905523
target distance 19.0
model initialize at round 3444
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.49239011,  15.67513766]), 'previousTarget': array([114.4327075,  15.23886  ]), 'currentState': array([95.23909805, 23.84932442,  1.08240712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.6517431364438055
running average episode reward sum: 0.6003467536628084
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.19355974,  15.54542995,   4.8194563 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.578756605337571}
episode index:3445
target Thresh 73.81771427628561
target distance 62.0
model initialize at round 3445
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.91146313, 11.69025457]), 'previousTarget': array([72.90700027, 10.9264839 ]), 'currentState': array([51.97020517, 10.15851337,  1.37422395]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.43774029666657754
running average episode reward sum: 0.600299566646849
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46685142,  15.930094  ,   2.97727373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0406849113119103}
episode index:3446
target Thresh 73.81989547123008
target distance 44.0
model initialize at round 3446
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.32936475, 16.73625081]), 'previousTarget': array([90.95367385, 16.63952224]), 'currentState': array([72.38776198, 18.26349623,  1.01222211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6007973285841244
running average episode reward sum: 0.6002997110512405
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81426775,  15.10537039,   6.23826124]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.213540130060291}
episode index:3447
target Thresh 73.82207448606985
target distance 71.0
model initialize at round 3447
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.79915095, 16.95328453]), 'previousTarget': array([63.98217027, 17.15568295]), 'currentState': array([45.81489342, 17.74666395,  5.41917974]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6003064470384329
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11750571,  14.99778372,   5.38014335]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8824970690287479}
episode index:3448
target Thresh 73.82425132298393
target distance 69.0
model initialize at round 3448
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.46309972,  8.78189216]), 'previousTarget': array([65.83200822,  8.58678368]), 'currentState': array([47.63203676,  6.18787178,  5.38779575]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.09493928484485037
running average episode reward sum: 0.6001599213317952
{'dynamicTrap': 10, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21205747,  15.58032499,   6.04589948]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9785859777091837}
episode index:3449
target Thresh 73.82642598414914
target distance 26.0
model initialize at round 3449
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.0028123,  13.4038011]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([87.38967282,  9.48911599,  1.83671093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6002114174008003
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0150126 ,  15.47200853,   5.23404313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.092241841568257}
episode index:3450
target Thresh 73.82859847174015
target distance 35.0
model initialize at round 3450
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.61587862,  9.6077702 ]), 'previousTarget': array([98.91891892,  9.48648649]), 'currentState': array([78.51371197,  3.68263483,  1.79149246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.600245471041831
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74268525,  14.7297266 ,   6.04186743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.37317367872852225}
episode index:3451
target Thresh 73.83076878792946
target distance 63.0
model initialize at round 3451
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.07815283,  8.36221515]), 'previousTarget': array([71.70193542,  7.44002047]), 'currentState': array([53.32424163,  5.23443759,  0.57124341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.6002399440844371
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44193595,  15.58452647,   4.95924728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8081501609754729}
episode index:3452
target Thresh 73.83293693488737
target distance 53.0
model initialize at round 3452
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.41358426, 20.03677715]), 'previousTarget': array([81.77598173, 20.01494615]), 'currentState': array([63.66311122, 23.18619215,  5.8767528 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6002598490408475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79749963,  15.28711516,   5.26856791]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3513424519942028}
episode index:3453
target Thresh 73.83510291478204
target distance 4.0
model initialize at round 3453
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.15052722,  17.68802383,   4.86671472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9238989381751503}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6003669825529955
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65120808,  14.96513117,   3.42402989]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6521409315942681}
episode index:3454
target Thresh 73.83726672977944
target distance 27.0
model initialize at round 3454
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.39566055,  16.06903268]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([89.74988182, 19.81648224,  0.38646631]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6004275790466767
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49878615,  15.85565927,   5.58941475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9916491831340986}
episode index:3455
target Thresh 73.8394283820434
target distance 26.0
model initialize at round 3455
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.2843355,  12.8483845]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([89.23799656,  6.74617403,  0.5664922 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6603882073478989
running average episode reward sum: 0.6004449287655137
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13913864,  15.60737523,   5.20629531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0535591803300455}
episode index:3456
target Thresh 73.84158787373553
target distance 58.0
model initialize at round 3456
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.99963071, 14.12153759]), 'previousTarget': array([76.99702801, 14.34477635]), 'currentState': array([57.       , 14.       ,  3.6740928], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5661854860638709
running average episode reward sum: 0.6004350185998494
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81158334,  14.81975179,   6.06111449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2607494119911521}
episode index:3457
target Thresh 73.84374520701539
target distance 46.0
model initialize at round 3457
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.08626592, 17.87156462]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.21780611, 20.16160938,  6.03496999]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6004818397958434
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36144646,  15.1599478 ,   5.37748333]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6582810392597935}
episode index:3458
target Thresh 73.84590038404026
target distance 67.0
model initialize at round 3458
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.35618537, 13.59774772]), 'previousTarget': array([67.97998109, 12.89462602]), 'currentState': array([49.36561688, 12.98360492,  5.89738232]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.48218634955907924
running average episode reward sum: 0.600447640463598
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03002608,  14.38581233,   5.8146672 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6149211782723153}
episode index:3459
target Thresh 73.84805340696535
target distance 27.0
model initialize at round 3459
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.62280754,  16.83685439]), 'previousTarget': array([107.6656401,  16.3582148]), 'currentState': array([88.21535865, 21.66913413,  0.43248105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6005104899714401
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14834861,  15.06757339,   5.04374846]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16301372036248496}
episode index:3460
target Thresh 73.85020427794365
target distance 20.0
model initialize at round 3460
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.73423223,  13.66538696]), 'previousTarget': array([111.76887233,  12.89976701]), 'currentState': array([95.50156199,  3.51476746,  0.26723742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.600570939950709
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14049027e+02, 1.58694788e+01, 1.07949227e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2885432222650413}
episode index:3461
target Thresh 73.85235299912605
target distance 71.0
model initialize at round 3461
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([62.8969572 , 19.42088869]), 'previousTarget': array([63.95059037, 18.59502885]), 'currentState': array([42.96856424, 21.11179199,  1.31999707]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6005757692981936
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62689613,  14.53049425,   6.02853077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.599701715994027}
episode index:3462
target Thresh 73.85449957266128
target distance 16.0
model initialize at round 3462
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.21946827,  13.42481743]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.        , 14.        ,  0.40254176], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.230333196981304}
done in step count: 13
reward sum = 0.6696140229989678
running average episode reward sum: 0.6005957052651877
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43260152,  15.18500352,   4.13595635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5967975679257304}
episode index:3463
target Thresh 73.8566440006959
target distance 27.0
model initialize at round 3463
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.20864261,  15.56298097]), 'previousTarget': array([107.98629668,  15.25976679]), 'currentState': array([89.30247697, 17.49807003,  1.06884497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6006681250301501
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42533186,  15.04438746,   5.44650712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5763798358453104}
episode index:3464
target Thresh 73.85878628537434
target distance 46.0
model initialize at round 3464
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.20961737, 16.94295127]), 'previousTarget': array([88.98112317, 16.13125551]), 'currentState': array([70.27076254, 18.50566306,  1.17226368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6007082497771509
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9054826 ,  14.41917996,   5.97337854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5884602454307982}
episode index:3465
target Thresh 73.8609264288389
target distance 12.0
model initialize at round 3465
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.67455523,   4.01391785,   3.42397034]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.00677181183031}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6008011627732994
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13026167,  15.31102852,   2.45137274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9236793308429007}
episode index:3466
target Thresh 73.86306443322968
target distance 50.0
model initialize at round 3466
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.35874403, 10.3603542 ]), 'previousTarget': array([84.74881264, 10.15981002]), 'currentState': array([66.616103  ,  7.16221018,  5.37322146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5669618991768227
running average episode reward sum: 0.6007914023857608
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4590825 ,  14.90635763,   5.40701398]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5489632362396966}
episode index:3467
target Thresh 73.86520030068475
target distance 53.0
model initialize at round 3467
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.81468367, 18.05413185]), 'previousTarget': array([81.91159005, 18.12154811]), 'currentState': array([63.90991171, 20.00350434,  5.75632483]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6008251218581404
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06647594,  15.05183858,   4.88745018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08429880879723527}
episode index:3468
target Thresh 73.86733403333992
target distance 5.0
model initialize at round 3468
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.30015892,  20.27650816,   3.92275476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.543554639263773}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6009316291738342
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.75417798,  15.89471185,   4.85490215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1701682419870654}
episode index:3469
target Thresh 73.86946563332894
target distance 44.0
model initialize at round 3469
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.25269752, 11.33859855]), 'previousTarget': array([90.81660336, 11.70226409]), 'currentState': array([69.45190455,  8.52282195,  2.43058777]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6009512372802883
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33261744,  15.74556148,   5.42943007]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0006304993647614}
episode index:3470
target Thresh 73.87159510278343
target distance 43.0
model initialize at round 3470
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.69311337, 15.77942633]), 'previousTarget': array([92., 15.]), 'currentState': array([72.70531098, 16.4778214 ,  2.13465595]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6009689067714158
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03410286,  15.3389137 ,   5.56019434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0236306813913252}
episode index:3471
target Thresh 73.87372244383285
target distance 15.0
model initialize at round 3471
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.12495321,  16.34407285]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.       ,   9.       ,   4.7431455], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.81367346728353}
done in step count: 34
reward sum = 0.5871892372679948
running average episode reward sum: 0.6009649379725956
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83565338,  15.36046126,   5.81262379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3961592310579068}
episode index:3472
target Thresh 73.87584765860453
target distance 17.0
model initialize at round 3472
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.43714371,  5.09861422,  0.42703729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.44559395170708}
done in step count: 20
reward sum = 0.6806860075972307
running average episode reward sum: 0.6009878924988337
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3397005 ,  15.2715221 ,   6.25769946]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7139465522916548}
episode index:3473
target Thresh 73.87797074922369
target distance 3.0
model initialize at round 3473
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.51684756,  12.27091731,   0.56194657]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.1060639885478456}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6010941996685232
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59538537,  15.37979088,   5.2781775 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5549361300706089}
episode index:3474
target Thresh 73.88009171781343
target distance 50.0
model initialize at round 3474
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.59233446, 12.40211422]), 'previousTarget': array([84.96409691, 13.19784581]), 'currentState': array([65.66992095, 10.64215967,  6.10575652]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6011137327789983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32562102,  15.80142228,   5.53285934]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0474085507661162}
episode index:3475
target Thresh 73.8822105664947
target distance 23.0
model initialize at round 3475
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.48093577,  12.91086219]), 'previousTarget': array([110.62485559,  13.28798697]), 'currentState': array([92.32697273,  4.51838783,  5.3221494 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6011351986352358
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57522714,  15.64413328,   4.93708266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7715825703853543}
episode index:3476
target Thresh 73.88432729738636
target distance 49.0
model initialize at round 3476
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.6683269 , 19.70020246]), 'previousTarget': array([85.73865762, 19.77736202]), 'currentState': array([67.95765872, 23.08982777,  5.97684887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.4607990998492268
running average episode reward sum: 0.6010948373758783
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.17853465,  15.51107703,   5.11577256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5413634216166965}
episode index:3477
target Thresh 73.88644191260515
target distance 1.0
model initialize at round 3477
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.18675191,  14.70405261,   2.6805985 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.828771759538844}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6012009915341946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.02905005,  15.92533349,   5.59019779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3412627878488166}
episode index:3478
target Thresh 73.88855441426567
target distance 6.0
model initialize at round 3478
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.46594383,  20.39336948,   5.62199265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.04599884019024}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6013042956498789
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13839311,  15.08745412,   5.40735327]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8660338666156165}
episode index:3479
target Thresh 73.89066480448044
target distance 33.0
model initialize at round 3479
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.97370069,  11.91138412]), 'previousTarget': array([101.43700211,  11.71200051]), 'currentState': array([83.60234287,  6.93639708,  5.26651192]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6013356890210347
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20151078,  15.97860749,   5.62181627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9991392381247937}
episode index:3480
target Thresh 73.89277308535983
target distance 22.0
model initialize at round 3480
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.87767657,  14.72276295]), 'previousTarget': array([112.81660336,  14.70226409]), 'currentState': array([91.95605346, 12.95388406,  1.42410588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.5510209492038813
running average episode reward sum: 0.601321234915945
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05854259,  15.08669026,   5.50285337]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9454402446863793}
episode index:3481
target Thresh 73.89487925901213
target distance 2.0
model initialize at round 3481
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.3119113 ,  13.81316339,   2.43546355]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.8743488956695553}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6014272021086746
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28572834,  14.65998045,   5.95975227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.791073509468022}
episode index:3482
target Thresh 73.89698332754351
target distance 64.0
model initialize at round 3482
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.59644494, 11.81590418]), 'previousTarget': array([70.93924283, 11.55775335]), 'currentState': array([52.65259314, 10.31831429,  1.20028466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5003814305591734
running average episode reward sum: 0.601398190977021
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22630098,  15.04306188,   5.56969005]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7748964418627691}
episode index:3483
target Thresh 73.89908529305805
target distance 26.0
model initialize at round 3483
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.18352677,  13.97596821]), 'previousTarget': array([108.48782391,  13.49719013]), 'currentState': array([89.48646354, 10.50815911,  0.24877787]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.601431581430988
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76910066,  14.39175097,   5.19304434]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.980552247186173}
episode index:3484
target Thresh 73.9011851576577
target distance 4.0
model initialize at round 3484
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.74662702,  16.49092414,   5.79242009]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.7019519902296976}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.601540238079071
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46836436,  15.87282375,   6.11605287]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0219871593303418}
episode index:3485
target Thresh 73.90328292344233
target distance 18.0
model initialize at round 3485
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.66715349, 20.65705172,  2.766927  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.185814953522144}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6016119298556102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93950325,  15.64581212,   5.11579831]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.140061246649777}
episode index:3486
target Thresh 73.90537859250972
target distance 42.0
model initialize at round 3486
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.8195481 , 16.31941926]), 'previousTarget': array([92.90990945, 17.10381815]), 'currentState': array([73.       , 19.       ,  1.8218321], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.41561839489119995
running average episode reward sum: 0.6015585907288639
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.274935  ,  15.52495176,   6.11225984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8951500409588334}
episode index:3487
target Thresh 73.90747216695553
target distance 70.0
model initialize at round 3487
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.78034068,  5.64078687]), 'previousTarget': array([64.7124451,  6.3792763]), 'currentState': array([46.13239926,  1.90469292,  0.24125069]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 79
reward sum = 0.45204365026647536
running average episode reward sum: 0.6015157252069424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.9061306 ,  14.03544869,   0.97902543]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3234167526323588}
episode index:3488
target Thresh 73.90956364887332
target distance 67.0
model initialize at round 3488
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.03391384,  8.10888513]), 'previousTarget': array([67.73578183,  7.24020299]), 'currentState': array([49.2549471 ,  5.14367441,  6.03752548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.38521281184039763
running average episode reward sum: 0.6014537295309991
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.3892769 ,  15.38765685,   5.23219259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7233675023676219}
episode index:3489
target Thresh 73.9116530403546
target distance 4.0
model initialize at round 3489
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.62370282,  11.44269315,   1.27617472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.910350739458201}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6015087898616904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32595411,  14.49387929,   0.45998012]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6020002078127096}
episode index:3490
target Thresh 73.91374034348873
target distance 49.0
model initialize at round 3490
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.99349309, 16.8540422 ]), 'previousTarget': array([85.98336106, 16.18435261]), 'currentState': array([67.03717451, 18.17515865,  1.80507201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6015526771425102
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76358205,  14.8458941 ,   5.4628582 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2822092775681734}
episode index:3491
target Thresh 73.91582556036303
target distance 19.0
model initialize at round 3491
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.97365685,  15.01290361]), 'previousTarget': array([114.4327075,  15.23886  ]), 'currentState': array([9.70126301e+01, 2.38107168e+01, 1.35301352e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6016368081783395
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56237972,  15.86174861,   5.40259928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9664999567909137}
episode index:3492
target Thresh 73.91790869306271
target distance 22.0
model initialize at round 3492
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.57091741,  15.17809604]), 'previousTarget': array([113.,  15.]), 'currentState': array([92.62445728, 16.6405352 ,  2.80583376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6017083286372334
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05930028,  15.53463872,   5.11384915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0820141055061445}
episode index:3493
target Thresh 73.91998974367093
target distance 68.0
model initialize at round 3493
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.13707633, 10.90005778]), 'previousTarget': array([66.89486598, 10.04800091]), 'currentState': array([48.21318118,  9.15695658,  1.77168053]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.4152081259428404
running average episode reward sum: 0.6016549513611331
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34519361,  14.11262325,   5.20179186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1028186190405895}
episode index:3494
target Thresh 73.92206871426872
target distance 68.0
model initialize at round 3494
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.7449311 , 16.65171295]), 'previousTarget': array([66.99135509, 16.41201897]), 'currentState': array([48.7576701 , 17.36543403,  1.00924843]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5024403779226349
running average episode reward sum: 0.6016265637864725
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00171353,  15.56827151,   4.99367826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5682740903434121}
episode index:3495
target Thresh 73.92414560693504
target distance 24.0
model initialize at round 3495
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.60494791,  16.676134  ]), 'previousTarget': array([110.40285  ,  16.1492875]), 'currentState': array([90.50547745, 22.60995471,  0.86086679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6017004859234447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15282232,  15.52681386,   5.16735685]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9976185934906447}
episode index:3496
target Thresh 73.9262204237468
target distance 12.0
model initialize at round 3496
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.78179336,   4.58584947,   2.25422744]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.56547768883663}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6017976491100268
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24305339,  14.06273445,   0.47023441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.204755117737075}
episode index:3497
target Thresh 73.9282931667788
target distance 55.0
model initialize at round 3497
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.61420403,  6.90934782]), 'previousTarget': array([79.54031523,  7.2633415 ]), 'currentState': array([60.       ,  3.       ,  1.8362156], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 71
reward sum = 0.3078816888199786
running average episode reward sum: 0.6017136251076569
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79864845,  14.71040329,   3.49928317]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8495325736700664}
episode index:3498
target Thresh 73.93036383810382
target distance 63.0
model initialize at round 3498
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.98499049, 15.83136557]), 'previousTarget': array([72., 15.]), 'currentState': array([52.98890473, 16.22703506,  1.7591828 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4994500984100759
running average episode reward sum: 0.6016843986067429
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27821172,  15.29254555,   4.93163722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7788204037384667}
episode index:3499
target Thresh 73.93243243979248
target distance 9.0
model initialize at round 3499
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.54748967,  11.66156165,   1.41399163]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.166093367547235}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6017842002213982
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1258155 ,  15.33712067,   0.67181562]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35983313708056264}
episode index:3500
target Thresh 73.9344989739134
target distance 45.0
model initialize at round 3500
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.43265169, 11.46564944]), 'previousTarget': array([89.69125016, 10.50066669]), 'currentState': array([69.62105017,  8.72695539,  2.63118577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6017886292247461
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.81536046,  14.22629768,   4.01454427]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1240231139446044}
episode index:3501
target Thresh 73.93656344253316
target distance 13.0
model initialize at round 3501
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.50282487,   3.63139329,   2.4956038 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.894345391189571}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6018802785865973
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26855131,  15.63669793,   1.87893546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.969742976880599}
episode index:3502
target Thresh 73.93862584771617
target distance 6.0
model initialize at round 3502
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.48184838,  19.71753164,   5.92186272]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.347540778073908}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.601964051916792
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40916419,  15.13973212,   4.55808967]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4323660452353636}
episode index:3503
target Thresh 73.94068619152485
target distance 39.0
model initialize at round 3503
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.36045966,  7.93064592]), 'previousTarget': array([94.97366596,  8.32455532]), 'currentState': array([76.54244258,  1.15700338,  5.65828657]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.25390066402429057
running average episode reward sum: 0.601864718758147
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04050806,  15.96122312,   4.85080776]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.358151192897353}
episode index:3504
target Thresh 73.94274447601956
target distance 42.0
model initialize at round 3504
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.5707449 , 15.80340632]), 'previousTarget': array([92.99433347, 15.52394444]), 'currentState': array([74.58619254, 16.58932415,  1.35027057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6761892283866643
running average episode reward sum: 0.601885924039068
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07723098,  15.75435027,   5.08847983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.191867019582887}
episode index:3505
target Thresh 73.94480070325855
target distance 29.0
model initialize at round 3505
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.54010038,  13.955753  ]), 'previousTarget': array([105.70920231,  13.39813833]), 'currentState': array([86.69074292, 11.50564921,  0.17091346]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6019295159852074
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93040732,  15.84787508,   4.76660832]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8507263356875023}
episode index:3506
target Thresh 73.9468548752981
target distance 20.0
model initialize at round 3506
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.8943102 ,  15.04581481]), 'previousTarget': array([113.56953382,  15.57218647]), 'currentState': array([96.54420476, 23.00028717,  5.93987206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.602010626722513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49985503,  15.59507989,   4.88591055]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7773448807414813}
episode index:3507
target Thresh 73.94890699419234
target distance 6.0
model initialize at round 3507
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35357081,  21.03457617,   4.03619313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.069100442772327}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6021156120056594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44405197,  15.98205283,   4.95999661]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1284972218099458}
episode index:3508
target Thresh 73.9509570619934
target distance 6.0
model initialize at round 3508
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.41228035,   9.48435084,   1.83549953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.739620144149761}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6022096415684698
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34895068,  15.06581561,   1.00430602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.654367562181484}
episode index:3509
target Thresh 73.95300508075137
target distance 40.0
model initialize at round 3509
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.15679703, 18.38237621]), 'previousTarget': array([94.84555753, 17.51930531]), 'currentState': array([75.44116469, 21.74300736,  2.12163424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6022509414125619
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54965852,  14.52580657,   5.30836121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.653962429808883}
episode index:3510
target Thresh 73.95505105251424
target distance 17.0
model initialize at round 3510
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.60843909, 23.65601367,  0.83761406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.42660453382808}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6023318681941919
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.2143353 ,  14.6775979 ,   5.75794925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8492420943129888}
episode index:3511
target Thresh 73.95709497932799
target distance 44.0
model initialize at round 3511
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.8274124 , 13.01712188]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([71.90023496, 11.31195348,  1.97940653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5870522205741436
running average episode reward sum: 0.6023275174972614
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14358429,  15.71206981,   5.01449361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7264019954251499}
episode index:3512
target Thresh 73.95913686323655
target distance 27.0
model initialize at round 3512
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.44620732,  17.59445311]), 'previousTarget': array([107.35993796,  16.98075682]), 'currentState': array([87.30720699, 23.39950834,  1.11044168]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6023936109341965
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1716483 ,  14.306207  ,   5.29766316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0805162019329027}
episode index:3513
target Thresh 73.9611767062818
target distance 16.0
model initialize at round 3513
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.29113463, 19.79721146,  2.31493926]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.34712921876523}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6024694083735348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29780109,  14.64737772,   5.57424117]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7857644571890388}
episode index:3514
target Thresh 73.96321451050359
target distance 57.0
model initialize at round 3514
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.39421328, 18.17628198]), 'previousTarget': array([77.92349448, 18.25232505]), 'currentState': array([56.46156292, 19.81623608,  2.37872648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6024921922104158
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28122743,  15.4316578 ,   4.89105354]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5151866870858117}
episode index:3515
target Thresh 73.96525027793973
target distance 9.0
model initialize at round 3515
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.30574452,   7.51320167,   0.9222682 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.735628375359418}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6025859274651646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49857697,  15.52832682,   5.23768643]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7283915727272807}
episode index:3516
target Thresh 73.96728401062597
target distance 21.0
model initialize at round 3516
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.64039729,  12.92543882]), 'previousTarget': array([112.05721038,  13.59867161]), 'currentState': array([92.5808719 ,  4.33163043,  2.58059549]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6026207282637124
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36009647,  15.67023778,   5.07811435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9266580870741845}
episode index:3517
target Thresh 73.96931571059605
target distance 42.0
model initialize at round 3517
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.24290333, 10.64835306]), 'previousTarget': array([92.64677133, 10.74224216]), 'currentState': array([74.6684417 ,  6.54464108,  0.66299396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6441111714463618
running average episode reward sum: 0.602632522022434
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10409393,  15.04945673,   5.20015561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8972701075353532}
episode index:3518
target Thresh 73.97134537988167
target distance 48.0
model initialize at round 3518
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.32428695, 11.70178943]), 'previousTarget': array([86.93091516, 12.6609096 ]), 'currentState': array([67.46481499,  9.33506737,  5.91561937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6026591718365935
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61448541,  15.4048404 ,   5.11026737]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5590324197900305}
episode index:3519
target Thresh 73.97337302051251
target distance 6.0
model initialize at round 3519
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.74575391,  14.42338908,   5.56250429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.2807701972594385}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6027581294724069
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13349164,  14.01510938,   1.72197842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9938961474094067}
episode index:3520
target Thresh 73.97539863451618
target distance 29.0
model initialize at round 3520
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.33110201,  14.23927712]), 'previousTarget': array([105.89383588,  14.05798302]), 'currentState': array([87.42877992, 12.26504931,  5.4909547 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6027887443823189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.90266416,  14.23782203,   4.35733666]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.181405027483817}
episode index:3521
target Thresh 73.97742222391834
target distance 21.0
model initialize at round 3521
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.13301459,  14.59926226]), 'previousTarget': array([113.79898987,  14.82842712]), 'currentState': array([92.32557279, 11.83064353,  2.23239279]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6028340464748699
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19257306,  15.88694591,   5.28881878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1994212388145087}
episode index:3522
target Thresh 73.97944379074255
target distance 23.0
model initialize at round 3522
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.64547696,  14.21263331]), 'previousTarget': array([111.13347761,  13.82323232]), 'currentState': array([93.67792373,  7.86976926,  0.93254107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6028837164473151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.71153211,  15.5177752 ,   4.90823226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8799824425809933}
episode index:3523
target Thresh 73.98146333701038
target distance 71.0
model initialize at round 3523
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.48854247,  9.65568667]), 'previousTarget': array([63.87423731,  9.23935068]), 'currentState': array([45.60404659,  7.50933547,  5.58064753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6028742719200818
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24304225,  15.10939385,   5.00605877]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.764821579904589}
episode index:3524
target Thresh 73.98348086474137
target distance 41.0
model initialize at round 3524
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([93.56303457, 12.15784541]), 'previousTarget': array([93.71472851, 11.36592926]), 'currentState': array([74.     ,  8.     ,  5.99938], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.10390282288970465
running average episode reward sum: 0.6027327197359598
{'dynamicTrap': 9, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29547989,  14.83763552,   5.33920561]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7229874173890817}
episode index:3525
target Thresh 73.98549637595306
target distance 41.0
model initialize at round 3525
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.21823191, 13.15087159]), 'previousTarget': array([93.94667447, 13.45951277]), 'currentState': array([72.28378903, 11.5328521 ,  4.24470186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6027422071909592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89229164,  15.27949347,   4.66123388]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2995291112093604}
episode index:3526
target Thresh 73.98750987266097
target distance 6.0
model initialize at round 3526
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.79966976,   7.43503122,   5.90284514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.214065145396551}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6028382485695273
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37035853,  15.83785165,   1.71820753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0480666793153421}
episode index:3527
target Thresh 73.98952135687857
target distance 47.0
model initialize at round 3527
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.63806689, 14.31602686]), 'previousTarget': array([87.95938166, 13.27400308]), 'currentState': array([68.64479517, 13.79729173,  1.75640154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6028708144096715
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21865294,  15.27556769,   5.17403515]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8285172214611193}
episode index:3528
target Thresh 73.99153083061736
target distance 44.0
model initialize at round 3528
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.10104444, 10.9683183 ]), 'previousTarget': array([90.59429865, 10.00792472]), 'currentState': array([71.37969749,  7.64138657,  0.33642495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5726887870472956
running average episode reward sum: 0.6028622618374521
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.06527485,  14.17259427,   5.15423963]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2483234133333043}
episode index:3529
target Thresh 73.99353829588682
target distance 7.0
model initialize at round 3529
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.16666753,  11.49007814,   5.14617634]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.583743300910081}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6029608816074414
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60960396,  15.01847788,   1.39453303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3908330863609148}
episode index:3530
target Thresh 73.99554375469441
target distance 9.0
model initialize at round 3530
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.40989188,  13.48872118,   4.92529732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.739102338436333}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6030594455180314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84120895,  14.73496254,   1.12151289]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3089651304865967}
episode index:3531
target Thresh 73.99754720904559
target distance 6.0
model initialize at round 3531
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41074076,  10.5880925 ,   0.98337841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.43098586892865}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6031552611193571
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.97944121,  15.51277269,   4.95133141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5131846578968017}
episode index:3532
target Thresh 73.9995486609438
target distance 10.0
model initialize at round 3532
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.63729151,  11.44689373,   0.14590836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.0142335886067}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6032457194927816
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39838737,  14.98431063,   6.13858736]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6018171752540415}
episode index:3533
target Thresh 74.00154811239052
target distance 27.0
model initialize at round 3533
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.6902668,  11.4795076]), 'previousTarget': array([106.27623097,  11.12276932]), 'currentState': array([89.67118858,  2.80121166,  0.89160603]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.611162037454471
running average episode reward sum: 0.6032479595374792
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83855818,  15.16752559,   5.21931997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2326548594139094}
episode index:3534
target Thresh 74.00354556538517
target distance 3.0
model initialize at round 3534
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37783102,  12.98670494,   1.61164504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.1072378252025805}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6033383405091597
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.97958349,  14.97382956,   3.89726369]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9799330092134666}
episode index:3535
target Thresh 74.00554102192523
target distance 70.0
model initialize at round 3535
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.23799364, 20.58498304]), 'previousTarget': array([64.9007438 , 20.00992562]), 'currentState': array([46.36790081, 22.86081497,  1.52941125]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6033422861540786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13164467,  15.86523323,   5.19691227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8751907596090686}
episode index:3536
target Thresh 74.00753448400614
target distance 66.0
model initialize at round 3536
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.3047946 , 20.23808044]), 'previousTarget': array([68.85467564, 20.59337265]), 'currentState': array([47.4243276 , 22.42143377,  4.50349593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 74
reward sum = 0.4294438965655169
running average episode reward sum: 0.6032931206495299
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04886725,  15.97456438,   5.14833761]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3617742986310002}
episode index:3537
target Thresh 74.00952595362138
target distance 74.0
model initialize at round 3537
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([60.86673507,  8.30495936]), 'previousTarget': array([60.85370283,  8.41463953]), 'currentState': array([41.       ,  6.       ,  1.3526281], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.46626822520718497
running average episode reward sum: 0.603254391170886
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68349185,  15.59668646,   5.30745541]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.675434773241777}
episode index:3538
target Thresh 74.0115154327624
target distance 62.0
model initialize at round 3538
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.91747683, 18.67019474]), 'previousTarget': array([72.95850618, 17.71235444]), 'currentState': array([51.98965734, 20.36784319,  1.22677767]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.27344909043604226
running average episode reward sum: 0.6031611995063663
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71152871,  15.85873461,   5.16471442]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9058922793887448}
episode index:3539
target Thresh 74.01350292341868
target distance 65.0
model initialize at round 3539
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.48174486, 14.66331425]), 'previousTarget': array([70., 15.]), 'currentState': array([51.48234339, 14.50858577,  0.63504141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6031392885707785
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38270103,  15.71920296,   5.13257907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9477926556574044}
episode index:3540
target Thresh 74.01548842757772
target distance 74.0
model initialize at round 3540
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([59.60253825, 21.51600923]), 'previousTarget': array([60.88414095, 20.85036314]), 'currentState': array([39.73947045, 23.85236056,  1.46522236]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5644597908380337
running average episode reward sum: 0.603128365244675
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50652692,  15.13561175,   0.90369852]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.511767749352403}
episode index:3541
target Thresh 74.01747194722503
target distance 45.0
model initialize at round 3541
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.09021836, 18.7073833 ]), 'previousTarget': array([89.87767469, 17.79136948]), 'currentState': array([70.30811589, 21.65160166,  0.37636065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5957127374909834
running average episode reward sum: 0.6031262716174153
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5346407 ,  14.30698543,   3.49695416]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8752770278326788}
episode index:3542
target Thresh 74.01945348434411
target distance 24.0
model initialize at round 3542
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.77760791,  14.92528972]), 'previousTarget': array([110.93091516,  14.6609096 ]), 'currentState': array([91.78298105, 14.46171978,  2.08903133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5442760055855419
running average episode reward sum: 0.6031096613249988
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.50817247,  15.77411123,   5.58226402]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.926006188819396}
episode index:3543
target Thresh 74.02143304091652
target distance 3.0
model initialize at round 3543
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.60605112,  17.02875662,   4.7330631 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.1173453675034226}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6032188290277851
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49736495,  15.67557106,   4.52938467]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8389089053966683}
episode index:3544
target Thresh 74.0234106189218
target distance 19.0
model initialize at round 3544
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.88564871,  15.13564398]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 13.       ,  2.2839496], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6470091525053181
running average episode reward sum: 0.6032311817283429
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.81220289,  15.44839272,   4.87519856]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9277551170798608}
episode index:3545
target Thresh 74.02538622033754
target distance 62.0
model initialize at round 3545
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.29924063, 20.05853856]), 'previousTarget': array([72.87373448, 19.75619127]), 'currentState': array([54.45194412, 22.5252816 ,  1.35712879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6032594435764173
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52566663,  15.73421116,   5.55952428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8741042100515986}
episode index:3546
target Thresh 74.02735984713934
target distance 12.0
model initialize at round 3546
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.11429872,   4.5531508 ,   0.53295147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.53285460191713}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6033495155388788
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33996626,  14.43487993,   1.00164914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8689103724467687}
episode index:3547
target Thresh 74.02933150130082
target distance 30.0
model initialize at round 3547
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.52149628,  15.69519941]), 'previousTarget': array([104.95570316,  15.66961979]), 'currentState': array([86.58839183, 17.32962488,  5.53082711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.603414668934006
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92760278,  15.64806577,   5.34286577]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6520970802390365}
episode index:3548
target Thresh 74.03130118479362
target distance 56.0
model initialize at round 3548
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.92672634, 12.71043189]), 'previousTarget': array([78.949174, 12.424941]), 'currentState': array([59.        , 11.        ,  0.57765275], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.2144938860273971
running average episode reward sum: 0.6033050829145903
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05863422,  15.74572167,   5.26604256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.748023249733312}
episode index:3549
target Thresh 74.03326889958745
target distance 38.0
model initialize at round 3549
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.86219378, 10.1183249 ]), 'previousTarget': array([96.46160575, 10.60932768]), 'currentState': array([75.42514364,  5.4065225 ,  4.5119803 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6033125529281926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28597811,  15.37222355,   5.29754839]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8052189912314427}
episode index:3550
target Thresh 74.03523464765003
target distance 23.0
model initialize at round 3550
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.6998155 ,  12.30025732]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([90.31655953,  4.42270059,  2.00220239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6033530653307282
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66721958,  15.59625504,   5.249951  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6828344430114944}
episode index:3551
target Thresh 74.03719843094707
target distance 51.0
model initialize at round 3551
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.95441233, 13.34960317]), 'previousTarget': array([83.96548746, 13.17444044]), 'currentState': array([64.        , 12.        ,  0.76730347], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.612554595010387
running average episode reward sum: 0.6033556558514713
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22405713,  15.12586701,   1.26957433]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7860851376655092}
episode index:3552
target Thresh 74.0391602514424
target distance 70.0
model initialize at round 3552
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.4319286 , 16.95930525]), 'previousTarget': array([64.99184173, 16.42880452]), 'currentState': array([46.44818307, 17.76547803,  1.49624794]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6033561203635135
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43013985,  15.18618093,   4.85587472]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4687041985635742}
episode index:3553
target Thresh 74.0411201110978
target distance 15.0
model initialize at round 3553
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.25086498,  8.46299312,  2.88736224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.051912288842036}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6033784046838981
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41413629,  15.95446968,   5.3974849 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1199324277670455}
episode index:3554
target Thresh 74.04307801187316
target distance 7.0
model initialize at round 3554
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09461308e+02, 9.17890723e+00, 6.09323978e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.035062844565262}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6034735106599085
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15818493,  15.33106084,   5.24056352]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3669110959303768}
episode index:3555
target Thresh 74.04503395572637
target distance 65.0
model initialize at round 3555
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.06066155,  7.34156797]), 'previousTarget': array([69.66764361,  6.63094959]), 'currentState': array([51.35770009,  3.90742908,  1.59917229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.37657504488643256
running average episode reward sum: 0.6034097034423119
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64256402,  15.02798209,   5.35789795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3585296025117448}
episode index:3556
target Thresh 74.04698794461336
target distance 42.0
model initialize at round 3556
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.54452527, 16.3036095 ]), 'previousTarget': array([92.99433347, 15.52394444]), 'currentState': array([73.58133972, 17.51654926,  0.1947422 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5775449952892246
running average episode reward sum: 0.6034024319471888
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13719027,  15.32910353,   5.12838331]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9234445072184568}
episode index:3557
target Thresh 74.04893998048814
target distance 64.0
model initialize at round 3557
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.50489947, 16.56454328]), 'previousTarget': array([70.99755904, 15.68753814]), 'currentState': array([50.51725178, 17.2673519 ,  2.9447391 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5165742855081551
running average episode reward sum: 0.6033780283085045
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27021094,  14.60907776,   4.94129096]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8278962879227867}
episode index:3558
target Thresh 74.05089006530274
target distance 65.0
model initialize at round 3558
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.36216293, 17.14354442]), 'previousTarget': array([69.99053926, 16.38490648]), 'currentState': array([51.3862483 , 18.124786  ,  1.50614875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.6033342365651652
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12810008,  15.33956634,   5.11249245]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.935689466519024}
episode index:3559
target Thresh 74.05283820100725
target distance 10.0
model initialize at round 3559
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.08062044,  14.26579852,   5.67635429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.946514095721835}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6034292213721415
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82493595,  14.35056856,   5.16634429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6726132733292716}
episode index:3560
target Thresh 74.05478438954978
target distance 15.0
model initialize at round 3560
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.46996256,  19.31358539,   0.43261939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.201018703565216}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6035188915414915
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81543666,  15.6276284 ,   5.71905921]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6542025929473632}
episode index:3561
target Thresh 74.05672863287654
target distance 38.0
model initialize at round 3561
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.25617753,  8.62026336]), 'previousTarget': array([96.0716533 ,  9.02262736]), 'currentState': array([78.4357132 ,  1.85342346,  4.89488629]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5843166891616117
running average episode reward sum: 0.6035135006929851
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38760174,  15.96100985,   5.24600328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1395488379527403}
episode index:3562
target Thresh 74.05867093293178
target distance 30.0
model initialize at round 3562
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([104.93036149,  12.6675404 ]), 'previousTarget': array([104.82455801,  13.6432744 ]), 'currentState': array([85.       , 11.       ,  2.5965881], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.700043145805155
running average episode reward sum: 0.603540592931299
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22980958,  15.74100563,   5.38228944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7758233020275048}
episode index:3563
target Thresh 74.06061129165779
target distance 39.0
model initialize at round 3563
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.77876999,  7.71804934]), 'previousTarget': array([94.97366596,  8.32455532]), 'currentState': array([74.86152005,  1.22669702,  2.92177451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.17194950294299083
running average episode reward sum: 0.6034194955435357
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54733596,  14.71977925,   5.52759127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.532379940486684}
episode index:3564
target Thresh 74.06254971099493
target distance 27.0
model initialize at round 3564
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.64050469,  15.97565301]), 'previousTarget': array([107.94535509,  15.52256629]), 'currentState': array([87.81397126, 18.60406926,  0.67625356]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.603464074286538
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43169963,  14.30912695,   5.11355301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8945786065584661}
episode index:3565
target Thresh 74.06448619288162
target distance 8.0
model initialize at round 3565
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.49865923,  16.06511723,   1.01107519]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.588012331975228}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6035642234552743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07671656,  15.37369423,   5.71824679]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9960420059515304}
episode index:3566
target Thresh 74.06642073925434
target distance 70.0
model initialize at round 3566
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([64.95776319, 16.70088934]), 'previousTarget': array([64.98165792, 17.14364323]), 'currentState': array([45.       , 18.       ,  2.8306155], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5164974608169849
running average episode reward sum: 0.6035398144946243
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12754346,  15.59592106,   5.31052898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0565520879729517}
episode index:3567
target Thresh 74.06835335204764
target distance 47.0
model initialize at round 3567
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.42021794, 14.11045539]), 'previousTarget': array([87.95938166, 13.27400308]), 'currentState': array([67.43061271, 13.46572058,  0.95460176]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5793048568341018
running average episode reward sum: 0.6035330221858629
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69167741,  15.0929571 ,   5.02726659]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32203080765339115}
episode index:3568
target Thresh 74.07028403319413
target distance 46.0
model initialize at round 3568
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.93473618, 12.89059709]), 'previousTarget': array([88.88288926, 12.16118362]), 'currentState': array([67.99520361, 11.33655688,  3.18998325]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5570860561173978
running average episode reward sum: 0.6035200081858437
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.06403594,  14.81210148,   3.62826154]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19851058929944906}
episode index:3569
target Thresh 74.0722127846245
target distance 27.0
model initialize at round 3569
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.2262134 ,  15.96419571]), 'previousTarget': array([107.78406925,  16.06902678]), 'currentState': array([89.49938748, 19.25848459,  5.44390077]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6035519999293766
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10092217,  14.83672237,   3.1157497 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9137836361478154}
episode index:3570
target Thresh 74.07413960826747
target distance 21.0
model initialize at round 3570
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.68966993,  13.55541027]), 'previousTarget': array([111.71663071,  13.28013989]), 'currentState': array([93.35904895,  5.5561394 ,  2.97150654]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6035215565408778
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64695779,  15.84027374,   5.41160297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9114267760498569}
episode index:3571
target Thresh 74.0760645060499
target distance 55.0
model initialize at round 3571
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.8228195,  9.6562807]), 'previousTarget': array([79.79172879,  9.87879691]), 'currentState': array([60.       ,  7.       ,  1.9047313], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.35059027300420487
running average episode reward sum: 0.6034507471109963
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21141548,  15.93545148,   5.29645276]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2234929545508302}
episode index:3572
target Thresh 74.07798747989668
target distance 5.0
model initialize at round 3572
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.94415847,  18.59077077,   5.16297704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.7128250617758067}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.603545353716731
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47324419,  14.47414015,   3.60945916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7074522206911468}
episode index:3573
target Thresh 74.07990853173078
target distance 67.0
model initialize at round 3573
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.84197753, 12.71122403]), 'previousTarget': array([67.99109528, 13.59674911]), 'currentState': array([4.88665197e+01, 1.17207278e+01, 3.45684846e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5401360962972451
running average episode reward sum: 0.6035276118987625
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42234463,  15.7480878 ,   5.65653396]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8590752865742808}
episode index:3574
target Thresh 74.08182766347325
target distance 27.0
model initialize at round 3574
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.74281324,  15.86352565]), 'previousTarget': array([107.6656401,  16.3582148]), 'currentState': array([8.89305902e+01, 1.85977197e+01, 2.50984987e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.2260130875260845
running average episode reward sum: 0.6034220134304066
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.81078258,  15.18540854,   5.03095737]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8317119216763419}
episode index:3575
target Thresh 74.08374487704323
target distance 69.0
model initialize at round 3575
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.44836016, 17.43839533]), 'previousTarget': array([65.99160369, 16.42053323]), 'currentState': array([45.4725317 , 18.42138937,  0.9161377 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5206948805864273
running average episode reward sum: 0.6033988794447118
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.44739658,  15.14248526,   5.1549854 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.46953780036832776}
episode index:3576
target Thresh 74.08566017435793
target distance 43.0
model initialize at round 3576
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.97142863, 17.99182011]), 'previousTarget': array([91.91402432, 17.14753262]), 'currentState': array([71.12467879, 20.46295767,  1.137182  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.38805412292360625
running average episode reward sum: 0.6033386768289666
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85980687,  15.94404089,   5.21403893]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9543936897362069}
episode index:3577
target Thresh 74.08757355733263
target distance 5.0
model initialize at round 3577
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.38262652,  11.76752909,   1.51676863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.851212119792666}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6034412370087237
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4163412 ,  14.57633196,   0.44335094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7212157786637298}
episode index:3578
target Thresh 74.08948502788073
target distance 29.0
model initialize at round 3578
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.48117032,  17.18682622]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([87.10926749, 22.15969065,  6.08080286]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6035081556889068
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7939796 ,  15.77489061,   5.10598028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1094408797880881}
episode index:3579
target Thresh 74.0913945879137
target distance 6.0
model initialize at round 3579
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.25222509,  10.81143736,   4.58910022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.3313050432739635}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6036052176705301
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21113101,  15.69654104,   0.14049581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.052370519666618}
episode index:3580
target Thresh 74.0933022393411
target distance 26.0
model initialize at round 3580
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.21133387,  11.19124054]), 'previousTarget': array([106.88854382,  10.94427191]), 'currentState': array([87.86047844,  3.23849856,  1.3045969 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6206362285316084
running average episode reward sum: 0.6036099736076597
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14764981e+02, 1.40626191e+01, 4.36770767e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9663938434994516}
episode index:3581
target Thresh 74.09520798407058
target distance 70.0
model initialize at round 3581
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.15483296, 21.36546526]), 'previousTarget': array([64.87065345, 20.72906818]), 'currentState': array([44.30974748, 23.84993349,  0.98660445]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.5067396712307267
running average episode reward sum: 0.6035829299721552
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.08905842,  15.54322501,   5.20915667]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5504768921629863}
episode index:3582
target Thresh 74.0971118240079
target distance 18.0
model initialize at round 3582
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.18605105,  13.77331638]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.      , 10.      ,  1.388323], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.553579505602723}
done in step count: 37
reward sum = 0.5501490858690777
running average episode reward sum: 0.6035680168144374
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45372152,  14.99099328,   4.24304413]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5463527222780222}
episode index:3583
target Thresh 74.09901376105687
target distance 13.0
model initialize at round 3583
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.74424347,   3.49354497,   1.90403745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.161780373099573}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6036544981846017
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21785238,  14.71461701,   0.31396231]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8325853458527188}
episode index:3584
target Thresh 74.10091379711945
target distance 21.0
model initialize at round 3584
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.21216243,  14.35656841]), 'previousTarget': array([113.64677133,  14.74224216]), 'currentState': array([92.72446789,  9.85881772,  4.83264256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6037052718645523
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12324517,  15.51958014,   5.13690406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5339970890566094}
episode index:3585
target Thresh 74.10281193409566
target distance 63.0
model initialize at round 3585
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.00474419, 20.80651675]), 'previousTarget': array([71.87767469, 19.79136948]), 'currentState': array([53.19322097, 23.54577716,  0.84072389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5970374666304942
running average episode reward sum: 0.603703412465435
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79272327,  15.3346024 ,   5.11572498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3936018396776991}
episode index:3586
target Thresh 74.10470817388364
target distance 49.0
model initialize at round 3586
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.3744937 , 17.24440645]), 'previousTarget': array([85.96262067, 16.77779873]), 'currentState': array([67.44017459, 18.86395013,  5.81352586]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5573294832809921
running average episode reward sum: 0.6036904841327938
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.36244527,  14.99622778,   4.88022006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6375658886962743}
episode index:3587
target Thresh 74.10660251837966
target distance 23.0
model initialize at round 3587
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.45302919,  15.99878629]), 'previousTarget': array([111.7042351,  15.5731765]), 'currentState': array([90.91874599, 20.2896813 ,  1.25851059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6037479081528853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.12544823,  15.77309163,   6.04395684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7832036340476666}
episode index:3588
target Thresh 74.10849496947802
target distance 31.0
model initialize at round 3588
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([103.76132524,  13.08058841]), 'previousTarget': array([103.74482241,  13.18464878]), 'currentState': array([84.       , 10.       ,  5.5462794], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5434436245487253
running average episode reward sum: 0.6037311056219284
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13359702,  15.54634785,   5.08996308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.024280279892966}
episode index:3589
target Thresh 74.1103855290712
target distance 25.0
model initialize at round 3589
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.48325563,  17.47848562]), 'previousTarget': array([109.04848294,  16.90448546]), 'currentState': array([88.48914914, 23.74138581,  1.67538333]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6037839978720738
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99159934,  14.6964875 ,   6.06980676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30362873925632694}
episode index:3590
target Thresh 74.11227419904975
target distance 13.0
model initialize at round 3590
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.01651751,   4.35481676,   1.91887205]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.02884209129049}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6038577828386281
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86477753,  15.32836526,   5.06338492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35511809660924504}
episode index:3591
target Thresh 74.11416098130233
target distance 53.0
model initialize at round 3591
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.66611291,  6.31137154]), 'previousTarget': array([81.50626598,  7.41651305]), 'currentState': array([62.31275135,  1.2668303 ,  5.71242285]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = -0.01642856672816434
running average episode reward sum: 0.6036850973292832
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.47799437,  15.88803342,   4.89972203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.030093798047667}
episode index:3592
target Thresh 74.11604587771576
target distance 64.0
model initialize at round 3592
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.19476595, 14.53597522]), 'previousTarget': array([70.99024152, 13.62469505]), 'currentState': array([52.19594098, 14.31918046,  1.66790141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4759251708132459
running average episode reward sum: 0.6036495393202335
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52697446,  14.54217603,   5.60177994]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6582977658997071}
episode index:3593
target Thresh 74.1179288901749
target distance 53.0
model initialize at round 3593
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.15624861, 16.11647849]), 'previousTarget': array([81.99644096, 15.62270866]), 'currentState': array([63.16853013, 16.81727086,  5.88350898]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6036773073101276
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62916887,  15.39690056,   5.57784841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5431811742286843}
episode index:3594
target Thresh 74.11981002056277
target distance 34.0
model initialize at round 3594
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.87238549,  17.09771277]), 'previousTarget': array([100.86301209,  16.66317505]), 'currentState': array([82.12293827, 20.25355404,  6.11348981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6037346231824255
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16662829,  14.8703846 ,   5.03911862]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21110457037833044}
episode index:3595
target Thresh 74.1216892707605
target distance 69.0
model initialize at round 3595
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([65.73729943,  5.23094585]), 'previousTarget': array([65.65421157,  5.7029674 ]), 'currentState': array([46.       ,  2.       ,  2.6989582], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.47168507596685355
running average episode reward sum: 0.6036979019512755
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.66517525,  15.31405486,   1.67958765]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7355872254324383}
episode index:3596
target Thresh 74.12356664264735
target distance 28.0
model initialize at round 3596
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.12011193,  11.30160352]), 'previousTarget': array([105.14017935,  10.42222613]), 'currentState': array([87.65742445,  3.61205407,  0.27494383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6037398873238783
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.95205098,  15.81598357,   5.06740716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2538860599687238}
episode index:3597
target Thresh 74.12544213810068
target distance 12.0
model initialize at round 3597
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34192297,   2.69224952,   2.32086849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.312499064667737}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6038209318950107
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.93687347,  15.3628644 ,   4.93232991]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.368314444921188}
episode index:3598
target Thresh 74.12731575899599
target distance 65.0
model initialize at round 3598
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.77395534,  7.08228791]), 'previousTarget': array([69.61161351,  5.9223227 ]), 'currentState': array([50.07357934,  3.63334812,  2.49729127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.49656933589465185
running average episode reward sum: 0.603791131507125
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5256494 ,  14.91892588,   5.14784986]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.481229162713951}
episode index:3599
target Thresh 74.12918750720692
target distance 61.0
model initialize at round 3599
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.37276523,  7.90425858]), 'previousTarget': array([73.62388913,  6.86043721]), 'currentState': array([53.65714582,  4.54355163,  0.77936816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.18055451254915478
running average episode reward sum: 0.6036735657796367
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.45455449,  15.25966851,   5.09970204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6041014336240272}
episode index:3600
target Thresh 74.13105738460519
target distance 62.0
model initialize at round 3600
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([72.91870112, 12.80148429]), 'previousTarget': array([72.95850618, 12.28764556]), 'currentState': array([53.      , 11.      ,  5.725411], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.1136670969383527
running average episode reward sum: 0.6035374906702667
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41067834,  14.76015844,   4.83315806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.636257798430597}
episode index:3601
target Thresh 74.13292539306069
target distance 70.0
model initialize at round 3601
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.93431751,  6.83390641]), 'previousTarget': array([64.66377472,  5.65184388]), 'currentState': array([45.19516509,  3.61430053,  0.44048262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.603507313315722
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01524178,  15.88025103,   5.53457613]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8803829738682135}
episode index:3602
target Thresh 74.13479153444142
target distance 26.0
model initialize at round 3602
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.21667958,  13.46110157]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([8.98892210e+01, 8.31820899e+00, 8.55885744e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.525114598707191
running average episode reward sum: 0.6034855556930164
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20335073,  14.75007698,   0.23540051]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32220030002530436}
episode index:3603
target Thresh 74.13665581061355
target distance 30.0
model initialize at round 3603
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.9678744 ,  10.67032934]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([86.60503981,  2.7452864 ,  5.20785511]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6035172551871633
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39021663,  14.33726058,   6.07935758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7690855329163351}
episode index:3604
target Thresh 74.13851822344131
target distance 62.0
model initialize at round 3604
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.84660472, 18.78939755]), 'previousTarget': array([72.95850618, 17.71235444]), 'currentState': array([52.92693009, 20.58008525,  0.67430544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6035317189778393
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29702699,  15.17003109,   0.20705171]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3422507861698468}
episode index:3605
target Thresh 74.14037877478715
target distance 33.0
model initialize at round 3605
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.38569163,  11.4904745 ]), 'previousTarget': array([101.5646825 ,  12.15008417]), 'currentState': array([82.01882265,  6.49804165,  3.75792813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6206362285316084
running average episode reward sum: 0.6035364623249146
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35157473,  15.8940567 ,   5.43918762]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9606987972453397}
episode index:3606
target Thresh 74.14223746651162
target distance 53.0
model initialize at round 3606
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.61889528,  8.88573646]), 'previousTarget': array([81.65323307,  8.70815718]), 'currentState': array([62.      ,  5.      ,  2.641159], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = -0.01984431966583622
running average episode reward sum: 0.6033636370457378
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18171556,  15.55738368,   5.33566842]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9900838284789315}
episode index:3607
target Thresh 74.14409430047338
target distance 12.0
model initialize at round 3607
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.69707561,   3.62209388,   1.68613935]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.519964957678935}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6034420797382739
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80442207,  14.04262637,   2.72110947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9771463536527105}
episode index:3608
target Thresh 74.1459492785293
target distance 31.0
model initialize at round 3608
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.62213848,  11.45392048]), 'previousTarget': array([103.03417236,  11.1400556 ]), 'currentState': array([85.69649109,  4.98709199,  5.2885415 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6034882424055132
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92149447,  14.10813959,   2.78130932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8953089498572063}
episode index:3609
target Thresh 74.14780240253434
target distance 7.0
model initialize at round 3609
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73431332,   9.20421295,   1.48360324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.842119784925024}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6035715924976471
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27281141,  15.95678947,   4.8827436 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9949231939883174}
episode index:3610
target Thresh 74.14965367434164
target distance 22.0
model initialize at round 3610
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.89161324,  15.00077316]), 'previousTarget': array([112.9793708 ,  14.90815322]), 'currentState': array([91.89161386, 15.00574782,  3.41470504]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7641216828154576
running average episode reward sum: 0.6036160538907012
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31983746,  14.69062716,   0.50831875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44498039602802086}
episode index:3611
target Thresh 74.15150309580244
target distance 68.0
model initialize at round 3611
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.96670825,  7.35992477]), 'previousTarget': array([66.69567118,  6.47570668]), 'currentState': array([48.22546523,  4.15316257,  1.56582278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.5309055429551132
running average episode reward sum: 0.60359592362743
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34505106,  14.8851322 ,   4.86608614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6649456582238518}
episode index:3612
target Thresh 74.1533506687662
target distance 55.0
model initialize at round 3612
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([81.75070792, 19.13983446]), 'previousTarget': array([79.88204353, 18.8310498 ]), 'currentState': array([61.90395323, 21.61093305,  0.27418599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6036177776743116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07296409,  15.10110856,   5.22722719]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9325333911507266}
episode index:3613
target Thresh 74.15519639508045
target distance 44.0
model initialize at round 3613
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.12147542, 17.95713056]), 'previousTarget': array([90.91786413, 17.18928508]), 'currentState': array([72.28647565, 20.52087472,  0.9578644 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6036616971365896
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22055844,  14.85619386,   5.26706936]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26329875540002384}
episode index:3614
target Thresh 74.15704027659098
target distance 47.0
model initialize at round 3614
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.99390151, 15.38368819]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([68.99607789, 15.67873164,  1.41995215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6011082622578465
running average episode reward sum: 0.6036609907922247
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99184759,  15.38269815,   5.35535611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38278497341185386}
episode index:3615
target Thresh 74.1588823151416
target distance 42.0
model initialize at round 3615
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.33079865, 19.43405838]), 'previousTarget': array([92.64677133, 19.25775784]), 'currentState': array([74.77570916, 23.62911202,  0.94200676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5911693117648533
running average episode reward sum: 0.6036575362349716
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96327053,  15.1339311 ,   5.19619627]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.13887617754907172}
episode index:3616
target Thresh 74.16072251257438
target distance 16.0
model initialize at round 3616
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.43056643, 18.65140802,  1.00500715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.966994704413935}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.603728423937047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71000753,  15.66149964,   6.11977089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7222723924870835}
episode index:3617
target Thresh 74.16256087072954
target distance 23.0
model initialize at round 3617
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.00465605,  14.958699  ]), 'previousTarget': array([111.92481176,  14.73259233]), 'currentState': array([92.00655697, 14.68295725,  2.57690024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6037922120347564
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61259092,  15.0210129 ,   5.63236915]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.38797852543059125}
episode index:3618
target Thresh 74.1643973914454
target distance 27.0
model initialize at round 3618
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.20403999,  14.14379686]), 'previousTarget': array([107.6656401,  13.6417852]), 'currentState': array([88.3609025 , 11.64381796,  2.36467397]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.43052100006830535
running average episode reward sum: 0.6037443338330526
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12281265,  15.88800531,   5.71963452]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2482031385149628}
episode index:3619
target Thresh 74.16623207655852
target distance 72.0
model initialize at round 3619
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([62.91191168, 19.12496048]), 'previousTarget': array([62.93091516, 19.3390904 ]), 'currentState': array([43.        , 21.        ,  0.14257583], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4356229998194988
running average episode reward sum: 0.6036978914755902
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.15677749,  14.09087929,   0.24921148]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9225397843938238}
episode index:3620
target Thresh 74.16806492790354
target distance 65.0
model initialize at round 3620
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.08638761, 14.14750613]), 'previousTarget': array([69.9787322 , 12.92209533]), 'currentState': array([50.08998933, 13.76795957,  2.3736186 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5167207618949434
running average episode reward sum: 0.6036738712796277
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37690719,  15.61987734,   5.46955749]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8789155654143558}
episode index:3621
target Thresh 74.16989594731335
target distance 28.0
model initialize at round 3621
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.24434509,  13.65655103]), 'previousTarget': array([106.55604828,  13.19058177]), 'currentState': array([86.47569966, 10.62329235,  0.91219878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.7176172745742565
running average episode reward sum: 0.6037053299773899
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.8484245 ,  15.31263486,   4.6076805 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9041928403586055}
episode index:3622
target Thresh 74.17172513661896
target distance 49.0
model initialize at round 3622
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.82733074, 16.22190315]), 'previousTarget': array([85.99583637, 15.59192171]), 'currentState': array([6.68461155e+01, 1.70885271e+01, 5.78427315e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6037328603017129
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70053646,  15.3310217 ,   4.96432288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4463785164295701}
episode index:3623
target Thresh 74.17355249764954
target distance 58.0
model initialize at round 3623
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.34031388, 10.04964917]), 'previousTarget': array([76.85591226, 10.3964032 ]), 'currentState': array([55.49431877,  7.57245764,  4.30611813]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6037028122330866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65860104,  15.48961351,   4.60879728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8206562717317967}
episode index:3624
target Thresh 74.17537803223247
target distance 64.0
model initialize at round 3624
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.06893299, 14.69993838]), 'previousTarget': array([70.99024152, 13.62469505]), 'currentState': array([51.06939951, 14.5633359 ,  2.53643352]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4346773683508578
running average episode reward sum: 0.6036561845244296
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20020201,  14.90996711,   5.39158208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8048495173700985}
episode index:3625
target Thresh 74.17720174219329
target distance 21.0
model initialize at round 3625
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.9890067 ,  13.50800676]), 'previousTarget': array([112.3829006 ,  13.87838597]), 'currentState': array([92.2438528 ,  6.53525943,  3.56342804]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6036916636157506
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85756219,  15.80056415,   5.88515669]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8131368155471919}
episode index:3626
target Thresh 74.17902362935568
target distance 23.0
model initialize at round 3626
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.42567309,  15.90273901]), 'previousTarget': array([111.13347761,  16.17676768]), 'currentState': array([93.55245161, 22.52101002,  5.60898215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6037671610955916
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92902805,  15.96362251,   5.58626539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9662325599242702}
episode index:3627
target Thresh 74.18084369554155
target distance 6.0
model initialize at round 3627
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.5029309,  18.6991106,   0.4326818]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.625797149544285}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6038655152435806
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92286922,  15.5369257 ,   0.21695197]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.542437426639338}
episode index:3628
target Thresh 74.18266194257099
target distance 47.0
model initialize at round 3628
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.43981114, 15.93952016]), 'previousTarget': array([88., 15.]), 'currentState': array([68.45231208, 16.64654294,  2.25700226]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.558315724978559
running average episode reward sum: 0.6038529636342488
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68307954,  15.17369923,   0.55727953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36140005574480444}
episode index:3629
target Thresh 74.1844783722622
target distance 6.0
model initialize at round 3629
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.11723356,  10.71944515,   2.44291234]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.779188958931039}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6039512399555617
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.54712106,  15.66617477,   1.20329416]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8620500458936091}
episode index:3630
target Thresh 74.18629298643164
target distance 69.0
model initialize at round 3630
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.67604528, 10.58911948]), 'previousTarget': array([65.92481176, 10.73259233]), 'currentState': array([47.76235706,  8.73304257,  0.67952722]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.36346253886083446
running average episode reward sum: 0.603885007870435
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01053561,  15.62788289,   5.56349376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6279712710999577}
episode index:3631
target Thresh 74.18810578689391
target distance 4.0
model initialize at round 3631
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.05584107,  17.85005523,   5.03494549]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.706796152991953}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6039832212520786
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.05215099,  15.94559005,   5.40337165]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.947027071678991}
episode index:3632
target Thresh 74.18991677546181
target distance 70.0
model initialize at round 3632
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.43875759,  7.70108501]), 'previousTarget': array([64.79898987,  7.82842712]), 'currentState': array([46.6609114 ,  4.72840946,  5.1111008 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.34905862298202217
running average episode reward sum: 0.603913052081071
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94822615,  14.74725822,   0.13154951]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.257990194159548}
episode index:3633
target Thresh 74.19172595394635
target distance 70.0
model initialize at round 3633
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.68546135, 10.9415222 ]), 'previousTarget': array([64.96742669, 12.14099581]), 'currentState': array([44.75020935,  9.33350234,  5.37561417]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6039100398065308
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31078155,  15.57331718,   5.29818242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6521332405585244}
episode index:3634
target Thresh 74.19353332415669
target distance 14.0
model initialize at round 3634
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.45168513,  13.56781791]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.       ,  17.       ,   2.1548188], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.882640483197498}
done in step count: 9
reward sum = 0.7056102474836408
running average episode reward sum: 0.6039380178554102
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68755239,  15.69307684,   5.22667581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7602493123808786}
episode index:3635
target Thresh 74.19533888790019
target distance 52.0
model initialize at round 3635
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.17394637, 19.86123955]), 'previousTarget': array([82.76743395, 19.95885631]), 'currentState': array([64.41809187, 22.97671915,  5.26233441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.603971307821885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43485711,  15.20870548,   5.29323308]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6024487244599219}
episode index:3636
target Thresh 74.19714264698243
target distance 1.0
model initialize at round 3636
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0093429 ,  15.84435082,   5.49709868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8444025078866131}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6040801966566879
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0093429 ,  15.84435082,   5.49709868]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8444025078866131}
episode index:3637
target Thresh 74.19894460320717
target distance 9.0
model initialize at round 3637
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.53957182,  21.32021571,   0.59420269]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.777684550428141}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6041703520033758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77444882,  15.22583728,   4.91998139]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8067053036169307}
episode index:3638
target Thresh 74.20074475837637
target distance 18.0
model initialize at round 3638
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.14228755, 22.17843488,  6.1897102 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.322456071304753}
done in step count: 11
reward sum = 0.7581173242587164
running average episode reward sum: 0.6042126567498048
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30724484,  15.98383966,   6.12445754]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2032664652169842}
episode index:3639
target Thresh 74.20254311429018
target distance 25.0
model initialize at round 3639
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.05179699,  13.07010976]), 'previousTarget': array([108.56953382,  12.42781353]), 'currentState': array([91.41882198,  5.80290657,  1.52496594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6042540047252042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5554152 ,  15.25242517,   5.05104318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5112475985958119}
episode index:3640
target Thresh 74.20433967274694
target distance 6.0
model initialize at round 3640
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.14479493,  19.57441193,   5.1820457 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.715485124943199}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6043414781362734
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58399554,  14.59563231,   3.17049441]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.710326705151389}
episode index:3641
target Thresh 74.20613443554323
target distance 42.0
model initialize at round 3641
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.10166611,  9.34947207]), 'previousTarget': array([92.23047895,  8.49442256]), 'currentState': array([7.37359795e+01, 4.35245477e+00, 6.24320507e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5501225638093719
running average episode reward sum: 0.6043265910098794
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76294133,  14.72643825,   5.5199493 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3619845907007813}
episode index:3642
target Thresh 74.2079274044738
target distance 72.0
model initialize at round 3642
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([61.712446 ,  7.8618816]), 'previousTarget': array([62.77059765,  7.02050797]), 'currentState': array([41.88950531,  5.20650549,  1.52292013]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6043186382673505
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35389632,  15.02356663,   5.62802384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6465333310273315}
episode index:3643
target Thresh 74.20971858133164
target distance 9.0
model initialize at round 3643
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.28393932,  17.63575441,   0.38892579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.105872565380169}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6044111633801753
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40939545,  14.85379826,   0.74606431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43471782182828506}
episode index:3644
target Thresh 74.2115079679079
target distance 8.0
model initialize at round 3644
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11103996e+02, 8.33265232e+00, 1.08921409e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.722199728533971}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6045062467509627
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6918694 ,  15.57514567,   2.06337118]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.652485256918898}
episode index:3645
target Thresh 74.21329556599198
target distance 23.0
model initialize at round 3645
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.9772742,  15.9579126]), 'previousTarget': array([111.7042351,  15.5731765]), 'currentState': array([91.52128149, 20.59087578,  0.85309243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7789969860600718
running average episode reward sum: 0.6045541048802301
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23591263,  14.94511259,   5.27616156]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7660562217980236}
episode index:3646
target Thresh 74.21508137737149
target distance 46.0
model initialize at round 3646
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.67807248, 19.30584398]), 'previousTarget': array([88.7042351, 19.5731765]), 'currentState': array([70.98430815, 22.79233888,  6.26226756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6045952798685282
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09875957,  15.37283348,   5.36475774]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9753148767819042}
episode index:3647
target Thresh 74.21686540383222
target distance 41.0
model initialize at round 3647
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.56354907, 15.21400832]), 'previousTarget': array([94., 15.]), 'currentState': array([72.56445882, 15.40476807,  1.85676551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6046262928215791
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65264834,  15.47998117,   5.37909614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5924821493914588}
episode index:3648
target Thresh 74.2186476471582
target distance 18.0
model initialize at round 3648
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.45924603,  14.19031632]), 'previousTarget': array([114.06563667,  14.42900019]), 'currentState': array([95.75501521,  4.88654229,  3.36708617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6291714973495734
running average episode reward sum: 0.6046330193780406
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14664627,  15.78152284,   5.47961018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.795162299070821}
episode index:3649
target Thresh 74.22042810913169
target distance 67.0
model initialize at round 3649
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.67636442, 16.19609935]), 'previousTarget': array([68., 15.]), 'currentState': array([47.68274955, 16.70143562,  0.82803583]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6046488136305354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61588809,  14.7791065 ,   5.62862477]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44309806414796754}
episode index:3650
target Thresh 74.22220679153314
target distance 34.0
model initialize at round 3650
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.92176069,  15.23267122]), 'previousTarget': array([100.96548746,  15.82555956]), 'currentState': array([81.      , 17.      ,  5.871377], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.5434436245487253
running average episode reward sum: 0.604632049678445
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6872895 ,  15.61952003,   5.44207254]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6939689703168916}
episode index:3651
target Thresh 74.22398369614122
target distance 59.0
model initialize at round 3651
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.99584827, 16.59250531]), 'previousTarget': array([75.98851894, 16.32242309]), 'currentState': array([56.    , 17.    ,  4.6292], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 54
reward sum = 0.44526280366003956
running average episode reward sum: 0.6045884107830402
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66763853,  14.79923456,   5.17661301]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3882922961734741}
episode index:3652
target Thresh 74.22575882473285
target distance 8.0
model initialize at round 3652
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.22253712,   9.47832439,   5.20597851]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.369800193722538}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6046755053036109
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.76139226,  15.25060379,   5.20806444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8015737221347842}
episode index:3653
target Thresh 74.22753217908317
target distance 49.0
model initialize at round 3653
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.3769269 , 16.69053729]), 'previousTarget': array([85.98336106, 16.18435261]), 'currentState': array([67.41427664, 17.91225526,  5.84606094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6047104335642433
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67221129,  14.77241251,   5.24873056]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.39905075655990485}
episode index:3654
target Thresh 74.22930376096552
target distance 48.0
model initialize at round 3654
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.40827813, 13.27733732]), 'previousTarget': array([86.98266146, 13.83261089]), 'currentState': array([68.45011346, 11.98440919,  0.24245756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6047393919209348
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95619801,  14.7874638 ,   5.37183161]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2170028865666739}
episode index:3655
target Thresh 74.23107357215147
target distance 50.0
model initialize at round 3655
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([84.98787424, 17.30366441]), 'previousTarget': array([84.96409691, 16.80215419]), 'currentState': array([65.       , 18.       ,  4.4691305], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6549803359578534
running average episode reward sum: 0.6047531339734614
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65184926,  15.81435973,   4.89288509]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.043115151444735}
episode index:3656
target Thresh 74.23284161441086
target distance 1.0
model initialize at round 3656
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22025841,  14.40170517,   3.50331473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.982829413230238}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6048612135102475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22025841,  14.40170517,   3.50331473]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.982829413230238}
episode index:3657
target Thresh 74.23460788951171
target distance 26.0
model initialize at round 3657
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.93520865,  14.42946656]), 'previousTarget': array([108.76743395,  14.04114369]), 'currentState': array([89.02312295, 12.55627588,  2.56593955]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6048881644346569
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47531184,  15.32118849,   5.35157131]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5736579061121683}
episode index:3658
target Thresh 74.2363723992203
target distance 42.0
model initialize at round 3658
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([92.78441866, 18.07138628]), 'previousTarget': array([92.79898987, 18.17157288]), 'currentState': array([73.      , 21.      ,  5.281998], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4905236312032323
running average episode reward sum: 0.6048569087546265
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92864485,  15.36529453,   5.20819983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9979084940629654}
episode index:3659
target Thresh 74.23813514530114
target distance 54.0
model initialize at round 3659
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([82.549183  , 18.59569515]), 'previousTarget': array([80.87767469, 18.79136948]), 'currentState': array([62.67084017, 20.79830413,  0.88986605]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6048536599944206
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22273828,  15.91902745,   5.29507972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9456340750640116}
episode index:3660
target Thresh 74.23989612951698
target distance 67.0
model initialize at round 3660
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.9746656 , 11.03362735]), 'previousTarget': array([67.89172986, 10.07824043]), 'currentState': array([49.04852101,  9.31643146,  1.90820092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6048195949292956
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.61785392,  14.65956538,   0.25109914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7054354684231033}
episode index:3661
target Thresh 74.2416553536288
target distance 20.0
model initialize at round 3661
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.29849471,  13.10637229]), 'previousTarget': array([112.14985851,  13.28991511]), 'currentState': array([93.49320828,  3.99748787,  3.33686531]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6048427051125123
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.03912489,  14.39471524,   3.54667557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6065479324827345}
episode index:3662
target Thresh 74.24341281939584
target distance 58.0
model initialize at round 3662
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.23741633,  6.95194171]), 'previousTarget': array([76.51579154,  6.37422914]), 'currentState': array([55.63491297,  2.98434013,  1.50685716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.4705866415856499
running average episode reward sum: 0.6048060531705175
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26686821,  15.38756112,   5.20346411]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8292682557926446}
episode index:3663
target Thresh 74.24516852857553
target distance 25.0
model initialize at round 3663
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.33522008,  11.96871769]), 'previousTarget': array([108.30630065,  12.05477229]), 'currentState': array([88.73685243,  4.6133985 ,  1.67952228]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6048388518284835
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45632216,  15.12179285,   0.83314615]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4722958907827027}
episode index:3664
target Thresh 74.24692248292362
target distance 28.0
model initialize at round 3664
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.06915367,  15.58596596]), 'previousTarget': array([106.98725709,  15.28616939]), 'currentState': array([88.14025073, 17.27084803,  1.80784803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6048818269615036
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6712952 ,  15.70885882,   1.47011697]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7813627032360367}
episode index:3665
target Thresh 74.24867468419406
target distance 27.0
model initialize at round 3665
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.77945854,  12.05051023]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([89.26459673,  4.48744921,  1.35879582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6049145870567016
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.84666356,  14.58819407,   0.54899713]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43942711682076735}
episode index:3666
target Thresh 74.25042513413902
target distance 15.0
model initialize at round 3666
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.94939117,  4.32987831,  3.22941208]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.273648868234158}
done in step count: 41
reward sum = 0.5602721815911259
running average episode reward sum: 0.604902412961947
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37825576,  14.83238817,   4.40862383]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4137283471915302}
episode index:3667
target Thresh 74.25217383450897
target distance 38.0
model initialize at round 3667
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.18246407, 16.76123563]), 'previousTarget': array([96.97235659, 15.94882334]), 'currentState': array([77.27946395, 18.72861614,  0.52052093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.32534647633291347
running average episode reward sum: 0.6048261981482531
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.5568773 ,  15.45947013,   5.64149284]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.721959231029546}
episode index:3668
target Thresh 74.2539207870526
target distance 27.0
model initialize at round 3668
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.80446723,  16.00572117]), 'previousTarget': array([107.94535509,  15.52256629]), 'currentState': array([87.99700785, 18.77421405,  0.72647965]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6048910978471455
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28653639,  15.04482394,   4.51434964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29002118703290114}
episode index:3669
target Thresh 74.25566599351689
target distance 11.0
model initialize at round 3669
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.52822755,  13.32405586]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  19.       ,   2.7372067], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.849761565182641}
done in step count: 8
reward sum = 0.85274469442792
running average episode reward sum: 0.6049586328870857
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29115413,  15.27684227,   6.2207059 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7609889072444734}
episode index:3670
target Thresh 74.25740945564702
target distance 59.0
model initialize at round 3670
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([75.5820635 ,  7.06728277]), 'previousTarget': array([75.59873325,  6.98618303]), 'currentState': array([56.       ,  3.       ,  5.3882337], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.12317117895289836
running average episode reward sum: 0.6048273914123011
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29757235,  15.89818254,   5.42989582]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1402352735870496}
episode index:3671
target Thresh 74.25915117518646
target distance 38.0
model initialize at round 3671
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([97.37585001, 18.23010437]), 'previousTarget': array([96.82908591, 17.39090975]), 'currentState': array([77.70352293, 21.83559285,  1.52919328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.22464668835029783
running average episode reward sum: 0.6047238563624477
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83592099,  15.24588763,   5.74007567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29560556050163195}
episode index:3672
target Thresh 74.26089115387693
target distance 5.0
model initialize at round 3672
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.23265344,  18.29717945,   3.94902372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.981976100402458}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6048207450511592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72665883,  15.23291634,   2.99779916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7630747549523257}
episode index:3673
target Thresh 74.26262939345841
target distance 32.0
model initialize at round 3673
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.48157978,  15.9279807 ]), 'previousTarget': array([102.96105157,  15.75243428]), 'currentState': array([84.55896367, 17.68564038,  1.31594199]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6048636198386649
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00321846,  15.72254649,   5.20051097]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2311161030189082}
episode index:3674
target Thresh 74.26436589566914
target distance 64.0
model initialize at round 3674
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.36180959, 13.9466348 ]), 'previousTarget': array([70.99024152, 13.62469505]), 'currentState': array([49.36713469, 13.48514203,  3.82803011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6048756576641152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.80205251,  15.43548245,   5.74052926]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47835987947994335}
episode index:3675
target Thresh 74.2661006622456
target distance 43.0
model initialize at round 3675
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.05753869, 11.12414443]), 'previousTarget': array([91.7401454 , 11.21351204]), 'currentState': array([70.29471823,  8.0531629 ,  2.13984859]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.6048645122704231
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02221577,  14.94844577,   3.91856852]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.0561371479456507}
episode index:3676
target Thresh 74.26783369492261
target distance 10.0
model initialize at round 3676
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.50852482,   6.60430377,   2.27383944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.252107160772837}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6049509632310316
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53346532,  15.80749148,   5.92791902]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9325755172364458}
episode index:3677
target Thresh 74.26956499543316
target distance 10.0
model initialize at round 3677
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.68810038,  23.53587302,   0.13959098]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.632204982085788}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6050373671818736
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22713795,  14.76246739,   5.9390633 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32865390706467995}
episode index:3678
target Thresh 74.27129456550855
target distance 12.0
model initialize at round 3678
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.07314502,  14.11904048]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,   3.       ,   3.4403977], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.41322308580594}
done in step count: 18
reward sum = 0.4248748072570875
running average episode reward sum: 0.6049883966572951
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.71265795,  14.88644298,   4.75352678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3089670680331234}
episode index:3679
target Thresh 74.27302240687837
target distance 7.0
model initialize at round 3679
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.63992919,  22.02471204,   5.28825313]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.267816921229889}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6050850291609207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08703877,  14.97152878,   5.18778974]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9134050655267405}
episode index:3680
target Thresh 74.27474852127047
target distance 13.0
model initialize at round 3680
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.68887657,   2.0941345 ,   1.05129796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.832865203541617}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6051638808928136
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.36005705,  14.71765281,   5.85081075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4575598502322129}
episode index:3681
target Thresh 74.27647291041092
target distance 62.0
model initialize at round 3681
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.76005098, 14.78416532]), 'previousTarget': array([73., 15.]), 'currentState': array([54.76033866, 14.67689302,  0.63384443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6051621948867227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17811409,  15.71225578,   4.99634539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0875682688029493}
episode index:3682
target Thresh 74.27819557602415
target distance 1.0
model initialize at round 3682
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98539489,  15.55062909,   5.57432675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5508227527781601}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6052694003727703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98539489,  15.55062909,   5.57432675]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5508227527781601}
episode index:3683
target Thresh 74.2799165198328
target distance 42.0
model initialize at round 3683
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.03422279, 13.71645002]), 'previousTarget': array([92.99433347, 14.47605556]), 'currentState': array([72.06538637, 12.60039811,  3.11362219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.2301538758600456
running average episode reward sum: 0.6051675774833803
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44671749,  15.93020203,   5.75021065]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0823111156962446}
episode index:3684
target Thresh 74.28163574355784
target distance 16.0
model initialize at round 3684
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([100.12114455,  19.94084018,   6.0399217 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.677762630843363}
done in step count: 10
reward sum = 0.7678541450088043
running average episode reward sum: 0.6052117258056396
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46514057,  14.1297714 ,   5.86400575]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0214560280333778}
episode index:3685
target Thresh 74.28335324891847
target distance 67.0
model initialize at round 3685
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.97305805,  7.43634074]), 'previousTarget': array([67.68673286,  6.52598201]), 'currentState': array([49.23775608,  4.19321631,  1.66487854]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 62
reward sum = 0.4083499730555855
running average episode reward sum: 0.6051583178423324
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98154191,  15.64581059,   4.93940826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6460743113870868}
episode index:3686
target Thresh 74.28506903763221
target distance 68.0
model initialize at round 3686
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.57757973,  7.70579314]), 'previousTarget': array([66.74334797,  7.19377688]), 'currentState': array([45.79190974,  4.78564538,  1.55499244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = 0.3610851950359398
running average episode reward sum: 0.6050921195448532
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18767517,  15.0063436 ,   5.02843777]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8123495991026182}
episode index:3687
target Thresh 74.28678311141485
target distance 48.0
model initialize at round 3687
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.30045112, 17.65611669]), 'previousTarget': array([86.96105157, 16.75243428]), 'currentState': array([66.3855581 , 19.49922161,  0.9497571 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6051187886271349
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.83879671,  15.372     ,   5.05994084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40542631708936067}
episode index:3688
target Thresh 74.28849547198045
target distance 30.0
model initialize at round 3688
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.51906723,  15.18661293]), 'previousTarget': array([104.98889814,  15.3337034 ]), 'currentState': array([86.52390715, 15.62658287,  0.73249548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6051720582939584
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53045011,  15.58937369,   5.02665288]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7535505641656207}
episode index:3689
target Thresh 74.2902061210414
target distance 40.0
model initialize at round 3689
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.20755472, 16.2825438 ]), 'previousTarget': array([94.99375293, 15.50015618]), 'currentState': array([74.24549462, 17.51386682,  1.03749514]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6052105406885485
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77521591,  15.15766913,   0.82447878]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7910873896640287}
episode index:3690
target Thresh 74.29191506030831
target distance 70.0
model initialize at round 3690
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.52956722, 11.00840623]), 'previousTarget': array([64.96742669, 12.14099581]), 'currentState': array([44.59182406,  9.43157467,  5.3694936 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.4998370298991989
running average episode reward sum: 0.6051819919183536
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0805945 ,  15.32713337,   0.37218466]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9758702293520999}
episode index:3691
target Thresh 74.29362229149014
target distance 9.0
model initialize at round 3691
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.06664116,  15.19344915,   4.7548067 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.9357170440928755}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6052730802600336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26519516,  14.23118882,   0.39267328]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8132644678550732}
episode index:3692
target Thresh 74.29532781629413
target distance 60.0
model initialize at round 3692
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.34417097, 19.87980591]), 'previousTarget': array([74.86526278, 19.68238601]), 'currentState': array([56.50164974, 22.3846714 ,  1.26077336]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3677779514976239
running average episode reward sum: 0.6052087707206991
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07871043,  15.07679925,   5.02938869]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.10997025137867152}
episode index:3693
target Thresh 74.29703163642577
target distance 6.0
model initialize at round 3693
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.64782384,  15.61249583,   0.76248711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.395064103829125}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.605307604025864
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72385622,  14.74027322,   5.61577022]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7690421536467383}
episode index:3694
target Thresh 74.29873375358893
target distance 72.0
model initialize at round 3694
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.36186851,  9.09857692]), 'previousTarget': array([62.84555753,  8.48069469]), 'currentState': array([44.49631875,  6.78342408,  1.41974979]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.25577317848502645
running average episode reward sum: 0.6052130074289653
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.42765198,  15.01114772,   4.48838743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4277972482392195}
episode index:3695
target Thresh 74.30043416948568
target distance 19.0
model initialize at round 3695
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.53673413,  16.27972188]), 'previousTarget': array([115.,  15.]), 'currentState': array([96.       , 12.       ,  5.0160904], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5856803359578534
running average episode reward sum: 0.6052077226152556
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31501977,  14.78711764,   5.29893848]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7172982755848564}
episode index:3696
target Thresh 74.30213288581648
target distance 9.0
model initialize at round 3696
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.45874183,   8.4180537 ,   4.00240231]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.783093624586707}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6052886461620216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42636391,  14.79530657,   0.72303974]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6090630223407981}
episode index:3697
target Thresh 74.30382990428001
target distance 61.0
model initialize at round 3697
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.94679946,  6.30595684]), 'previousTarget': array([73.56072872,  6.16867989]), 'currentState': array([52.34252333,  2.34712404,  3.9274739 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 60
reward sum = 0.5056489911426563
running average episode reward sum: 0.6052617019610969
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50348982,  15.45953207,   5.13140772]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6765294376629967}
episode index:3698
target Thresh 74.30552522657331
target distance 30.0
model initialize at round 3698
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.11221146,  10.00736923]), 'previousTarget': array([103.56953382,  10.42781353]), 'currentState': array([8.59324296e+01, 1.67097125e+00, 5.09940942e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6053000664899887
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42909818,  15.54376491,   5.29090311]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.788421948833026}
episode index:3699
target Thresh 74.30721885439169
target distance 24.0
model initialize at round 3699
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.18029945,  13.59217008]), 'previousTarget': array([110.57960839,  14.07908508]), 'currentState': array([89.74100019,  8.88965543,  3.01217723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6053445916465603
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23583903,  15.76880443,   5.17637863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8041643526395033}
episode index:3700
target Thresh 74.3089107894288
target distance 66.0
model initialize at round 3700
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.61788174, 10.24935732]), 'previousTarget': array([68.88845169, 10.10938124]), 'currentState': array([50.73148118,  8.12072564,  0.98782771]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.2934532491191134
running average episode reward sum: 0.6052603194653856
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41948533,  14.89751313,   5.67687753]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5894920190297082}
episode index:3701
target Thresh 74.31060103337653
target distance 36.0
model initialize at round 3701
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([98.50190835, 17.61798167]), 'previousTarget': array([98.87767469, 16.79136948]), 'currentState': array([78.74905636, 20.75244143,  2.56657141]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7260760968305775
running average episode reward sum: 0.6052929547374994
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76013589,  15.05976741,   0.86638905]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24719817387108015}
episode index:3702
target Thresh 74.31228958792516
target distance 31.0
model initialize at round 3702
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.69173226,  18.18463817]), 'previousTarget': array([103.50882004,  17.59478257]), 'currentState': array([84.44057276, 23.60615619,  0.31364858]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.6217005041524029
running average episode reward sum: 0.6052973856177087
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91281379,  15.25210169,   5.3534178 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2667521319982958}
episode index:3703
target Thresh 74.31397645476325
target distance 6.0
model initialize at round 3703
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.32094009,  19.66606551,   0.39372444]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.350094480924514}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6053933085724555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15637026,  14.63980384,   5.57743784]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9173071550937852}
episode index:3704
target Thresh 74.31566163557765
target distance 18.0
model initialize at round 3704
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.52428547, 18.72309437,  0.76401406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.891139713747762}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.60547156631758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15129530e+02, 1.51784302e+01, 1.14591401e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.22048884646870834}
episode index:3705
target Thresh 74.31734513205355
target distance 27.0
model initialize at round 3705
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.75651585,  11.77389053]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([86.87355326,  5.18346386,  1.42034006]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6055118382336312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22491718,  15.13830002,   5.52510933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.78732475900942}
episode index:3706
target Thresh 74.31902694587443
target distance 11.0
model initialize at round 3706
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.35455233,  13.49384648]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.        ,   8.        ,   0.31152588], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.721736349124097}
done in step count: 17
reward sum = 0.5684668898909268
running average episode reward sum: 0.6055018449915641
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73698944,  15.43746637,   5.25224841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5104423359654402}
episode index:3707
target Thresh 74.32070707872212
target distance 42.0
model initialize at round 3707
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.74529051,  8.61216282]), 'previousTarget': array([92.23047895,  8.49442256]), 'currentState': array([74.59160063,  2.85576081,  0.91962164]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6055033593913866
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54331446,  15.19902338,   5.64587635]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.49816863164613057}
episode index:3708
target Thresh 74.32238553227674
target distance 46.0
model initialize at round 3708
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.21293015, 11.85677101]), 'previousTarget': array([88.77237675, 11.00883994]), 'currentState': array([68.34921414,  9.52593721,  2.9750604 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6055297665996929
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.425961  ,  15.15587049,   5.54218953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5948246644677647}
episode index:3709
target Thresh 74.32406230821677
target distance 11.0
model initialize at round 3709
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.61138334,   4.4926902 ,   0.69424265]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 14.090765840437925}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6056127820931926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12426967,  14.855167  ,   1.54266947]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8876261676429855}
episode index:3710
target Thresh 74.32573740821896
target distance 52.0
model initialize at round 3710
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.05603761,  7.92332259]), 'previousTarget': array([82.40285  ,  6.8507125]), 'currentState': array([63.52945525,  3.59751846,  2.26076555]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.531035076466161
running average episode reward sum: 0.6055926857025629
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19417807,  14.81173723,   5.28823345]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8275215172689818}
episode index:3711
target Thresh 74.32741083395841
target distance 56.0
model initialize at round 3711
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([77.77434344,  8.71889661]), 'previousTarget': array([78.62497641,  7.85490608]), 'currentState': array([58.05310491,  5.39132231,  1.15836406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6055925276695442
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85225504,  15.30113824,   5.36204886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9038931889598835}
episode index:3712
target Thresh 74.32908258710857
target distance 13.0
model initialize at round 3712
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.57790095,   3.23375705,   2.76462954]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.84929175413247}
done in step count: 33
reward sum = 0.39818909408735476
running average episode reward sum: 0.6055366689478684
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34481218,  15.12959873,   6.2152661 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6678824082627074}
episode index:3713
target Thresh 74.33075266934117
target distance 61.0
model initialize at round 3713
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.16792752, 11.98785236]), 'previousTarget': array([73.90394822, 10.9577654 ]), 'currentState': array([54.2221253 , 10.51646731,  2.4353551 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.38881845071999893
running average episode reward sum: 0.6054783172466761
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76282529,  14.93759162,   6.18275082]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24524813258211267}
episode index:3714
target Thresh 74.3324210823263
target distance 15.0
model initialize at round 3714
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([98.22666239, 23.2126055 ,  2.1994946 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.67596700787734}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6055563414558315
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68278407,  15.28783686,   5.28850755]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42834099606510406}
episode index:3715
target Thresh 74.33408782773238
target distance 24.0
model initialize at round 3715
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.41331539,  14.1521183 ]), 'previousTarget': array([110.72787848,  14.28797975]), 'currentState': array([91.94976877,  9.55099308,  6.21650124]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6055698244695877
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.87112929,  14.79455496,   4.80498912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8950273227860719}
episode index:3716
target Thresh 74.33575290722614
target distance 7.0
model initialize at round 3716
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.282683  ,  20.87809463,   2.36044478]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.513972240599058}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6056576629208758
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12184439,  14.52370382,   5.285404  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9990071705014111}
episode index:3717
target Thresh 74.33741632247268
target distance 69.0
model initialize at round 3717
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.89440949, 19.07427948]), 'previousTarget': array([65.92481176, 19.26740767]), 'currentState': array([47.96880185, 20.79769476,  5.92789194]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.3465082633426423
running average episode reward sum: 0.6055879616299725
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22476717,  15.29997527,   5.11647904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.831246721783119}
episode index:3718
target Thresh 74.33907807513539
target distance 57.0
model initialize at round 3718
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([76.78365435, 17.91210442]), 'previousTarget': array([77.95093522, 17.59993437]), 'currentState': array([56.84146786, 19.43170889,  1.77489758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6055911082229578
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29500298,  14.67840161,   5.2278696 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7748847170358596}
episode index:3719
target Thresh 74.34073816687604
target distance 47.0
model initialize at round 3719
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.93545117, 15.19388281]), 'previousTarget': array([87.9954746 , 14.42543563]), 'currentState': array([66.93592842, 15.33204871,  1.27210557]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6056045673929447
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5768343 ,  15.05222467,   0.15204111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4263761551885606}
episode index:3720
target Thresh 74.34239659935473
target distance 54.0
model initialize at round 3720
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.92502047, 13.73019058]), 'previousTarget': array([80.96920706, 13.10940039]), 'currentState': array([61.      , 12.      ,  3.793265], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 42
reward sum = 0.25768854721485823
running average episode reward sum: 0.6055110667156596
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86205575,  14.87735242,   5.96636536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1845834325872219}
episode index:3721
target Thresh 74.34405337422987
target distance 27.0
model initialize at round 3721
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.70147078,  10.47461953]), 'previousTarget': array([106.0200334 ,  10.67631238]), 'currentState': array([86.39125794,  2.42874445,  1.88415623]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6055491271744494
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52706534,  15.40957971,   5.32445896]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6256378626030825}
episode index:3722
target Thresh 74.34570849315824
target distance 33.0
model initialize at round 3722
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.97259738,  15.04659161]), 'previousTarget': array([101.99082358,  14.60578253]), 'currentState': array([82.      , 14.      ,  4.896157], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.3594050182452608
running average episode reward sum: 0.6054830127213392
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.985748  ,  14.75880917,   4.1587019 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2416115381105776}
episode index:3723
target Thresh 74.34736195779499
target distance 28.0
model initialize at round 3723
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.76199181,  16.24838036]), 'previousTarget': array([106.55604828,  16.80941823]), 'currentState': array([8.80529937e+01, 1.96477031e+01, 4.77924903e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.6919265145485043
running average episode reward sum: 0.6055062252621092
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69488447,  14.52381086,   3.86367118]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8423897681694339}
episode index:3724
target Thresh 74.34901376979354
target distance 67.0
model initialize at round 3724
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.62012294, 12.04040354]), 'previousTarget': array([67.94453985, 11.4883985 ]), 'currentState': array([49.66252194, 10.7388036 ,  6.06293971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5929664464014994
running average episode reward sum: 0.6055028588785224
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26121142,  15.93117707,   5.22372587]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1886544132672905}
episode index:3725
target Thresh 74.35066393080572
target distance 49.0
model initialize at round 3725
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.7520236 , 11.91747049]), 'previousTarget': array([85.93369232, 12.62724019]), 'currentState': array([66.8700508 ,  9.74787196,  4.30965539]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6055128182835697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26907232,  15.42360162,   5.02973401]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5018348809252515}
episode index:3726
target Thresh 74.3523124424817
target distance 10.0
model initialize at round 3726
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.92171445,   6.97085293,   1.31928825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.599014165052095}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.605602962456126
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46106815,  15.73005639,   5.84718594]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9074303646029768}
episode index:3727
target Thresh 74.35395930646999
target distance 68.0
model initialize at round 3727
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.27300495, 18.92737255]), 'previousTarget': array([66.96548746, 17.82555956]), 'currentState': array([46.33765297, 20.53415241,  1.01445162]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6056011794743691
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49929117,  15.33007791,   5.21351447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5997172356763959}
episode index:3728
target Thresh 74.35560452441744
target distance 8.0
model initialize at round 3728
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58151606,  23.45043341,   5.67758257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.460789180651362}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6056494704267243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46426119,  15.09346949,   3.02081712]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5438314233306885}
episode index:3729
target Thresh 74.35724809796929
target distance 72.0
model initialize at round 3729
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.17565073,  9.29817046]), 'previousTarget': array([62.84555753,  8.48069469]), 'currentState': array([44.30033416,  7.06841899,  1.6978032 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.49483865960020695
running average episode reward sum: 0.6056197624345456
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6954476 ,  15.46381379,   5.28504862]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5548652034162774}
episode index:3730
target Thresh 74.3588900287691
target distance 63.0
model initialize at round 3730
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.10881477, 13.92083502]), 'previousTarget': array([71.97736275, 12.95130299]), 'currentState': array([53.11544784, 13.40578296,  1.70571944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.30676431766652
running average episode reward sum: 0.60553966180609
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18488761,  14.22752151,   5.97896724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7942961940050363}
episode index:3731
target Thresh 74.36053031845879
target distance 21.0
model initialize at round 3731
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.24500343,  13.15993966]), 'previousTarget': array([111.36486284,  12.92277877]), 'currentState': array([93.2854091 ,  4.35920268,  3.01321256]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6006617507176383
running average episode reward sum: 0.6055383547559592
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89301314,  15.69818598,   5.25598914]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7063355072729851}
episode index:3732
target Thresh 74.36216896867867
target distance 64.0
model initialize at round 3732
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.16137201, 10.19855686]), 'previousTarget': array([70.88143384, 10.17453183]), 'currentState': array([49.27019595,  8.11502263,  2.15325153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.525596487525562
running average episode reward sum: 0.6055169398437624
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08572281,  14.31578985,   5.70393481]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1419484726320275}
episode index:3733
target Thresh 74.36380598106737
target distance 5.0
model initialize at round 3733
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.99163365,  18.70783174,   4.35694496]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.764230064039725}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6056120333280034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87354202,  15.73633968,   4.13277886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7471196333147097}
episode index:3734
target Thresh 74.36544135726193
target distance 12.0
model initialize at round 3734
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([116.79502948,  15.21202267]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,   8.       ,   2.4955273], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.566506003967916}
done in step count: 11
reward sum = 0.8253382542587164
running average episode reward sum: 0.6056708623028176
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54173433,  15.35314354,   0.75070414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5785479925186382}
episode index:3735
target Thresh 74.3670750988977
target distance 48.0
model initialize at round 3735
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.69504867, 11.27103606]), 'previousTarget': array([86.89236818, 12.07212169]), 'currentState': array([67.87898604,  8.56480955,  6.27191448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6056914414603946
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05909753,  14.67293908,   5.758885  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9961256473853981}
episode index:3736
target Thresh 74.36870720760844
target distance 20.0
model initialize at round 3736
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([113.87716713,  15.39299151]), 'currentState': array([9.63712615e+01, 2.08307272e+01, 8.42182001e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.51991996875111}
done in step count: 12
reward sum = 0.7512222210161292
running average episode reward sum: 0.6057303846714076
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43272428,  15.24011842,   5.9888687 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6160021076995343}
episode index:3737
target Thresh 74.37033768502624
target distance 9.0
model initialize at round 3737
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.06681549e+02, 7.34757411e+00, 9.25512314e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.302930798084832}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6058176866947452
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15274915,  14.4735248 ,   6.13645757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9975019504299046}
episode index:3738
target Thresh 74.37196653278161
target distance 24.0
model initialize at round 3738
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.66041853,  11.61510965]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([89.49877541,  3.23926823,  2.60061836]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6515472037354172
running average episode reward sum: 0.6058299171085031
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27581036,  15.62441916,   0.78749822]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9562164619759974}
episode index:3739
target Thresh 74.37359375250337
target distance 46.0
model initialize at round 3739
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.36207601, 19.41691613]), 'previousTarget': array([88.83200822, 18.41321632]), 'currentState': array([68.63147323, 22.68850808,  0.85408723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.48942582247815714
running average episode reward sum: 0.6057987930190297
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88096819,  14.10582106,   5.74174093]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.902066819242898}
episode index:3740
target Thresh 74.37521934581873
target distance 45.0
model initialize at round 3740
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.77100863,  8.91534388]), 'previousTarget': array([89.32469879,  8.15325301]), 'currentState': array([71.37333268,  4.04398188,  5.99592263]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 53
reward sum = 0.18904049309647303
running average episode reward sum: 0.605687390105391
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94165683,  14.06480404,   6.07730676]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9370140935474011}
episode index:3741
target Thresh 74.37684331435332
target distance 26.0
model initialize at round 3741
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.71414726,  12.24379498]), 'previousTarget': array([107.66691212,  12.17958159]), 'currentState': array([87.73653694,  5.93108506,  1.50805855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6544237873476586
running average episode reward sum: 0.6057004142628581
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37687386,  15.06364403,   5.46093137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6263678978533237}
episode index:3742
target Thresh 74.37846565973109
target distance 47.0
model initialize at round 3742
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.37790023, 14.10792469]), 'previousTarget': array([87.98191681, 13.85029433]), 'currentState': array([66.38760721, 13.48488002,  1.88642645]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5841800945372019
running average episode reward sum: 0.6056946647785606
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40431504,  15.64095501,   5.63019909]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8750222319111907}
episode index:3743
target Thresh 74.38008638357438
target distance 60.0
model initialize at round 3743
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.99419962, 15.51835531]), 'previousTarget': array([74.9972228 , 15.66671295]), 'currentState': array([55.      , 16.      ,  5.650954], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.270737325581795
running average episode reward sum: 0.6056051996772794
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38521013,  14.67672125,   6.22667424]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6946047301292247}
episode index:3744
target Thresh 74.38170548750391
target distance 2.0
model initialize at round 3744
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.53203164,  17.61659942,   0.84257126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.596868134195931}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6057025811993949
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48091791,  14.64297466,   4.66628047]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6300105650621127}
episode index:3745
target Thresh 74.3833229731388
target distance 46.0
model initialize at round 3745
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.27977107, 16.16367601]), 'previousTarget': array([88.99527578, 15.56532009]), 'currentState': array([70.30189377, 17.10411133,  1.67234498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6057286743958179
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70070516,  14.07140934,   3.09848937]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9756321126613287}
episode index:3746
target Thresh 74.38493884209652
target distance 41.0
model initialize at round 3746
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.51369664, 19.63020718]), 'previousTarget': array([93.62981184, 19.16979281]), 'currentState': array([73.9625084 , 23.84340527,  0.62116361]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6057644287857278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64526489,  14.49918502,   5.89693126]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6137203311109722}
episode index:3747
target Thresh 74.38655309599295
target distance 68.0
model initialize at round 3747
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.66986997,  9.56973907]), 'previousTarget': array([66.82709532,  8.62417438]), 'currentState': array([4.78002179e+01, 7.29006235e+00, 5.58829308e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5753547499769285
running average episode reward sum: 0.6057563152108055
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07909724,  14.74322142,   0.86516329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2686849674365114}
episode index:3748
target Thresh 74.38816573644235
target distance 26.0
model initialize at round 3748
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.0288455 ,  15.06390134]), 'previousTarget': array([108.98522349,  14.76866244]), 'currentState': array([88.0296857 , 15.24722435,  1.22196484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7340122396670141
running average episode reward sum: 0.6057905259135147
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47371974,  15.17114033,   1.04207711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5036858218731833}
episode index:3749
target Thresh 74.38977676505735
target distance 64.0
model initialize at round 3749
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.04177871, 12.5101706 ]), 'previousTarget': array([70.93924283, 11.55775335]), 'currentState': array([50.07237884, 11.40424609,  1.14457536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 72
reward sum = 0.39993818013653804
running average episode reward sum: 0.6057356319546408
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.57423608,  14.72022433,   5.80661548]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5094598508825826}
episode index:3750
target Thresh 74.39138618344897
target distance 46.0
model initialize at round 3750
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.9447466 , 16.76984418]), 'previousTarget': array([88.98112317, 16.13125551]), 'currentState': array([69.99445742, 18.17908569,  6.25948435]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6057616815582252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23669898,  15.71573861,   5.42429727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7538621673732475}
episode index:3751
target Thresh 74.39299399322665
target distance 7.0
model initialize at round 3751
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.49333645,  11.62289501,   0.95300817]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.231332567849185}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6058461653036594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59821006,  15.45479141,   4.8313284 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6068528530224008}
episode index:3752
target Thresh 74.3946001959982
target distance 23.0
model initialize at round 3752
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.45186182,  15.33569767]), 'previousTarget': array([111.7042351,  15.5731765]), 'currentState': array([93.90609882, 19.57399317,  0.12319949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6059162158358911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.26624916,  14.02567625,   6.24427355]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0100472207001974}
episode index:3753
target Thresh 74.39620479336979
target distance 70.0
model initialize at round 3753
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.52178449, 19.66954323]), 'previousTarget': array([64.9007438 , 20.00992562]), 'currentState': array([46.61392408, 21.58711824,  0.69265741]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5532270410003567
running average episode reward sum: 0.6059021803604421
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76351297,  15.40807054,   5.54772492]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47164359602434347}
episode index:3754
target Thresh 74.39780778694605
target distance 25.0
model initialize at round 3754
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.19289559,  15.51714318]), 'previousTarget': array([109.98401917,  15.20063923]), 'currentState': array([90.3076329 , 17.65637857,  0.38007379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.6400718150582082
running average episode reward sum: 0.605911280130002
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13818642,  14.8034433 ,   3.93424952]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24027072383260567}
episode index:3755
target Thresh 74.39940917832996
target distance 24.0
model initialize at round 3755
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.46147019,  12.45566234]), 'previousTarget': array([109.18129646,  12.33309421]), 'currentState': array([89.82289481,  5.20283476,  1.33538389]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 42
reward sum = 0.5972432572726375
running average episode reward sum: 0.6059089723496885
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51483314,  14.36568495,   4.60504213]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8169508763835347}
episode index:3756
target Thresh 74.40100896912293
target distance 61.0
model initialize at round 3756
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.9475404 , 10.44762972]), 'previousTarget': array([73.90394822, 10.9577654 ]), 'currentState': array([54.       ,  9.       ,  1.0233849], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5170367819374844
running average episode reward sum: 0.6058853172550885
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14212304e+02, 1.58015433e+01, 1.09602347e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1238045608926845}
episode index:3757
target Thresh 74.40260716092472
target distance 56.0
model initialize at round 3757
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([78.29698978, 17.91062373]), 'previousTarget': array([78.97136265, 16.93010557]), 'currentState': array([58.35958285, 19.49170086,  2.98632044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4770853413777182
running average episode reward sum: 0.605851043711747
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46951527,  15.7481951 ,   3.9190596 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9171749854923011}
episode index:3758
target Thresh 74.40420375533354
target distance 46.0
model initialize at round 3758
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.24163659, 16.57406235]), 'previousTarget': array([88.92481176, 17.26740767]), 'currentState': array([70.28193481, 17.84304021,  0.19996136]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6058886390963227
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44838515,  14.84423379,   5.23270641]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5731858805940838}
episode index:3759
target Thresh 74.40579875394597
target distance 26.0
model initialize at round 3759
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.37612833,  17.29279618]), 'previousTarget': array([108.11558017,  17.11828302]), 'currentState': array([89.47633412, 23.83479732,  0.36214244]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6059562374249251
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16907018,  14.47900006,   5.36040459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9807575173811084}
episode index:3760
target Thresh 74.40739215835704
target distance 10.0
model initialize at round 3760
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.21333312,  23.76834997,   6.03881294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.413278214433902}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6060355848956999
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.88068736,  15.57057194,   2.43848497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.049362932829763}
episode index:3761
target Thresh 74.40898397016014
target distance 43.0
model initialize at round 3761
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.90107987, 17.38217942]), 'previousTarget': array([91.95150202, 16.60803474]), 'currentState': array([70.99808288, 19.34959164,  3.21940625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6060541105374236
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.64583766,  14.4932867 ,   5.42804521]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6182146294707884}
episode index:3762
target Thresh 74.41057419094706
target distance 71.0
model initialize at round 3762
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.84833582, 10.18684944]), 'previousTarget': array([63.9035   ,  9.9623169]), 'currentState': array([45.94354366,  8.23768326,  6.04530451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.3950128251044931
running average episode reward sum: 0.605998027283256
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72279263,  14.71877932,   0.64475622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3948784556004596}
episode index:3763
target Thresh 74.41216282230805
target distance 13.0
model initialize at round 3763
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.19470376,  18.97776678,   0.20188075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.457433439081479}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6060797273948926
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.01347512,  14.47942443,   3.10244924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5207499401820653}
episode index:3764
target Thresh 74.41374986583172
target distance 30.0
model initialize at round 3764
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.43038902,  14.24921694]), 'previousTarget': array([104.9007438 ,  13.99007438]), 'currentState': array([86.50670498, 12.50370428,  1.33262795]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6061316665349042
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27226666,  15.68665335,   5.47990021]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0005441688604897}
episode index:3765
target Thresh 74.41533532310514
target distance 70.0
model initialize at round 3765
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.51275018, 16.38982035]), 'previousTarget': array([64.99184173, 16.42880452]), 'currentState': array([46.52096115, 16.96285749,  0.98558443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.2885400964467695
running average episode reward sum: 0.6060473352629744
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37684786,  15.84528644,   5.13001241]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0501560619273178}
episode index:3766
target Thresh 74.41691919571375
target distance 37.0
model initialize at round 3766
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.04230489, 16.87167363]), 'previousTarget': array([97.93458096, 16.38368262]), 'currentState': array([79.17847037, 19.20149727,  6.06825847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.3814821872345512
running average episode reward sum: 0.6059877214726829
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22702312,  15.61872625,   5.39160133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9901087968131971}
episode index:3767
target Thresh 74.4185014852414
target distance 27.0
model initialize at round 3767
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.32296031,  12.27862136]), 'previousTarget': array([106.5218472 ,  11.54593775]), 'currentState': array([88.47229199,  5.59638341,  0.29411292]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6060292169591145
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46631386,  15.56309734,   5.02513384]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7311136896870485}
episode index:3768
target Thresh 74.42008219327042
target distance 13.0
model initialize at round 3768
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.34818493,   2.50324334,   2.02181339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.19943521902578}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6061083766455166
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28104892,  15.38566936,   0.25259028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8158624395836216}
episode index:3769
target Thresh 74.4216613213815
target distance 52.0
model initialize at round 3769
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.24537334, 18.52406598]), 'previousTarget': array([82.90818058, 18.08575187]), 'currentState': array([64.37539506, 20.80089787,  1.53730601]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 33
reward sum = 0.3611392912463991
running average episode reward sum: 0.6060433981082755
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15315937e+02, 1.47475677e+01, 1.04858573e-01]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4043985344352891}
episode index:3770
target Thresh 74.42323887115377
target distance 53.0
model initialize at round 3770
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([83.23280709,  9.64706049]), 'previousTarget': array([81.71773129,  9.34829399]), 'currentState': array([63.51084031,  6.3238051 ,  1.20383912]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4994500984100759
running average episode reward sum: 0.6060151315212433
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43455808,  14.56425322,   0.92907044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6153990402999263}
episode index:3771
target Thresh 74.42481484416479
target distance 66.0
model initialize at round 3771
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.16200655, 11.65138242]), 'previousTarget': array([68.91786413, 10.81071492]), 'currentState': array([50.21754921, 10.16187865,  6.03519112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.49995898312853265
running average episode reward sum: 0.6059870148329102
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67858352,  14.26698894,   5.46473703]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8003835157741142}
episode index:3772
target Thresh 74.4263892419905
target distance 59.0
model initialize at round 3772
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.46689205, 19.51050557]), 'previousTarget': array([75.89737675, 18.97653796]), 'currentState': array([54.58958509, 21.72244313,  1.66782224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6059867548414721
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69137238,  14.89208873,   0.92368408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6997432474990288}
episode index:3773
target Thresh 74.42796206620534
target distance 61.0
model initialize at round 3773
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.33074957, 18.74735692]), 'previousTarget': array([73.90394822, 19.0422346 ]), 'currentState': array([52.40743567, 20.49708928,  2.45007646]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6059864949878144
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38930336,  15.31450381,   5.29371684]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6869228754005942}
episode index:3774
target Thresh 74.42953331838211
target distance 24.0
model initialize at round 3774
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.26846466,  15.25176112]), 'previousTarget': array([111.,  15.]), 'currentState': array([91.31382984, 16.59807052,  2.3868835 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.6279803072246495
running average episode reward sum: 0.6059923211632414
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.00651835,  15.72279703,   5.51556429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7228264224983548}
episode index:3775
target Thresh 74.43110300009207
target distance 60.0
model initialize at round 3775
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.99150156,  7.75126914]), 'previousTarget': array([74.67213115,  7.60655738]), 'currentState': array([53.28276182,  4.35044894,  2.05321455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.6059904577324425
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11528276,  15.65399144,   5.31796882]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.100195168458099}
episode index:3776
target Thresh 74.43267111290488
target distance 64.0
model initialize at round 3776
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.28584326, 14.05340537]), 'previousTarget': array([70.97806349, 12.93647173]), 'currentState': array([51.29053066, 13.62042309,  2.4033882 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6059951002892756
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.162719  ,  14.71423265,   3.50969243]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8847047251933707}
episode index:3777
target Thresh 74.4342376583887
target distance 2.0
model initialize at round 3777
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.0327502 ,  12.99244759,   1.90494823]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.85698093206182}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.606094122232026
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11903398,  14.61997777,   1.53302312]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9594363021521772}
episode index:3778
target Thresh 74.43580263811003
target distance 48.0
model initialize at round 3778
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.92325764,  9.82125287]), 'previousTarget': array([86.57960839,  9.07908508]), 'currentState': array([68.27932864,  6.06411918,  5.96700579]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6506173448953149
running average episode reward sum: 0.6061059039792245
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49347979,  15.32414461,   5.38581942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6013588340241706}
episode index:3779
target Thresh 74.43736605363387
target distance 67.0
model initialize at round 3779
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.75889986, 20.65023171]), 'previousTarget': array([67.85893586, 20.62878378]), 'currentState': array([49.91307758, 23.12880747,  6.11484838]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4863979996240202
running average episode reward sum: 0.6060742352214585
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.69394383,  15.25889706,   2.77567446]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7406657285727156}
episode index:3780
target Thresh 74.43892790652365
target distance 22.0
model initialize at round 3780
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.65129243,  13.73319635]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([92.94505907,  6.65669768,  2.61394709]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6061115528250318
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98828469,  14.80129435,   6.1343465 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.19905070530131905}
episode index:3781
target Thresh 74.44048819834119
target distance 53.0
model initialize at round 3781
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.90845124, 12.91143122]), 'previousTarget': array([81.94328241, 12.50515339]), 'currentState': array([62.       , 11.       ,  0.2766112], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = -0.15769365217439046
running average episode reward sum: 0.6059095948120758
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07556606,  14.3974462 ,   5.56128273]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6072736656687331}
episode index:3782
target Thresh 74.4420469306468
target distance 8.0
model initialize at round 3782
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.72688596,   8.10071282,   1.41869307]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.324812242968298}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6059982996903706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.03577924,  14.18101456,   1.46366061]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2650924174458271}
episode index:3783
target Thresh 74.44360410499921
target distance 12.0
model initialize at round 3783
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.43242172,   2.38755699,   2.50405073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.414040774204215}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6060820064543075
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41282912,  14.70836302,   1.67672329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5054503041740924}
episode index:3784
target Thresh 74.4451597229556
target distance 12.0
model initialize at round 3784
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.74769867,  15.93290107]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.       ,  22.       ,   4.6711617], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.341908935499962}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.6061496374560122
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4111102 ,  15.50044791,   5.70902872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.772812594723442}
episode index:3785
target Thresh 74.44671378607157
target distance 50.0
model initialize at round 3785
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.06007678, 15.08711705]), 'previousTarget': array([84.9960012 , 14.39992002]), 'currentState': array([66.06016739, 15.14732223,  6.0997842 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.2977488343948268
running average episode reward sum: 0.6060681792407294
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.92312724,  15.61515435,   5.03726826]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.109314554093389}
episode index:3786
target Thresh 74.4482662959012
target distance 10.0
model initialize at round 3786
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.63442609,  20.35721462,   5.50342393]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.668113428678428}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6061567485489312
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16638589,  15.31580438,   5.70276137]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8914285639126595}
episode index:3787
target Thresh 74.44981725399701
target distance 8.0
model initialize at round 3787
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.96754836,  20.67092825,   5.06194186]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.665111804954478}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6062452710940346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19122454,  14.89654942,   5.44760215]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8153648089646308}
episode index:3788
target Thresh 74.45136666190994
target distance 49.0
model initialize at round 3788
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.37329731,  9.92145042]), 'previousTarget': array([85.59608118,  8.99920024]), 'currentState': array([66.68078685,  6.42788038,  0.11589551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6059204946650831
running average episode reward sum: 0.6062451853784292
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.64824637,  14.49798017,   5.46781475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8199068623129584}
episode index:3789
target Thresh 74.45291452118941
target distance 25.0
model initialize at round 3789
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.99960379,  16.84529756]), 'previousTarget': array([109.04848294,  16.90448546]), 'currentState': array([91.2364522 , 23.76946846,  6.04664499]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6063076386786945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.94079581,  15.14067671,   5.18745745]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9512553242872491}
episode index:3790
target Thresh 74.45446083338327
target distance 24.0
model initialize at round 3790
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.60083708,  15.33898039]), 'previousTarget': array([110.72787848,  15.71202025]), 'currentState': array([91.69955149, 17.32362949,  5.86787403]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6063723050285801
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58529617,  14.66585443,   2.01735209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6739620678795001}
episode index:3791
target Thresh 74.45600560003781
target distance 56.0
model initialize at round 3791
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([78.89449607, 10.05159111]), 'previousTarget': array([78.84555753, 10.48069469]), 'currentState': array([59.       ,  8.       ,  1.5657359], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 66
reward sum = -0.17180550197135036
running average episode reward sum: 0.6061670893621771
{'dynamicTrap': 13, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04946313,  15.89171062,   6.08522423]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3033296492679973}
episode index:3792
target Thresh 74.45754882269786
target distance 45.0
model initialize at round 3792
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.19044995,  8.50628172]), 'previousTarget': array([89.21428366,  7.55079306]), 'currentState': array([69.79493008,  3.62634226,  0.68622708]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6061818837074504
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16230699,  14.68879932,   5.24154074]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8936304830491364}
episode index:3793
target Thresh 74.45909050290658
target distance 13.0
model initialize at round 3793
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.16816892,   1.22276722,   0.20638865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.160516783167154}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6062534016672005
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32420485,  14.69538064,   5.44532177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4448614804568465}
episode index:3794
target Thresh 74.46063064220567
target distance 9.0
model initialize at round 3794
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.70912797,  20.65764337,   0.73450112]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.877924051871531}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6063417354610697
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67400894,  14.73395977,   0.2932204 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.42077022067036945}
episode index:3795
target Thresh 74.46216924213529
target distance 16.0
model initialize at round 3795
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([99.62123339, 18.53083135,  2.08058718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.77888565327122}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6064226563019608
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00730617,  15.7810221 ,   5.81548092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.263105920924432}
episode index:3796
target Thresh 74.46370630423401
target distance 29.0
model initialize at round 3796
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.01489065,  12.41149772]), 'previousTarget': array([105.58520839,  13.05211208]), 'currentState': array([85.65484447,  7.39267198,  5.51086259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6064577570965581
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25849491,  15.63566419,   3.37920609]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6862132212674571}
episode index:3797
target Thresh 74.46524183003892
target distance 60.0
model initialize at round 3797
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.99340723, 15.51348532]), 'previousTarget': array([75., 15.]), 'currentState': array([55.       , 15.       ,  5.1988792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5598236312032323
running average episode reward sum: 0.6064454784957436
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04060306,  15.92288664,   5.38435723]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3312258420828}
episode index:3798
target Thresh 74.46677582108553
target distance 22.0
model initialize at round 3798
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.96959922,  12.89454121]), 'previousTarget': array([110.21853056,  12.17458624]), 'currentState': array([93.24266839,  3.63409181,  2.35058057]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6280332322654911
running average episode reward sum: 0.606451160978968
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12933124,  15.44885239,   5.49958425]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9795573262025071}
episode index:3799
target Thresh 74.46830827890783
target distance 71.0
model initialize at round 3799
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([63.74539626,  5.18108886]), 'previousTarget': array([63.67294975,  5.60208939]), 'currentState': array([44.       ,  2.       ,  0.7200652], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4855680986708194
running average episode reward sum: 0.6061637874895865
{'dynamicTrap': 11, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.61830489,  20.88202803,   4.67896555]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.678956518926489}
episode index:3800
target Thresh 74.46983920503827
target distance 27.0
model initialize at round 3800
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.49619129,  12.72892606]), 'previousTarget': array([106.75497521,  11.94628712]), 'currentState': array([88.35371655,  6.93533814,  1.70669121]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6945152023787424
running average episode reward sum: 0.6061870317450164
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41837519,  14.62720306,   3.65430174]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6908436713962}
episode index:3801
target Thresh 74.4713686010078
target distance 63.0
model initialize at round 3801
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.34169674, 20.9934991 ]), 'previousTarget': array([71.84067458, 20.48054926]), 'currentState': array([53.54553168, 23.84162992,  0.64974201]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.48499600942521226
running average episode reward sum: 0.6061551561473522
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67379298,  15.67800712,   3.25107341]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7523992766030666}
episode index:3802
target Thresh 74.4728964683458
target distance 42.0
model initialize at round 3802
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.44197821, 14.28583576]), 'previousTarget': array([92.97736275, 13.95130299]), 'currentState': array([71.45116195, 13.67981178,  1.80098724]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6061864012643151
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52097327,  14.49533049,   5.29846054]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6958145737541339}
episode index:3803
target Thresh 74.47442280858014
target distance 68.0
model initialize at round 3803
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.7634097 ,  8.21960068]), 'previousTarget': array([66.74334797,  7.19377688]), 'currentState': array([47.96632035,  5.37790169,  6.23238475]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5829956081033546
running average episode reward sum: 0.606180304841297
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19846924,  15.20673829,   5.88094732]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8277634204845927}
episode index:3804
target Thresh 74.47594762323716
target distance 8.0
model initialize at round 3804
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.07903703e+02, 1.84545066e+01, 5.51912785e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.8924677135401895}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6062734495732703
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1072069 ,  15.39615961,   6.1401771 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9767404712182647}
episode index:3805
target Thresh 74.47747091384169
target distance 7.0
model initialize at round 3805
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.21343815,  12.38298982,   1.49173406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.350829890833756}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.606366545358984
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7160264 ,  14.9865768 ,   0.44134381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7161522121019556}
episode index:3806
target Thresh 74.47899268191699
target distance 3.0
model initialize at round 3806
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.58969436,  13.57627683,   0.20280981]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.0039834472424882}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6064673159013116
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50233989,  15.09857723,   1.87537944]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5073293361130248}
episode index:3807
target Thresh 74.48051292898485
target distance 14.0
model initialize at round 3807
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.72117089,   3.52062508,   0.11795235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.552873016621174}
done in step count: 10
reward sum = 0.8364611450088044
running average episode reward sum: 0.606527713440468
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.12243716,  14.8587539 ,   0.52085239]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8888571276261573}
episode index:3808
target Thresh 74.48203165656552
target distance 12.0
model initialize at round 3808
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.06198756,   4.6635874 ,   0.52355218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.061164828178871}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6066131788210053
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24249949,  15.47903278,   1.14695294]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8962585690721431}
episode index:3809
target Thresh 74.48354886617771
target distance 38.0
model initialize at round 3809
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.91423348, 10.98439067]), 'previousTarget': array([96.34149075, 10.08986599]), 'currentState': array([77.38970391,  6.64933067,  2.21028876]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.14429707098238526
running average episode reward sum: 0.6064918360105489
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43569364,  15.23182554,   5.69836622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6100694678459254}
episode index:3810
target Thresh 74.48506455933865
target distance 15.0
model initialize at round 3810
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.11530004,   2.73964532,   0.16361874]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 18.522990847941603}
done in step count: 15
reward sum = 0.7221513546412884
running average episode reward sum: 0.606522184874005
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.99230061,  15.03482449,   5.96888614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.03566547189584296}
episode index:3811
target Thresh 74.48657873756402
target distance 74.0
model initialize at round 3811
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([62.60168681, 18.31945616]), 'previousTarget': array([60.95450197, 18.65172284]), 'currentState': array([42.64169919, 19.58393014,  5.08936227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4303698868157362
running average episode reward sum: 0.606250177509973
{'dynamicTrap': 12, 'scaleFactor': 20, 'currentTarget': array([109.65690957,  16.95140459]), 'previousTarget': array([108.06670867,  17.37273002]), 'currentState': array([9.08706157e+01, 2.38125387e+01, 8.10209031e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:3812
target Thresh 74.48809140236798
target distance 49.0
model initialize at round 3812
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.72237272, 14.0997975 ]), 'previousTarget': array([85.98336106, 13.81564739]), 'currentState': array([67.73325482, 13.4401267 ,  5.72592825]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6062738237309381
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69554142,  15.00722517,   1.94050428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30454429653268106}
episode index:3813
target Thresh 74.48960255526325
target distance 11.0
model initialize at round 3813
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.64989166,  16.61242202]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.       ,  15.       ,   3.4386168], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.76094725856119}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.6063408902029297
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.07831498,  15.77745686,   0.28419102]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2057953609645282}
episode index:3814
target Thresh 74.49111219776093
target distance 19.0
model initialize at round 3814
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.52891523,  14.30815776]), 'previousTarget': array([113.30852571,  14.02072541]), 'currentState': array([95.43048957,  5.79657807,  2.25337458]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.4535225247170458
running average episode reward sum: 0.6063008329642702
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.92136623,  14.9755378 ,   0.52091747]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08235088678604924}
episode index:3815
target Thresh 74.49262033137069
target distance 23.0
model initialize at round 3815
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.98412174,  12.49426007]), 'previousTarget': array([110.04268443,  12.62910995]), 'currentState': array([90.52162894,  4.80424318,  1.63336277]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.60634172444262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41939436,  15.18115956,   5.43432544]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6082118867118939}
episode index:3816
target Thresh 74.49412695760066
target distance 30.0
model initialize at round 3816
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.07224917,  16.74469733]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([85.37411811, 20.20643577,  5.64405942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6068659884688123
running average episode reward sum: 0.6063418617923779
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58737708,  14.79754323,   5.98062927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4596155135830762}
episode index:3817
target Thresh 74.49563207795744
target distance 66.0
model initialize at round 3817
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.59195416, 18.42714411]), 'previousTarget': array([68.96336993, 17.79009879]), 'currentState': array([50.65124762, 19.96604711,  0.34195554]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.4695208750845765
running average episode reward sum: 0.6063060260179651
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98400084,  14.33766665,   5.89771849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6625265559027094}
episode index:3818
target Thresh 74.49713569394619
target distance 10.0
model initialize at round 3818
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.05880566e+02, 5.76818470e+00, 5.98279794e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.976536468240992}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6063888850565642
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67004387,  15.24950922,   0.30676094]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41367366197350475}
episode index:3819
target Thresh 74.49863780707051
target distance 43.0
model initialize at round 3819
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.98446086,  9.46182101]), 'previousTarget': array([91.37605409,  8.956665  ]), 'currentState': array([73.58874069,  4.58267773,  1.34591359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3491169113477977
running average episode reward sum: 0.6063215363723472
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.35422542,  14.9218967 ,   0.2589416 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3627337517638497}
episode index:3820
target Thresh 74.5001384188325
target distance 73.0
model initialize at round 3820
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([61.99104509, 16.40157205]), 'previousTarget': array([61.99249812, 16.45226033]), 'currentState': array([42.       , 17.       ,  3.9624867], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.09748977198504635
running average episode reward sum: 0.6061373407930859
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.79973327,  14.35976896,   5.6697997 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3603440723422107}
episode index:3821
target Thresh 74.5016375307328
target distance 10.0
model initialize at round 3821
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72625491,   4.2718692 ,   2.68762028]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.731622753546413}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6062226176133669
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79875361,  15.41293276,   6.0769286 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4593621357721466}
episode index:3822
target Thresh 74.50313514427049
target distance 12.0
model initialize at round 3822
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.55391869,   2.54697492,   2.43518591]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.921041368839266}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6063054117741868
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.75236028,  14.30510092,   1.30422318]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7377060192114304}
episode index:3823
target Thresh 74.50463126094321
target distance 51.0
model initialize at round 3823
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([83.67537592,  7.58881352]), 'previousTarget': array([83.55042088,  8.21675745]), 'currentState': array([64.       ,  4.       ,  2.7177618], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4905236312032323
running average episode reward sum: 0.6062751341119037
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04207133,  15.53170191,   5.61249431]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5333637780674771}
episode index:3824
target Thresh 74.50612588224706
target distance 8.0
model initialize at round 3824
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.92567927,  11.37311929,   0.13094819]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.949860217409235}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.606365255658515
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27206163,  14.1729817 ,   0.49186632]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.101750216129311}
episode index:3825
target Thresh 74.50761900967667
target distance 5.0
model initialize at round 3825
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.31219843,  17.57905203,   2.44995379]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.1678587625715755}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6064553300950654
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.32113453,  15.25875441,   5.43780092]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7265068274519817}
episode index:3826
target Thresh 74.50911064472517
target distance 29.0
model initialize at round 3826
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.21070143,  16.58509219]), 'previousTarget': array([105.58520839,  16.94788792]), 'currentState': array([87.61237521, 20.5732759 ,  5.06258971]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.606508445469543
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.5905657 ,  14.6436498 ,   0.20284962]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.54279085393748}
episode index:3827
target Thresh 74.51060078888419
target distance 18.0
model initialize at round 3827
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([9.89901979e+01, 2.18941823e+01, 9.32328065e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 17.431107611893104}
done in step count: 14
reward sum = 0.7335831620689782
running average episode reward sum: 0.6065416415815074
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28403081,  15.85226155,   3.21371567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8983447279744163}
episode index:3828
target Thresh 74.51208944364387
target distance 29.0
model initialize at round 3828
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.87188279,  14.05956695]), 'previousTarget': array([105.95260657,  14.37604183]), 'currentState': array([87.00442129, 11.76087749,  0.12036913]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.606590498369719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87801097,  15.89471771,   5.77143459]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9029956293358189}
episode index:3829
target Thresh 74.51357661049288
target distance 2.0
model initialize at round 3829
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.68459279,  16.63330417,   4.42346025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.770974254523156}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6066880204328079
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.56584049,  15.07330397,   2.97762603]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4403043894650186}
episode index:3830
target Thresh 74.51506229091835
target distance 67.0
model initialize at round 3830
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.11352625, 13.14941978]), 'previousTarget': array([67.96445232, 12.1919076 ]), 'currentState': array([47.12844403, 12.37709317,  1.18022585]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.6066828909004416
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67079395,  14.68930008,   0.35742959]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.45267103350966326}
episode index:3831
target Thresh 74.51654648640601
target distance 26.0
model initialize at round 3831
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.7762271 ,  15.64456117]), 'previousTarget': array([108.98522349,  15.23133756]), 'currentState': array([88.88262776, 17.70482941,  0.67373502]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.606746767434939
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6758976 ,  15.46123116,   5.85910221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5637167272974962}
episode index:3832
target Thresh 74.51802919844003
target distance 70.0
model initialize at round 3832
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.27145711, 17.96008732]), 'previousTarget': array([64.98165792, 17.14364323]), 'currentState': array([46.30825664, 19.17278142,  1.60978847]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6067283801293748
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23548256,  14.6441545 ,   0.31756734]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8432751220860383}
episode index:3833
target Thresh 74.51951042850312
target distance 9.0
model initialize at round 3833
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.20450517,  17.08000091,   5.13602173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.068218105642043}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6068181719055278
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37674633,  14.64698652,   0.50185319]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7162846134486983}
episode index:3834
target Thresh 74.52099017807653
target distance 25.0
model initialize at round 3834
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.42079345,  14.67100649]), 'previousTarget': array([109.85753677,  14.38290441]), 'currentState': array([89.45547504, 13.49369567,  2.79146457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6068732146084462
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62136964,  14.75920372,   5.14173733]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44871349201984645}
episode index:3835
target Thresh 74.52246844863997
target distance 69.0
model initialize at round 3835
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([64.95269777, 20.34446319]), 'previousTarget': array([65.92481176, 19.26740767]), 'currentState': array([45.06576889, 22.46815327,  1.29599059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5591099216973463
running average episode reward sum: 0.6068607632807842
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08873845,  14.77334482,   5.80420132]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9390261853388998}
episode index:3836
target Thresh 74.52394524167175
target distance 29.0
model initialize at round 3836
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.44328113,  14.5629331 ]), 'previousTarget': array([105.98811999,  14.68924552]), 'currentState': array([87.47664999, 13.40809941,  0.62876957]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6818244607375143
running average episode reward sum: 0.6068803003403247
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.49935354,  14.58913964,   5.64179153]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6476520018299557}
episode index:3837
target Thresh 74.52542055864865
target distance 65.0
model initialize at round 3837
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.97882815, 17.98299094]), 'previousTarget': array([69.9787322 , 17.07790467]), 'currentState': array([51.02458838, 19.33514297,  1.92625206]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6068947353952083
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.31822453,  15.71930341,   5.75694491]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7865521274509254}
episode index:3838
target Thresh 74.52689440104598
target distance 12.0
model initialize at round 3838
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.58035396,   3.57867905,   1.36100739]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.509212268856583}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6069794373000044
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.1605903 ,  14.97514147,   5.7711419 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.16250289304876941}
episode index:3839
target Thresh 74.5283667703376
target distance 25.0
model initialize at round 3839
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.57321656,  15.41356413]), 'previousTarget': array([109.85753677,  15.61709559]), 'currentState': array([91.71729521, 17.80989203,  0.82218498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.6989156712611155
running average episode reward sum: 0.6070033790275985
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51194005,  15.45098358,   3.52109527]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6822527448046916}
episode index:3840
target Thresh 74.52983766799585
target distance 45.0
model initialize at round 3840
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.11460305, 13.00390535]), 'previousTarget': array([89.87767469, 12.20863052]), 'currentState': array([69.17380295, 11.46621503,  3.09393728]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6070360007382537
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28877273,  14.89487097,   5.66021083]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7189550360610902}
episode index:3841
target Thresh 74.53130709549166
target distance 5.0
model initialize at round 3841
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.77527406,   8.7123535 ,   3.111588  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.335262300261821}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6071255254777544
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48307972,  14.67571725,   0.97198391]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5818292857147987}
episode index:3842
target Thresh 74.53277505429443
target distance 26.0
model initialize at round 3842
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.48737026,  13.09960479]), 'previousTarget': array([107.89972147,  12.54221128]), 'currentState': array([90.57936991,  6.58135943,  1.36270398]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6071619674680885
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.31270157,  14.59807919,   5.67129045]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7961905967206881}
episode index:3843
target Thresh 74.53424154587213
target distance 37.0
model initialize at round 3843
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.09178379, 10.36392016]), 'previousTarget': array([97.43335206, 10.72703158]), 'currentState': array([76.66714284,  5.60121883,  4.44230866]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.46770005898356853
running average episode reward sum: 0.6071256870548511
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.86699742,  15.27350487,   3.99613177]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.30412925241501243}
episode index:3844
target Thresh 74.53570657169128
target distance 64.0
model initialize at round 3844
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.02329425, 11.81676078]), 'previousTarget': array([70.91268452, 10.86681417]), 'currentState': array([50.07319825, 10.40478703,  1.14399195]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.5471566423907612
running average episode reward sum: 0.6071100904242493
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.4408523 ,  14.36329561,   0.78035218]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7744309087849988}
episode index:3845
target Thresh 74.53717013321686
target distance 53.0
model initialize at round 3845
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.99181361, 15.5721787 ]), 'previousTarget': array([82., 15.]), 'currentState': array([62.       , 15.       ,  3.9614894], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.4596782697986555
running average episode reward sum: 0.6070717566175343
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18438356,  14.11715143,   2.62374405]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9018973893698283}
episode index:3846
target Thresh 74.53863223191244
target distance 25.0
model initialize at round 3846
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.78409161,  12.40029969]), 'previousTarget': array([108.03046115,  11.65462135]), 'currentState': array([90.33283264,  4.68336748,  0.42218626]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6071024061052755
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.20198803,  14.16342388,   0.80241267]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8606153391015997}
episode index:3847
target Thresh 74.54009286924015
target distance 63.0
model initialize at round 3847
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.31529867,  7.77834887]), 'previousTarget': array([71.64677133,  6.74224216]), 'currentState': array([51.58310598,  4.51635995,  2.67992449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5179294735641086
running average episode reward sum: 0.6070792322662575
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00534732,  14.96964761,   5.37381018]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9951156776805048}
episode index:3848
target Thresh 74.5415520466606
target distance 61.0
model initialize at round 3848
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.91470589, 18.15487422]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([54.        , 20.        ,  0.39792714], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.2742793495590793
running average episode reward sum: 0.6069927682801034
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.90379499,  14.48470388,   6.23579277]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5241998621624724}
episode index:3849
target Thresh 74.54300976563297
target distance 55.0
model initialize at round 3849
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.8158602 , 11.70770838]), 'previousTarget': array([79.88204353, 11.1689502 ]), 'currentState': array([60.      ,  9.      ,  5.658792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3183569667928823
running average episode reward sum: 0.6069177979420548
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28805071,  14.952768  ,   0.97447695]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.713514296321769}
episode index:3850
target Thresh 74.544466027615
target distance 50.0
model initialize at round 3850
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([85.88110906, 14.82034842]), 'previousTarget': array([84.9960012 , 15.60007998]), 'currentState': array([6.58814897e+01, 1.46969590e+01, 3.39738687e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6426116020847181
running average episode reward sum: 0.6069270666525567
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22950653,  15.37598742,   3.33348648]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.44049947632521136}
episode index:3851
target Thresh 74.54592083406293
target distance 13.0
model initialize at round 3851
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25781743,   2.8251514 ,   1.59059447]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.17757809057569}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6070019397542198
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62139026,  15.24853726,   4.85039863]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6692507905660338}
episode index:3852
target Thresh 74.54737418643157
target distance 49.0
model initialize at round 3852
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([85.69833651,  8.46056911]), 'previousTarget': array([85.59608118,  8.99920024]), 'currentState': array([66.       ,  5.       ,  2.8001928], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4598165100414968
running average episode reward sum: 0.6069637395388777
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.30091065,  14.86350702,   5.68291368]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7122894419623989}
episode index:3853
target Thresh 74.54882608617427
target distance 22.0
model initialize at round 3853
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.68169136,  12.7037158 ]), 'previousTarget': array([111.20732955,  13.27605889]), 'currentState': array([91.32011991,  4.77574672,  2.30397224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6070020777712765
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.72865877,  15.76212107,   2.52371037]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0544060549390284}
episode index:3854
target Thresh 74.55027653474295
target distance 62.0
model initialize at round 3854
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.8966041 , 15.32693576]), 'previousTarget': array([72.99739905, 14.32253869]), 'currentState': array([53.89723673, 15.48601041,  1.94546793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.5021519972801305
running average episode reward sum: 0.606974879306817
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.29887373,  15.6221063 ,   5.34222385]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6901751668137529}
episode index:3855
target Thresh 74.55172553358803
target distance 37.0
model initialize at round 3855
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([97.80676793, 10.77343546]), 'previousTarget': array([97.65140491, 11.71783336]), 'currentState': array([78.        ,  8.        ,  0.81777376], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.4591057349423374
running average episode reward sum: 0.6069365314996685
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.84750179,  14.5472176 ,   0.26555616]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9608700185171615}
episode index:3856
target Thresh 74.55317308415853
target distance 3.0
model initialize at round 3856
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.63034908,  15.07106824,   3.39063513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.6318973034655877}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6070307400733009
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81647429,  15.64279157,   0.68443322]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6684778905211817}
episode index:3857
target Thresh 74.55461918790199
target distance 43.0
model initialize at round 3857
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.56951502,  9.35936578]), 'previousTarget': array([91.26392603,  8.37597936]), 'currentState': array([72.12503509,  4.67831889,  0.48643351]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6070450613021529
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52986619,  14.45496329,   6.01575931]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7601468222492319}
episode index:3858
target Thresh 74.55606384626452
target distance 23.0
model initialize at round 3858
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.98339798,  16.0126989 ]), 'previousTarget': array([111.35234545,  15.95156206]), 'currentState': array([93.02328527, 22.37776966,  6.2621916 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.607115150952761
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0120631 ,  14.42799034,   5.43239958]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1415841522344163}
episode index:3859
target Thresh 74.55750706069077
target distance 3.0
model initialize at round 3859
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.80924255,  13.14787706,   5.86311778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.2018770821427904}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6072143439188354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.22619797,  14.05596594,   1.57993247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9707552862581148}
episode index:3860
target Thresh 74.55894883262397
target distance 67.0
model initialize at round 3860
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.46235342, 13.39385407]), 'previousTarget': array([67.96445232, 12.1919076 ]), 'currentState': array([48.47425415, 12.70400817,  0.44283366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.5151371174238033
running average episode reward sum: 0.6071904958933252
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88550112,  14.70908916,   5.83181976]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.31263254635984367}
episode index:3861
target Thresh 74.56038916350587
target distance 10.0
model initialize at round 3861
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.33816629,   3.90869344,   0.23561495]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.031200056497935}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6072746167768088
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7634404 ,  14.84109306,   0.56659698]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.28497694703562704}
episode index:3862
target Thresh 74.56182805477683
target distance 58.0
model initialize at round 3862
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.6556962 ,  6.69507872]), 'previousTarget': array([76.58520839,  7.05211208]), 'currentState': array([57.       ,  3.       ,  3.5001142], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.28009142071598936
running average episode reward sum: 0.6071899201172021
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.60554899,  14.23161436,   5.70801815]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.863717598907082}
episode index:3863
target Thresh 74.56326550787571
target distance 48.0
model initialize at round 3863
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.0394487 , 10.89940883]), 'previousTarget': array([86.84555753, 11.48069469]), 'currentState': array([68.26684279,  7.89207517,  0.29142159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6072007670913872
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33738434,  14.39511913,   0.14200134]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6926103271769127}
episode index:3864
target Thresh 74.56470152423998
target distance 33.0
model initialize at round 3864
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.01755467,  17.28190529]), 'previousTarget': array([101.5646825 ,  17.84991583]), 'currentState': array([83.37064314, 21.02341131,  0.3909313 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6970981399936143
running average episode reward sum: 0.6072240264375456
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.81139125,  15.5287112 ,   5.6290352 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5613455219086427}
episode index:3865
target Thresh 74.56613610530566
target distance 42.0
model initialize at round 3865
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.17743778, 18.05108095]), 'previousTarget': array([92.85976548, 17.6357422 ]), 'currentState': array([74.38874468, 20.9506715 ,  1.6102845 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.48973445121800774
running average episode reward sum: 0.6071936359628379
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.1000289 ,  15.42423042,   5.8323215 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9949469455661315}
episode index:3866
target Thresh 74.56756925250733
target distance 28.0
model initialize at round 3866
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.04800589,  15.6605893 ]), 'previousTarget': array([106.949174,  15.575059]), 'currentState': array([88.13768998, 17.55249852,  6.0506863 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6072524205828242
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.20189494,  14.56658862,   5.79490224]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9081944237215733}
episode index:3867
target Thresh 74.56900096727813
target distance 8.0
model initialize at round 3867
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.85288107,   8.48107221,   1.93504333]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.96010550725503}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6073363949694128
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.85190816,  15.44523572,   5.09324032]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9612400176001339}
episode index:3868
target Thresh 74.57043125104977
target distance 33.0
model initialize at round 3868
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([101.91412742,  15.14864133]), 'previousTarget': array([101.96336993,  15.79009879]), 'currentState': array([82.       , 17.       ,  3.1237552], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6543142836436554
running average episode reward sum: 0.6073485370962347
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59274829,  14.19389444,   5.24993835]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9031390388025121}
episode index:3869
target Thresh 74.57186010525255
target distance 73.0
model initialize at round 3869
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([61.91903855,  8.79774952]), 'previousTarget': array([61.88097346,  9.17873682]), 'currentState': array([42.       ,  7.       ,  1.9983219], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.44612636880166046
running average episode reward sum: 0.6073068776212233
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.55528563,  14.53237608,   6.17176358]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6453239473296657}
episode index:3870
target Thresh 74.57328753131532
target distance 39.0
model initialize at round 3870
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.090016  ,  8.88369131]), 'previousTarget': array([94.97366596,  8.32455532]), 'currentState': array([74.89435572,  3.26883364,  1.27757597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5231610844081905
running average episode reward sum: 0.6072851401391222
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0450788 ,  14.73845358,   5.80224853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9900914215532609}
episode index:3871
target Thresh 74.57471353066549
target distance 29.0
model initialize at round 3871
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.58348845,  15.98782061]), 'previousTarget': array([105.81242258,  16.26725206]), 'currentState': array([87.75856378, 18.62834358,  5.44788278]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7138751520752917
running average episode reward sum: 0.6073126685512957
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.29927767,  14.79927116,   5.5493468 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7289059270750098}
episode index:3872
target Thresh 74.57613810472907
target distance 26.0
model initialize at round 3872
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.0903759 ,  16.16759865]), 'previousTarget': array([108.64012894,  16.22305213]), 'currentState': array([90.6330383 , 20.79491037,  6.00214941]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6073757062746481
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82704881,  14.18853387,   5.82493891]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8296923454128102}
episode index:3873
target Thresh 74.57756125493066
target distance 22.0
model initialize at round 3873
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.16379082,  14.1853797 ]), 'previousTarget': array([112.29527642,  14.26234812]), 'currentState': array([91.60001868, 10.03100615,  1.61987209]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.683586232120547
running average episode reward sum: 0.6073953785838493
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53657449,  14.3708398 ,   0.86955715]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8268946349704104}
episode index:3874
target Thresh 74.57898298269336
target distance 45.0
model initialize at round 3874
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.56162415, 15.9118409 ]), 'previousTarget': array([90., 15.]), 'currentState': array([69.57446048, 16.62828259,  0.81642151]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5084931554539462
running average episode reward sum: 0.6073698554294932
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50539426,  14.67819491,   3.26314875]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5900791092137483}
episode index:3875
target Thresh 74.58040328943893
target distance 6.0
model initialize at round 3875
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.10780173e+02, 2.24982173e+01, 9.11539793e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.604080775056952}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6074585087304403
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76187137,  14.07326068,   5.63984189]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9568442991120639}
episode index:3876
target Thresh 74.58182217658768
target distance 12.0
model initialize at round 3876
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.16515047,   4.8417607 ,   2.1687609 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.25013758034329}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.607544663396592
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14345602e+02, 1.40166819e+01, 4.86268103e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.181165102048714}
episode index:3877
target Thresh 74.58323964555849
target distance 8.0
model initialize at round 3877
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.21699255,  12.1624617 ,   6.03567011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.352605906990551}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6076165664931158
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.58403134,  14.54844371,   3.38405716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7382382333715821}
episode index:3878
target Thresh 74.58465569776882
target distance 10.0
model initialize at round 3878
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.84840135,   6.2851298 ,   3.23776972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.908734484983444}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6077026359911586
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.61816498,  15.14789547,   0.34624498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4094765549988402}
episode index:3879
target Thresh 74.58607033463473
target distance 27.0
model initialize at round 3879
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.27859288,  14.55880888]), 'previousTarget': array([107.87767469,  14.20863052]), 'currentState': array([89.33779213, 13.02112698,  5.92513961]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6077526174224853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16443787,  14.13065446,   5.49833889]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2057884327519388}
episode index:3880
target Thresh 74.58748355757086
target distance 9.0
model initialize at round 3880
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.06270837e+02, 2.28357082e+01, 6.48480257e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.730158138586784}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6078361816405953
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6098007 ,  14.95003465,   5.63371807]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3933853444560802}
episode index:3881
target Thresh 74.58889536799043
target distance 8.0
model initialize at round 3881
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.43297697,  18.22960987,   6.08130038]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.318208208317082}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6079270522815945
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40498393,  14.27184923,   0.26277282]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9403444438242514}
episode index:3882
target Thresh 74.59030576730524
target distance 8.0
model initialize at round 3882
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.4535382 ,  18.37262672,   5.89475429]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.26581494560745}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6080154022681046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.18358392,  14.07112513,   1.12581089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9468429573113133}
episode index:3883
target Thresh 74.59171475692571
target distance 68.0
model initialize at round 3883
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.76865815, 18.74138176]), 'previousTarget': array([66.96548746, 17.82555956]), 'currentState': array([47.8311129 , 20.32071373,  6.2823028 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.1055401921401053
running average episode reward sum: 0.6078860317196679
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04349856,  14.66544971,   5.79526928]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0133207292020237}
episode index:3884
target Thresh 74.59312233826083
target distance 49.0
model initialize at round 3884
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.2824458 ,  7.68459073]), 'previousTarget': array([85.33123116,  7.12869398]), 'currentState': array([64.82656499,  3.05115767,  1.4011054 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.46125068711285805
running average episode reward sum: 0.6078482877442223
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.27653373,  15.36709145,   5.56674192]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8112703461574109}
episode index:3885
target Thresh 74.59452851271816
target distance 63.0
model initialize at round 3885
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.20986222, 20.53962991]), 'previousTarget': array([71.87767469, 19.79136948]), 'currentState': array([53.38329693, 23.16780584,  0.51040742]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.4068648961185939
running average episode reward sum: 0.607796567880191
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.56751324,  14.38051045,   1.19293039]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.840141998381375}
episode index:3886
target Thresh 74.59593328170388
target distance 52.0
model initialize at round 3886
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([80.99984225,  9.11598251]), 'previousTarget': array([82.64012894,  8.77694787]), 'currentState': array([61.29277124,  5.70550567,  2.73926365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5016952525708709
running average episode reward sum: 0.6077692714265482
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15711928e+02, 1.42997809e+01, 5.19302487e-03]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9985730522187576}
episode index:3887
target Thresh 74.59733664662276
target distance 33.0
model initialize at round 3887
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([99.36630704, 10.27828319]), 'previousTarget': array([101.14048809,  10.80014791]), 'currentState': array([80.22046836,  4.49582165,  3.55279297]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3339700339051366
running average episode reward sum: 0.6076988498119594
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10667677,  15.22709787,   5.4081218 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9217373984260637}
episode index:3888
target Thresh 74.5987386088782
target distance 3.0
model initialize at round 3888
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([117.02448969,  18.26189341,   3.18215358]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.8390763359351174}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6077895922033679
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.22908869,  14.96718348,   3.12143087]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.771609468458609}
episode index:3889
target Thresh 74.60013916987211
target distance 27.0
model initialize at round 3889
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.19434825,  15.26226626]), 'previousTarget': array([108.,  15.]), 'currentState': array([89.21472424, 16.16483179,  6.03761942]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6078415043565859
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.53030985,  14.97425653,   1.11764497]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5309343328304358}
episode index:3890
target Thresh 74.60153833100507
target distance 27.0
model initialize at round 3890
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.62484854,  14.67973331]), 'previousTarget': array([107.94535509,  14.47743371]), 'currentState': array([89.66025542, 13.4901865 ,  1.21357196]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6078954918747665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02505311,  14.32443745,   1.1204715 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6760269362335166}
episode index:3891
target Thresh 74.60293609367625
target distance 61.0
model initialize at round 3891
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.89736378, 17.97641046]), 'previousTarget': array([73.93315042, 18.36613521]), 'currentState': array([54.       , 20.       ,  1.8013855], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.11511024770853273
running average episode reward sum: 0.6077688769610547
{'dynamicTrap': 8, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24557955,  14.6500222 ,   5.72984048]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8316457575634433}
episode index:3892
target Thresh 74.6043324592834
target distance 10.0
model initialize at round 3892
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.84816861,  18.05730498,   0.16078025]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.648996399306043}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6078570406325007
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05467501,  14.72805842,   5.93106188]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9836623170055289}
episode index:3893
target Thresh 74.6057274292229
target distance 41.0
model initialize at round 3893
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.23894834, 12.72393402]), 'previousTarget': array([93.90549268, 12.94199929]), 'currentState': array([72.33820114, 10.73389356,  2.29037619]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.597689912374029
running average episode reward sum: 0.6078544296596556
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46408656,  15.15542588,   0.96592845]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4894216380791656}
episode index:3894
target Thresh 74.60712100488972
target distance 44.0
model initialize at round 3894
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.47244822, 17.48179606]), 'previousTarget': array([90.95367385, 16.63952224]), 'currentState': array([71.58279833, 19.57984924,  2.22579925]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6078960698948662
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.08098492,  15.11735928,   5.86381999]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9264782346684048}
episode index:3895
target Thresh 74.60851318767742
target distance 45.0
model initialize at round 3895
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.6478474 , 13.50240119]), 'previousTarget': array([89.95570316, 13.33038021]), 'currentState': array([71.68884893, 12.22240903,  1.10614806]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.44464147498033846
running average episode reward sum: 0.6078541667647548
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.95599021,  14.64762312,   1.13439993]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35511452699305485}
episode index:3896
target Thresh 74.60990397897818
target distance 35.0
model initialize at round 3896
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.11197369,  18.1271173 ]), 'previousTarget': array([99.61161351, 18.0776773 ]), 'currentState': array([81.60047425, 22.52045197,  5.58758551]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6078957856970207
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27113478,  14.2988034 ,   1.65711714]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7517916905825058}
episode index:3897
target Thresh 74.61129338018283
target distance 21.0
model initialize at round 3897
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.94365924,  14.67428068]), 'previousTarget': array([113.90990945,  14.89618185]), 'currentState': array([93.18993241, 11.54533894,  3.1942811 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7204335664537295
running average episode reward sum: 0.6079246563437002
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68457392,  14.35459029,   5.4633915 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7183643216559584}
episode index:3898
target Thresh 74.61268139268073
target distance 13.0
model initialize at round 3898
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.43144312,   3.60731032,   0.93812943]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.938508126810998}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6080054001339245
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05930847,  15.59014575,   0.59777343]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1104830303073376}
episode index:3899
target Thresh 74.6140680178599
target distance 28.0
model initialize at round 3899
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.39551833,  13.13611189]), 'previousTarget': array([106.23047895,  12.49442256]), 'currentState': array([87.97049672,  8.37496335,  1.96616524]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6080489426875821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73582449,  14.81865091,   0.27519372]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7578424429972216}
episode index:3900
target Thresh 74.61545325710698
target distance 28.0
model initialize at round 3900
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.31774618,  15.79538401]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.457938  , 18.15928311,  0.48692816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6080904690149643
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.62454005,  14.61291203,   5.68616107]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5392654912636832}
episode index:3901
target Thresh 74.6168371118072
target distance 31.0
model initialize at round 3901
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.32733426,  16.60225648]), 'previousTarget': array([103.74482241,  16.81535122]), 'currentState': array([85.5962057 , 19.87067592,  5.22443146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6081421444119931
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4035572 ,  14.05342196,   5.7527081 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1188181291633421}
episode index:3902
target Thresh 74.61821958334441
target distance 63.0
model initialize at round 3902
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.53971972, 19.815962  ]), 'previousTarget': array([71.90990945, 19.10381815]), 'currentState': array([50.65603059, 21.96977414,  1.50653338]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6081460858033534
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.82762637,  15.19893475,   5.69191797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8511994155030449}
episode index:3903
target Thresh 74.6196006731011
target distance 66.0
model initialize at round 3903
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.56594081, 16.77579846]), 'previousTarget': array([68.99082358, 16.39421747]), 'currentState': array([50.58189356, 17.57445699,  1.3583371 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6081633970132041
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13953324,  14.60388528,   0.52834665]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9472644375519604}
episode index:3904
target Thresh 74.62098038245834
target distance 22.0
model initialize at round 3904
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.51569221,  14.63345451]), 'previousTarget': array([112.50265712,  14.43242207]), 'currentState': array([92.72989446, 11.71417251,  2.64595592]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6082048514943288
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48537207,  14.2285462 ,   0.13429365]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9273526152568791}
episode index:3905
target Thresh 74.62235871279586
target distance 65.0
model initialize at round 3905
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.01308446, 12.46777353]), 'previousTarget': array([69.94108971, 11.53392998]), 'currentState': array([51.04614274, 11.31832221,  1.91000622]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.44755601943275614
running average episode reward sum: 0.6081637227610821
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.91054173,  14.33509533,   1.16222716]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.127468161678272}
episode index:3906
target Thresh 74.62373566549198
target distance 30.0
model initialize at round 3906
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.88164685,  16.28152811]), 'previousTarget': array([104.72787848,  16.71202025]), 'currentState': array([8.60762936e+01, 1.90650480e+01, 6.41400178e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6819275199781069
running average episode reward sum: 0.6081826026682275
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32971717,  14.15732587,   0.92593149]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.904882924546577}
episode index:3907
target Thresh 74.62511124192363
target distance 46.0
model initialize at round 3907
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.63011447,  7.35795842]), 'previousTarget': array([88.24618806,  7.4391401 ]), 'currentState': array([67.31848222,  2.15595306,  2.04881513]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.42314089568670393
running average episode reward sum: 0.6081352532038002
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.30604319,  14.5578608 ,   5.14233067]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5377262397171102}
episode index:3908
target Thresh 74.62648544346642
target distance 10.0
model initialize at round 3908
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.76427234,   8.85747418,   4.89782166]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.80516299779575}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6082157365604706
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.6485552 ,  15.33801174,   5.38152498]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.48761192092943934}
episode index:3909
target Thresh 74.62785827149455
target distance 65.0
model initialize at round 3909
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.05365851,  6.98833818]), 'previousTarget': array([69.61161351,  5.9223227 ]), 'currentState': array([49.35094504,  3.55277679,  2.97671986]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6081973356624262
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.94187294,  14.24468288,   1.30360536]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7575504628132638}
episode index:3910
target Thresh 74.62922972738082
target distance 20.0
model initialize at round 3910
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.20783571,  13.84494977]), 'previousTarget': array([112.52431817,  13.638375  ]), 'currentState': array([96.39684177,  3.01026091,  0.29700404]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6082235069463969
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44974684,  14.28041484,   0.27019578]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9058594556741493}
episode index:3911
target Thresh 74.6305998124967
target distance 22.0
model initialize at round 3911
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.17511408,  14.46131328]), 'previousTarget': array([112.6773982 ,  14.57770876]), 'currentState': array([91.3705628 , 11.67209316,  3.61868262]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6082609547429861
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.73657498,  14.93241338,   0.85705332]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.27195715485922967}
episode index:3912
target Thresh 74.63196852821228
target distance 4.0
model initialize at round 3912
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.64002954,  17.36947194,   5.97534454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 3.344227545239025}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6083437056740274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.89716029,  14.14513186,   0.95699611]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2392320747506422}
episode index:3913
target Thresh 74.6333358758963
target distance 47.0
model initialize at round 3913
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.68140341, 15.58218765]), 'previousTarget': array([88., 15.]), 'currentState': array([66.68562859, 15.99327063,  1.48445344]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6083698195017223
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.27715915,  15.65272585,   0.94759668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7091320230900914}
episode index:3914
target Thresh 74.63470185691605
target distance 8.0
model initialize at round 3914
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.57798834,   8.35583665,   0.15782809]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.575728369380101}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6084573342476735
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10624475,  15.01837137,   2.0525832 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8939440489172188}
episode index:3915
target Thresh 74.63606647263755
target distance 28.0
model initialize at round 3915
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([104.93932394,  17.83016493]), 'previousTarget': array([106.23047895,  17.50557744]), 'currentState': array([85.68660432, 23.24614012,  1.94633412]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.6084908488133375
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13767629,  15.39477016,   5.63755624]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41808879773937185}
episode index:3916
target Thresh 74.63742972442542
target distance 63.0
model initialize at round 3916
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.50050684, 13.53339546]), 'previousTarget': array([71.97736275, 12.95130299]), 'currentState': array([50.51136016, 12.87459754,  1.67575455]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.44318084924756646
running average episode reward sum: 0.6084486455967009
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09478434,  14.77417109,   5.68643609]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.24491379843762284}
episode index:3917
target Thresh 74.6387916136429
target distance 33.0
model initialize at round 3917
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([100.84963792,   9.685144  ]), 'previousTarget': array([100.79586847,   9.83486126]), 'currentState': array([82.       ,  3.       ,  5.8007274], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5085795494741819
running average episode reward sum: 0.6084231557814578
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15073086e+02, 1.50906979e+01, 5.40042182e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11648009108710154}
episode index:3918
target Thresh 74.64015214165187
target distance 35.0
model initialize at round 3918
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([99.9702068 , 14.90874364]), 'previousTarget': array([99.99184173, 15.42880452]), 'currentState': array([80.      , 16.      ,  3.457898], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.3678928113514323
running average episode reward sum: 0.6083617803427157
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.28153714,  14.96913986,   1.28725223]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2832234285518939}
episode index:3919
target Thresh 74.64151130981287
target distance 11.0
model initialize at round 3919
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.89100551,   5.22630586,   1.93975073]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.774301864471608}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6084491855135211
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.48698318,  14.04007626,   1.99423802]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0884116195526636}
episode index:3920
target Thresh 74.64286911948507
target distance 61.0
model initialize at round 3920
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([73.17994895, 11.87898444]), 'previousTarget': array([73.90394822, 10.9577654 ]), 'currentState': array([53.23541322, 10.39053081,  3.08352625]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4456009753452659
running average episode reward sum: 0.6084076531977425
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.44562081,  14.72069977,   5.52711076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6207615496766216}
episode index:3921
target Thresh 74.64422557202629
target distance 41.0
model initialize at round 3921
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.80954881,  8.76107013]), 'previousTarget': array([93.19474814,  8.61797507]), 'currentState': array([72.49626117,  3.56521403,  1.77205729]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4991341285707952
running average episode reward sum: 0.6083797915137479
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.04141198,  15.34367774,   5.45134266]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0183346115878964}
episode index:3922
target Thresh 74.64558066879295
target distance 19.0
model initialize at round 3922
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([114.4327075,  15.23886  ]), 'currentState': array([97.46492331, 23.84570068,  0.74845779]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.639891418016887}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6084506569433177
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76922434,  14.39392566,   0.30899058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6485241024300343}
episode index:3923
target Thresh 74.64693441114017
target distance 66.0
model initialize at round 3923
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.43554127,  9.28000646]), 'previousTarget': array([68.81660336,  8.70226409]), 'currentState': array([47.57861054,  6.89205717,  1.57112932]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 69
reward sum = 0.3812169817728026
running average episode reward sum: 0.6083927482595332
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.65422197,  14.72324232,   5.69140743]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7103528711370651}
episode index:3924
target Thresh 74.64828680042169
target distance 7.0
model initialize at round 3924
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.15932986,  20.33995604,   5.81807518]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.913820708450944}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6084824815746263
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.42127799,  14.49202323,   5.95706274]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7700386729719961}
episode index:3925
target Thresh 74.6496378379899
target distance 43.0
model initialize at round 3925
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([92.76677072, 17.34166654]), 'previousTarget': array([91.95150202, 16.60803474]), 'currentState': array([7.28767852e+01, 1.94365360e+01, 1.28035545e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.48468613904579494
running average episode reward sum: 0.6084509491389336
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.91839513,  15.35340825,   5.85317113]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36270751699522824}
episode index:3926
target Thresh 74.65098752519583
target distance 46.0
model initialize at round 3926
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.45047319, 17.84249031]), 'previousTarget': array([88.88288926, 17.83881638]), 'currentState': array([70.58320396, 20.14284096,  1.0999791 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5967142408795563
running average episode reward sum: 0.6084479604177064
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52446407,  14.31030595,   0.95820897]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8664527949918427}
episode index:3927
target Thresh 74.65233586338917
target distance 1.0
model initialize at round 3927
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00329398,  15.6829668 ,   0.55883908]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2082493666251652}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6085476427088424
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.00329398,  15.6829668 ,   0.55883908]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2082493666251652}
episode index:3928
target Thresh 74.65368285391827
target distance 4.0
model initialize at round 3928
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.74347465,  10.64347231,   0.6338709 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.692444714687025}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6086397148282853
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.7009887 ,  15.28900349,   1.40915525]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4158494577926014}
episode index:3929
target Thresh 74.65502849813011
target distance 8.0
model initialize at round 3929
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.60444478,   9.42932034,   4.06974983]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.258871897189989}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6087172918086047
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02809835,  14.4352102 ,   1.06238449]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.565488316991804}
episode index:3930
target Thresh 74.65637279737034
target distance 6.0
model initialize at round 3930
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.6814125 ,  13.69346162,   5.97545123]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.476715758669286}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6088092739271983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.24822982,  14.32459946,   5.42950416]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.010605907426929}
episode index:3931
target Thresh 74.65771575298325
target distance 65.0
model initialize at round 3931
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.52649396, 19.54658063]), 'previousTarget': array([69.91533358, 19.16166152]), 'currentState': array([51.63498044, 21.62689064,  1.07812529]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.49986815410390695
running average episode reward sum: 0.6087815676403663
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.63069475,  15.62262384,   5.92962773]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.723910776763654}
episode index:3932
target Thresh 74.6590573663118
target distance 47.0
model initialize at round 3932
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.02336086, 16.850833  ]), 'previousTarget': array([87.98191681, 16.14970567]), 'currentState': array([69.07393388, 18.27222772,  1.82036155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.26228292968482847
running average episode reward sum: 0.6086934673001793
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.35067545,  14.6284767 ,   6.2765787 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7480988813507301}
episode index:3933
target Thresh 74.6603976386976
target distance 61.0
model initialize at round 3933
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([73.65430618,  5.70246519]), 'previousTarget': array([73.56072872,  6.16867989]), 'currentState': array([54.       ,  2.       ,  2.9970818], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = -0.6607801445026011
running average episode reward sum: 0.6083707744654556
{'dynamicTrap': 17, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26846694,  15.94714289,   6.13387044]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1967540503817966}
episode index:3934
target Thresh 74.66173657148093
target distance 58.0
model initialize at round 3934
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([76.96157925, 14.23909403]), 'previousTarget': array([76.98811999, 13.68924552]), 'currentState': array([57.     , 13.     ,  5.86696], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.37325941411810953
running average episode reward sum: 0.6083110257080611
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.46991261,  14.53012943,   0.64836457]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6645270568750601}
episode index:3935
target Thresh 74.66307416600073
target distance 5.0
model initialize at round 3935
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.19421829,  17.18584601,   6.05506581]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.388837795118548}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6084029941974646
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.33618854,  14.99472432,   1.19793653]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3362299301657045}
episode index:3936
target Thresh 74.66441042359457
target distance 6.0
model initialize at round 3936
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.37039423,  10.15649027,   0.20904887]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.110303463389267}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.608494915966782
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39503097,  14.52085087,   1.01638593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6209938403041194}
episode index:3937
target Thresh 74.66574534559874
target distance 23.0
model initialize at round 3937
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([110.17533639,  16.03700497]), 'previousTarget': array([111.35234545,  15.95156206]), 'currentState': array([90.6219062 , 20.23978591,  2.63569546]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.608556612984336
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.990379  ,  14.49484692,   6.18907783]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5052446911140274}
episode index:3938
target Thresh 74.66707893334812
target distance 30.0
model initialize at round 3938
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.14784796,  17.08423063]), 'previousTarget': array([104.47682419,  17.45540769]), 'currentState': array([86.68017718, 21.66787486,  5.12889421]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.608605628972291
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.33966775,  14.49742469,   5.70395773]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8298316810024191}
episode index:3939
target Thresh 74.66841118817635
target distance 21.0
model initialize at round 3939
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([114.76906017,  15.01680896]), 'previousTarget': array([114.,  15.]), 'currentState': array([94.8218272 , 16.46866883,  2.07063324]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.46666495134983815
running average episode reward sum: 0.608569603419595
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.2003944 ,  14.02171366,   1.58984146]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9986000554593389}
episode index:3940
target Thresh 74.66974211141564
target distance 22.0
model initialize at round 3940
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.36071187,  14.72991441]), 'previousTarget': array([112.91786413,  14.81071492]), 'currentState': array([91.41556235, 13.24970801,  3.98915339]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6086066878356782
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85215067,  15.26246282,   1.08952717]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3012410233255776}
episode index:3941
target Thresh 74.67107170439692
target distance 46.0
model initialize at round 3941
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.99578773,  9.68074091]), 'previousTarget': array([88.6278531 ,  9.84023213]), 'currentState': array([70.43354398,  5.51917638,  0.68638247]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.29864048137678356
running average episode reward sum: 0.6085280561242477
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14602801e+02, 1.46487880e+01, 2.34138966e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5302046760098555}
episode index:3942
target Thresh 74.67239996844982
target distance 28.0
model initialize at round 3942
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.3041873 ,  13.37747921]), 'previousTarget': array([106.40285  ,  12.8507125]), 'currentState': array([8.77343970e+01, 9.25154826e+00, 3.01461220e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7194053960976405
running average episode reward sum: 0.6085561761698914
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9092277 ,  14.93211421,   1.17219811]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.11334942039346668}
episode index:3943
target Thresh 74.67372690490257
target distance 9.0
model initialize at round 3943
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.95178222,  19.40841404,   6.27834016]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 9.176487545788603}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6086430001743869
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.21354114,  15.47770361,   5.72704221]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9201729592207901}
episode index:3944
target Thresh 74.6750525150821
target distance 70.0
model initialize at round 3944
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.01785946, 10.19513233]), 'previousTarget': array([64.87065345,  9.27093182]), 'currentState': array([46.11339541,  8.24261832,  6.11337668]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.611117239532865
running average episode reward sum: 0.6086436273580013
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.7527283 ,  15.30311049,   0.45830169]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8114652574607151}
episode index:3945
target Thresh 74.67637680031405
target distance 28.0
model initialize at round 3945
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.41929032,  14.82720482]), 'previousTarget': array([106.98725709,  15.28616939]), 'currentState': array([87.42448399, 14.37144192,  5.97736597]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6086925343428419
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.47552156,  14.83677234,   0.30532264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.502756424454056}
episode index:3946
target Thresh 74.67769976192268
target distance 58.0
model initialize at round 3946
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.69560781,  7.40097468]), 'previousTarget': array([76.70920231,  8.39813833]), 'currentState': array([56.0592398 ,  3.60452218,  5.12922406]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.17373281663715667
running average episode reward sum: 0.608582334262349
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.37520396,  14.08076026,   5.79822657]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1114728081482117}
episode index:3947
target Thresh 74.67902140123097
target distance 64.0
model initialize at round 3947
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.20779717,  5.43864911]), 'previousTarget': array([70.59974639,  5.98119849]), 'currentState': array([51.66810369,  1.17245894,  6.05547678]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4256279252428257
running average episode reward sum: 0.6085359932266297
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17862992,  15.06079846,   5.78832325]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8236171831820545}
episode index:3948
target Thresh 74.68034171956056
target distance 47.0
model initialize at round 3948
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.11225189,  9.03939534]), 'previousTarget': array([87.56211858,  9.16215289]), 'currentState': array([69.62220496,  4.55184918,  0.74046248]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.47680134153931164
running average episode reward sum: 0.6085026342365848
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09433348,  15.49040324,   5.77430105]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0299161051081365}
episode index:3949
target Thresh 74.68166071823174
target distance 32.0
model initialize at round 3949
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([102.09243991,  11.59932813]), 'previousTarget': array([102.53800035,  12.27393758]), 'currentState': array([82.75240493,  6.50393345,  5.55837822]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5047046020847181
running average episode reward sum: 0.6084763562537614
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53496042,  15.2155977 ,   5.87196453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5125857783923975}
episode index:3950
target Thresh 74.68297839856353
target distance 50.0
model initialize at round 3950
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.72457342, 18.43077148]), 'previousTarget': array([84.85753677, 18.61709559]), 'currentState': array([66.87018706, 20.83978421,  5.93499059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6084951054916143
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.05456918,  15.45384602,   5.54818304]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.048720953323606}
episode index:3951
target Thresh 74.68429476187362
target distance 61.0
model initialize at round 3951
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.23183132,  6.73320094]), 'previousTarget': array([73.56072872,  6.16867989]), 'currentState': array([52.5953003 ,  2.9375917 ,  1.55295718]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.42225131442295505
running average episode reward sum: 0.6084479790262629
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40713472,  14.23320714,   5.79612157]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8681762292056292}
episode index:3952
target Thresh 74.68560980947835
target distance 44.0
model initialize at round 3952
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.70593821,  7.82301889]), 'previousTarget': array([90.18035416,  7.66692282]), 'currentState': array([69.41175622,  2.55665948,  1.79448366]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.608459922168572
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39141756,  15.85630364,   6.04318933]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.05053724791022}
episode index:3953
target Thresh 74.6869235426928
target distance 67.0
model initialize at round 3953
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([68.27400032,  7.70472127]), 'previousTarget': array([67.68673286,  6.52598201]), 'currentState': array([48.51339477,  4.61951974,  2.13308302]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6084590486594594
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38668663,  15.72039059,   5.92278853]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9461056452868367}
episode index:3954
target Thresh 74.68823596283066
target distance 2.0
model initialize at round 3954
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.43766472,  15.7474234 ,   3.66016972]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.620346138152911}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6085408707325941
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.79508014,  15.3731433 ,   3.73505747]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8782871691342493}
episode index:3955
target Thresh 74.68954707120439
target distance 23.0
model initialize at round 3955
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([111.55760219,  14.81024134]), 'previousTarget': array([111.83200822,  14.58678368]), 'currentState': array([91.58791964, 13.70943298,  1.02108407]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6085917268998056
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51377753,  15.72171399,   5.87630235]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8859110722945487}
episode index:3956
target Thresh 74.69085686912507
target distance 38.0
model initialize at round 3956
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.21847355, 19.02108228]), 'previousTarget': array([96.66906377, 18.37675141]), 'currentState': array([75.6192994 , 23.00509697,  1.7448647 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.608615698587473
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37500992,  15.75705036,   5.9197334 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8448418106443784}
episode index:3957
target Thresh 74.69216535790252
target distance 23.0
model initialize at round 3957
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.29336772,  15.20351208]), 'previousTarget': array([112.,  15.]), 'currentState': array([92.34966471, 16.70308207,  0.45538855]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6086584488807555
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18376696,  15.98320767,   5.2442696 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.2778629395946528}
episode index:3958
target Thresh 74.69347253884521
target distance 66.0
model initialize at round 3958
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.50307802, 10.06301883]), 'previousTarget': array([68.85467564,  9.40662735]), 'currentState': array([47.6102522 ,  7.99529524,  1.55564451]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.19654975002051395
running average episode reward sum: 0.608554354741109
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0829968 ,  15.00765718,   5.68653752]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.08334927709095126}
episode index:3959
target Thresh 74.69477841326034
target distance 41.0
model initialize at round 3959
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.74963992, 16.90872119]), 'previousTarget': array([93.97624702, 16.02554893]), 'currentState': array([73.82983251, 18.69793126,  2.64069599]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6446496168142221
running average episode reward sum: 0.608563469706279
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.53242748,  14.52056907,   5.70719076]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6696850577045305}
episode index:3960
target Thresh 74.69608298245376
target distance 11.0
model initialize at round 3960
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([105.2014637,  17.2453387,   6.1588133]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.05250513864245}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6086499192342248
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69546365,  14.87526461,   6.0780814 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.32909163564419464}
episode index:3961
target Thresh 74.69738624773007
target distance 4.0
model initialize at round 3961
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([113.28965104,  19.60855358,   3.56222808]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.915695240522491}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6087411986589513
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.39035742,  14.62962329,   5.6175617 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7133323089982148}
episode index:3962
target Thresh 74.6986882103925
target distance 10.0
model initialize at round 3962
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([103.42284745,   5.41308142,   1.89281964]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 15.031283013441872}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6088181040459875
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34117386,  14.87986943,   5.49937147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6696889118184598}
episode index:3963
target Thresh 74.69998887174306
target distance 38.0
model initialize at round 3963
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.97795404, 15.06119632]), 'previousTarget': array([96.99307839, 15.47386636]), 'currentState': array([77.     , 16.     ,  3.54066], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5922820409839835
running average episode reward sum: 0.6088139324861838
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.72820809,  15.1263715 ,   6.05642704]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2997342094090917}
episode index:3964
target Thresh 74.70128823308235
target distance 59.0
model initialize at round 3964
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.19517125, 10.04811247]), 'previousTarget': array([75.86070472, 10.3563548 ]), 'currentState': array([54.3408357 ,  7.63868104,  4.34046841]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.4103277753147375
running average episode reward sum: 0.608763872925737
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98238836,  15.36121283,   0.34543264]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36164192393928907}
episode index:3965
target Thresh 74.70258629570979
target distance 18.0
model initialize at round 3965
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([97.62719302, 23.82582789,  0.18762219]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.48614021222562}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6088361307122556
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.55971509,  14.67660073,   6.20771701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6464271606610722}
episode index:3966
target Thresh 74.70388306092342
target distance 4.0
model initialize at round 3966
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.73405496,  12.56076444,   0.19334984]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.076305571114344}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.608927248148426
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.25973734,  15.12529587,   5.05594486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.750791490226198}
episode index:3967
target Thresh 74.70517853001999
target distance 63.0
model initialize at round 3967
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([70.68928725,  7.58134607]), 'previousTarget': array([71.64677133,  6.74224216]), 'currentState': array([50.96383497,  4.27884212,  1.33759844]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.47635514887559216
running average episode reward sum: 0.6088938378411496
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.23236777,  15.20910339,   5.65829403]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3126003965736712}
episode index:3968
target Thresh 74.70647270429498
target distance 6.0
model initialize at round 3968
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.89387333,   8.09541208,   2.95896637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.21866357675733}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6089776338380152
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.4028482 ,  15.50447688,   0.14134537]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7817206625727147}
episode index:3969
target Thresh 74.70776558504257
target distance 10.0
model initialize at round 3969
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.13828187,   5.99955875,   1.3308233 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.317734681524094}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6090566683620932
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.17576526,  15.30928803,   0.38953757]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8803533305848548}
episode index:3970
target Thresh 74.70905717355564
target distance 23.0
model initialize at round 3970
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.16563869,  15.93786403]), 'previousTarget': array([111.35234545,  15.95156206]), 'currentState': array([93.17810524, 22.22066384,  6.09913892]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.6037866037644188
running average episode reward sum: 0.609055341224194
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52742728,  15.03605165,   6.00586879]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4739458803428473}
episode index:3971
target Thresh 74.71034747112577
target distance 28.0
model initialize at round 3971
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.30576858,  16.0170891 ]), 'previousTarget': array([106.79898987,  16.17157288]), 'currentState': array([88.53269022, 19.02131486,  1.02390593]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6091038244186339
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.0303397 ,  14.07227456,   0.26009476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9282214126321938}
episode index:3972
target Thresh 74.71163647904326
target distance 10.0
model initialize at round 3972
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.79004523,   5.84265797,   0.18284237]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.064377585774013}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6091781456495904
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38846842,  14.78093228,   3.57915923]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6495856695171522}
episode index:3973
target Thresh 74.71292419859715
target distance 33.0
model initialize at round 3973
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.86656689,  10.72973472]), 'previousTarget': array([101.29527642,  11.26234812]), 'currentState': array([82.8466943 ,  4.54552524,  6.2495656 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.42558261226221766
running average episode reward sum: 0.6091319464715864
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.13097008,  14.03264487,   0.61139443]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.3003803151364552}
episode index:3974
target Thresh 74.7142106310751
target distance 60.0
model initialize at round 3974
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.93753357, 19.42052066]), 'previousTarget': array([74.9007438 , 19.00992562]), 'currentState': array([55.      , 21.      ,  4.710649], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.32521726773456006
running average episode reward sum: 0.6090605213951745
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69591572,  14.61398885,   5.28159614]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.491397859448584}
episode index:3975
target Thresh 74.71549577776359
target distance 43.0
model initialize at round 3975
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([90.53896887, 11.03403572]), 'previousTarget': array([91.80809791, 11.76392064]), 'currentState': array([70.7967714 ,  7.83315471,  2.90164089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.609067343569387
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.51091345,  15.07668168,   0.37985408]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5166358805627987}
episode index:3976
target Thresh 74.71677963994775
target distance 32.0
model initialize at round 3976
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([103.33771631,  10.5486346 ]), 'previousTarget': array([101.72658355,  10.02246883]), 'currentState': array([84.65254435,  3.41670999,  1.22076672]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6091039671408314
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.46179869,  14.93068181,   5.92882681]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5426469036827128}
episode index:3977
target Thresh 74.71806221891143
target distance 12.0
model initialize at round 3977
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.46869507,   3.68439872,   1.38860052]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.595164412197034}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6091851540138243
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79877922,  15.84300925,   0.66389334]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8666916437945751}
episode index:3978
target Thresh 74.71934351593724
target distance 18.0
model initialize at round 3978
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.97868625,  13.64365907]), 'previousTarget': array([115.,  15.]), 'currentState': array([9.7000000e+01, 1.4000000e+01, 3.4615308e-02], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 16.982425199661165}
done in step count: 23
reward sum = 0.5857072836436554
running average episode reward sum: 0.609179253568896
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.09612342,  15.15061437,   5.82077691]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.17867400770702188}
episode index:3979
target Thresh 74.72062353230645
target distance 44.0
model initialize at round 3979
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.20015588, 12.42788277]), 'previousTarget': array([90.8721051 , 12.25819376]), 'currentState': array([69.29881214, 10.44381693,  2.0096283 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6017300378696803
running average episode reward sum: 0.60917738190666
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.13698146,  15.03196685,   5.36271796]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.14066200262509884}
episode index:3980
target Thresh 74.72190226929908
target distance 20.0
model initialize at round 3980
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([114.83082261,  13.40414277]), 'previousTarget': array([114.97504678,  15.00124766]), 'currentState': array([95.     , 16.     ,  2.72792], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.6106142978203272
running average episode reward sum: 0.6091777428501198
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.87714284,  14.7537115 ,   6.05966333]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2752306483184419}
episode index:3981
target Thresh 74.72317972819387
target distance 50.0
model initialize at round 3981
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([84.98772396, 12.70063598]), 'previousTarget': array([84.96409691, 13.19784581]), 'currentState': array([65.      , 12.      ,  1.905713], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.640553227272292
running average episode reward sum: 0.6091856221782018
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48170601,  15.62088905,   6.07422834]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.785839611890121}
episode index:3982
target Thresh 74.72445591026829
target distance 47.0
model initialize at round 3982
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([86.18555158, 16.15838353]), 'previousTarget': array([87.98191681, 16.14970567]), 'currentState': array([66.20169361, 16.96176424,  2.36562872]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5316787773654661
running average episode reward sum: 0.6091661627644903
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.85993138,  15.03130463,   5.67892939]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.14352420956301234}
episode index:3983
target Thresh 74.72573081679852
target distance 30.0
model initialize at round 3983
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([106.0634566 ,  16.56183136]), 'previousTarget': array([104.82455801,  16.3567256 ]), 'currentState': array([86.36207522, 20.00502341,  1.62922042]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.708533338401361
running average episode reward sum: 0.6091911043246401
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80906418,  15.35949133,   0.35666637]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8853354549655106}
episode index:3984
target Thresh 74.72700444905946
target distance 6.0
model initialize at round 3984
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.85389502,  10.2540274 ,   1.32820106]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.6940523721062855}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6092744892794898
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.09384283,  14.72153546,   3.30955357]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9479785379546203}
episode index:3985
target Thresh 74.72827680832474
target distance 55.0
model initialize at round 3985
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([79.994721  , 15.54050889]), 'previousTarget': array([79.99669503, 15.63642373]), 'currentState': array([60.       , 16.       ,  5.4276433], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.4806148998611943
running average episode reward sum: 0.6092422114095907
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.45999175,  15.33851287,   5.73960927]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5711246525712037}
episode index:3986
target Thresh 74.72954789586672
target distance 58.0
model initialize at round 3986
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.5596451, 16.3828013]), 'previousTarget': array([76.99702801, 15.65522365]), 'currentState': array([55.57192621, 17.08358211,  1.68294072]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 49
reward sum = 0.33749266518573257
running average episode reward sum: 0.6091740525066001
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.41285672,  15.78771464,   5.11301904]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9824620042582334}
episode index:3987
target Thresh 74.7308177129565
target distance 27.0
model initialize at round 3987
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.63782776,  11.8902669 ]), 'previousTarget': array([106.27623097,  11.12276932]), 'currentState': array([89.21396696,  4.10814937,  0.15452588]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 43
reward sum = 0.5374354872456131
running average episode reward sum: 0.6091560638994634
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.28466199,  15.98315685,   5.26218727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.215856016525362}
episode index:3988
target Thresh 74.73208626086387
target distance 25.0
model initialize at round 3988
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.5719453 ,  14.05082717]), 'previousTarget': array([109.44774604,  13.66745905]), 'currentState': array([89.87088208, 10.60581509,  2.62032771]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6091983465004911
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.58179297,  15.3178498 ,   5.153909  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5252862275455231}
episode index:3989
target Thresh 74.7333535408574
target distance 52.0
model initialize at round 3989
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([84.85195818, 16.50877934]), 'previousTarget': array([82.98522349, 16.23133756]), 'currentState': array([64.87695694, 17.50844191,  6.13423127]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6092255475496384
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.26207409,  15.02846496,   5.51006711]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7384747178628167}
episode index:3990
target Thresh 74.73461955420436
target distance 7.0
model initialize at round 3990
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.32146812,  15.10845017,   6.04337746]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.679567387889483}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6093160194745821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.78469074,  15.02754558,   4.97246011]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.21706412846864082}
episode index:3991
target Thresh 74.73588430217077
target distance 12.0
model initialize at round 3991
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.94987693,   7.44107857,   0.27370024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.575462893694334}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6093945336717147
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.93896194,  14.97437155,   5.80393997]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9393116297981832}
episode index:3992
target Thresh 74.7371477860214
target distance 45.0
model initialize at round 3992
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.70590187, 10.07209487]), 'previousTarget': array([89.5237412 ,  9.33860916]), 'currentState': array([69.0481551 ,  6.3879407 ,  3.18192601]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6094111463727888
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43950756,  14.50432545,   5.256695  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6624652129527415}
episode index:3993
target Thresh 74.7384100070197
target distance 68.0
model initialize at round 3993
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([67.14103549,  7.70363665]), 'previousTarget': array([66.69567118,  6.47570668]), 'currentState': array([47.36948759,  4.68935503,  2.28521687]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.24773358524418534
running average episode reward sum: 0.609320591149672
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.16584683,  15.23887299,   2.69983305]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.29080143986823087}
episode index:3994
target Thresh 74.7396709664279
target distance 48.0
model initialize at round 3994
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.29019527, 10.01492399]), 'previousTarget': array([86.65744374,  9.6857707 ]), 'currentState': array([68.62969016,  6.34551819,  1.09389382]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2597390106112685
running average episode reward sum: 0.6092330863735672
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.34646312,  15.09547042,   6.26542103]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6604733595437665}
episode index:3995
target Thresh 74.74093066550694
target distance 68.0
model initialize at round 3995
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([65.773771  , 21.30972299]), 'previousTarget': array([66.86301209, 20.66317505]), 'currentState': array([45.93607001, 23.8524812 ,  1.34567916]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.5203405226503064
running average episode reward sum: 0.60921084098725
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.67608044,  15.52403697,   1.39590183]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.616067064209223}
episode index:3996
target Thresh 74.74218910551656
target distance 14.0
model initialize at round 3996
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.32931237,  15.20295548]), 'previousTarget': array([115.,  15.]), 'currentState': array([101.        ,  11.        ,   0.86911213], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 13.026003926924}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.609247575391371
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.68250375,  15.47280308,   4.79761453]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5695143721454229}
episode index:3997
target Thresh 74.74344628771517
target distance 63.0
model initialize at round 3997
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([71.66170801, 16.72401461]), 'previousTarget': array([71.99748095, 15.68257967]), 'currentState': array([51.67751406, 17.51899385,  0.76999259]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3254075465059231
running average episode reward sum: 0.6091765798863972
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43920138,  15.37298743,   6.02615075]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5762095747684722}
episode index:3998
target Thresh 74.74470221335999
target distance 37.0
model initialize at round 3998
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([96.26759851, 17.7574013 ]), 'previousTarget': array([97.74210955, 17.79857683]), 'currentState': array([76.48081684, 20.67000691,  2.41284323]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6133610701385961
running average episode reward sum: 0.6091776262705563
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98988867,  14.7372063 ,   0.72823748]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.26298814951211896}
episode index:3999
target Thresh 74.7459568837069
target distance 23.0
model initialize at round 3999
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.87337548,  14.94340551]), 'previousTarget': array([111.92481176,  14.73259233]), 'currentState': array([92.8804539 , 14.41134673,  1.86267987]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6092277638310439
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.54808905,  15.46233823,   5.66755517]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6465138404055369}
episode index:4000
target Thresh 74.74721030001058
target distance 46.0
model initialize at round 4000
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.6796393, 11.8209724]), 'previousTarget': array([88.77237675, 11.00883994]), 'currentState': array([68.8239454 ,  9.42276063,  0.68391204]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6092377300556221
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02370089,  15.72106702,   4.94758059]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7214564320931331}
episode index:4001
target Thresh 74.74846246352446
target distance 4.0
model initialize at round 4001
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.3736798 ,  11.32781104,   0.54098159]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 4.514701485124993}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6093279502630046
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.19291138,  15.3552207 ,   0.60314671]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.881801444757134}
episode index:4002
target Thresh 74.7497133755007
target distance 24.0
model initialize at round 4002
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.9067978 ,  16.95012182]), 'previousTarget': array([110.2,  16.4]), 'currentState': array([89.8585845 , 23.04647893,  1.42718363]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6093780126956696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32240442,  15.15016602,   1.17326758]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.35566057790628036}
episode index:4003
target Thresh 74.75096303719023
target distance 40.0
model initialize at round 4003
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([94.40328886, 10.54839607]), 'previousTarget': array([94.70060934, 11.44760664]), 'currentState': array([74.85466468,  6.32331705,  3.61592484]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.40495448733988465
running average episode reward sum: 0.6093269578691572
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.48519916,  15.72486065,   4.58447147]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8722621106959243}
episode index:4004
target Thresh 74.75221144984268
target distance 19.0
model initialize at round 4004
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.73266557,  13.27286502]), 'previousTarget': array([112.50614522,  13.29367831]), 'currentState': array([96.82282052,  1.15358985,  5.51078701]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4316942024771592
running average episode reward sum: 0.6092826051212441
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39189143,  15.78050836,   5.42812778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8733683036307246}
episode index:4005
target Thresh 74.75345861470647
target distance 5.0
model initialize at round 4005
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([108.06608709,  13.28813981,   2.62239486]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.142101479925783}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6093679040340696
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.65992597,  15.47896749,   0.84298617]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5874182528841324}
episode index:4006
target Thresh 74.75470453302877
target distance 21.0
model initialize at round 4006
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([111.87307313,  11.97514663]), 'previousTarget': array([111.36486284,  12.92277877]), 'currentState': array([94.        ,  3.        ,  0.35799852], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.640553227272292
running average episode reward sum: 0.6093756867451348
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14477545,  15.63241344,   5.72838824]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6487732183811019}
episode index:4007
target Thresh 74.75594920605552
target distance 23.0
model initialize at round 4007
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.46477082,  12.98741158]), 'previousTarget': array([110.34140113,  12.97452223]), 'currentState': array([90.66866004,  6.15321671,  3.29843211]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7105532272722921
running average episode reward sum: 0.6094009306424719
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.88878308,  14.88468074,   4.97332886]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.1602115323044601}
episode index:4008
target Thresh 74.75719263503136
target distance 5.0
model initialize at round 4008
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.14804136,   8.76939481,   0.18990534]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.8523068465968295}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6094861362097599
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52149273,  14.84693084,   5.10437024]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5434931763245804}
episode index:4009
target Thresh 74.75843482119971
target distance 72.0
model initialize at round 4009
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([62.9991663, 15.817388 ]), 'previousTarget': array([62.99807127, 15.72224901]), 'currentState': array([43.       , 16.       ,  0.6816336], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.27783181729288897
running average episode reward sum: 0.6094034293970624
{'dynamicTrap': 6, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.76327453,  14.97105818,   0.47263084]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23848810298395656}
episode index:4010
target Thresh 74.7596757658028
target distance 72.0
model initialize at round 4010
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([62.82794143,  4.94137187]), 'previousTarget': array([62.68175722,  5.55365061]), 'currentState': array([43.18959751,  1.15515366,  5.58580756]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.43589082257534406
running average episode reward sum: 0.6093601702081266
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.40992497,  14.94083555,   1.10205454]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.41417256060003865}
episode index:4011
target Thresh 74.76091547008156
target distance 19.0
model initialize at round 4011
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([113.08621319,  13.95399555]), 'previousTarget': array([114.07475678,  14.56172689]), 'currentState': array([95.53648349,  4.36196827,  3.66223907]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6093784140827034
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.14147789e+02, 1.43549671e+01, 1.65645068e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0687991115175035}
episode index:4012
target Thresh 74.76215393527566
target distance 12.0
model initialize at round 4012
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.13430686,   4.24327633,   1.84119814]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.25207982980975}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6094565018674891
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.0899263 ,  15.46751135,   2.98895359]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.023132933652303}
episode index:4013
target Thresh 74.76339116262362
target distance 7.0
model initialize at round 4013
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.07885086,  18.29169227,   6.14812929]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 6.774602954013555}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6095345507445595
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.23435879,  15.22141161,   1.49996227]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7970129025862462}
episode index:4014
target Thresh 74.76462715336262
target distance 60.0
model initialize at round 4014
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([74.15472772, 16.67994025]), 'previousTarget': array([74.9972228 , 15.66671295]), 'currentState': array([54.17162253, 17.50183274,  3.0289104 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6095411885864821
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.43755596,  15.91222432,   5.86118628]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.0117353553970907}
episode index:4015
target Thresh 74.76586190872868
target distance 24.0
model initialize at round 4015
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([109.39768142,  14.8961175 ]), 'previousTarget': array([110.98266146,  14.83261089]), 'currentState': array([89.40111888, 14.52532584,  3.83414567]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.5435209858916336
running average episode reward sum: 0.6095247492929824
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.73550663,  15.56238824,   5.30605476]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9258782505142904}
episode index:4016
target Thresh 74.76709542995654
target distance 26.0
model initialize at round 4016
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([108.33000397,  13.3057363 ]), 'previousTarget': array([108.11558017,  12.88171698]), 'currentState': array([88.94559415,  8.38185003,  2.40381849]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6095705769092011
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82480692,  15.88297009,   0.36134261]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9001826477705316}
episode index:4017
target Thresh 74.76832771827975
target distance 43.0
model initialize at round 4017
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.39659405, 18.34718536]), 'previousTarget': array([91.80809791, 18.23607936]), 'currentState': array([73.63241405, 21.40940499,  1.2557289 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6166509845523169
running average episode reward sum: 0.6095723390813373
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.16245203,  15.73744325,   5.8885735 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 1.1159342106725152}
episode index:4018
target Thresh 74.76955877493054
target distance 48.0
model initialize at round 4018
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.5425579 , 11.29147532]), 'previousTarget': array([86.72787848, 10.28797975]), 'currentState': array([67.72252344,  8.61449301,  0.29352474]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 52
reward sum = 0.489926184388512
running average episode reward sum: 0.6095425689507843
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59055121,  15.14471196,   5.17411085]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.43426934025256253}
episode index:4019
target Thresh 74.77078860114003
target distance 53.0
model initialize at round 4019
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([81.65477437,  7.69997897]), 'previousTarget': array([81.58267675,  8.06432914]), 'currentState': array([62.       ,  4.       ,  6.1193447], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 49
reward sum = 0.19503758081241152
running average episode reward sum: 0.6094394582572175
{'dynamicTrap': 7, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.25920195,  15.31150482,   0.33557058]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.40524178105647557}
episode index:4020
target Thresh 74.772017198138
target distance 6.0
model initialize at round 4020
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.74147222,   9.17855973,   0.99302178]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.212782103643421}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6095196934946335
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.14221123,  15.18498   ,   5.39808686]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.2333273128088087}
episode index:4021
target Thresh 74.77324456715307
target distance 7.0
model initialize at round 4021
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([109.54967237,  13.53277736,   0.70310229]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.644361219122077}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6096093949631829
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.10558754,  14.56537349,   0.81728924]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9944213640836986}
episode index:4022
target Thresh 74.77447070941261
target distance 58.0
model initialize at round 4022
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([75.28639688,  8.79541308]), 'previousTarget': array([76.70920231,  8.39813833]), 'currentState': array([55.52610585,  5.70819777,  1.683671  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2769988911646184
running average episode reward sum: 0.6095267177313164
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.9360568 ,  14.39055195,   5.5957184 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6127933245404152}
episode index:4023
target Thresh 74.77569562614276
target distance 48.0
model initialize at round 4023
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.88113829,  8.53234047]), 'previousTarget': array([86.40285  ,  7.8507125]), 'currentState': array([68.4267591 ,  3.8926066 ,  1.56904381]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.617897245391792
running average episode reward sum: 0.6095287978823255
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.96335276,  15.50595854,   1.34656602]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5072840035444318}
episode index:4024
target Thresh 74.77691931856843
target distance 18.0
model initialize at round 4024
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([112.73653087,  13.74827599]), 'previousTarget': array([114.06563667,  14.42900019]), 'currentState': array([95.2345144 ,  4.06946306,  3.96246982]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6095745334067383
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.34775835,  15.32695085,   5.77889316]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4773182692955427}
episode index:4025
target Thresh 74.77814178791333
target distance 38.0
model initialize at round 4025
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([95.70738736,  9.30007688]), 'previousTarget': array([95.9232797 ,  8.47375358]), 'currentState': array([76.52698809,  3.63330675,  2.83307725]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 60
reward sum = 0.4127773902698527
running average episode reward sum: 0.6095256518510659
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.14850453,  14.83863238,   5.57124797]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8666510542481282}
episode index:4026
target Thresh 74.7793630353999
target distance 54.0
model initialize at round 4026
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([80.99937203, 14.84151165]), 'previousTarget': array([81., 15.]), 'currentState': array([61.       , 15.       ,  3.8179219], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.3243876088541394
running average episode reward sum: 0.60945484528464
{'dynamicTrap': 4, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.66565347,  14.77945183,   4.53238562]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.400536009441461}
episode index:4027
target Thresh 74.7805830622494
target distance 48.0
model initialize at round 4027
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.24034261, 19.3385026 ]), 'previousTarget': array([86.79065962, 19.11386214]), 'currentState': array([68.498128  , 22.53927786,  5.69493002]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6094599020835274
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.79725997,  14.70045364,   5.31172116]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.36170642979827067}
episode index:4028
target Thresh 74.78180186968186
target distance 30.0
model initialize at round 4028
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([105.54693562,  12.81334729]), 'previousTarget': array([104.32469879,  12.15325301]), 'currentState': array([86.06145212,  8.30602737,  6.16152448]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.5503750279177817
running average episode reward sum: 0.6094452371854967
{'dynamicTrap': 3, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.02540938,  15.85885656,   1.13683257]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8592323456331912}
episode index:4029
target Thresh 74.7830194589161
target distance 47.0
model initialize at round 4029
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([88.30326177, 12.86607847]), 'previousTarget': array([87.98191681, 13.85029433]), 'currentState': array([68.36684835, 11.27252269,  5.75175953]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5824314032808505
running average episode reward sum: 0.609438534000905
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.32969273,  14.93089257,   5.13799649]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.3368577334047281}
episode index:4030
target Thresh 74.7842358311697
target distance 45.0
model initialize at round 4030
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.02415788, 10.03692126]), 'previousTarget': array([89.61161351,  9.9223227 ]), 'currentState': array([71.43936443,  5.98280414,  0.99805373]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6094549791795354
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.40670866,  14.75561224,   5.33199296]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6416541080375617}
episode index:4031
target Thresh 74.78545098765902
target distance 45.0
model initialize at round 4031
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.91591314, 14.83605888]), 'previousTarget': array([89.98027613, 13.88801227]), 'currentState': array([69.91634028, 14.70534843,  2.5881592 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6094714162008352
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.62159321,  15.35777873,   0.64282246]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7172055044691117}
episode index:4032
target Thresh 74.78666492959923
target distance 3.0
model initialize at round 4032
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([116.04055032,  17.20687679,   5.55183145]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.4398873217997554}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.6094703089979928
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.59070663,  15.35738596,   5.09549214]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6904049898809821}
episode index:4033
target Thresh 74.78787765820427
target distance 6.0
model initialize at round 4033
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([111.83765698,  10.11373624,   1.43536329]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 5.820308147400389}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6095526118835661
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.82629645,  15.16430728,   3.11517089]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.23910208093667074}
episode index:4034
target Thresh 74.78908917468686
target distance 26.0
model initialize at round 4034
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.98913278,  14.83232282]), 'previousTarget': array([108.94108971,  14.53392998]), 'currentState': array([87.99485044, 14.35412451,  1.273211  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6095867183228346
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.18483011,  15.51386621,   6.1620704 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9636184066595087}
episode index:4035
target Thresh 74.79029948025853
target distance 42.0
model initialize at round 4035
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([93.20772308,  8.51605833]), 'previousTarget': array([92.34744447,  9.06718784]), 'currentState': array([74.03823544,  2.81248545,  0.157633  ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5542253948911999
running average episode reward sum: 0.6095730014438872
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.50514717,  14.87188993,   1.1300499 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5111668139635427}
episode index:4036
target Thresh 74.79150857612957
target distance 34.0
model initialize at round 4036
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([101.60254276,  17.14007178]), 'previousTarget': array([100.86301209,  16.66317505]), 'currentState': array([81.85292094, 20.2948203 ,  1.91961688]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6096246075712475
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.15404943e+02, 1.58536317e+01, 3.33825126e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9448100345341178}
episode index:4037
target Thresh 74.7927164635091
target distance 42.0
model initialize at round 4037
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([91.00011676,  9.57298745]), 'previousTarget': array([92.55604828, 10.19058177]), 'currentState': array([71.49263953,  5.1618275 ,  2.7111665 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6096393047359325
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.59539387,  14.7539415 ,   0.23428326]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.47355138143441067}
episode index:4038
target Thresh 74.79392314360499
target distance 2.0
model initialize at round 4038
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([112.09707317,  15.42025041,   1.12708998]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 2.9331884664347907}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6097310256310216
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52187551,  15.47945285,   0.59933865]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.708681232941021}
episode index:4039
target Thresh 74.79512861762392
target distance 28.0
model initialize at round 4039
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([107.7113978 ,  12.62390351]), 'previousTarget': array([106.04057123,  12.12018361]), 'currentState': array([88.69632308,  6.42495737,  5.6978634 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.609772632149281
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.98850756,  14.4720261 ,   5.70530193]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5280989651392648}
episode index:4040
target Thresh 74.79633288677137
target distance 5.0
model initialize at round 4040
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.11274437e+02, 8.76485064e+00, 7.51975139e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.263395170421482}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.609854717652189
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.77023519,  15.60288257,   6.23809303]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6451815673185485}
episode index:4041
target Thresh 74.7975359522516
target distance 69.0
model initialize at round 4041
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([66.60284821, 10.99122064]), 'previousTarget': array([65.89786813, 10.0186243 ]), 'currentState': array([46.67110674,  9.34025666,  0.14260793]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.23081171963810704
running average episode reward sum: 0.6097609415517402
{'dynamicTrap': 5, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.74661334,  15.77177802,   5.48525173]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.812309118977008}
episode index:4042
target Thresh 74.79873781526769
target distance 13.0
model initialize at round 4042
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.50194967,  15.76696369]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.      ,  17.      ,   4.820004], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.5678530780607}
done in step count: 7
reward sum = 0.86206534790699
running average episode reward sum: 0.6098233467969432
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.52944553,  15.03180388,   5.36429203]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5303998973067235}
episode index:4043
target Thresh 74.79993847702148
target distance 8.0
model initialize at round 4043
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.69963199,  22.73937112,   6.02467901]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.745197626798181}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6098363187786906
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.11429747,  15.31201152,   0.57157155]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.9390527956677646}
episode index:4044
target Thresh 74.80113793871365
target distance 13.0
model initialize at round 4044
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([113.55246796,  15.3447844 ]), 'previousTarget': array([115.,  15.]), 'currentState': array([102.        ,  15.        ,   0.71924895], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 11.55761187802314}
done in step count: 11
reward sum = 0.7560382542587164
running average episode reward sum: 0.6098724626440751
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.70075841,  15.39213335,   1.63926111]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.4932687798915288}
episode index:4045
target Thresh 74.80233620154367
target distance 47.0
model initialize at round 4045
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([89.31366178, 14.93196729]), 'previousTarget': array([87.9954746 , 15.57456437]), 'currentState': array([69.31373193, 14.87899557,  0.22743064]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.682554595010387
running average episode reward sum: 0.6098904265917682
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.38764016,  14.7992751 ,   5.24496786]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6444183874065083}
episode index:4046
target Thresh 74.80353326670979
target distance 66.0
model initialize at round 4046
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.30860719, 11.73994362]), 'previousTarget': array([68.9793708 , 12.90815322]), 'currentState': array([49.3593211 , 10.31657276,  5.86657727]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5526834771623851
running average episode reward sum: 0.6098762909482225
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.52624023,  14.48588224,   0.26841428]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6991175813152496}
episode index:4047
target Thresh 74.80472913540908
target distance 8.0
model initialize at round 4047
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([107.21430161,  18.48399682,   5.82245803]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 8.52967369325776}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6099605581811651
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.43806676,  15.49471868,   1.35936242]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7486758628787782}
episode index:4048
target Thresh 74.80592380883739
target distance 7.0
model initialize at round 4048
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([1.09337027e+02, 1.91444562e+01, 7.69273599e-02]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.0175337445694685}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6100471562181665
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([114.15804137,  15.31211912,   6.01556414]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8979491492515491}
episode index:4049
target Thresh 74.80711728818943
target distance 60.0
model initialize at round 4049
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([74.99968845, 13.88836704]), 'previousTarget': array([74.9972228 , 14.33328705]), 'currentState': array([55.       , 14.       ,  6.2036023], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.49445979083803365
running average episode reward sum: 0.6100186161279492
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.39470522,  15.78050419,   1.35659559]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8746307788431558}
episode index:4050
target Thresh 74.80830957465864
target distance 12.0
model initialize at round 4050
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([104.37259214,   7.54858981,   5.38503949]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 12.979418767068914}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6100890480307215
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.07618221,  15.05676349,   4.85116778]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.09500432947051883}
episode index:4051
target Thresh 74.80950066943734
target distance 67.0
model initialize at round 4051
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([69.61787105, 13.6045311 ]), 'previousTarget': array([67.99109528, 13.59674911]), 'currentState': array([49.62731955, 12.98983555,  0.96859735]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6298236312032323
running average episode reward sum: 0.6100939183622054
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.49759092,  15.19671794,   1.3306265 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.5350651094893352}
episode index:4052
target Thresh 74.8106905737166
target distance 9.0
model initialize at round 4052
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([106.20632081,   9.44013198,   5.85389376]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 10.403889952916316}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6101642969301542
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.37672671,  15.58914503,   0.22959726]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.6992959914402872}
episode index:4053
target Thresh 74.81187928868634
target distance 47.0
model initialize at round 4053
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([87.63360307, 13.58339878]), 'previousTarget': array([87.92796014, 12.69599661]), 'currentState': array([67.66034473, 12.54949789,  1.11532295]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5292964995668706
running average episode reward sum: 0.6101443492741692
{'dynamicTrap': 2, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.80661168,  14.96019034,   5.86957816]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8075934654953768}
episode index:4054
target Thresh 74.81306681553527
target distance 7.0
model initialize at round 4054
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([110.58601688,  20.78603451,   5.53877488]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 7.277461255515401}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6102102868015983
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.04176298,  14.21471239,   3.58187212]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.7863973368969794}
episode index:4055
target Thresh 74.81425315545091
target distance 38.0
model initialize at round 4055
at step 0:
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([96.53683385,  9.27926666]), 'previousTarget': array([96.34149075, 10.08986599]), 'currentState': array([77.       ,  5.       ,  1.0813558], dtype=float32), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5726116020847181
running average episode reward sum: 0.6102010169089165
{'dynamicTrap': 1, 'scaleFactor': 20, 'currentTarget': array([115.,  15.]), 'previousTarget': array([115.,  15.]), 'currentState': array([115.41994768,  14.25701942,   1.04286568]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 0.8534495878895254}
episode index:4056
target Thresh 74.81543830961962
target distance 61.0
model initialize at round 4056
at step 0:
{'dynamicTrap': 0, 'scaleFactor': 20, 'currentTarget': array([72.22571135, 19.10258155]), 'previousTarget': array([73.90394822, 19.0422346 ]), 'currentState': array([52.31707323, 21.01206536,  2.1244235 ]), 'targetState': array([115,  15], dtype=int32), 'currentDistance': 20.0}
Traceback (most recent call last):
  File "/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length120/DDPGHER_CNN.py", line 212, in <module>
    agent.train()
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 238, in train
    self.update_net(state, action, nextState, reward, info)
  File "/home/yangyutu/Dropbox/pythonScripts/DeepReinforcementLearning-PyTorch/Agents/DDPG/DDPG.py", line 193, in update_net
    torch.nn.utils.clip_grad_norm_(self.actorNet.parameters(), self.netGradClip)
  File "/home/yangyutu/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py", line 33, in clip_grad_norm_
    total_norm += param_norm.item() ** norm_type
KeyboardInterrupt

Process finished with exit code 1
