/home/yangyutu/anaconda3/bin/python /snap/pycharm-community/132/helpers/pydev/pydevconsole.py --mode=client --port=38505
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel'])
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.4.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.4.0
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
[GCC 7.3.0] on linux
runfile('/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length120/DDPGHER_CNN.py', wdir='/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length120')
Backend Qt5Agg is interactive backend. Turning interactive mode on.
episode index:0
target Thresh 7.599999999999998
target distance 5.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.,  6.]), 'dynamicTrap': False, 'previousTarget': array([74.,  6.]), 'currentState': array([70.42085619,  3.1246059 ,  6.21471292]), 'targetState': array([74,  6], dtype=int32), 'currentDistance': 4.59109591177425}
done in step count: 99
reward sum = -0.10180805067378063
running average episode reward sum: -0.10180805067378063
{'scaleFactor': 20, 'currentTarget': array([74.,  6.]), 'dynamicTrap': False, 'previousTarget': array([74.,  6.]), 'currentState': array([63.69760194,  5.26802813,  1.51814221]), 'targetState': array([74,  6], dtype=int32), 'currentDistance': 10.328368147369288}
episode index:1
target Thresh 7.941146423220525
target distance 4.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.23904631, 12.48258954]), 'dynamicTrap': True, 'previousTarget': array([89., 14.]), 'currentState': array([85.      , 14.      ,  2.603752], dtype=float32), 'targetState': array([89, 14], dtype=int32), 'currentDistance': 5.454368956571117}
done in step count: 99
reward sum = -0.10980502473704444
running average episode reward sum: -0.10580653770541254
{'scaleFactor': 20, 'currentTarget': array([89., 14.]), 'dynamicTrap': False, 'previousTarget': array([89., 14.]), 'currentState': array([90.542576  , 23.77869154,  0.09654065]), 'targetState': array([89, 14], dtype=int32), 'currentDistance': 9.899613577672117}
episode index:2
target Thresh 8.280591371556902
target distance 5.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.,  3.]), 'dynamicTrap': False, 'previousTarget': array([40.,  3.]), 'currentState': array([45.32059659,  3.36661679,  5.00385737]), 'targetState': array([40,  3], dtype=int32), 'currentDistance': 5.333212530169999}
done in step count: 79
reward sum = 0.41913079984886187
running average episode reward sum: 0.06917257481267892
{'scaleFactor': 20, 'currentTarget': array([40.,  3.]), 'dynamicTrap': False, 'previousTarget': array([40.,  3.]), 'currentState': array([39.17325893,  2.9448224 ,  5.53225742]), 'targetState': array([40,  3], dtype=int32), 'currentDistance': 0.8285803349315604}
episode index:3
target Thresh 8.618343331150516
target distance 7.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.84149492,  1.90148438]), 'dynamicTrap': True, 'previousTarget': array([56.,  2.]), 'currentState': array([63.        ,  5.        ,  0.47102737], dtype=float32), 'targetState': array([56,  2], dtype=int32), 'currentDistance': 7.800320118809187}
done in step count: 30
reward sum = 0.6339980212373394
running average episode reward sum: 0.21037893641884403
{'scaleFactor': 20, 'currentTarget': array([56.,  2.]), 'dynamicTrap': False, 'previousTarget': array([56.,  2.]), 'currentState': array([55.8574825 ,  1.96603992,  3.73033741]), 'targetState': array([56,  2], dtype=int32), 'currentDistance': 0.1465077649651127}
episode index:4
target Thresh 8.954410745817937
target distance 7.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  11.]), 'dynamicTrap': False, 'previousTarget': array([110.,  11.]), 'currentState': array([107.90557241,  17.42111772,   6.14908516]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 6.754063942663185}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16830314913507521
{'scaleFactor': 20, 'currentTarget': array([110.,  11.]), 'dynamicTrap': False, 'previousTarget': array([110.,  11.]), 'currentState': array([114.80910218,  10.22531234,   5.26335993]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 4.871098927051706}
episode index:5
target Thresh 9.28880201726205
target distance 1.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([105.05252266,  20.0262506 ,   3.52996337]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 2.177350655700134}
done in step count: 46
reward sum = 0.51444873414297
running average episode reward sum: 0.22599407996972434
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([107.50297965,  21.33436841,   6.13880692]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 0.603979105728356}
episode index:6
target Thresh 9.621525505282044
target distance 9.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  13.]), 'dynamicTrap': False, 'previousTarget': array([111.,  13.]), 'currentState': array([114.49121863,  22.74563185,   3.69260001]), 'targetState': array([111,  13], dtype=int32), 'currentDistance': 10.352098707295282}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.31781575608390344
{'scaleFactor': 20, 'currentTarget': array([111.,  13.]), 'dynamicTrap': False, 'previousTarget': array([111.,  13.]), 'currentState': array([110.97202695,  12.70711329,   4.061809  ]), 'targetState': array([111,  13], dtype=int32), 'currentDistance': 0.29421950722558}
episode index:7
target Thresh 9.952589527982456
target distance 1.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([108.79919068,   6.60734287,   0.88601917]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 1.841539226203633}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.3981632878234155
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([106.8135119 ,   7.65695923,   3.96261215]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 0.6829152527402849}
episode index:8
target Thresh 10.282002361981094
target distance 7.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([110.44031957,  12.27014396,   4.13683224]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 6.29366475621402}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.46173392250970263
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([107.90742154,   7.22461704,   3.6723645 ]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 0.9348083582902876}
episode index:9
target Thresh 10.60977224261596
target distance 5.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 18.]), 'dynamicTrap': False, 'previousTarget': array([91., 18.]), 'currentState': array([87.298944  , 14.63441358,  2.34606895]), 'targetState': array([91, 18], dtype=int32), 'currentDistance': 5.002498126976505}
done in step count: 26
reward sum = 0.7252717110904925
running average episode reward sum: 0.4880877013677816
{'scaleFactor': 20, 'currentTarget': array([91., 18.]), 'dynamicTrap': False, 'previousTarget': array([91., 18.]), 'currentState': array([91.46925618, 17.77098218,  5.05217097]), 'targetState': array([91, 18], dtype=int32), 'currentDistance': 0.5221594804997621}
episode index:10
target Thresh 10.935907364151157
target distance 5.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([11.9606481 , 17.34472408,  5.88541484]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 6.163437163480803}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5267631146510415
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([15.89786635, 21.44910053,  4.19491366]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.5602869886193078}
episode index:11
target Thresh 11.260415879981709
target distance 4.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([51.68524412,  5.05062128,  1.4574008 ]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 3.988655268049173}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5597615796324481
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([49.89383743,  8.48707464,  3.72839964]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 1.017932730277514}
episode index:12
target Thresh 11.583305902837385
target distance 6.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.,  3.]), 'dynamicTrap': False, 'previousTarget': array([69.,  3.]), 'currentState': array([67.66160584,  9.84696887,  3.58738637]), 'targetState': array([69,  3], dtype=int32), 'currentDistance': 6.976552279060402}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.590594997353029
{'scaleFactor': 20, 'currentTarget': array([69.,  3.]), 'dynamicTrap': False, 'previousTarget': array([69.,  3.]), 'currentState': array([69.04233779,  3.46199253,  6.0026809 ]), 'targetState': array([69,  3], dtype=int32), 'currentDistance': 0.46392842976551324}
episode index:13
target Thresh 11.904585504985604
target distance 8.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'dynamicTrap': False, 'previousTarget': array([13.,  4.]), 'currentState': array([ 6.94626791, 11.11881752,  4.76919222]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 9.344797223608946}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6117228455218219
{'scaleFactor': 20, 'currentTarget': array([13.,  4.]), 'dynamicTrap': False, 'previousTarget': array([13.,  4.]), 'currentState': array([13.43674725,  3.60871798,  4.58310347]), 'targetState': array([13,  4], dtype=int32), 'currentDistance': 0.5863870545811074}
episode index:14
target Thresh 12.224262718433135
target distance 3.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.93174592, 21.81213986]), 'dynamicTrap': True, 'previousTarget': array([66., 22.]), 'currentState': array([64.       , 19.       ,  1.2829132], dtype=float32), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 3.4117111384722745}
done in step count: 3
reward sum = 0.960299
running average episode reward sum: 0.6349612558203671
{'scaleFactor': 20, 'currentTarget': array([66., 22.]), 'dynamicTrap': False, 'previousTarget': array([66., 22.]), 'currentState': array([66.72765372, 22.38022069,  1.19333088]), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 0.821004088000648}
episode index:15
target Thresh 12.542345535126984
target distance 5.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([9.623591  , 5.96844533, 3.53845274]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 4.724482561765692}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6559198648315943
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([6.67472346, 9.65621862, 2.96663155]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 0.9412091257018225}
episode index:16
target Thresh 12.858841907154112
target distance 6.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([111.51656273,  17.09399045,   3.49279857]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 7.508741776319572}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6684390382396757
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([106.96101112,  11.95830134,   3.68298995]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 0.9619153531783611}
episode index:17
target Thresh 13.173759746940313
target distance 13.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 12.]), 'dynamicTrap': False, 'previousTarget': array([70., 12.]), 'currentState': array([55.7254329 , 20.90725095,  2.9467622 ]), 'targetState': array([70, 12], dtype=int32), 'currentDistance': 16.825646652622968}
done in step count: 40
reward sum = 0.5886522239553815
running average episode reward sum: 0.6640064374461038
{'scaleFactor': 20, 'currentTarget': array([70., 12.]), 'dynamicTrap': False, 'previousTarget': array([70., 12.]), 'currentState': array([70.13154009, 11.77864208,  0.52990233]), 'targetState': array([70, 12], dtype=int32), 'currentDistance': 0.25749199312016696}
episode index:18
target Thresh 13.487106927447993
target distance 10.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  12.]), 'dynamicTrap': False, 'previousTarget': array([107.,  12.]), 'currentState': array([104.65043634,   2.85862688,   6.1022715 ]), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 9.438493098805452}
done in step count: 22
reward sum = 0.748091003433632
running average episode reward sum: 0.6684319409191316
{'scaleFactor': 20, 'currentTarget': array([107.,  12.]), 'dynamicTrap': False, 'previousTarget': array([107.,  12.]), 'currentState': array([106.1918547 ,  11.56644945,   1.30513336]), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 0.9170959099389397}
episode index:19
target Thresh 13.798891282372969
target distance 6.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  19.]), 'dynamicTrap': False, 'previousTarget': array([115.,  19.]), 'currentState': array([118.1693785 ,  14.85432705,   0.82301605]), 'targetState': array([115,  19], dtype=int32), 'currentDistance': 5.218387131668268}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.682559846368175
{'scaleFactor': 20, 'currentTarget': array([115.,  19.]), 'dynamicTrap': False, 'previousTarget': array([115.,  19.]), 'currentState': array([115.04646111,  19.56837392,   3.44971061]), 'targetState': array([115,  19], dtype=int32), 'currentDistance': 0.5702697159166082}
episode index:20
target Thresh 14.10912060634037
target distance 13.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.83392007,  2.60652336]), 'dynamicTrap': True, 'previousTarget': array([92.,  2.]), 'currentState': array([105.       ,  15.       ,   2.7804172], dtype=float32), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 17.366973375069083}
done in step count: 29
reward sum = 0.6211011956906949
running average episode reward sum: 0.6796332439549617
{'scaleFactor': 20, 'currentTarget': array([92.,  2.]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([91.99692319,  1.30498217,  5.2115683 ]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 0.6950246433762854}
episode index:21
target Thresh 14.417802655099432
target distance 12.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([42.13577403,  4.1114802 ,  0.78573666]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 12.743531597826916}
done in step count: 29
reward sum = 0.6928634869554239
running average episode reward sum: 0.6802346186368009
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([2.98313808e+01, 7.41452140e+00, 7.87036022e-03]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 0.6092763051439593}
episode index:22
target Thresh 14.72494514571747
target distance 13.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'dynamicTrap': False, 'previousTarget': array([3., 6.]), 'currentState': array([ 6.15140584, 17.27300865,  4.74004877]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 11.705216053521282}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6888122883916777
{'scaleFactor': 20, 'currentTarget': array([3., 6.]), 'dynamicTrap': False, 'previousTarget': array([3., 6.]), 'currentState': array([2.59576259, 6.47891653, 4.32491207]), 'targetState': array([3, 6], dtype=int32), 'currentDistance': 0.6267127999164466}
episode index:23
target Thresh 15.030555756772735
target distance 15.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.84208401, 21.12218592]), 'dynamicTrap': True, 'previousTarget': array([57., 21.]), 'currentState': array([42.      , 20.      ,  5.525663], dtype=float32), 'targetState': array([57, 21], dtype=int32), 'currentDistance': 14.884446885714215}
done in step count: 16
reward sum = 0.8315577710948755
running average episode reward sum: 0.6947600168376443
{'scaleFactor': 20, 'currentTarget': array([57., 21.]), 'dynamicTrap': False, 'previousTarget': array([57., 21.]), 'currentState': array([57.24715569, 21.60812663,  3.58284757]), 'targetState': array([57, 21], dtype=int32), 'currentDistance': 0.6564327273625128}
episode index:24
target Thresh 15.334642128546424
target distance 7.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  14.]), 'dynamicTrap': False, 'previousTarget': array([118.,  14.]), 'currentState': array([115.30074487,   7.14440155,   1.45763892]), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 7.367849644581635}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7046288221401785
{'scaleFactor': 20, 'currentTarget': array([118.,  14.]), 'dynamicTrap': False, 'previousTarget': array([118.,  14.]), 'currentState': array([118.7804409 ,  13.18895356,   1.66985929]), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 1.1255595548188821}
episode index:25
target Thresh 15.637211863213668
target distance 2.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 14.]), 'dynamicTrap': False, 'previousTarget': array([48., 14.]), 'currentState': array([49.65331933, 14.35312399,  3.49491024]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 1.6906097633520962}
done in step count: 19
reward sum = 0.8074312763923075
running average episode reward sum: 0.7085827626883373
{'scaleFactor': 20, 'currentTarget': array([48., 14.]), 'dynamicTrap': False, 'previousTarget': array([48., 14.]), 'currentState': array([47.12602729, 14.30060528,  3.89219641]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 0.9242249881812188}
episode index:26
target Thresh 15.938272525033607
target distance 11.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  22.]), 'dynamicTrap': False, 'previousTarget': array([117.,  22.]), 'currentState': array([107.04952048,  21.49134056,   0.21277094]), 'targetState': array([117,  22], dtype=int32), 'currentDistance': 9.963472136413335}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.7114381470631107
{'scaleFactor': 20, 'currentTarget': array([117.,  22.]), 'dynamicTrap': False, 'previousTarget': array([117.,  22.]), 'currentState': array([117.84219502,  21.34152497,   1.03346461]), 'targetState': array([117,  22], dtype=int32), 'currentDistance': 1.0690565018927178}
episode index:27
target Thresh 16.237831640538445
target distance 7.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,   6.]), 'dynamicTrap': False, 'previousTarget': array([109.,   6.]), 'currentState': array([1.03759416e+02, 1.19818209e+01, 8.00477823e-02]), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 7.952729427319876}
done in step count: 19
reward sum = 0.7785105733122375
running average episode reward sum: 0.7138335908577224
{'scaleFactor': 20, 'currentTarget': array([109.,   6.]), 'dynamicTrap': False, 'previousTarget': array([109.,   6.]), 'currentState': array([108.39243859,   5.45178675,   5.23599046]), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 0.8183328359956973}
episode index:28
target Thresh 16.535896698721682
target distance 9.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  10.]), 'dynamicTrap': False, 'previousTarget': array([107.,  10.]), 'currentState': array([115.0774095 ,  19.09109147,   5.01065481]), 'targetState': array([107,  10], dtype=int32), 'currentDistance': 12.161105556073966}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.715506319253908
{'scaleFactor': 20, 'currentTarget': array([107.,  10.]), 'dynamicTrap': False, 'previousTarget': array([107.,  10.]), 'currentState': array([107.80255904,  10.91287792,   3.80807155]), 'targetState': array([107,  10], dtype=int32), 'currentDistance': 1.2155028217911197}
episode index:29
target Thresh 16.832475151225292
target distance 8.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([10.24850663,  1.49458817,  1.60206002]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 10.43417726117998}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7221066835282323
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([3.28237645, 9.46783211, 4.54693818]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.5464460998028634}
episode index:30
target Thresh 17.127574412526048
target distance 10.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.67971747,  8.13334975]), 'dynamicTrap': True, 'previousTarget': array([98.,  7.]), 'currentState': array([93.        , 17.        ,  0.18296929], dtype=float32), 'targetState': array([98,  7], dtype=int32), 'currentDistance': 9.599885792192305}
done in step count: 9
reward sum = 0.8741132574836408
running average episode reward sum: 0.7270101213977616
{'scaleFactor': 20, 'currentTarget': array([98.,  7.]), 'dynamicTrap': False, 'previousTarget': array([98.,  7.]), 'currentState': array([9.75018028e+01, 7.62450072e+00, 6.42718941e-02]), 'targetState': array([98,  7], dtype=int32), 'currentDistance': 0.798875183562044}
episode index:31
target Thresh 17.42120186012084
target distance 17.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   6.]), 'dynamicTrap': False, 'previousTarget': array([106.,   6.]), 'currentState': array([110.92286903,  22.80473289,   5.34619832]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 17.51095905801076}
done in step count: 24
reward sum = 0.7692373851928906
running average episode reward sum: 0.7283297233913594
{'scaleFactor': 20, 'currentTarget': array([106.,   6.]), 'dynamicTrap': False, 'previousTarget': array([106.,   6.]), 'currentState': array([105.48471344,   6.32334772,   4.93372014]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 0.6083370667313329}
episode index:32
target Thresh 17.71336483471114
target distance 16.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  23.]), 'dynamicTrap': False, 'previousTarget': array([110.,  23.]), 'currentState': array([103.67607414,   6.84780425,   5.18262875]), 'targetState': array([110,  23], dtype=int32), 'currentDistance': 17.346050437469053}
done in step count: 32
reward sum = 0.6955763459578533
running average episode reward sum: 0.7273371968024652
{'scaleFactor': 20, 'currentTarget': array([110.,  23.]), 'dynamicTrap': False, 'previousTarget': array([110.,  23.]), 'currentState': array([110.35028337,  22.34133502,   1.35773201]), 'targetState': array([110,  23], dtype=int32), 'currentDistance': 0.7460147410627714}
episode index:33
target Thresh 18.00407064038655
target distance 15.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,   3.]), 'dynamicTrap': False, 'previousTarget': array([112.,   3.]), 'currentState': array([115.49812845,  16.97947891,   4.96194613]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 14.410507740340572}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7320150695940436
{'scaleFactor': 20, 'currentTarget': array([112.,   3.]), 'dynamicTrap': False, 'previousTarget': array([112.,   3.]), 'currentState': array([111.73576849,   3.87803233,   4.10389453]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 0.9169291489862978}
episode index:34
target Thresh 18.293326544807353
target distance 7.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.,  3.]), 'dynamicTrap': False, 'previousTarget': array([84.,  3.]), 'currentState': array([89.7994029 ,  5.82061258,  4.92807937]), 'targetState': array([84,  3], dtype=int32), 'currentDistance': 6.448947924300746}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7379997861599565
{'scaleFactor': 20, 'currentTarget': array([84.,  3.]), 'dynamicTrap': False, 'previousTarget': array([84.,  3.]), 'currentState': array([83.97622601,  3.59029551,  3.83747269]), 'targetState': array([84,  3], dtype=int32), 'currentDistance': 0.590774063406159}
episode index:35
target Thresh 18.58113977938622
target distance 4.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  8.]), 'dynamicTrap': False, 'previousTarget': array([35.,  8.]), 'currentState': array([35.40503946,  5.61756585,  2.33544067]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 2.41661942444451}
done in step count: 33
reward sum = 0.5964835872561332
running average episode reward sum: 0.7340687806348504
{'scaleFactor': 20, 'currentTarget': array([35.,  8.]), 'dynamicTrap': False, 'previousTarget': array([35.,  8.]), 'currentState': array([34.54949485,  8.15033003,  1.34998564]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 0.4749252648698803}
episode index:36
target Thresh 18.867517539468995
target distance 9.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,   5.]), 'dynamicTrap': False, 'previousTarget': array([112.,   5.]), 'currentState': array([115.74359672,  14.3181338 ,   3.18055725]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 10.042018417715758}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7381854317451552
{'scaleFactor': 20, 'currentTarget': array([112.,   5.]), 'dynamicTrap': False, 'previousTarget': array([112.,   5.]), 'currentState': array([111.78580064,   5.58403504,   2.42626583]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 0.6220757931166674}
episode index:37
target Thresh 19.152466984514604
target distance 18.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 22.]), 'dynamicTrap': False, 'previousTarget': array([93., 22.]), 'currentState': array([110.17497716,  21.53312393,   3.19004869]), 'targetState': array([93, 22], dtype=int32), 'currentDistance': 17.181321656240073}
done in step count: 44
reward sum = 0.5769254844174826
running average episode reward sum: 0.7339417489207427
{'scaleFactor': 20, 'currentTarget': array([93., 22.]), 'dynamicTrap': False, 'previousTarget': array([93., 22.]), 'currentState': array([92.90704096, 21.92080112,  5.26284087]), 'targetState': array([93, 22], dtype=int32), 'currentDistance': 0.12212225451193125}
episode index:38
target Thresh 19.43599523827402
target distance 3.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 16.]), 'dynamicTrap': False, 'previousTarget': array([93., 16.]), 'currentState': array([93.63625693, 18.62531311,  4.95181012]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 2.701312977177618}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7400021912561083
{'scaleFactor': 20, 'currentTarget': array([93., 16.]), 'dynamicTrap': False, 'previousTarget': array([93., 16.]), 'currentState': array([93.86139337, 16.75419208,  5.63452721]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 1.1449035926071083}
episode index:39
target Thresh 19.718109388968344
target distance 18.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.58959537, 21.7732716 ]), 'dynamicTrap': False, 'previousTarget': array([89.48314552, 21.71285862]), 'currentState': array([72.08342569, 12.10197267,  0.40409994]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6416795931467895
running average episode reward sum: 0.7375441263033753
{'scaleFactor': 20, 'currentTarget': array([90., 22.]), 'dynamicTrap': False, 'previousTarget': array([90., 22.]), 'currentState': array([90.07938155, 22.69841163,  1.57607591]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 0.7029084165268262}
episode index:40
target Thresh 19.998816489466044
target distance 3.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  12.]), 'dynamicTrap': False, 'previousTarget': array([118.,  12.]), 'currentState': array([116.50368376,  15.23193886,   0.27024949]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 3.561515273856768}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7429844161496345
{'scaleFactor': 20, 'currentTarget': array([118.,  12.]), 'dynamicTrap': False, 'previousTarget': array([118.,  12.]), 'currentState': array([117.76240389,  12.4758727 ,   3.57350701]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 0.5318897802484714}
episode index:41
target Thresh 20.278123557459246
target distance 14.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78., 19.]), 'dynamicTrap': False, 'previousTarget': array([78., 19.]), 'currentState': array([65.1282456 , 22.68007297,  4.99687708]), 'targetState': array([78, 19], dtype=int32), 'currentDistance': 13.387494101113585}
done in step count: 18
reward sum = 0.7632268381170431
running average episode reward sum: 0.7434663785774299
{'scaleFactor': 20, 'currentTarget': array([78., 19.]), 'dynamicTrap': False, 'previousTarget': array([78., 19.]), 'currentState': array([78.08050636, 18.97530241,  1.49519962]), 'targetState': array([78, 19], dtype=int32), 'currentDistance': 0.08420953367195841}
episode index:42
target Thresh 20.556037575639206
target distance 7.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   8.]), 'dynamicTrap': False, 'previousTarget': array([118.,   8.]), 'currentState': array([112.56525865,   4.67498626,   5.10019833]), 'targetState': array([118,   8], dtype=int32), 'currentDistance': 6.371195329307889}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7480713499919315
{'scaleFactor': 20, 'currentTarget': array([118.,   8.]), 'dynamicTrap': False, 'previousTarget': array([118.,   8.]), 'currentState': array([118.70489543,   7.25916273,   0.88489604]), 'targetState': array([118,   8], dtype=int32), 'currentDistance': 1.0226032608485318}
episode index:43
target Thresh 20.832565491870838
target distance 9.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.,  8.]), 'dynamicTrap': False, 'previousTarget': array([69.,  8.]), 'currentState': array([77.64857517,  9.42572905,  3.27087355]), 'targetState': array([69,  8], dtype=int32), 'currentDistance': 8.765304093708762}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7512148391220269
{'scaleFactor': 20, 'currentTarget': array([69.,  8.]), 'dynamicTrap': False, 'previousTarget': array([69.,  8.]), 'currentState': array([69.71043903,  8.44697876,  2.08165568]), 'targetState': array([69,  8], dtype=int32), 'currentDistance': 0.8393530982062589}
episode index:44
target Thresh 21.107714219366468
target distance 5.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  19.]), 'dynamicTrap': False, 'previousTarget': array([118.,  19.]), 'currentState': array([113.96264754,  14.32088066,   0.56149783]), 'targetState': array([118,  19], dtype=int32), 'currentDistance': 6.1801596042691385}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7548215593078406
{'scaleFactor': 20, 'currentTarget': array([118.,  19.]), 'dynamicTrap': False, 'previousTarget': array([118.,  19.]), 'currentState': array([118.82986465,  19.26105028,   1.22935759]), 'targetState': array([118,  19], dtype=int32), 'currentDistance': 0.8699555087315167}
episode index:45
target Thresh 21.381490636858604
target distance 7.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'dynamicTrap': False, 'previousTarget': array([5., 6.]), 'currentState': array([11.43003651, 11.10171437,  3.29161894]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 8.208097167919172}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7572981735135175
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'dynamicTrap': False, 'previousTarget': array([5., 6.]), 'currentState': array([4.03787656, 5.90706623, 3.11498332]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 0.9666013681945833}
episode index:46
target Thresh 21.653901588771944
target distance 14.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 20.]), 'dynamicTrap': False, 'previousTarget': array([68., 20.]), 'currentState': array([72.4058521 ,  6.15893085,  1.38324231]), 'targetState': array([68, 20], dtype=int32), 'currentDistance': 14.525382198582772}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.759856106481293
{'scaleFactor': 20, 'currentTarget': array([68., 20.]), 'dynamicTrap': False, 'previousTarget': array([68., 20.]), 'currentState': array([68.67760323, 20.25232835,  3.63513898]), 'targetState': array([68, 20], dtype=int32), 'currentDistance': 0.7230599805064118}
episode index:47
target Thresh 21.924953885394487
target distance 11.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'dynamicTrap': False, 'previousTarget': array([5., 5.]), 'currentState': array([14.3935556 , 15.87031386,  3.69744992]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 14.366718838968037}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.7602303825837483
{'scaleFactor': 20, 'currentTarget': array([5., 5.]), 'dynamicTrap': False, 'previousTarget': array([5., 5.]), 'currentState': array([4.06792806, 5.53275677, 0.79001784]), 'targetState': array([5, 5], dtype=int32), 'currentDistance': 1.0735864547729916}
episode index:48
target Thresh 22.19465430304774
target distance 4.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.90489671,  9.85288202]), 'dynamicTrap': True, 'previousTarget': array([82., 10.]), 'currentState': array([82.        ,  6.        ,  0.95135725], dtype=float32), 'targetState': array([82, 10], dtype=int32), 'currentDistance': 3.8540555925467874}
done in step count: 17
reward sum = 0.7782717586692641
running average episode reward sum: 0.7605985739324324
{'scaleFactor': 20, 'currentTarget': array([82., 10.]), 'dynamicTrap': False, 'previousTarget': array([82., 10.]), 'currentState': array([82.07720081,  9.38848302,  1.23617275]), 'targetState': array([82, 10], dtype=int32), 'currentDistance': 0.6163708163835141}
episode index:49
target Thresh 22.46300958425622
target distance 22.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.4756456 , 18.58697755]), 'dynamicTrap': False, 'previousTarget': array([26.48906088, 18.57265691]), 'currentState': array([45.00304074, 11.05467185,  2.52523547]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5553395247384707
running average episode reward sum: 0.7564933929485531
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'dynamicTrap': False, 'previousTarget': array([23., 20.]), 'currentState': array([23.08327174, 19.65044904,  3.28075598]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.3593327930601134}
episode index:50
target Thresh 22.730026437915907
target distance 21.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.8203778,  6.142856 ]), 'dynamicTrap': False, 'previousTarget': array([86.09009055,  6.10381815]), 'currentState': array([105.52387616,   9.57391302,   0.85455394]), 'targetState': array([85,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.43387006116497745
running average episode reward sum: 0.7501674452665221
{'scaleFactor': 20, 'currentTarget': array([85.,  6.]), 'dynamicTrap': False, 'previousTarget': array([85.,  6.]), 'currentState': array([85.21760029,  5.79524153,  4.722386  ]), 'targetState': array([85,  6], dtype=int32), 'currentDistance': 0.2987907602697785}
episode index:51
target Thresh 22.995711539462064
target distance 8.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.03698941, 22.13126377]), 'dynamicTrap': True, 'previousTarget': array([27., 22.]), 'currentState': array([19.      , 23.      ,  4.338717], dtype=float32), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 8.083804887115559}
done in step count: 8
reward sum = 0.9127446944279201
running average episode reward sum: 0.7532939308273183
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'dynamicTrap': False, 'previousTarget': array([27., 22.]), 'currentState': array([27.22389606, 22.79267203,  6.0798166 ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.8236858621058656}
episode index:52
target Thresh 23.26007153103607
target distance 19.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.32937248, 19.71571613]), 'dynamicTrap': False, 'previousTarget': array([63.31492866, 19.69836445]), 'currentState': array([54.07190746,  1.98722659,  2.61930358]), 'targetState': array([64, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.2180736866089615
running average episode reward sum: 0.743195435653387
{'scaleFactor': 20, 'currentTarget': array([64., 21.]), 'dynamicTrap': False, 'previousTarget': array([64., 21.]), 'currentState': array([63.43310458, 20.29978379,  4.83470249]), 'targetState': array([64, 21], dtype=int32), 'currentDistance': 0.900929051795599}
episode index:53
target Thresh 23.52311302165147
target distance 21.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.03382242, 13.50582633]), 'dynamicTrap': False, 'previousTarget': array([82.99469707, 13.52709229]), 'currentState': array([100.04469899,   2.98774294,   6.06315948]), 'targetState': array([79, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6404481193229766
running average episode reward sum: 0.7412927075731942
{'scaleFactor': 20, 'currentTarget': array([79., 16.]), 'dynamicTrap': False, 'previousTarget': array([79., 16.]), 'currentState': array([79.99951909, 16.25498405,  3.12038352]), 'targetState': array([79, 16], dtype=int32), 'currentDistance': 1.0315305558793049}
episode index:54
target Thresh 23.784842587359247
target distance 9.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([26.28309999,  9.65898839,  0.39177823]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 10.65086837782147}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7436100367585721
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([34.08451174, 16.57555066,  3.39425537]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 0.43278109980317553}
episode index:55
target Thresh 24.04526677141216
target distance 23.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.3280829 , 16.29376702]), 'dynamicTrap': False, 'previousTarget': array([11.45647272, 16.24859289]), 'currentState': array([30.89243599, 12.14213023,  3.2286346 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.3291849214628402
running average episode reward sum: 0.7362095882711482
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.13825552, 16.20481912,  0.54967087]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 1.172568197707041}
episode index:56
target Thresh 24.30439208442838
target distance 9.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  12.]), 'dynamicTrap': False, 'previousTarget': array([118.,  12.]), 'currentState': array([108.84782783,  11.43895122,   6.05609608]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 9.169352826344547}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.739482133993197
{'scaleFactor': 20, 'currentTarget': array([118.,  12.]), 'dynamicTrap': False, 'previousTarget': array([118.,  12.]), 'currentState': array([118.01964459,  11.63022563,   1.72107201]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 0.37029582077186957}
episode index:57
target Thresh 24.56222500455422
target distance 6.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([ 7.85057697, 10.61810422,  2.40436402]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 5.913147728656738}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7421693084805334
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.4752574 , 13.11475882,  0.92938551]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 1.0290805297108208}
episode index:58
target Thresh 24.818771977626135
target distance 13.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.,  5.]), 'dynamicTrap': False, 'previousTarget': array([82.,  5.]), 'currentState': array([68.96948978, 11.94229043,  5.23605704]), 'targetState': array([82,  5], dtype=int32), 'currentDistance': 14.76447063411901}
done in step count: 34
reward sum = 0.6058053241034849
running average episode reward sum: 0.7398580545080411
{'scaleFactor': 20, 'currentTarget': array([83.95004199,  4.56676826]), 'dynamicTrap': True, 'previousTarget': array([83.75385961,  4.60531991]), 'currentState': array([83.65568935,  3.57774479,  3.14970326]), 'targetState': array([82,  5], dtype=int32), 'currentDistance': 1.0318967437344857}
episode index:59
target Thresh 25.074039417331797
target distance 14.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'dynamicTrap': False, 'previousTarget': array([18.,  6.]), 'currentState': array([28.12838047, 19.85246972,  4.31926346]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 17.160274135424796}
done in step count: 23
reward sum = 0.7065672879015997
running average episode reward sum: 0.7393032083979337
{'scaleFactor': 20, 'currentTarget': array([18.,  6.]), 'dynamicTrap': False, 'previousTarget': array([18.,  6.]), 'currentState': array([17.73891174,  6.6683775 ,  4.18569561]), 'targetState': array([18,  6], dtype=int32), 'currentDistance': 0.7175622343434725}
episode index:60
target Thresh 25.3280337053705
target distance 13.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([44.42946511,  9.90859287,  1.85258663]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 15.967963826776735}
done in step count: 18
reward sum = 0.7800139954679253
running average episode reward sum: 0.7399705983499009
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([34.52067697, 22.58122633,  4.06629509]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 0.7803387451264623}
episode index:61
target Thresh 25.580761191612655
target distance 20.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.14156445,  5.20340526]), 'dynamicTrap': True, 'previousTarget': array([76.1565257 ,  5.25304229]), 'currentState': array([57.        , 11.        ,  0.06233392], dtype=float32), 'targetState': array([77,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.31381927422602435
running average episode reward sum: 0.73309718989629
{'scaleFactor': 20, 'currentTarget': array([77.,  5.]), 'dynamicTrap': False, 'previousTarget': array([77.,  5.]), 'currentState': array([76.13094286,  5.63861478,  0.91929842]), 'targetState': array([77,  5], dtype=int32), 'currentDistance': 1.0784661061537584}
episode index:62
target Thresh 25.83222819425862
target distance 24.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.01175025,  4.68547209]), 'dynamicTrap': True, 'previousTarget': array([55.01733854,  4.83261089]), 'currentState': array([75.       ,  4.       ,  1.9203016], dtype=float32), 'targetState': array([51,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6855763459578534
running average episode reward sum: 0.7323428907861561
{'scaleFactor': 20, 'currentTarget': array([51.,  5.]), 'dynamicTrap': False, 'previousTarget': array([51.,  5.]), 'currentState': array([51.02357235,  5.56938021,  1.68282309]), 'targetState': array([51,  5], dtype=int32), 'currentDistance': 0.5698679533027325}
episode index:63
target Thresh 26.08244099999651
target distance 3.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63., 14.]), 'dynamicTrap': False, 'previousTarget': array([63., 14.]), 'currentState': array([65.96713867, 17.08077652,  1.99927789]), 'targetState': array([63, 14], dtype=int32), 'currentDistance': 4.277276684847843}
done in step count: 13
reward sum = 0.8395669999251314
running average episode reward sum: 0.7340182674914526
{'scaleFactor': 20, 'currentTarget': array([63., 14.]), 'dynamicTrap': False, 'previousTarget': array([63., 14.]), 'currentState': array([62.0648141 , 14.69263205,  0.204803  ]), 'targetState': array([63, 14], dtype=int32), 'currentDistance': 1.1637490331161802}
episode index:64
target Thresh 26.331405864159542
target distance 22.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.99200398, 12.22702627]), 'dynamicTrap': False, 'previousTarget': array([93.48906088, 12.42734309]), 'currentState': array([111.4963807 ,  19.81570483,   4.50239801]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.26583416081265676
running average episode reward sum: 0.7268154350810097
{'scaleFactor': 20, 'currentTarget': array([90., 11.]), 'dynamicTrap': False, 'previousTarget': array([90., 11.]), 'currentState': array([90.73958705, 11.51833488,  5.32355824]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 0.9031389963230663}
episode index:65
target Thresh 26.57912901088226
target distance 21.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.59221236,  5.7974565 ]), 'dynamicTrap': False, 'previousTarget': array([84.79898987,  5.82842712]), 'currentState': array([64.79604944,  2.94931091,  4.03071237]), 'targetState': array([86,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6723156148046894
running average episode reward sum: 0.7259896802283381
{'scaleFactor': 20, 'currentTarget': array([86.,  6.]), 'dynamicTrap': False, 'previousTarget': array([86.,  6.]), 'currentState': array([86.39953503,  6.04675509,  2.07602446]), 'targetState': array([86,  6], dtype=int32), 'currentDistance': 0.40226145829550125}
episode index:66
target Thresh 26.825616633256253
target distance 4.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 14.]), 'currentState': array([ 3.92933425, 11.25627448,  5.53731465]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 4.909006967329255}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7294912672398555
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.97604897, 14.93018474,  0.50570744]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9304930381247779}
episode index:67
target Thresh 27.0708748934849
target distance 5.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([ 7.01395227, 13.23497089,  2.52148724]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 5.638209887640514}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7314113714663472
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([ 3.64025321, 17.98530795,  5.08925456]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.360046680529853}
episode index:68
target Thresh 27.314909923037497
target distance 8.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  17.]), 'dynamicTrap': False, 'previousTarget': array([108.,  17.]), 'currentState': array([111.44835222,   9.65749138,   1.94137192]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 8.111939711053983}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7336573642235904
{'scaleFactor': 20, 'currentTarget': array([108.,  17.]), 'dynamicTrap': False, 'previousTarget': array([108.,  17.]), 'currentState': array([108.89968907,  17.99856922,   4.83890134]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 1.3440911094337802}
episode index:69
target Thresh 27.55772782280248
target distance 22.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.51370636, 18.052365  ]), 'dynamicTrap': False, 'previousTarget': array([ 8.18339664, 18.29773591]), 'currentState': array([26.41060038, 20.08056818,  2.94895911]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7626206394390459
running average episode reward sum: 0.7340711252980969
{'scaleFactor': 20, 'currentTarget': array([ 6., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 18.]), 'currentState': array([ 6.82197798, 18.76276248,  3.11269727]), 'targetState': array([ 6, 18], dtype=int32), 'currentDistance': 1.1213627398684267}
episode index:70
target Thresh 27.799334663240003
target distance 4.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 15.]), 'dynamicTrap': False, 'previousTarget': array([49., 15.]), 'currentState': array([48.02507978, 12.45411042,  2.56355053]), 'targetState': array([49, 15], dtype=int32), 'currentDistance': 2.7261737294125337}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7375363207164336
{'scaleFactor': 20, 'currentTarget': array([49., 15.]), 'dynamicTrap': False, 'previousTarget': array([49., 15.]), 'currentState': array([48.7038753 , 14.62147789,  5.75533748]), 'targetState': array([49, 15], dtype=int32), 'currentDistance': 0.4805921585602066}
episode index:71
target Thresh 28.039736484533645
target distance 10.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  4.]), 'dynamicTrap': False, 'previousTarget': array([34.,  4.]), 'currentState': array([25.68556023,  5.96528676,  5.47160852]), 'targetState': array([34,  4], dtype=int32), 'currentDistance': 8.543550831291963}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7405009558439831
{'scaleFactor': 20, 'currentTarget': array([34.,  4.]), 'dynamicTrap': False, 'previousTarget': array([34.,  4.]), 'currentState': array([3.33485336e+01, 3.83890712e+00, 6.95281824e-03]), 'targetState': array([34,  4], dtype=int32), 'currentDistance': 0.6710881779886065}
episode index:72
target Thresh 28.278939296741473
target distance 7.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.84557662, 15.12657123]), 'dynamicTrap': True, 'previousTarget': array([85., 15.]), 'currentState': array([78.       , 22.       ,  5.6955843], dtype=float32), 'targetState': array([85, 15], dtype=int32), 'currentDistance': 9.70082173125968}
done in step count: 12
reward sum = 0.8481222522970137
running average episode reward sum: 0.7419752201789562
{'scaleFactor': 20, 'currentTarget': array([85., 15.]), 'dynamicTrap': False, 'previousTarget': array([85., 15.]), 'currentState': array([84.33457341, 15.94919794,  1.63825873]), 'targetState': array([85, 15], dtype=int32), 'currentDistance': 1.159210631079073}
episode index:73
target Thresh 28.51694907994624
target distance 9.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.02316028, 21.8759134 ]), 'dynamicTrap': True, 'previousTarget': array([26., 22.]), 'currentState': array([19.      , 13.      ,  1.846844], dtype=float32), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 11.318419452379041}
done in step count: 47
reward sum = 0.45311888634760505
running average episode reward sum: 0.7380717562082623
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'dynamicTrap': False, 'previousTarget': array([26., 22.]), 'currentState': array([26.2323109 , 22.28735134,  5.17181007]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.36951204321733067}
episode index:74
target Thresh 28.75377178440494
target distance 23.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.58215009, 10.26436868]), 'dynamicTrap': False, 'previousTarget': array([56.58076497, 10.26237106]), 'currentState': array([72.000533  , 23.00302907,  2.04841161]), 'targetState': array([49,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6612043200001896
running average episode reward sum: 0.7370468570588212
{'scaleFactor': 20, 'currentTarget': array([49.,  4.]), 'dynamicTrap': False, 'previousTarget': array([49.,  4.]), 'currentState': array([49.92246029,  4.65688543,  0.21347505]), 'targetState': array([49,  4], dtype=int32), 'currentDistance': 1.1324448983871285}
episode index:75
target Thresh 28.9894133306975
target distance 18.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.,  5.]), 'dynamicTrap': False, 'previousTarget': array([82.,  5.]), 'currentState': array([63.52942318,  5.42669391,  5.03508568]), 'targetState': array([82,  5], dtype=int32), 'currentDistance': 18.47550475411711}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7386654293954326
{'scaleFactor': 20, 'currentTarget': array([82.,  5.]), 'dynamicTrap': False, 'previousTarget': array([82.,  5.]), 'currentState': array([81.06426038,  4.71248714,  0.41182685]), 'targetState': array([82,  5], dtype=int32), 'currentDistance': 0.9789138243948633}
episode index:76
target Thresh 29.223879609874857
target distance 12.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.00891078, 15.99114374]), 'dynamicTrap': True, 'previousTarget': array([90., 16.]), 'currentState': array([78.       ,  8.       ,  2.3776395], dtype=float32), 'targetState': array([90, 16], dtype=int32), 'currentDistance': 14.42471200404451}
done in step count: 36
reward sum = 0.5232122565378071
running average episode reward sum: 0.7358673362414375
{'scaleFactor': 20, 'currentTarget': array([90., 16.]), 'dynamicTrap': False, 'previousTarget': array([90., 16.]), 'currentState': array([89.3678066 , 16.37982235,  5.5703187 ]), 'targetState': array([90, 16], dtype=int32), 'currentDistance': 0.7375184858127785}
episode index:77
target Thresh 29.457176483606197
target distance 17.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55., 16.]), 'dynamicTrap': False, 'previousTarget': array([55., 16.]), 'currentState': array([38.19123386, 20.94222271,  5.90611362]), 'targetState': array([55, 16], dtype=int32), 'currentDistance': 17.520279232342187}
done in step count: 43
reward sum = 0.5525995499878741
running average episode reward sum: 0.7335177492381867
{'scaleFactor': 20, 'currentTarget': array([55.14961705, 14.3236937 ]), 'dynamicTrap': True, 'previousTarget': array([55., 16.]), 'currentState': array([55.24227521, 14.1338923 ,  0.66981445]), 'targetState': array([55, 16], dtype=int32), 'currentDistance': 0.21121104128433657}
episode index:78
target Thresh 29.689309784325534
target distance 27.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.17823821,  11.83948211]), 'dynamicTrap': False, 'previousTarget': array([106.75509063,  12.79365671]), 'currentState': array([90.224621  , 20.65240608,  4.69935799]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6590634109666579
running average episode reward sum: 0.7325752892600661
{'scaleFactor': 20, 'currentTarget': array([116.,   8.]), 'dynamicTrap': False, 'previousTarget': array([116.,   8.]), 'currentState': array([116.97023176,   7.26692941,   1.22026704]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 1.2160354267032956}
episode index:79
target Thresh 29.92028531537745
target distance 2.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 20.]), 'dynamicTrap': False, 'previousTarget': array([44., 20.]), 'currentState': array([47.24663575, 18.99333107,  6.16715884]), 'targetState': array([44, 20], dtype=int32), 'currentDistance': 3.3991213581723523}
done in step count: 6
reward sum = 0.93206534790699
running average episode reward sum: 0.7350689149931526
{'scaleFactor': 20, 'currentTarget': array([45.51637405, 18.69618975]), 'dynamicTrap': True, 'previousTarget': array([44., 20.]), 'currentState': array([45.96466822, 19.60282228,  2.40892505]), 'targetState': array([44, 20], dtype=int32), 'currentDistance': 1.0114100036058347}
episode index:80
target Thresh 30.150108851162273
target distance 15.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([15.67546914, 11.1587175 ,  5.36763364]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 13.325476122141604}
done in step count: 15
reward sum = 0.8029710917431029
running average episode reward sum: 0.7359072134715471
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([28.92653691, 11.4914728 ,  6.09557544]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.49693293024862073}
episode index:81
target Thresh 30.37878613728035
target distance 4.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.,  2.]), 'dynamicTrap': False, 'previousTarget': array([84.,  2.]), 'currentState': array([86.98399851,  5.71883057,  4.29491746]), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 4.768012992329837}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.738299385842711
{'scaleFactor': 20, 'currentTarget': array([84.,  2.]), 'dynamicTrap': False, 'previousTarget': array([84.,  2.]), 'currentState': array([83.40246651,  1.22078565,  0.13987821]), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 0.9819476966168355}
episode index:82
target Thresh 30.606322890675756
target distance 23.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.5016154 , 21.26123624]), 'dynamicTrap': False, 'previousTarget': array([23.92481176, 21.26740767]), 'currentState': array([ 3.55714443, 22.75055749,  4.10194063]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6149045562516543
running average episode reward sum: 0.7368127011488428
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'dynamicTrap': False, 'previousTarget': array([27., 21.]), 'currentState': array([26.02972703, 21.35923994,  5.02751517]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.0346414638078856}
episode index:83
target Thresh 30.832724799779175
target distance 18.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 15.]), 'currentState': array([19.62808979, 11.64395275,  1.08453941]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 17.9447096010748}
done in step count: 33
reward sum = 0.6113286523495909
running average episode reward sum: 0.7353188434250422
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.7774679 , 14.32706968,  4.11443014]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 1.0282468316837627}
episode index:84
target Thresh 31.05799752465012
target distance 8.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   8.]), 'dynamicTrap': False, 'previousTarget': array([118.,   8.]), 'currentState': array([110.44737249,  14.58348994,   4.00830221]), 'targetState': array([118,   8], dtype=int32), 'currentDistance': 10.019207657136537}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7373078226201452
{'scaleFactor': 20, 'currentTarget': array([118.,   8.]), 'dynamicTrap': False, 'previousTarget': array([118.,   8.]), 'currentState': array([118.37933514,   8.34999135,   0.7954337 ]), 'targetState': array([118,   8], dtype=int32), 'currentDistance': 0.5161289465054147}
episode index:85
target Thresh 31.282146697118446
target distance 17.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 18.]), 'dynamicTrap': False, 'previousTarget': array([24., 18.]), 'currentState': array([39.46579303, 12.52168424,  2.63193154]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 16.407397652076327}
done in step count: 23
reward sum = 0.7024289407316748
running average episode reward sum: 0.7369022542260931
{'scaleFactor': 20, 'currentTarget': array([24.39762047, 16.04906661]), 'dynamicTrap': True, 'previousTarget': array([24., 18.]), 'currentState': array([23.83786239, 15.85975429,  1.93427561]), 'targetState': array([24, 18], dtype=int32), 'currentDistance': 0.590904616488923}
episode index:86
target Thresh 31.505177920925146
target distance 11.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   4.]), 'dynamicTrap': False, 'previousTarget': array([118.,   4.]), 'currentState': array([106.83675861,   8.05836061,   5.48363388]), 'targetState': array([118,   4], dtype=int32), 'currentDistance': 11.878057465767542}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7391455081764483
{'scaleFactor': 20, 'currentTarget': array([118.,   4.]), 'dynamicTrap': False, 'previousTarget': array([118.,   4.]), 'currentState': array([117.00358372,   4.21449448,   6.2246888 ]), 'targetState': array([118,   4], dtype=int32), 'currentDistance': 1.0192415271685242}
episode index:87
target Thresh 31.72709677186243
target distance 19.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 10.]), 'currentState': array([26.2060843 ,  6.41482505,  3.29920232]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 19.537839015385767}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.7393224829381208
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 10.]), 'currentState': array([7.16523568, 9.0515427 , 1.13788634]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.962742992884844}
episode index:88
target Thresh 31.94790879791313
target distance 26.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.523226  , 12.81405327]), 'dynamicTrap': False, 'previousTarget': array([70.11558017, 13.11828302]), 'currentState': array([51.26437536, 18.20818638,  5.3728106 ]), 'targetState': array([77, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6085040475079411
running average episode reward sum: 0.7378526128771075
{'scaleFactor': 20, 'currentTarget': array([75.11882644, 11.2366134 ]), 'dynamicTrap': True, 'previousTarget': array([77., 11.]), 'currentState': array([74.70873703, 10.82482949,  5.68180337]), 'targetState': array([77, 11], dtype=int32), 'currentDistance': 0.5811534290004701}
episode index:89
target Thresh 32.167619519389405
target distance 24.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.3240895 , 22.50761311]), 'dynamicTrap': False, 'previousTarget': array([11.15444247, 22.48069469]), 'currentState': array([31.19567176, 20.24482307,  1.90652054]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7527367542471035
running average episode reward sum: 0.738017992225663
{'scaleFactor': 20, 'currentTarget': array([ 7., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 23.]), 'currentState': array([ 7.00738049, 22.90586823,  2.50916935]), 'targetState': array([ 7, 23], dtype=int32), 'currentDistance': 0.0944206652142581}
episode index:90
target Thresh 32.386234429070704
target distance 13.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.19993936,  1.99840271]), 'dynamicTrap': True, 'previousTarget': array([65.,  2.]), 'currentState': array([78.      ,  8.      ,  3.093656], dtype=float32), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 14.137210563385615}
done in step count: 29
reward sum = 0.5986298654264718
running average episode reward sum: 0.7364862545685291
{'scaleFactor': 20, 'currentTarget': array([65.,  2.]), 'dynamicTrap': False, 'previousTarget': array([65.,  2.]), 'currentState': array([65.77118537,  1.95436295,  4.16629342]), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 0.7725345362182664}
episode index:91
target Thresh 32.60375899234119
target distance 16.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.01419918,  3.01161366]), 'dynamicTrap': True, 'previousTarget': array([33.,  3.]), 'currentState': array([17.       ,  8.       ,  3.7281594], dtype=float32), 'targetState': array([33,  3], dtype=int32), 'currentDistance': 16.773150382558274}
done in step count: 55
reward sum = 0.3598660440576741
running average episode reward sum: 0.7323925566281936
{'scaleFactor': 20, 'currentTarget': array([33.,  3.]), 'dynamicTrap': False, 'previousTarget': array([33.,  3.]), 'currentState': array([32.10917629,  3.83125189,  3.79241779]), 'targetState': array([33,  3], dtype=int32), 'currentDistance': 1.2184197045824903}
episode index:92
target Thresh 32.820198647326265
target distance 24.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.21658974, 11.97055874]), 'dynamicTrap': False, 'previousTarget': array([89.59715  , 12.1492875]), 'currentState': array([107.36394619,  17.74799251,   3.50954294]), 'targetState': array([85, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7571250792392806
running average episode reward sum: 0.7326584977315387
{'scaleFactor': 20, 'currentTarget': array([85., 11.]), 'dynamicTrap': False, 'previousTarget': array([85., 11.]), 'currentState': array([85.56453706, 11.08416605,  3.86864441]), 'targetState': array([85, 11], dtype=int32), 'currentDistance': 0.5707766798247199}
episode index:93
target Thresh 33.03555880502857
target distance 7.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 15.]), 'dynamicTrap': False, 'previousTarget': array([73., 15.]), 'currentState': array([70.54321013,  7.84065987,  6.25338221]), 'targetState': array([73, 15], dtype=int32), 'currentDistance': 7.569145760529417}
done in step count: 12
reward sum = 0.877521022998968
running average episode reward sum: 0.7341995884258731
{'scaleFactor': 20, 'currentTarget': array([71.63655597, 14.30514415]), 'dynamicTrap': True, 'previousTarget': array([73., 15.]), 'currentState': array([71.95664633, 14.83634655,  0.20758976]), 'targetState': array([73, 15], dtype=int32), 'currentDistance': 0.6201885362446454}
episode index:94
target Thresh 33.249844849463265
target distance 9.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'dynamicTrap': False, 'previousTarget': array([12., 17.]), 'currentState': array([6.34754577, 8.48117838, 1.08972245]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 10.223529752533079}
done in step count: 30
reward sum = 0.6865765130646281
running average episode reward sum: 0.7336982928957546
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'dynamicTrap': False, 'previousTarget': array([12., 17.]), 'currentState': array([11.45014074, 16.37833468,  2.54532376]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 0.8299475726127387}
episode index:95
target Thresh 33.463062137792626
target distance 10.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.07076502, 19.02237284]), 'dynamicTrap': True, 'previousTarget': array([31., 19.]), 'currentState': array([21.       , 18.       ,  3.3488054], dtype=float32), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 10.122527071140073}
done in step count: 38
reward sum = 0.5600756180093549
running average episode reward sum: 0.7318897233656879
{'scaleFactor': 20, 'currentTarget': array([31., 19.]), 'dynamicTrap': False, 'previousTarget': array([31., 19.]), 'currentState': array([31.19076418, 19.10563626,  5.15830841]), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 0.21805960705314345}
episode index:96
target Thresh 33.67521600045996
target distance 10.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  13.]), 'dynamicTrap': False, 'previousTarget': array([111.,  13.]), 'currentState': array([99.91881363, 19.1329626 ,  4.16948736]), 'targetState': array([111,  13], dtype=int32), 'currentDistance': 12.665145932831276}
done in step count: 29
reward sum = 0.681783293552732
running average episode reward sum: 0.7313731622335955
{'scaleFactor': 20, 'currentTarget': array([111.,  13.]), 'dynamicTrap': False, 'previousTarget': array([111.,  13.]), 'currentState': array([110.42311335,  12.49272979,   0.50850694]), 'targetState': array([111,  13], dtype=int32), 'currentDistance': 0.768193516084667}
episode index:97
target Thresh 33.8863117413229
target distance 4.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84., 10.]), 'dynamicTrap': False, 'previousTarget': array([84., 10.]), 'currentState': array([82.82248077,  4.15528813,  4.8408829 ]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 5.962147964022291}
done in step count: 19
reward sum = 0.8079510561246379
running average episode reward sum: 0.7321545693141163
{'scaleFactor': 20, 'currentTarget': array([84., 10.]), 'dynamicTrap': False, 'previousTarget': array([84., 10.]), 'currentState': array([84.04234498, 10.51821357,  6.03456548]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 0.5199407710326018}
episode index:98
target Thresh 34.096354637785936
target distance 29.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.00981927,  2.04368044]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([72.01005533,  2.14085295,  4.41840386]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.7330207548523295
{'scaleFactor': 20, 'currentTarget': array([101.,   2.]), 'dynamicTrap': False, 'previousTarget': array([101.,   2.]), 'currentState': array([100.94147316,   1.15766795,   5.27117875]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 0.8443628788720526}
episode index:99
target Thresh 34.30534994093245
target distance 15.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 18.]), 'dynamicTrap': False, 'previousTarget': array([98., 18.]), 'currentState': array([113.38598809,   8.3884682 ,   2.9201932 ]), 'targetState': array([98, 18], dtype=int32), 'currentDistance': 18.141393909820923}
done in step count: 35
reward sum = 0.5934707529882202
running average episode reward sum: 0.7316252548336886
{'scaleFactor': 20, 'currentTarget': array([99.46732133, 17.1865255 ]), 'dynamicTrap': True, 'previousTarget': array([99.28427461, 17.26711017]), 'currentState': array([100.3366261 ,  17.77577916,   2.84291181]), 'targetState': array([98, 18], dtype=int32), 'currentDistance': 1.0501955307888031}
episode index:100
target Thresh 34.51330287565587
target distance 9.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'dynamicTrap': False, 'previousTarget': array([10., 23.]), 'currentState': array([ 5.67683685, 15.25512081,  0.82627106]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 8.869774136604644}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.733246175620075
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'dynamicTrap': False, 'previousTarget': array([10., 23.]), 'currentState': array([10.84827556, 22.29849721,  0.46330269]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 1.100762273936882}
episode index:101
target Thresh 34.72021864079043
target distance 34.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.3326078 , 18.36573572]), 'dynamicTrap': False, 'previousTarget': array([94.03451254, 18.82555956]), 'currentState': array([112.32381882,  18.95859493,   3.98369718]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6165005693038021
running average episode reward sum: 0.7321016108522683
{'scaleFactor': 20, 'currentTarget': array([80., 18.]), 'dynamicTrap': False, 'previousTarget': array([80., 18.]), 'currentState': array([80.23433039, 17.23402958,  3.33148194]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 0.8010127466439363}
episode index:102
target Thresh 34.92610240924101
target distance 7.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  22.]), 'dynamicTrap': False, 'previousTarget': array([116.,  22.]), 'currentState': array([108.98719618,  16.04004586,   0.87025332]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 9.203285868192486}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.73210356967559
{'scaleFactor': 20, 'currentTarget': array([116.,  22.]), 'dynamicTrap': False, 'previousTarget': array([116.,  22.]), 'currentState': array([116.66515237,  21.41431858,   1.57223156]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 0.8862563948300392}
episode index:103
target Thresh 35.13095932811256
target distance 12.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  14.]), 'dynamicTrap': False, 'previousTarget': array([109.,  14.]), 'currentState': array([98.52016976,  3.600566  ,  2.26657245]), 'targetState': array([109,  14], dtype=int32), 'currentDistance': 14.763978782094146}
done in step count: 51
reward sum = 0.3661035727264777
running average episode reward sum: 0.7285843389356946
{'scaleFactor': 20, 'currentTarget': array([109.,  14.]), 'dynamicTrap': False, 'previousTarget': array([109.,  14.]), 'currentState': array([108.35716499,  14.68547918,   1.26527501]), 'targetState': array([109,  14], dtype=int32), 'currentDistance': 0.9397438827714318}
episode index:104
target Thresh 35.334794518838706
target distance 13.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([43.44235231, 11.62379571,  0.29483151]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 15.6718767967686}
done in step count: 37
reward sum = 0.6212805872302529
running average episode reward sum: 0.7275623984432619
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([56.96693394, 20.29116304,  3.67871571]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 1.1989207990911421}
episode index:105
target Thresh 35.53761307730984
target distance 30.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.09446555,  3.36898869]), 'dynamicTrap': False, 'previousTarget': array([16.17544199,  3.3567256 ]), 'currentState': array([35.91304258,  6.0567395 ,  2.6477097 ]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5103635434271621
running average episode reward sum: 0.7255133526412232
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'dynamicTrap': False, 'previousTarget': array([6., 2.]), 'currentState': array([5.39404597, 1.96294828, 2.83639554]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 0.6070857592473035}
episode index:106
target Thresh 35.7394200740005
target distance 5.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,   7.]), 'dynamicTrap': False, 'previousTarget': array([117.,   7.]), 'currentState': array([113.14396249,  12.59546755,   0.72215855]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 6.795460423964529}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7268519737639125
{'scaleFactor': 20, 'currentTarget': array([117.,   7.]), 'dynamicTrap': False, 'previousTarget': array([117.,   7.]), 'currentState': array([116.0803595 ,   6.13926223,   1.58406584]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 1.259606355083175}
episode index:107
target Thresh 35.94022055409611
target distance 15.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([46.42080819,  7.63782837,  3.03672004]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 14.660076645055824}
done in step count: 53
reward sum = 0.4542205586223742
running average episode reward sum: 0.7243276088088982
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([31.63201674,  4.19177751,  2.48536728]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 0.8880513888441481}
episode index:108
target Thresh 36.140019537619104
target distance 4.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  20.]), 'dynamicTrap': False, 'previousTarget': array([111.,  20.]), 'currentState': array([106.88965125,  19.89470975,   0.46128917]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 4.1116970773959185}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7263198339519451
{'scaleFactor': 20, 'currentTarget': array([111.,  20.]), 'dynamicTrap': False, 'previousTarget': array([111.,  20.]), 'currentState': array([110.06496956,  19.47903504,   5.75671441]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 1.0703674204348463}
episode index:109
target Thresh 36.338822019554506
target distance 30.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.01646573,  3.04795634]), 'dynamicTrap': False, 'previousTarget': array([82.9007438 ,  3.00992562]), 'currentState': array([63.12574727,  5.13585466,  0.72301548]), 'targetState': array([93,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.7251619809748017
{'scaleFactor': 20, 'currentTarget': array([93.,  2.]), 'dynamicTrap': False, 'previousTarget': array([93.,  2.]), 'currentState': array([93.01982154,  2.6839155 ,  5.38413416]), 'targetState': array([93,  2], dtype=int32), 'currentDistance': 0.6842026802160635}
episode index:110
target Thresh 36.53663296997471
target distance 22.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.83750579,  7.14731374]), 'dynamicTrap': False, 'previousTarget': array([11.44208854,  7.42295739]), 'currentState': array([28.29090091, 16.91353096,  4.12206113]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5188813252823402
running average episode reward sum: 0.7233035966892839
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'dynamicTrap': False, 'previousTarget': array([7., 5.]), 'currentState': array([7.81618469, 4.75380585, 4.15815003]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.8525074846885445}
episode index:111
target Thresh 36.73345733416378
target distance 29.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.92536021,  17.54222893]), 'dynamicTrap': False, 'previousTarget': array([108.98811999,  17.68924552]), 'currentState': array([88.95075882, 16.53460826,  5.61697531]), 'targetState': array([118,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6952745020650722
running average episode reward sum: 0.7230533369158535
{'scaleFactor': 20, 'currentTarget': array([118.,  18.]), 'dynamicTrap': False, 'previousTarget': array([118.,  18.]), 'currentState': array([1.17258072e+02, 1.78286369e+01, 5.72679838e-03]), 'targetState': array([118,  18], dtype=int32), 'currentDistance': 0.7614602786818618}
episode index:112
target Thresh 36.92930003274106
target distance 36.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.10244109, 21.39362303]), 'dynamicTrap': False, 'previousTarget': array([49.06908484, 21.3390904 ]), 'currentState': array([69.02795491, 23.11812274,  2.20205608]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.23901285954317358
running average episode reward sum: 0.7187697928683077
{'scaleFactor': 20, 'currentTarget': array([33., 20.]), 'dynamicTrap': False, 'previousTarget': array([33., 20.]), 'currentState': array([33.45456672, 20.1337308 ,  5.81917488]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 0.47382995704671976}
episode index:113
target Thresh 37.12416596178423
target distance 8.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  18.]), 'dynamicTrap': False, 'previousTarget': array([108.,  18.]), 'currentState': array([116.20467687,  10.3040937 ,   1.84839281]), 'targetState': array([108,  18], dtype=int32), 'currentDistance': 11.249164253334083}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.719426323489144
{'scaleFactor': 20, 'currentTarget': array([108.,  18.]), 'dynamicTrap': False, 'previousTarget': array([108.,  18.]), 'currentState': array([108.05488908,  17.33537499,   2.79101992]), 'targetState': array([108,  18], dtype=int32), 'currentDistance': 0.6668877052278449}
episode index:114
target Thresh 37.31805999295166
target distance 23.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.25785946,  9.63306862]), 'dynamicTrap': False, 'previousTarget': array([82.7042351,  9.5731765]), 'currentState': array([63.77045129, 14.13205995,  1.8587368 ]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7107738624238289
running average episode reward sum: 0.7193510846972718
{'scaleFactor': 20, 'currentTarget': array([86.,  9.]), 'dynamicTrap': False, 'previousTarget': array([86.,  9.]), 'currentState': array([86.63011795,  8.31965935,  0.15941065]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 0.9273144221486475}
episode index:115
target Thresh 37.51098697360423
target distance 18.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.30747657, 22.57911042]), 'dynamicTrap': False, 'previousTarget': array([64.48314552, 22.71285862]), 'currentState': array([47.21642402, 12.19181419,  5.55399132]), 'targetState': array([65, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.44773338196795337
running average episode reward sum: 0.7170095527771914
{'scaleFactor': 20, 'currentTarget': array([65., 23.]), 'dynamicTrap': False, 'previousTarget': array([65., 23.]), 'currentState': array([64.11539248, 23.7184102 ,  3.81494753]), 'targetState': array([65, 23], dtype=int32), 'currentDistance': 1.1395804863845065}
episode index:116
target Thresh 37.7029517269265
target distance 18.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'dynamicTrap': False, 'previousTarget': array([23., 23.]), 'currentState': array([ 5.03018721, 23.32314375,  5.62655926]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 17.972718039091706}
done in step count: 61
reward sum = 0.4730626373301944
running average episode reward sum: 0.7149245364058495
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'dynamicTrap': False, 'previousTarget': array([23., 23.]), 'currentState': array([22.26114413, 22.79350586,  0.22166163]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.7671687066770012}
episode index:117
target Thresh 37.8939590520473
target distance 15.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.8956111 , 14.09829647]), 'dynamicTrap': True, 'previousTarget': array([92., 14.]), 'currentState': array([77.       ,  8.       ,  4.9379506], dtype=float32), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 16.095603428965433}
done in step count: 31
reward sum = 0.6561610437868859
running average episode reward sum: 0.7144265407056888
{'scaleFactor': 20, 'currentTarget': array([92., 14.]), 'dynamicTrap': False, 'previousTarget': array([92., 14.]), 'currentState': array([91.49306867, 14.50890523,  1.74555489]), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 0.7183062755419464}
episode index:118
target Thresh 38.08401372415972
target distance 35.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.24627449,  7.46376295]), 'dynamicTrap': False, 'previousTarget': array([91.,  7.]), 'currentState': array([71.2561479 ,  8.09212492,  0.62421107]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.29877422349888505
running average episode reward sum: 0.7109336640905055
{'scaleFactor': 20, 'currentTarget': array([106.,   7.]), 'dynamicTrap': False, 'previousTarget': array([106.,   7.]), 'currentState': array([105.90021234,   6.39958164,   2.91304744]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 0.6086540798419179}
episode index:119
target Thresh 38.27312049464044
target distance 14.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 16.]), 'dynamicTrap': False, 'previousTarget': array([93., 16.]), 'currentState': array([106.99968027,  13.10606492,   4.08250844]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 14.29566046006276}
done in step count: 29
reward sum = 0.6918690743620961
running average episode reward sum: 0.7107747925094354
{'scaleFactor': 20, 'currentTarget': array([93., 16.]), 'dynamicTrap': False, 'previousTarget': array([93., 16.]), 'currentState': array([93.8479733 , 16.57197602,  0.60932818]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 1.0228466565801155}
episode index:120
target Thresh 38.461284091168594
target distance 8.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.,  7.]), 'dynamicTrap': False, 'previousTarget': array([71.,  7.]), 'currentState': array([62.98681949,  6.36772022,  0.89168191]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 8.038086810707835}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7125266098806626
{'scaleFactor': 20, 'currentTarget': array([71.,  7.]), 'dynamicTrap': False, 'previousTarget': array([71.,  7.]), 'currentState': array([71.37810179,  6.79466045,  4.86330658]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 0.4302618939550158}
episode index:121
target Thresh 38.64850921784388
target distance 12.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.,  2.]), 'dynamicTrap': False, 'previousTarget': array([66.,  2.]), 'currentState': array([71.02083361, 13.92665978,  4.00165534]), 'targetState': array([66,  2], dtype=int32), 'currentDistance': 12.94040121607494}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7140250659821221
{'scaleFactor': 20, 'currentTarget': array([66.,  2.]), 'dynamicTrap': False, 'previousTarget': array([66.,  2.]), 'currentState': array([66.64256589,  1.76657629,  4.29378259]), 'targetState': array([66,  2], dtype=int32), 'currentDistance': 0.6836501641454962}
episode index:122
target Thresh 38.83480055530422
target distance 36.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.72191846,   8.67645788]), 'dynamicTrap': True, 'previousTarget': array([100.72787848,   8.71202025]), 'currentState': array([81.        , 12.        ,  0.37119475], dtype=float32), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.36928129854341124
running average episode reward sum: 0.7112222711248968
{'scaleFactor': 20, 'currentTarget': array([117.,   6.]), 'dynamicTrap': False, 'previousTarget': array([117.,   6.]), 'currentState': array([117.11731076,   5.79824598,   4.72998002]), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 0.233380590354066}
episode index:123
target Thresh 39.020162760842744
target distance 32.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.04079182, 12.65246086]), 'dynamicTrap': False, 'previousTarget': array([95.59715  , 13.1492875]), 'currentState': array([114.57249358,  16.95509105,   3.31403446]), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 50
reward sum = 0.5208176392053655
running average episode reward sum: 0.7096867498997392
{'scaleFactor': 20, 'currentTarget': array([84.08888642,  8.52273781]), 'dynamicTrap': True, 'previousTarget': array([83., 10.]), 'currentState': array([84.61816821,  7.97464518,  1.50097456]), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 0.7619348622713518}
episode index:124
target Thresh 39.20460046852427
target distance 25.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.87711087,  3.67082223]), 'dynamicTrap': False, 'previousTarget': array([28.14246323,  3.61709559]), 'currentState': array([47.69056731,  6.39606417,  3.24401116]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.23158569339952484
running average episode reward sum: 0.7021565703533451
{'scaleFactor': 20, 'currentTarget': array([24.36048865,  1.91469749]), 'dynamicTrap': True, 'previousTarget': array([24.34966652,  1.92947449]), 'currentState': array([24.08110619,  3.79766001,  4.62771208]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 1.9035762095622222}
episode index:125
target Thresh 39.38811828930106
target distance 8.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.,  6.]), 'dynamicTrap': False, 'previousTarget': array([85.,  6.]), 'currentState': array([92.20875307, 13.19596993,  3.14541101]), 'targetState': array([85,  6], dtype=int32), 'currentDistance': 10.185681320815439}
done in step count: 29
reward sum = 0.7148582514138053
running average episode reward sum: 0.7022573773458884
{'scaleFactor': 20, 'currentTarget': array([85.,  6.]), 'dynamicTrap': False, 'previousTarget': array([85.,  6.]), 'currentState': array([84.58858743,  6.83929666,  4.19095784]), 'targetState': array([85,  6], dtype=int32), 'currentDistance': 0.9347080754425245}
episode index:126
target Thresh 39.57072081112823
target distance 31.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.45635998, 15.45126084]), 'dynamicTrap': False, 'previousTarget': array([25.49117996, 15.40521743]), 'currentState': array([44.97906083, 11.10797274,  3.8939414 ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.3027610385852518
running average episode reward sum: 0.6991117368832063
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'dynamicTrap': False, 'previousTarget': array([14., 18.]), 'currentState': array([13.2079113 , 17.09852418,  1.48470466]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 1.2000263205551716}
episode index:127
target Thresh 39.75241259907831
target distance 14.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'dynamicTrap': False, 'previousTarget': array([20.,  8.]), 'currentState': array([32.29627597, 11.05130658,  3.70802271]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 12.669209703193578}
done in step count: 94
reward sum = 0.147999966661415
running average episode reward sum: 0.6948061761783485
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'dynamicTrap': False, 'previousTarget': array([20.,  8.]), 'currentState': array([20.43893732,  7.98728691,  1.87039746]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.4391213843357328}
episode index:128
target Thresh 39.93319819545548
target distance 2.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.59787185, 16.35309716,  1.44365447]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 2.387210354527697}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6958891807153387
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.26579592, 14.14088035,  4.33214989]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.3008234424781464}
episode index:129
target Thresh 40.113082119909045
target distance 23.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.07097257, 17.52200154]), 'dynamicTrap': False, 'previousTarget': array([41.54352728, 17.24859289]), 'currentState': array([23.65808104, 12.71163332,  0.8378064 ]), 'targetState': array([45, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5693414589077923
running average episode reward sum: 0.6949157367014345
{'scaleFactor': 20, 'currentTarget': array([45., 18.]), 'dynamicTrap': False, 'previousTarget': array([45., 18.]), 'currentState': array([44.16590312, 17.23273618,  5.95478774]), 'targetState': array([45, 18], dtype=int32), 'currentDistance': 1.1333187448709963}
episode index:130
target Thresh 40.2920688695465
target distance 3.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.03885856, 11.02091918]), 'dynamicTrap': True, 'previousTarget': array([98., 11.]), 'currentState': array([101.       ,  12.       ,   0.5101689], dtype=float32), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 3.118807121102214}
done in step count: 18
reward sum = 0.7520955802232258
running average episode reward sum: 0.6953522240565626
{'scaleFactor': 20, 'currentTarget': array([98., 11.]), 'dynamicTrap': False, 'previousTarget': array([98., 11.]), 'currentState': array([98.11513017, 11.87948913,  0.10746536]), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 0.8869927240891174}
episode index:131
target Thresh 40.470162919045904
target distance 37.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.38536208, 12.61610357]), 'dynamicTrap': False, 'previousTarget': array([92.0073006 , 12.45965677]), 'currentState': array([110.36934549,  13.41635796,   3.90160215]), 'targetState': array([75, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4098881408963102
running average episode reward sum: 0.6931896173659546
{'scaleFactor': 20, 'currentTarget': array([75., 12.]), 'dynamicTrap': False, 'previousTarget': array([75., 12.]), 'currentState': array([75.57870084, 11.60182096,  1.71287203]), 'targetState': array([75, 12], dtype=int32), 'currentDistance': 0.7024537045137024}
episode index:132
target Thresh 40.64736872076777
target distance 36.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.00415626, 15.59228294]), 'dynamicTrap': True, 'previousTarget': array([85.00771159, 15.44465866]), 'currentState': array([105.       ,  16.       ,   5.6055803], dtype=float32), 'targetState': array([69, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4882752616181177
running average episode reward sum: 0.6916489079242416
{'scaleFactor': 20, 'currentTarget': array([69., 15.]), 'dynamicTrap': False, 'previousTarget': array([69., 15.]), 'currentState': array([68.02827774, 14.49928155,  1.02025197]), 'targetState': array([69, 15], dtype=int32), 'currentDistance': 1.0931436829094556}
episode index:133
target Thresh 40.82369070486638
target distance 25.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.50169488, 14.19337774]), 'dynamicTrap': False, 'previousTarget': array([26.06369443, 14.40509555]), 'currentState': array([44.47126751, 15.29617824,  4.58305788]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.42281532016201406
running average episode reward sum: 0.6896426871200458
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'dynamicTrap': False, 'previousTarget': array([21., 14.]), 'currentState': array([21.1747651 , 14.890225  ,  2.07756265]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.9072173890285429}
episode index:134
target Thresh 40.999133279400496
target distance 17.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.95848168, 16.55800594]), 'dynamicTrap': True, 'previousTarget': array([26.20859686, 17.86502556]), 'currentState': array([43.       ,  7.       ,  1.3714916], dtype=float32), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 18.67313009002632}
done in step count: 23
reward sum = 0.7281327320632573
running average episode reward sum: 0.6899277985640697
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([25.2459426 , 17.95898576,  1.17386853]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.755171989029649}
episode index:135
target Thresh 41.17370083044363
target distance 23.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.66179367, 13.94452762]), 'dynamicTrap': False, 'previousTarget': array([24.98112317, 13.86874449]), 'currentState': array([ 6.67895489, 13.11618334,  5.34227497]), 'targetState': array([28, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5388956268212889
running average episode reward sum: 0.6888172678894904
{'scaleFactor': 20, 'currentTarget': array([28., 14.]), 'dynamicTrap': False, 'previousTarget': array([28., 14.]), 'currentState': array([27.66127097, 14.44388435,  0.98997367]), 'targetState': array([28, 14], dtype=int32), 'currentDistance': 0.558364284670235}
episode index:136
target Thresh 41.347397722193676
target distance 7.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 21.]), 'currentState': array([ 9.030752  , 14.77516447,  3.92584276]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 9.390423371456917}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6897595282523206
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.17863471, 20.75919976,  0.33808482]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 0.29982514479965244}
episode index:137
target Thresh 41.52022829708193
target distance 35.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.4056022 ,  9.36534931]), 'dynamicTrap': True, 'previousTarget': array([20.43046618,  9.42781353]), 'currentState': array([39.       ,  2.       ,  1.3615506], dtype=float32), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.45792947489866775
running average episode reward sum: 0.688079600329468
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.65596498, 16.81001952,  4.11279348]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 1.0423155402985909}
episode index:138
target Thresh 41.6921968758818
target distance 36.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.22302936, 20.59594468]), 'dynamicTrap': False, 'previousTarget': array([73.03079294, 20.89059961]), 'currentState': array([91.20549635, 21.43320969,  4.0542233 ]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5382667120639241
running average episode reward sum: 0.6870018097664066
{'scaleFactor': 20, 'currentTarget': array([57., 20.]), 'dynamicTrap': False, 'previousTarget': array([57., 20.]), 'currentState': array([57.26926728, 19.55611772,  5.35120896]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 0.5191688952785318}
episode index:139
target Thresh 41.8633077578167
target distance 10.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77., 20.]), 'dynamicTrap': False, 'previousTarget': array([77., 20.]), 'currentState': array([71.7680821 , 11.4974772 ,  0.08686829]), 'targetState': array([77, 20], dtype=int32), 'currentDistance': 9.983278961761178}
done in step count: 24
reward sum = 0.7129438041751971
running average episode reward sum: 0.6871871097264693
{'scaleFactor': 20, 'currentTarget': array([77., 20.]), 'dynamicTrap': False, 'previousTarget': array([77., 20.]), 'currentState': array([77.83400465, 20.79616282,  5.33099691]), 'targetState': array([77, 20], dtype=int32), 'currentDistance': 1.1530130016724356}
episode index:140
target Thresh 42.03356522066758
target distance 27.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.62508568,  20.93577961]), 'dynamicTrap': False, 'previousTarget': array([100.98629668,  20.74023321]), 'currentState': array([82.62651312, 20.69683326,  1.25988918]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.707929532598275
running average episode reward sum: 0.6873342191085389
{'scaleFactor': 20, 'currentTarget': array([108.,  21.]), 'dynamicTrap': False, 'previousTarget': array([108.,  21.]), 'currentState': array([107.0832904 ,  20.19983494,   3.08287292]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 1.216807555615323}
episode index:141
target Thresh 42.20297352087991
target distance 6.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  11.]), 'dynamicTrap': False, 'previousTarget': array([110.,  11.]), 'currentState': array([107.71839239,  16.98646319,   3.85828209]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 6.406518153787813}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6889920393572669
{'scaleFactor': 20, 'currentTarget': array([110.,  11.]), 'dynamicTrap': False, 'previousTarget': array([110.,  11.]), 'currentState': array([109.42155673,  11.28620072,   0.18459083]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 0.645373895293295}
episode index:142
target Thresh 42.37153689366999
target distance 18.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 18.]), 'dynamicTrap': False, 'previousTarget': array([48., 18.]), 'currentState': array([65.65605234, 22.60716616,  4.40458989]), 'targetState': array([48, 18], dtype=int32), 'currentDistance': 18.24725086806628}
done in step count: 46
reward sum = 0.5380611860745105
running average episode reward sum: 0.6879365788448001
{'scaleFactor': 20, 'currentTarget': array([48., 18.]), 'dynamicTrap': False, 'previousTarget': array([48., 18.]), 'currentState': array([48.14309197, 17.40332539,  4.91871901]), 'targetState': array([48, 18], dtype=int32), 'currentDistance': 0.6135926198251532}
episode index:143
target Thresh 42.539259553130925
target distance 42.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.02945209,  8.60791947]), 'dynamicTrap': False, 'previousTarget': array([47.64677133,  8.25775784]), 'currentState': array([29.49546856, 12.90017848,  1.4421404 ]), 'targetState': array([70,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.33139951846078625
running average episode reward sum: 0.6854606270365777
{'scaleFactor': 20, 'currentTarget': array([70.,  4.]), 'dynamicTrap': False, 'previousTarget': array([70.,  4.]), 'currentState': array([69.71180495,  4.15576863,  6.04145479]), 'targetState': array([70,  4], dtype=int32), 'currentDistance': 0.32759770133847554}
episode index:144
target Thresh 42.70614569233794
target distance 8.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'dynamicTrap': False, 'previousTarget': array([27., 20.]), 'currentState': array([34.8116165 , 14.03306824,  3.06882615]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 9.829833514086655}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6867246628002495
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'dynamicTrap': False, 'previousTarget': array([27., 20.]), 'currentState': array([27.27234612, 19.08944023,  1.50688477]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.9504164869979974}
episode index:145
target Thresh 42.8721994834532
target distance 26.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.16184437, 10.27080109]), 'dynamicTrap': True, 'previousTarget': array([54.89972147,  9.45778872]), 'currentState': array([36.       , 16.       ,  6.0435033], dtype=float32), 'targetState': array([62,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.41377284349482235
running average episode reward sum: 0.6848551297913082
{'scaleFactor': 20, 'currentTarget': array([62.,  7.]), 'dynamicTrap': False, 'previousTarget': array([62.,  7.]), 'currentState': array([61.06975547,  6.76146006,  0.38430237]), 'targetState': array([62,  7], dtype=int32), 'currentDistance': 0.9603416992412163}
episode index:146
target Thresh 43.03742507783015
target distance 21.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.36608434, 20.88315939]), 'dynamicTrap': False, 'previousTarget': array([94.3829006 , 20.87838597]), 'currentState': array([75.95299587, 13.07558786,  6.16645455]), 'targetState': array([97, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6656191087159921
running average episode reward sum: 0.6847242725050816
{'scaleFactor': 20, 'currentTarget': array([97., 22.]), 'dynamicTrap': False, 'previousTarget': array([97., 22.]), 'currentState': array([97.46073948, 21.95203429,  5.05203566]), 'targetState': array([97, 22], dtype=int32), 'currentDistance': 0.4632295045258598}
episode index:147
target Thresh 43.201826606117244
target distance 21.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  23.]), 'dynamicTrap': False, 'previousTarget': array([116.10381815,  21.90990945]), 'currentState': array([118.85085249,   3.6485701 ,   0.48055923]), 'targetState': array([116,  23], dtype=int32), 'currentDistance': 19.560296494443026}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6856241553773258
{'scaleFactor': 20, 'currentTarget': array([116.,  23.]), 'dynamicTrap': False, 'previousTarget': array([116.,  23.]), 'currentState': array([115.66916118,  22.92847176,   2.76855946]), 'targetState': array([116,  23], dtype=int32), 'currentDistance': 0.33848281345731696}
episode index:148
target Thresh 43.36540817836125
target distance 31.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.60432847, 10.84713482]), 'dynamicTrap': False, 'previousTarget': array([99.84855491, 10.6881969 ]), 'currentState': array([80.64031117,  4.49370676,  1.19042587]), 'targetState': array([112,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6864027220495521
{'scaleFactor': 20, 'currentTarget': array([112.,  15.]), 'dynamicTrap': False, 'previousTarget': array([112.,  15.]), 'currentState': array([112.51690751,  15.43749446,   0.46447694]), 'targetState': array([112,  15], dtype=int32), 'currentDistance': 0.6771962576654063}
episode index:149
target Thresh 43.52817388410999
target distance 13.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'dynamicTrap': False, 'previousTarget': array([20.,  7.]), 'currentState': array([11.53684354, 20.03459732,  5.60645223]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 15.54109856399929}
done in step count: 25
reward sum = 0.6828511150602273
running average episode reward sum: 0.6863790446696233
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'dynamicTrap': False, 'previousTarget': array([21.71781085,  6.53007995]), 'currentState': array([20.9830499 ,  7.91447641,  1.56510781]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 1.342629590985599}
episode index:150
target Thresh 43.690127792514595
target distance 34.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.71577669,  3.17008939]), 'dynamicTrap': False, 'previousTarget': array([22.922597  ,  3.24212379]), 'currentState': array([4.80589223, 5.06653363, 0.81670385]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7359838322305526
running average episode reward sum: 0.6867075531965169
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([36.7889565,  1.1845384,  3.2554568]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 0.8423283074702217}
episode index:151
target Thresh 43.8512739524312
target distance 4.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.,  4.]), 'dynamicTrap': False, 'previousTarget': array([64.,  4.]), 'currentState': array([67.82920172,  4.50499957,  2.90693367]), 'targetState': array([64,  4], dtype=int32), 'currentDistance': 3.8623581346569855}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6885732863991714
{'scaleFactor': 20, 'currentTarget': array([64.,  4.]), 'dynamicTrap': False, 'previousTarget': array([64.,  4.]), 'currentState': array([64.35201384,  3.67291901,  1.7659831 ]), 'targetState': array([64,  4], dtype=int32), 'currentDistance': 0.4805160974748886}
episode index:152
target Thresh 44.0116163925222
target distance 8.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'dynamicTrap': False, 'previousTarget': array([21.,  3.]), 'currentState': array([28.53699035, 11.00790234,  4.13452697]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 10.996941554427682}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6900435083670438
{'scaleFactor': 20, 'currentTarget': array([21.,  3.]), 'dynamicTrap': False, 'previousTarget': array([21.,  3.]), 'currentState': array([21.6177571 ,  3.54350835,  1.06468338]), 'targetState': array([21,  3], dtype=int32), 'currentDistance': 0.8228153831347259}
episode index:153
target Thresh 44.17115912135696
target distance 37.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.35582611, 12.98444296]), 'dynamicTrap': False, 'previousTarget': array([72.70572515, 13.92215805]), 'currentState': array([55.53539115, 19.75136454,  5.34661973]), 'targetState': array([91,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4615132381039742
running average episode reward sum: 0.6885595455731277
{'scaleFactor': 20, 'currentTarget': array([91.,  7.]), 'dynamicTrap': False, 'previousTarget': array([91.,  7.]), 'currentState': array([90.70187707,  7.49180178,  5.86265805]), 'targetState': array([91,  7], dtype=int32), 'currentDistance': 0.5751054388308456}
episode index:154
target Thresh 44.329906127512004
target distance 42.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.05189608, 11.43984375]), 'dynamicTrap': True, 'previousTarget': array([24.050826, 11.424941]), 'currentState': array([44.        , 10.        ,  0.47399595], dtype=float32), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.405870722900486
running average episode reward sum: 0.6867357467171752
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.23760262, 13.07561108,  4.91042263]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.7661375864420344}
episode index:155
target Thresh 44.48786137967074
target distance 19.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.51251442, 16.28746865]), 'dynamicTrap': False, 'previousTarget': array([86.50614522, 16.29367831]), 'currentState': array([70.03905313,  4.94616921,  1.18841863]), 'targetState': array([89, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6606163358501151
running average episode reward sum: 0.6865683145962326
{'scaleFactor': 20, 'currentTarget': array([89., 18.]), 'dynamicTrap': False, 'previousTarget': array([89., 18.]), 'currentState': array([88.07689184, 17.11470776,  2.84649594]), 'targetState': array([89, 18], dtype=int32), 'currentDistance': 1.2790117386159912}
episode index:156
target Thresh 44.64502882672271
target distance 14.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  17.]), 'dynamicTrap': False, 'previousTarget': array([108.,  17.]), 'currentState': array([106.05569272,   3.01372762,   2.39437902]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 14.120770014662023}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6880138492006109
{'scaleFactor': 20, 'currentTarget': array([108.,  17.]), 'dynamicTrap': False, 'previousTarget': array([108.,  17.]), 'currentState': array([107.98458523,  16.88026643,   1.82764614]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 0.12072175836139785}
episode index:157
target Thresh 44.80141239786228
target distance 4.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  19.]), 'dynamicTrap': False, 'previousTarget': array([108.,  19.]), 'currentState': array([105.9865293 ,  15.0239739 ,   1.07272243]), 'targetState': array([108,  19], dtype=int32), 'currentDistance': 4.456775494197212}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6891577223877525
{'scaleFactor': 20, 'currentTarget': array([108.,  19.]), 'dynamicTrap': False, 'previousTarget': array([108.,  19.]), 'currentState': array([107.95452407,  19.57093279,   5.70948708]), 'targetState': array([108,  19], dtype=int32), 'currentDistance': 0.5727410548885727}
episode index:158
target Thresh 44.95701600268686
target distance 4.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.9837843 , 11.97986289]), 'dynamicTrap': True, 'previousTarget': array([28., 12.]), 'currentState': array([24.      , 11.      ,  4.028489], dtype=float32), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 4.10251979135711}
done in step count: 4
reward sum = 0.9505960099999999
running average episode reward sum: 0.6908019883475779
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([27.33565096, 11.09695051,  0.27039863]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 1.121096792291645}
episode index:159
target Thresh 45.111843531294696
target distance 23.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.05235681,  13.38021657]), 'dynamicTrap': False, 'previousTarget': array([105.73169692,  13.25132013]), 'currentState': array([88.37725284,  4.02122626,  2.48018706]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6853917660121079
running average episode reward sum: 0.6907681744579811
{'scaleFactor': 20, 'currentTarget': array([111.,  16.]), 'dynamicTrap': False, 'previousTarget': array([111.,  16.]), 'currentState': array([110.93662468,  16.22866158,   1.17150132]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 0.2372815869513921}
episode index:160
target Thresh 45.265898854382044
target distance 7.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  22.]), 'dynamicTrap': False, 'previousTarget': array([117.,  22.]), 'currentState': array([117.74132074,  15.10341283,   1.88985574]), 'targetState': array([117,  22], dtype=int32), 'currentDistance': 6.936315376452544}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.692325391693652
{'scaleFactor': 20, 'currentTarget': array([117.,  22.]), 'dynamicTrap': False, 'previousTarget': array([117.,  22.]), 'currentState': array([116.83330026,  21.02552405,   1.05336633]), 'targetState': array([117,  22], dtype=int32), 'currentDistance': 0.9886314642097288}
episode index:161
target Thresh 45.419185823340015
target distance 24.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.81057873, 14.1715561 ]), 'dynamicTrap': False, 'previousTarget': array([69.8, 14.4]), 'currentState': array([87.92746937, 20.04901239,  2.08592653]), 'targetState': array([65, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6805758014147677
running average episode reward sum: 0.6922528633585971
{'scaleFactor': 20, 'currentTarget': array([65., 13.]), 'dynamicTrap': False, 'previousTarget': array([65., 13.]), 'currentState': array([64.86664738, 13.45954865,  0.3718187 ]), 'targetState': array([65, 13], dtype=int32), 'currentDistance': 0.47850588786975434}
episode index:162
target Thresh 45.57170827035083
target distance 31.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.74453664, 15.14812483]), 'dynamicTrap': False, 'previousTarget': array([87.84855491, 15.3118031 ]), 'currentState': array([68.80027445, 21.56021731,  5.03103888]), 'targetState': array([100,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6395052326665261
running average episode reward sum: 0.6919292582623268
{'scaleFactor': 20, 'currentTarget': array([100.,  11.]), 'dynamicTrap': False, 'previousTarget': array([100.,  11.]), 'currentState': array([100.80062659,  11.82603408,   0.99161298]), 'targetState': array([100,  11], dtype=int32), 'currentDistance': 1.1503630858271991}
episode index:163
target Thresh 45.72347000848359
target distance 33.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.34725894, 22.44332177]), 'dynamicTrap': False, 'previousTarget': array([37.08213587, 21.81071492]), 'currentState': array([56.32696315, 21.54253346,  0.97221518]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7070142223951085
running average episode reward sum: 0.6920212397509413
{'scaleFactor': 20, 'currentTarget': array([24.94653538, 22.53172159]), 'dynamicTrap': True, 'previousTarget': array([24., 23.]), 'currentState': array([25.13530862, 22.47384309,  2.55444858]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.1974468491223427}
episode index:164
target Thresh 45.87447483178965
target distance 12.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([25.98770192, 13.78241575,  0.78981972]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 16.826188699900904}
done in step count: 18
reward sum = 0.8084505095459952
running average episode reward sum: 0.6927268716890932
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([38.51481411,  1.64698181,  2.15828709]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 0.6242238463784964}
episode index:165
target Thresh 46.02472651539747
target distance 30.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.95402425, 10.96373033]), 'dynamicTrap': False, 'previousTarget': array([22.11145618, 10.94427191]), 'currentState': array([39.80702817,  1.94872887,  3.30028515]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6519509023106183
running average episode reward sum: 0.6924812333193433
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([ 9.15331396, 16.12081558,  2.34445551]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 1.2205910376945812}
episode index:166
target Thresh 46.17422881560696
target distance 45.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.56515309, 15.48256954]), 'dynamicTrap': False, 'previousTarget': array([91.4762588 , 15.33860916]), 'currentState': array([111.11504145,  11.2633439 ,   2.16893265]), 'targetState': array([66, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.40731248375986284
running average episode reward sum: 0.6907736360165919
{'scaleFactor': 20, 'currentTarget': array([66., 21.]), 'dynamicTrap': False, 'previousTarget': array([66., 21.]), 'currentState': array([66.69208533, 21.04856146,  2.80745597]), 'targetState': array([66, 21], dtype=int32), 'currentDistance': 0.6937869455990062}
episode index:167
target Thresh 46.32298546998342
target distance 44.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.15753239, 14.44410533]), 'dynamicTrap': False, 'previousTarget': array([78.59715  , 14.1492875]), 'currentState': array([96.42543019, 19.80583216,  3.52504098]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 59
reward sum = 0.3777496346813022
running average episode reward sum: 0.6889103979134057
{'scaleFactor': 20, 'currentTarget': array([54.,  8.]), 'dynamicTrap': False, 'previousTarget': array([54.,  8.]), 'currentState': array([54.43025821,  7.61117845,  1.91697089]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 0.5799175123246995}
episode index:168
target Thresh 46.47100019745094
target distance 45.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.99443491, 13.26190405]), 'dynamicTrap': True, 'previousTarget': array([75.97366596, 13.32455532]), 'currentState': array([57.     ,  7.     ,  1.40253], dtype=float32), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 55
reward sum = 0.4673569227086606
running average episode reward sum: 0.6875994306045019
{'scaleFactor': 20, 'currentTarget': array([102.,  22.]), 'dynamicTrap': False, 'previousTarget': array([102.,  22.]), 'currentState': array([101.74616186,  21.73465521,   1.29735029]), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 0.36720792337092806}
episode index:169
target Thresh 46.61827669838544
target distance 43.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.3341493 ,  8.81761989]), 'dynamicTrap': False, 'previousTarget': array([82.25556298,  8.97467424]), 'currentState': array([101.04626439,   1.75668872,   3.89029074]), 'targetState': array([58, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.47281179533861484
running average episode reward sum: 0.6863359739264673
{'scaleFactor': 20, 'currentTarget': array([58., 18.]), 'dynamicTrap': False, 'previousTarget': array([58., 18.]), 'currentState': array([58.72786584, 18.63955757,  2.82440726]), 'targetState': array([58, 18], dtype=int32), 'currentDistance': 0.9689285655129546}
episode index:170
target Thresh 46.76481865470709
target distance 22.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.13765159, 17.95129959]), 'dynamicTrap': False, 'previousTarget': array([27.11145618, 17.94427191]), 'currentState': array([45.06093073,  9.07683821,  0.97067291]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6334958471968762
running average episode reward sum: 0.6860269673374054
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'dynamicTrap': False, 'previousTarget': array([23., 20.]), 'currentState': array([22.95405518, 20.07006734,  2.00872741]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.08378757756911952}
episode index:171
target Thresh 46.91062972997244
target distance 28.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.26583056, 14.59799315]), 'dynamicTrap': False, 'previousTarget': array([18.27466942, 14.62476387]), 'currentState': array([3.5166937, 2.2705973, 4.8249209]), 'targetState': array([30, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6317709430537413
running average episode reward sum: 0.6857115253357562
{'scaleFactor': 20, 'currentTarget': array([30., 23.]), 'dynamicTrap': False, 'previousTarget': array([30., 23.]), 'currentState': array([30.37176207, 22.76886252,  2.31782186]), 'targetState': array([30, 23], dtype=int32), 'currentDistance': 0.4377574294356577}
episode index:172
target Thresh 47.05571356946598
target distance 13.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.,  9.]), 'dynamicTrap': False, 'previousTarget': array([71.,  9.]), 'currentState': array([84.06237593,  6.52107879,  2.33342016]), 'targetState': array([71,  9], dtype=int32), 'currentDistance': 13.29551485310797}
done in step count: 14
reward sum = 0.8037621214293867
running average episode reward sum: 0.6863938987235806
{'scaleFactor': 20, 'currentTarget': array([71.,  9.]), 'dynamicTrap': False, 'previousTarget': array([71.,  9.]), 'currentState': array([71.47189777,  9.40549626,  3.71950814]), 'targetState': array([71,  9], dtype=int32), 'currentDistance': 0.6221854450006324}
episode index:173
target Thresh 47.200073800291236
target distance 29.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.03217608,   3.83644409]), 'dynamicTrap': False, 'previousTarget': array([102.44164417,   4.30718934]), 'currentState': array([84.54312859,  8.32832876,  4.86265159]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6871972017414656
{'scaleFactor': 20, 'currentTarget': array([112.,   2.]), 'dynamicTrap': False, 'previousTarget': array([112.,   2.]), 'currentState': array([111.72286946,   1.23674327,   0.19390547]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 0.8120111908141414}
episode index:174
target Thresh 47.343714031461495
target distance 3.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 21.]), 'dynamicTrap': False, 'previousTarget': array([86., 21.]), 'currentState': array([84.76246785, 20.4772087 ,  1.00418418]), 'targetState': array([86, 21], dtype=int32), 'currentDistance': 1.343427168313479}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6889275034458
{'scaleFactor': 20, 'currentTarget': array([86., 21.]), 'dynamicTrap': False, 'previousTarget': array([86., 21.]), 'currentState': array([86.31564679, 21.67082556,  0.29923814]), 'targetState': array([86, 21], dtype=int32), 'currentDistance': 0.7413769815947993}
episode index:175
target Thresh 47.486637853990025
target distance 9.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.11364018,  2.83582702]), 'dynamicTrap': True, 'previousTarget': array([52.,  3.]), 'currentState': array([61.       ,  5.       ,  2.0772667], dtype=float32), 'targetState': array([52,  3], dtype=int32), 'currentDistance': 9.146094009232847}
done in step count: 19
reward sum = 0.7867646338355867
running average episode reward sum: 0.6894833962321056
{'scaleFactor': 20, 'currentTarget': array([52.,  3.]), 'dynamicTrap': False, 'previousTarget': array([52.,  3.]), 'currentState': array([52.83869802,  2.79739199,  2.65117946]), 'targetState': array([52,  3], dtype=int32), 'currentDistance': 0.8628234858102806}
episode index:176
target Thresh 47.62884884097984
target distance 35.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.93644457, 19.084174  ]), 'dynamicTrap': False, 'previousTarget': array([28.00815827, 18.57119548]), 'currentState': array([46.93607978, 19.20496875,  1.56389666]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5941471898228063
running average episode reward sum: 0.6889447735970249
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'dynamicTrap': False, 'previousTarget': array([13., 19.]), 'currentState': array([12.87146139, 18.0868024 ,  2.45418194]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.922199561610051}
episode index:177
target Thresh 47.77035054771301
target distance 26.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.96359007, 10.85248297]), 'dynamicTrap': False, 'previousTarget': array([10.11145618, 11.05572809]), 'currentState': array([26.46396838, 20.53425755,  3.81686413]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6896692801363519
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'dynamicTrap': False, 'previousTarget': array([2., 7.]), 'currentState': array([1.26929102, 6.96695683, 5.08281422]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.7314557115225491}
episode index:178
target Thresh 47.91114651173957
target distance 36.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.54210851, 18.94949834]), 'dynamicTrap': False, 'previousTarget': array([76.99228841, 18.55534134]), 'currentState': array([58.54223052, 18.87963843,  5.9579348 ]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6901617498529038
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([93.3494472 , 18.14547707,  5.13520027]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 0.9232132935552616}
episode index:179
target Thresh 48.05124025296597
target distance 9.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 16.]), 'dynamicTrap': False, 'previousTarget': array([36., 16.]), 'currentState': array([43.88954937, 17.26462938,  3.30137026]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 7.990261365083652}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6916641624092765
{'scaleFactor': 20, 'currentTarget': array([36., 16.]), 'dynamicTrap': False, 'previousTarget': array([36., 16.]), 'currentState': array([36.65505283, 16.4632285 ,  4.83896729]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 0.80229349164843}
episode index:180
target Thresh 48.19063527374302
target distance 32.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.15913485,  9.17602905]), 'dynamicTrap': False, 'previousTarget': array([32.00975848,  9.37530495]), 'currentState': array([50.15613322,  9.52252043,  2.8509419 ]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7593562967695787
running average episode reward sum: 0.6920381521018749
{'scaleFactor': 20, 'currentTarget': array([20.,  9.]), 'dynamicTrap': False, 'previousTarget': array([20.,  9.]), 'currentState': array([20.45007002,  8.40551889,  2.24052641]), 'targetState': array([20,  9], dtype=int32), 'currentDistance': 0.7456345050089548}
episode index:181
target Thresh 48.32933505895352
target distance 7.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83., 23.]), 'dynamicTrap': False, 'previousTarget': array([83., 23.]), 'currentState': array([87.53266922, 17.59644968,  2.25875303]), 'targetState': array([83, 23], dtype=int32), 'currentDistance': 7.052903393416693}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6935137447276888
{'scaleFactor': 20, 'currentTarget': array([83., 23.]), 'dynamicTrap': False, 'previousTarget': array([83., 23.]), 'currentState': array([82.13722186, 22.2386768 ,  1.91836885]), 'targetState': array([83, 23], dtype=int32), 'currentDistance': 1.1506516115666958}
episode index:182
target Thresh 48.4673430760993
target distance 48.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.00291067, 21.65879853]), 'dynamicTrap': True, 'previousTarget': array([36.00433887, 21.58342373]), 'currentState': array([56.      , 22.      ,  4.340128], dtype=float32), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 43
reward sum = 0.6096986383684022
running average episode reward sum: 0.6930557386820095
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 21.]), 'currentState': array([ 8.19002189, 21.1367565 ,  5.54343214]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.23411676310423424}
episode index:183
target Thresh 48.60466277538799
target distance 13.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'dynamicTrap': False, 'previousTarget': array([11., 11.]), 'currentState': array([22.6335888 , 20.34551081,  2.71655989]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 14.922431461779343}
done in step count: 23
reward sum = 0.7460599780715755
running average episode reward sum: 0.693343805200431
{'scaleFactor': 20, 'currentTarget': array([11., 11.]), 'dynamicTrap': False, 'previousTarget': array([11., 11.]), 'currentState': array([10.04427051, 11.42808008,  6.24479419]), 'targetState': array([11, 11], dtype=int32), 'currentDistance': 1.0472208004889578}
episode index:184
target Thresh 48.741297589819226
target distance 4.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94., 11.]), 'dynamicTrap': False, 'previousTarget': array([94., 11.]), 'currentState': array([97.08389323, 12.62280076,  6.18953735]), 'targetState': array([94, 11], dtype=int32), 'currentDistance': 3.4848069938519766}
done in step count: 10
reward sum = 0.8749780850088044
running average episode reward sum: 0.6943256121183141
{'scaleFactor': 20, 'currentTarget': array([94., 11.]), 'dynamicTrap': False, 'previousTarget': array([94., 11.]), 'currentState': array([94.50441324, 10.69980254,  1.24004599]), 'targetState': array([94, 11], dtype=int32), 'currentDistance': 0.5869848645378593}
episode index:185
target Thresh 48.87725093527049
target distance 43.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.89402866,  4.974252  ]), 'dynamicTrap': False, 'previousTarget': array([82.13385239,  5.31001716]), 'currentState': array([101.72161299,   2.35377508,   3.65071094]), 'targetState': array([59,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 38
reward sum = 0.6463554795347068
running average episode reward sum: 0.6940677081796927
{'scaleFactor': 20, 'currentTarget': array([60.11954129,  8.77510766]), 'dynamicTrap': True, 'previousTarget': array([59.,  8.]), 'currentState': array([60.34395236,  7.84052403,  3.99271346]), 'targetState': array([59,  8], dtype=int32), 'currentDistance': 0.9611487357309384}
episode index:186
target Thresh 49.01252621058249
target distance 12.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.8664818 , 22.03190674]), 'dynamicTrap': True, 'previousTarget': array([70., 22.]), 'currentState': array([58.     , 19.     ,  5.97252], dtype=float32), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 12.247687483966113}
done in step count: 17
reward sum = 0.7750085412909168
running average episode reward sum: 0.6945005468594319
{'scaleFactor': 20, 'currentTarget': array([70., 22.]), 'dynamicTrap': False, 'previousTarget': array([70., 22.]), 'currentState': array([69.72860202, 21.80847628,  0.22133822]), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 0.33217194057396965}
episode index:187
target Thresh 49.14712679764414
target distance 12.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 14.]), 'currentState': array([10.61753918,  3.84032161,  1.78553495]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 10.28763811863089}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6958142681495466
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 14.]), 'currentState': array([ 8.26465689, 13.67857571,  2.95579967]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.8025229381537462}
episode index:188
target Thresh 49.28105606147716
target distance 1.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'dynamicTrap': False, 'previousTarget': array([17., 19.]), 'currentState': array([18.38777564, 19.00537475,  4.82935732]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 1.387786046563203}
done in step count: 4
reward sum = 0.9311920199999999
running average episode reward sum: 0.6970596530799723
{'scaleFactor': 20, 'currentTarget': array([17., 19.]), 'dynamicTrap': False, 'previousTarget': array([17.36959473, 19.44721787]), 'currentState': array([17.99119888, 18.55338697,  3.29720455]), 'targetState': array([17, 19], dtype=int32), 'currentDistance': 1.0871699078352337}
episode index:189
target Thresh 49.4143173503201
target distance 47.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.99583658, 20.93278855]), 'dynamicTrap': False, 'previousTarget': array([62., 21.]), 'currentState': array([81.9957746 , 20.88299477,  3.84406984]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6701762270261951
running average episode reward sum: 0.6969181613638997
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([34.65322971, 20.94950956,  3.14371651]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 0.3504267662666246}
episode index:190
target Thresh 49.54691399571212
target distance 33.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.25376934, 19.56161106]), 'dynamicTrap': False, 'previousTarget': array([28.14048809, 18.80014791]), 'currentState': array([ 9.94400887, 14.35266196,  1.34919602]), 'targetState': array([42, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6875319349397349
running average episode reward sum: 0.6968690188171763
{'scaleFactor': 20, 'currentTarget': array([42., 23.]), 'dynamicTrap': False, 'previousTarget': array([42., 23.]), 'currentState': array([41.44696874, 22.65865115,  5.87997815]), 'targetState': array([42, 23], dtype=int32), 'currentDistance': 0.6498943091086996}
episode index:191
target Thresh 49.67884931257627
target distance 43.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.1025487 , 14.89031633]), 'dynamicTrap': False, 'previousTarget': array([35.85577145, 13.78779003]), 'currentState': array([54.43783219,  9.77692094,  1.22461295]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5654934851664227
running average episode reward sum: 0.6961847712460787
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([12.50217201, 20.98042856,  4.99311757]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 0.5025532500616892}
episode index:192
target Thresh 49.81012659930233
target distance 40.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.12182147, 12.26836986]), 'dynamicTrap': False, 'previousTarget': array([90.97504678, 13.00124766]), 'currentState': array([72.12384208, 12.55265868,  4.64719224]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.535436837221134
running average episode reward sum: 0.6953518803962085
{'scaleFactor': 20, 'currentTarget': array([111.,  12.]), 'dynamicTrap': False, 'previousTarget': array([111.,  12.]), 'currentState': array([110.74765121,  11.84700986,   0.14805017]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 0.29510318976235433}
episode index:193
target Thresh 49.94074913782933
target distance 45.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.41112172,  4.37988401]), 'dynamicTrap': False, 'previousTarget': array([88.04429684,  4.66961979]), 'currentState': array([106.37647111,   5.55666941,   2.39815998]), 'targetState': array([63,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7458554384864713
running average episode reward sum: 0.6956122080152305
{'scaleFactor': 20, 'currentTarget': array([63.,  3.]), 'dynamicTrap': False, 'previousTarget': array([63.,  3.]), 'currentState': array([62.69810587,  2.5839316 ,  2.16567152]), 'targetState': array([63,  3], dtype=int32), 'currentDistance': 0.5140554262344672}
episode index:194
target Thresh 50.070720193727524
target distance 11.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([26.98959746,  3.7383337 ,  4.02273536]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 11.21991028276901}
done in step count: 26
reward sum = 0.6938894471744846
running average episode reward sum: 0.6956033733442524
{'scaleFactor': 20, 'currentTarget': array([16.,  6.]), 'dynamicTrap': False, 'previousTarget': array([16.,  6.]), 'currentState': array([16.22734008,  5.22490666,  1.1813109 ]), 'targetState': array([16,  6], dtype=int32), 'currentDistance': 0.8077457543204043}
episode index:195
target Thresh 50.200043016280055
target distance 11.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([22.50370925,  6.48134145,  3.38148463]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 11.065606461951758}
done in step count: 12
reward sum = 0.8388305661440494
running average episode reward sum: 0.6963341243279249
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([12.42938176,  3.26409862,  4.24423448]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.5040999659948437}
episode index:196
target Thresh 50.32872083856427
target distance 39.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.07233514,   9.01488798]), 'dynamicTrap': False, 'previousTarget': array([98.94108971,  9.53392998]), 'currentState': array([80.19382813,  6.8137612 ,  0.22779053]), 'targetState': array([118,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6787953868694155
running average episode reward sum: 0.6962450952037699
{'scaleFactor': 20, 'currentTarget': array([118.,  11.]), 'dynamicTrap': False, 'previousTarget': array([118.,  11.]), 'currentState': array([117.1248264 ,  10.16780632,   5.70179678]), 'targetState': array([118,  11], dtype=int32), 'currentDistance': 1.207673440172683}
episode index:197
target Thresh 50.45675687753239
target distance 3.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'dynamicTrap': False, 'previousTarget': array([10., 12.]), 'currentState': array([12.04338792, 11.67805148,  3.11861598]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 2.0685949481332098}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.697728705834054
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'dynamicTrap': False, 'previousTarget': array([10., 12.]), 'currentState': array([10.59025592, 12.3650161 ,  2.2728942 ]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 0.6940020218156981}
episode index:198
target Thresh 50.58415433409208
target distance 24.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.56538291, 14.93810429]), 'dynamicTrap': False, 'previousTarget': array([83.2, 14.6]), 'currentState': array([65.45778029,  9.03052253,  0.86405938]), 'targetState': array([88, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7162966210028773
running average episode reward sum: 0.6978220119404299
{'scaleFactor': 20, 'currentTarget': array([88., 16.]), 'dynamicTrap': False, 'previousTarget': array([88., 16.]), 'currentState': array([88.78845631, 16.12724628,  6.09903637]), 'targetState': array([88, 16], dtype=int32), 'currentDistance': 0.7986582315672833}
episode index:199
target Thresh 50.710916393186366
target distance 29.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.86165194, 10.68996284]), 'dynamicTrap': False, 'previousTarget': array([53.25018649, 10.81888192]), 'currentState': array([34.42902261, 18.4512885 ,  4.82613075]), 'targetState': array([64,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.4826130774917211
running average episode reward sum: 0.6967459672681864
{'scaleFactor': 20, 'currentTarget': array([62.84780291,  5.69245481]), 'dynamicTrap': True, 'previousTarget': array([64.,  6.]), 'currentState': array([62.49972023,  5.33320084,  5.72170133]), 'targetState': array([64,  6], dtype=int32), 'currentDistance': 0.5002249144016929}
episode index:200
target Thresh 50.83704622387334
target distance 21.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.72887893, 11.94213298]), 'dynamicTrap': False, 'previousTarget': array([91.64677133, 11.74224216]), 'currentState': array([73.16943234,  7.76744149,  0.81369656]), 'targetState': array([93, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.49126062761677447
running average episode reward sum: 0.6957236521455425
{'scaleFactor': 20, 'currentTarget': array([93., 12.]), 'dynamicTrap': False, 'previousTarget': array([93., 12.]), 'currentState': array([93.01271377, 11.45443852,  3.48356925]), 'targetState': array([93, 12], dtype=int32), 'currentDistance': 0.5457095994557368}
episode index:201
target Thresh 50.962546979405346
target distance 5.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51., 12.]), 'dynamicTrap': False, 'previousTarget': array([51., 12.]), 'currentState': array([49.30543488,  6.00773213,  3.63940912]), 'targetState': array([51, 12], dtype=int32), 'currentDistance': 6.227264662145429}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.696847518691495
{'scaleFactor': 20, 'currentTarget': array([51., 12.]), 'dynamicTrap': False, 'previousTarget': array([51., 12.]), 'currentState': array([50.82972197, 12.67502884,  3.94996202]), 'targetState': array([51, 12], dtype=int32), 'currentDistance': 0.6961742172151958}
episode index:202
target Thresh 51.08742179730781
target distance 4.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 22.]), 'dynamicTrap': False, 'previousTarget': array([70., 22.]), 'currentState': array([66.1700835 , 17.89495237,  5.78283694]), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 5.614238725984542}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.698146772343261
{'scaleFactor': 20, 'currentTarget': array([70., 22.]), 'dynamicTrap': False, 'previousTarget': array([70., 22.]), 'currentState': array([70.13607654, 21.88705223,  2.15372287]), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 0.17684462741938428}
episode index:203
target Thresh 51.21167379945766
target distance 24.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.57271297, 12.43230709]), 'dynamicTrap': False, 'previousTarget': array([24.46153846, 12.30769231]), 'currentState': array([ 6.32175499, 20.61170393,  5.10579717]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6797912072953298
running average episode reward sum: 0.6980567940832221
{'scaleFactor': 20, 'currentTarget': array([30., 10.]), 'dynamicTrap': False, 'previousTarget': array([30., 10.]), 'currentState': array([29.81337916, 10.78701328,  0.27626164]), 'targetState': array([30, 10], dtype=int32), 'currentDistance': 0.808836963068094}
episode index:204
target Thresh 51.335306092161446
target distance 51.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.92001858, 15.81786703]), 'dynamicTrap': False, 'previousTarget': array([33.44957912, 16.21675745]), 'currentState': array([52.4013696 , 11.29271925,  4.33970881]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.3713012640959193
running average episode reward sum: 0.6964628646686499
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.64055934, 23.7708547 ,  1.98388363]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 1.0022640523606747}
episode index:205
target Thresh 51.45832176623291
target distance 11.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 20.]), 'dynamicTrap': False, 'previousTarget': array([72., 20.]), 'currentState': array([66.05492517,  9.1922663 ,  1.25657794]), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 12.334951253077884}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6976065660435933
{'scaleFactor': 20, 'currentTarget': array([72., 20.]), 'dynamicTrap': False, 'previousTarget': array([72., 20.]), 'currentState': array([71.45333487, 19.29228709,  1.35416642]), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 0.8942596562994752}
episode index:206
target Thresh 51.58072389707032
target distance 29.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.72043006, 22.37155223]), 'dynamicTrap': False, 'previousTarget': array([36.01188001, 22.68924552]), 'currentState': array([54.65449693, 20.74890688,  3.17605972]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7181720355391135
running average episode reward sum: 0.6977059161377746
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'dynamicTrap': False, 'previousTarget': array([27., 23.]), 'currentState': array([26.37861047, 22.08912627,  2.31316535]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 1.1026404237755776}
episode index:207
target Thresh 51.702515544733316
target distance 42.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.02423809, 15.9439737 ]), 'dynamicTrap': False, 'previousTarget': array([51., 16.]), 'currentState': array([71.02417337, 15.89309691,  4.14100432]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 35
reward sum = 0.6664271292974996
running average episode reward sum: 0.6975555373548885
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([29.77193409, 16.0564338 ,  3.57856877]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 0.7739941922612157}
episode index:208
target Thresh 51.82369975401944
target distance 49.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.73113654,  6.73162874]), 'dynamicTrap': True, 'previousTarget': array([71.79898987,  7.17157288]), 'currentState': array([52.       , 10.       ,  1.5394511], dtype=float32), 'targetState': array([101,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5422642831776324
running average episode reward sum: 0.6968125169999735
{'scaleFactor': 20, 'currentTarget': array([101.,   3.]), 'dynamicTrap': False, 'previousTarget': array([101.,   3.]), 'currentState': array([101.25398297,   2.14466819,   0.50212685]), 'targetState': array([101,   3], dtype=int32), 'currentDistance': 0.8922442804925067}
episode index:209
target Thresh 51.944279554540216
target distance 17.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.85093062,  6.21792121]), 'dynamicTrap': False, 'previousTarget': array([32.28585494,  6.56139529]), 'currentState': array([43.30558936, 22.61276166,  2.00371015]), 'targetState': array([31,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7893762755042207
running average episode reward sum: 0.6972532968023747
{'scaleFactor': 20, 'currentTarget': array([31.,  5.]), 'dynamicTrap': False, 'previousTarget': array([31.,  5.]), 'currentState': array([31.47142412,  5.78600504,  6.19451814]), 'targetState': array([31,  5], dtype=int32), 'currentDistance': 0.916539488540835}
episode index:210
target Thresh 52.06425796079698
target distance 48.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.84581673, 11.47201877]), 'dynamicTrap': False, 'previousTarget': array([49.8, 10.6]), 'currentState': array([68.15272046,  6.25249117,  3.17555892]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.2888639409604783
running average episode reward sum: 0.695317802224925
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'dynamicTrap': False, 'previousTarget': array([21., 19.]), 'currentState': array([20.84597739, 19.14405437,  4.989393  ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.2108900815904113}
episode index:211
target Thresh 52.183637972256086
target distance 11.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([30.22643074, 15.70621175,  4.06028259]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 11.510242689047942}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6962612949231975
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([26.77275046,  5.99156195,  2.40948158]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.2571151047452114}
episode index:212
target Thresh 52.30242257342407
target distance 29.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.39181672, 12.60841541]), 'dynamicTrap': False, 'previousTarget': array([23.10128274, 11.9279843 ]), 'currentState': array([5.12170249, 7.25465996, 6.11447519]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7376041259462709
running average episode reward sum: 0.696455392721428
{'scaleFactor': 20, 'currentTarget': array([33., 15.]), 'dynamicTrap': False, 'previousTarget': array([33., 15.]), 'currentState': array([33.25667557, 14.76427994,  1.88572074]), 'targetState': array([33, 15], dtype=int32), 'currentDistance': 0.3484914589986533}
episode index:213
target Thresh 52.420614733922136
target distance 11.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([11.55246614,  8.45826369,  0.85856676]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 9.557716812343854}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6975563738204259
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([10.6781137 , 17.97699065,  3.66979653]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.32270764151839776}
episode index:214
target Thresh 52.53821740856047
target distance 13.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 19.]), 'currentState': array([3.07380277, 7.76324446, 0.66482067]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 12.703719479437838}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6986037613581352
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.21353166, 18.66811067,  2.26799509]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.8536292997655918}
episode index:215
target Thresh 52.65523353741203
target distance 35.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.1670692 , 13.09746684]), 'dynamicTrap': False, 'previousTarget': array([95.91891892, 13.51351351]), 'currentState': array([77.12941127, 19.22670667,  0.85411632]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4803399033098399
running average episode reward sum: 0.6975932805338375
{'scaleFactor': 20, 'currentTarget': array([112.,   8.]), 'dynamicTrap': False, 'previousTarget': array([112.,   8.]), 'currentState': array([112.67491243,   7.92166166,   1.62580269]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 0.679443653214615}
episode index:216
target Thresh 52.771666045886164
target distance 21.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.15841416,  7.73663008]), 'dynamicTrap': False, 'previousTarget': array([64.83071558,  8.58173352]), 'currentState': array([49.62357126, 20.3330048 ,  5.42882085]), 'targetState': array([71,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.606366784649433
running average episode reward sum: 0.6971728819352918
{'scaleFactor': 20, 'currentTarget': array([71.,  3.]), 'dynamicTrap': False, 'previousTarget': array([71.,  3.]), 'currentState': array([70.14390427,  3.92439332,  3.83710285]), 'targetState': array([71,  3], dtype=int32), 'currentDistance': 1.259921786674017}
episode index:217
target Thresh 52.887517844801636
target distance 23.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.65532547, 15.04351609]), 'dynamicTrap': False, 'previousTarget': array([15.65859887, 15.02547777]), 'currentState': array([33.96861921, 23.08237623,  4.21333349]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5877514914769568
running average episode reward sum: 0.696670948951538
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'dynamicTrap': False, 'previousTarget': array([11., 13.]), 'currentState': array([10.56832806, 13.45240564,  5.7202676 ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.625309145548069}
episode index:218
target Thresh 53.002791830459444
target distance 24.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.89600954, 19.02664651]), 'dynamicTrap': False, 'previousTarget': array([84., 19.]), 'currentState': array([103.89554178,  19.16343206,   2.24050899]), 'targetState': array([80, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7896615821251509
running average episode reward sum: 0.6970955637148878
{'scaleFactor': 20, 'currentTarget': array([80., 19.]), 'dynamicTrap': False, 'previousTarget': array([80., 19.]), 'currentState': array([79.52650409, 19.23986055,  4.87403373]), 'targetState': array([80, 19], dtype=int32), 'currentDistance': 0.5307838160192723}
episode index:219
target Thresh 53.11749088471524
target distance 45.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.94449581,  5.57125834]), 'dynamicTrap': False, 'previousTarget': array([30.23767062,  5.92585987]), 'currentState': array([48.72569025,  8.52157127,  2.49075031]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5665063135834569
running average episode reward sum: 0.6913519188180771
{'scaleFactor': 20, 'currentTarget': array([23.58553228,  3.0364479 ]), 'dynamicTrap': True, 'previousTarget': array([23.58762532,  3.0118226 ]), 'currentState': array([43.51472222,  4.7179319 ,  4.75348956]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:220
target Thresh 53.231617875051356
target distance 36.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.08229715, 11.52811054]), 'dynamicTrap': False, 'previousTarget': array([73.72964181, 11.35287728]), 'currentState': array([91.25719917,  5.84276689,  2.33937919]), 'targetState': array([57, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4815818274195207
running average episode reward sum: 0.690402732884147
{'scaleFactor': 20, 'currentTarget': array([57., 16.]), 'dynamicTrap': False, 'previousTarget': array([57., 16.]), 'currentState': array([57.00154421, 16.08567611,  2.34908597]), 'targetState': array([57, 16], dtype=int32), 'currentDistance': 0.08569002693808886}
episode index:221
target Thresh 53.3451756546485
target distance 26.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.16581761,  9.28410678]), 'dynamicTrap': True, 'previousTarget': array([80.48782391, 10.50280987]), 'currentState': array([61.       , 15.       ,  0.5584596], dtype=float32), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6432893912304545
running average episode reward sum: 0.6901905106244457
{'scaleFactor': 20, 'currentTarget': array([87.,  9.]), 'dynamicTrap': False, 'previousTarget': array([87.,  9.]), 'currentState': array([86.42591073,  9.73005357,  4.443581  ]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 0.9287393065346856}
episode index:222
target Thresh 53.45816706245707
target distance 42.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.19944708, 13.31977481]), 'dynamicTrap': False, 'previousTarget': array([30.77784154, 13.7566426 ]), 'currentState': array([47.33560257, 21.75066229,  4.30029345]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.19395287032937536
running average episode reward sum: 0.6879652297262616
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([6.49689605, 2.80174392, 5.29348664]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.5407578511548148}
episode index:223
target Thresh 53.570594923268146
target distance 8.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 15.]), 'currentState': array([ 2.89025857, 21.03769302,  4.33012742]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 6.395681816993727}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6891823314239122
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.11843875, 14.95700206,  4.47932404]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.12600222440338507}
episode index:224
target Thresh 53.68246204778411
target distance 11.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63., 23.]), 'dynamicTrap': False, 'previousTarget': array([63., 23.]), 'currentState': array([52.09753177, 19.78276631,  1.01063251]), 'targetState': array([63, 23], dtype=int32), 'currentDistance': 11.36725147627197}
done in step count: 8
reward sum = 0.91284469442792
running average episode reward sum: 0.6901763863705966
{'scaleFactor': 20, 'currentTarget': array([63., 23.]), 'dynamicTrap': False, 'previousTarget': array([63., 23.]), 'currentState': array([63.00370203, 23.82175317,  0.15876751]), 'targetState': array([63, 23], dtype=int32), 'currentDistance': 0.8217615059038481}
episode index:225
target Thresh 53.793771232688876
target distance 40.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.43254873,  6.11606435]), 'dynamicTrap': False, 'previousTarget': array([31.00624707,  6.50015618]), 'currentState': array([49.43215226,  6.24199598,  2.6842432 ]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6626260405848385
running average episode reward sum: 0.6900544821857039
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([11.,  6.]), 'currentState': array([10.6943462 ,  6.66593397,  2.75319429]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.7327293436801391}
episode index:226
target Thresh 53.90452526071789
target distance 5.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42., 17.]), 'dynamicTrap': False, 'previousTarget': array([42., 17.]), 'currentState': array([45.62428258, 21.28008528,  2.61370707]), 'targetState': array([42, 17], dtype=int32), 'currentDistance': 5.60843598711041}
done in step count: 17
reward sum = 0.7573252683927313
running average episode reward sum: 0.6903508292615057
{'scaleFactor': 20, 'currentTarget': array([42., 17.]), 'dynamicTrap': False, 'previousTarget': array([42., 17.]), 'currentState': array([42.55127521, 17.79724211,  4.16100209]), 'targetState': array([42, 17], dtype=int32), 'currentDistance': 0.9692777453734859}
episode index:227
target Thresh 54.01472690072764
target distance 21.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.37303752,  3.81637388]), 'dynamicTrap': False, 'previousTarget': array([48.3102453 ,  3.88009345]), 'currentState': array([67.28519544, 10.32254656,  6.10603666]), 'targetState': array([46,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7555735117785243
running average episode reward sum: 0.6906368936585101
{'scaleFactor': 20, 'currentTarget': array([46.,  3.]), 'dynamicTrap': False, 'previousTarget': array([46.,  3.]), 'currentState': array([46.98930931,  3.29077694,  3.16671863]), 'targetState': array([46,  3], dtype=int32), 'currentDistance': 1.0311566944152248}
episode index:228
target Thresh 54.12437890776481
target distance 7.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([112.50796532,   4.85615998,   3.10240471]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 8.295342233806116}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.691691166384486
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([106.13480057,   9.07156967,   3.16463858]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 0.9381652707402138}
episode index:229
target Thresh 54.23348402313533
target distance 24.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.30161886, 17.44608792]), 'dynamicTrap': False, 'previousTarget': array([59.93091516, 17.6609096 ]), 'currentState': array([40.52222926, 14.48369914,  6.05407524]), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7907847184524244
running average episode reward sum: 0.6921220079152162
{'scaleFactor': 20, 'currentTarget': array([64., 18.]), 'dynamicTrap': False, 'previousTarget': array([64., 18.]), 'currentState': array([64.83005216, 18.1806236 ,  0.15670099]), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 0.8494771809623155}
episode index:230
target Thresh 54.342044974472756
target distance 39.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.60947616, 14.20049667]), 'dynamicTrap': False, 'previousTarget': array([68.50337237, 14.59112713]), 'currentState': array([51.39706519,  5.93562604,  0.22910231]), 'targetState': array([89, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 59
reward sum = 0.3978303619849397
running average episode reward sum: 0.6908480181059942
{'scaleFactor': 20, 'currentTarget': array([89., 23.]), 'dynamicTrap': False, 'previousTarget': array([89., 23.]), 'currentState': array([8.92309535e+01, 2.32468781e+01, 5.54392201e-02]), 'targetState': array([89, 23], dtype=int32), 'currentDistance': 0.3380655728050597}
episode index:231
target Thresh 54.45006447580652
target distance 12.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([22.80046123, 10.99284911,  3.27841444]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 16.13717001079175}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6918077992671047
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([11.23392923, 22.29723673,  2.45242029]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.37824933157121604}
episode index:232
target Thresh 54.557545227629795
target distance 8.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,   6.]), 'dynamicTrap': False, 'previousTarget': array([109.,   6.]), 'currentState': array([115.04469323,  10.66429248,   3.08948827]), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 7.635046861020704}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6929201694414949
{'scaleFactor': 20, 'currentTarget': array([109.,   6.]), 'dynamicTrap': False, 'previousTarget': array([109.,   6.]), 'currentState': array([108.80848666,   6.17863161,   2.51898313]), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 0.26189046221795964}
episode index:233
target Thresh 54.664489916966964
target distance 53.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.32194983,  3.89929732]), 'dynamicTrap': False, 'previousTarget': array([82.98577525,  4.7541802 ]), 'currentState': array([64.36578097,  2.57592112,  5.18969876]), 'targetState': array([116,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4607590107952339
running average episode reward sum: 0.6919280277378784
{'scaleFactor': 20, 'currentTarget': array([116.,   6.]), 'dynamicTrap': False, 'previousTarget': array([116.,   6.]), 'currentState': array([116.11151139,   5.42791195,   1.10989242]), 'targetState': array([116,   6], dtype=int32), 'currentDistance': 0.5828546359563418}
episode index:234
target Thresh 54.77090121744082
target distance 20.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 12.]), 'dynamicTrap': False, 'previousTarget': array([86.1565257 , 12.25304229]), 'currentState': array([68.50383703, 16.81638108,  6.133515  ]), 'targetState': array([87, 12], dtype=int32), 'currentDistance': 19.11296866478176}
done in step count: 33
reward sum = 0.614094063832961
running average episode reward sum: 0.6915968193808363
{'scaleFactor': 20, 'currentTarget': array([87., 12.]), 'dynamicTrap': False, 'previousTarget': array([87., 12.]), 'currentState': array([87.13143827, 12.05777911,  0.37069972]), 'targetState': array([87, 12], dtype=int32), 'currentDistance': 0.14357730860116136}
episode index:235
target Thresh 54.87678178933944
target distance 3.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'dynamicTrap': False, 'previousTarget': array([18., 18.]), 'currentState': array([16.84339385, 18.3167539 ,  6.25192904]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 1.1991959103021803}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6928612396376972
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'dynamicTrap': False, 'previousTarget': array([18., 18.]), 'currentState': array([17.6537012 , 17.51910335,  4.74431145]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.5926081708167485}
episode index:236
target Thresh 54.98213427968262
target distance 20.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.97808138,  2.13998724]), 'dynamicTrap': False, 'previousTarget': array([62.96680906,  2.22127294]), 'currentState': array([59.88425912, 21.89924491,  3.92138081]), 'targetState': array([63,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7339545496343808
running average episode reward sum: 0.693034629131354
{'scaleFactor': 20, 'currentTarget': array([63.,  2.]), 'dynamicTrap': False, 'previousTarget': array([63.,  2.]), 'currentState': array([62.25605062,  1.16426103,  5.80958859]), 'targetState': array([63,  2], dtype=int32), 'currentDistance': 1.1188924444447792}
episode index:237
target Thresh 55.08696132228812
target distance 27.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.39796787,  3.43831939]), 'dynamicTrap': False, 'previousTarget': array([25.05464491,  3.52256629]), 'currentState': array([43.33235643,  5.05700797,  3.82596481]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 22
reward sum = 0.7300751824949883
running average episode reward sum: 0.6931902617085122
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([18.84479737,  3.65916325,  4.37837408]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.0715310486390128}
episode index:238
target Thresh 55.19126553783744
target distance 10.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  12.]), 'dynamicTrap': False, 'previousTarget': array([112.,  12.]), 'currentState': array([113.37564929,  20.83324086,   3.77548186]), 'targetState': array([112,  12], dtype=int32), 'currentDistance': 8.939717844089108}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6942291315314933
{'scaleFactor': 20, 'currentTarget': array([112.,  12.]), 'dynamicTrap': False, 'previousTarget': array([112.,  12.]), 'currentState': array([111.99485611,  11.90022045,   4.51466434]), 'targetState': array([112,  12], dtype=int32), 'currentDistance': 0.09991204993347037}
episode index:239
target Thresh 55.29504953394142
target distance 38.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.66190431, 10.34144895]), 'dynamicTrap': False, 'previousTarget': array([84.66906377, 10.37675141]), 'currentState': array([64.9858766 , 13.92668721,  0.37048197]), 'targetState': array([103,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5436181222064588
running average episode reward sum: 0.6936015856593056
{'scaleFactor': 20, 'currentTarget': array([103.,   7.]), 'dynamicTrap': False, 'previousTarget': array([103.,   7.]), 'currentState': array([103.95263602,   7.23372514,   1.13826849]), 'targetState': array([103,   7], dtype=int32), 'currentDistance': 0.9808887928478838}
episode index:240
target Thresh 55.39831590520537
target distance 8.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.20039757, 22.56052656]), 'dynamicTrap': True, 'previousTarget': array([76., 22.]), 'currentState': array([84.       , 23.       ,  5.3590336], dtype=float32), 'targetState': array([76, 22], dtype=int32), 'currentDistance': 7.811973820879844}
done in step count: 8
reward sum = 0.88334070442792
running average episode reward sum: 0.6943888849073082
{'scaleFactor': 20, 'currentTarget': array([76., 22.]), 'dynamicTrap': False, 'previousTarget': array([76., 22.]), 'currentState': array([76.87367753, 21.96883028,  3.53731015]), 'targetState': array([76, 22], dtype=int32), 'currentDistance': 0.8742333681084369}
episode index:241
target Thresh 55.50106723329395
target distance 6.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([110.33731084,   6.68055206,   0.78714347]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 4.369987120242734}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6955290093498401
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([111.99967261,  10.44029773,   1.49019736]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 1.145692787473235}
episode index:242
target Thresh 55.6033060869957
target distance 40.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.76765481, 15.61264281]), 'dynamicTrap': False, 'previousTarget': array([89.0992562 , 15.99007438]), 'currentState': array([107.60777895,  13.08886115,   4.21246731]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5070344881445834
running average episode reward sum: 0.6947533117317114
{'scaleFactor': 20, 'currentTarget': array([69., 18.]), 'dynamicTrap': False, 'previousTarget': array([69., 18.]), 'currentState': array([69.7258017 , 18.03020072,  3.06348684]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 0.7264297546938829}
episode index:243
target Thresh 55.705035022287305
target distance 31.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.84457673,  11.81919044]), 'dynamicTrap': False, 'previousTarget': array([102.65136197,  11.21988205]), 'currentState': array([83.93198478,  5.31427956,  0.80443776]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7252444389279828
running average episode reward sum: 0.6948782753677616
{'scaleFactor': 20, 'currentTarget': array([115.,  16.]), 'dynamicTrap': False, 'previousTarget': array([115.,  16.]), 'currentState': array([115.3981327 ,  15.57029521,   0.36140202]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 0.5857950615385797}
episode index:244
target Thresh 55.806256582397424
target distance 42.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.09078242,  4.31452357]), 'dynamicTrap': False, 'previousTarget': array([31.,  5.]), 'currentState': array([11.10056408,  3.68908711,  5.65164077]), 'targetState': array([53,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.641432712553823
running average episode reward sum: 0.6946601302134191
{'scaleFactor': 20, 'currentTarget': array([53.,  5.]), 'dynamicTrap': False, 'previousTarget': array([53.,  5.]), 'currentState': array([53.27837379,  5.0272606 ,  5.89387223]), 'targetState': array([53,  5], dtype=int32), 'currentDistance': 0.27970539719979376}
episode index:245
target Thresh 55.90697329787036
target distance 7.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.80201789, 12.02827353]), 'dynamicTrap': True, 'previousTarget': array([89., 12.]), 'currentState': array([82.      , 16.      ,  6.157814], dtype=float32), 'targetState': array([89, 12], dtype=int32), 'currentDistance': 7.876678145358519}
done in step count: 16
reward sum = 0.7835231190018656
running average episode reward sum: 0.6950213618751607
{'scaleFactor': 20, 'currentTarget': array([89., 12.]), 'dynamicTrap': False, 'previousTarget': array([89., 12.]), 'currentState': array([88.34625994, 12.78646802,  3.93363541]), 'targetState': array([89, 12], dtype=int32), 'currentDistance': 1.022696447214319}
episode index:246
target Thresh 56.00718768662922
target distance 14.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 21.]), 'dynamicTrap': False, 'previousTarget': array([98., 21.]), 'currentState': array([88.36004958,  8.55655117,  0.9508636 ]), 'targetState': array([98, 21], dtype=int32), 'currentDistance': 15.740650014249615}
done in step count: 18
reward sum = 0.7703470586002126
running average episode reward sum: 0.69532632421008
{'scaleFactor': 20, 'currentTarget': array([98., 21.]), 'dynamicTrap': False, 'previousTarget': array([98., 21.]), 'currentState': array([98.44045302, 21.09474862,  1.39850673]), 'targetState': array([98, 21], dtype=int32), 'currentDistance': 0.45052875692756017}
episode index:247
target Thresh 56.10690225403895
target distance 10.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.97435356, 14.8600591 ]), 'dynamicTrap': True, 'previousTarget': array([24., 16.]), 'currentState': array([14.       , 14.       ,  0.3341947], dtype=float32), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 9.015471341129192}
done in step count: 14
reward sum = 0.8199270212749682
running average episode reward sum: 0.6958287463756642
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'dynamicTrap': False, 'previousTarget': array([24., 16.]), 'currentState': array([23.8271138 , 15.053576  ,  0.60837133]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.962085245038001}
episode index:248
target Thresh 56.20611949296893
target distance 28.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.38957985, 14.56612833]), 'dynamicTrap': False, 'previousTarget': array([53.38497013, 14.68695255]), 'currentState': array([72.08651538, 21.66715655,  2.83509707]), 'targetState': array([44, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7568871899953818
running average episode reward sum: 0.6960739610086751
{'scaleFactor': 20, 'currentTarget': array([44., 11.]), 'dynamicTrap': False, 'previousTarget': array([44., 11.]), 'currentState': array([43.8644395 , 10.58754414,  4.28524669]), 'targetState': array([44, 11], dtype=int32), 'currentDistance': 0.4341618177617746}
episode index:249
target Thresh 56.30484188385532
target distance 52.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.40524813,  8.27139115]), 'dynamicTrap': False, 'previousTarget': array([42.03320117,  8.8480693 ]), 'currentState': array([60.3877862 ,  9.10695815,  2.94590163]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.059934938008144645
running average episode reward sum: 0.6930499254126078
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([9.48991951, 7.43976318, 2.49467132]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.6734788472329873}
episode index:250
target Thresh 56.403071894763
target distance 10.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.,  8.]), 'dynamicTrap': False, 'previousTarget': array([41.,  8.]), 'currentState': array([51.26265834,  9.17790465,  0.77094007]), 'targetState': array([41,  8], dtype=int32), 'currentDistance': 10.33003463260352}
done in step count: 8
reward sum = 0.9037239328339101
running average episode reward sum: 0.693889264087593
{'scaleFactor': 20, 'currentTarget': array([41.,  8.]), 'dynamicTrap': False, 'previousTarget': array([41.,  8.]), 'currentState': array([41.8400442 ,  7.97095161,  3.17399212]), 'targetState': array([41,  8], dtype=int32), 'currentDistance': 0.8405462866379122}
episode index:251
target Thresh 56.50081198144738
target distance 26.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.13758898,  9.72057041]), 'dynamicTrap': False, 'previousTarget': array([72.13182128,  9.70751784]), 'currentState': array([92.0011634 , 12.05261057,  3.84527063]), 'targetState': array([66,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8073813856825759
running average episode reward sum: 0.6943396296494779
{'scaleFactor': 20, 'currentTarget': array([66.,  9.]), 'dynamicTrap': False, 'previousTarget': array([66.,  9.]), 'currentState': array([65.29023432,  8.35928477,  2.16441988]), 'targetState': array([66,  9], dtype=int32), 'currentDistance': 0.9561816373533633}
episode index:252
target Thresh 56.598064587415706
target distance 28.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.39257435, 14.04867038]), 'dynamicTrap': False, 'previousTarget': array([30.31144849, 13.51581277]), 'currentState': array([49.22899929, 11.4959766 ,  3.05667436]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5106569293714578
running average episode reward sum: 0.6895767974003834
{'scaleFactor': 20, 'currentTarget': array([26.41846104, 14.69044764]), 'dynamicTrap': True, 'previousTarget': array([26.36689349, 14.74561293]), 'currentState': array([25.50209253, 16.4346967 ,  3.29828882]), 'targetState': array([22, 15], dtype=int32), 'currentDistance': 1.970313693357054}
episode index:253
target Thresh 56.6948321439882
target distance 30.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.42200807, 19.49395   ]), 'dynamicTrap': False, 'previousTarget': array([83.9007438 , 18.99007438]), 'currentState': array([65.4567204 , 18.31611795,  1.07121712]), 'targetState': array([94, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8345137614500875
running average episode reward sum: 0.690147415369083
{'scaleFactor': 20, 'currentTarget': array([92.78001236, 20.28800288]), 'dynamicTrap': True, 'previousTarget': array([94., 20.]), 'currentState': array([92.88670044, 19.64730234,  5.21201407]), 'targetState': array([94, 20], dtype=int32), 'currentDistance': 0.6495225471397214}
episode index:254
target Thresh 56.79111707035882
target distance 13.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([109.59615486,   6.65020433,   5.227391  ]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 13.852039917037434}
done in step count: 12
reward sum = 0.8578542096231192
running average episode reward sum: 0.69080508907204
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([95.69187017,  3.68258356,  3.29871339]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 0.4423767487970549}
episode index:255
target Thresh 56.88692177365574
target distance 32.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.67461729, 17.55964531]), 'dynamicTrap': False, 'previousTarget': array([47.59715  , 16.8507125]), 'currentState': array([67.31390429, 13.77832196,  0.61348939]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7004578701557224
running average episode reward sum: 0.6908427952481482
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([34.27020912, 19.90915629,  4.48275062]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 0.7354232143913212}
episode index:256
target Thresh 56.98224864900152
target distance 34.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.80509201,  8.7030606 ]), 'dynamicTrap': False, 'previousTarget': array([94.66589544,  8.88214879]), 'currentState': array([112.01784999,  14.25913212,   2.95673647]), 'targetState': array([80,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 33
reward sum = 0.6305065668286622
running average episode reward sum: 0.6906080239313409
{'scaleFactor': 20, 'currentTarget': array([80.33956018,  5.76235437]), 'dynamicTrap': True, 'previousTarget': array([80.,  5.]), 'currentState': array([79.89382884,  6.17896071,  4.08915074]), 'targetState': array([80,  5], dtype=int32), 'currentDistance': 0.610112506307716}
episode index:257
target Thresh 57.07710007957302
target distance 46.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.58510464, 15.55107738]), 'dynamicTrap': False, 'previousTarget': array([50.98112317, 14.86874449]), 'currentState': array([32.58848469, 15.18339466,  0.67710532]), 'targetState': array([77, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5236707645477614
running average episode reward sum: 0.6899609802903193
{'scaleFactor': 20, 'currentTarget': array([77., 16.]), 'dynamicTrap': False, 'previousTarget': array([77., 16.]), 'currentState': array([76.43894393, 16.13196577,  1.47767065]), 'targetState': array([77, 16], dtype=int32), 'currentDistance': 0.5763669603560826}
episode index:258
target Thresh 57.17147843666094
target distance 39.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.28927692,  7.45678013]), 'dynamicTrap': False, 'previousTarget': array([23.31457586,  7.53328126]), 'currentState': array([42.96016744,  3.84346219,  3.47957468]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 89
reward sum = -0.06226986442056415
running average episode reward sum: 0.6870566140945245
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.67781923, 11.22708329,  2.06315103]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.7148466480878498}
episode index:259
target Thresh 57.26538607972912
target distance 26.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.82440179, 15.13180848]), 'dynamicTrap': False, 'previousTarget': array([68.44384383, 15.38419958]), 'currentState': array([52.81309539,  3.14689849,  0.14582139]), 'targetState': array([78, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.31458183560984376
running average episode reward sum: 0.6856240187926603
{'scaleFactor': 20, 'currentTarget': array([78., 22.]), 'dynamicTrap': False, 'previousTarget': array([78., 22.]), 'currentState': array([77.81030976, 22.0252627 ,  2.1548223 ]), 'targetState': array([78, 22], dtype=int32), 'currentDistance': 0.19136507429531177}
episode index:260
target Thresh 57.35882535647354
target distance 9.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 18.]), 'currentState': array([14.32400433, 22.84694247,  4.24266338]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 8.78259020864786}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6866407468811943
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.07084242, 18.14769519,  3.90210763]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.1638063374138382}
episode index:261
target Thresh 57.45179860288096
target distance 9.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'dynamicTrap': False, 'previousTarget': array([13., 14.]), 'currentState': array([ 5.21478776, 19.83523446,  0.24562043]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 9.729310906379625}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6876497136866095
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'dynamicTrap': False, 'previousTarget': array([13., 14.]), 'currentState': array([12.90071559, 14.59222165,  5.76267867]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.6004863653929388}
episode index:262
target Thresh 57.544308143287424
target distance 23.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.82974052, 16.45973731]), 'dynamicTrap': False, 'previousTarget': array([95.90788956, 16.1235743 ]), 'currentState': array([109.27468629,   2.62703904,   2.49106133]), 'targetState': array([88, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6144301556561386
running average episode reward sum: 0.687371312325277
{'scaleFactor': 20, 'currentTarget': array([88., 23.]), 'dynamicTrap': False, 'previousTarget': array([88., 23.]), 'currentState': array([88.65816668, 23.13455237,  3.58009262]), 'targetState': array([88, 23], dtype=int32), 'currentDistance': 0.6717795179583153}
episode index:263
target Thresh 57.63635629043621
target distance 30.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.59290423, 14.92658664]), 'dynamicTrap': False, 'previousTarget': array([18.38838649, 15.0776773 ]), 'currentState': array([36.10840934, 19.30209352,  3.30747604]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6879286700871134
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([ 7.63677905, 13.32264262,  4.2581239 ]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.4858268379736877}
episode index:264
target Thresh 57.72794534553584
target distance 20.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.1327443 , 6.98909267]), 'dynamicTrap': False, 'previousTarget': array([8.0992562 , 6.99007438]), 'currentState': array([28.06556856,  5.3512535 ,  3.51784277]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6886441129282903
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'dynamicTrap': False, 'previousTarget': array([8., 7.]), 'currentState': array([8.05940404, 6.53468593, 2.1402012 ]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.4690906355608394}
episode index:265
target Thresh 57.81907759831742
target distance 15.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.,  6.]), 'dynamicTrap': False, 'previousTarget': array([84.,  6.]), 'currentState': array([70.94366352, 20.86975893,  0.34093916]), 'targetState': array([84,  6], dtype=int32), 'currentDistance': 19.788321124963044}
done in step count: 24
reward sum = 0.6789937887435185
running average episode reward sum: 0.6886078335140619
{'scaleFactor': 20, 'currentTarget': array([84.,  6.]), 'dynamicTrap': False, 'previousTarget': array([84.,  6.]), 'currentState': array([83.14353437,  6.49219642,  5.23868016]), 'targetState': array([84,  6], dtype=int32), 'currentDistance': 0.9878211798448285}
episode index:266
target Thresh 57.909755327092036
target distance 25.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.35743179, 17.47617325]), 'dynamicTrap': False, 'previousTarget': array([26.25928039, 17.39259851]), 'currentState': array([ 8.35486951, 11.23897619,  0.47548395]), 'targetState': array([32, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7895179855315589
running average episode reward sum: 0.6889857741583221
{'scaleFactor': 20, 'currentTarget': array([32., 19.]), 'dynamicTrap': False, 'previousTarget': array([32., 19.]), 'currentState': array([31.64809983, 18.08302688,  0.07306841]), 'targetState': array([32, 19], dtype=int32), 'currentDistance': 0.9821779050019881}
episode index:267
target Thresh 57.999980798807634
target distance 18.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  4.]), 'dynamicTrap': False, 'previousTarget': array([32.,  4.]), 'currentState': array([48.40557935, 10.72862511,  4.29594421]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 17.731819692339716}
done in step count: 22
reward sum = 0.6935455442818087
running average episode reward sum: 0.6890027882259471
{'scaleFactor': 20, 'currentTarget': array([32.,  4.]), 'dynamicTrap': False, 'previousTarget': array([32.,  4.]), 'currentState': array([32.89207594,  3.94521868,  4.01535661]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 0.8937563842700859}
episode index:268
target Thresh 58.089756269105706
target distance 55.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.48396849, 17.74403592]), 'dynamicTrap': False, 'previousTarget': array([36.29527642, 17.73765188]), 'currentState': array([17.19668841, 23.03561671,  0.12037199]), 'targetState': array([72,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6267947456826318
running average episode reward sum: 0.6887715315622173
{'scaleFactor': 20, 'currentTarget': array([72.,  8.]), 'dynamicTrap': False, 'previousTarget': array([72.,  8.]), 'currentState': array([71.86830878,  8.01770741,  4.88607495]), 'targetState': array([72,  8], dtype=int32), 'currentDistance': 0.13287637129054083}
episode index:269
target Thresh 58.17908398237767
target distance 34.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.66275238, 10.18156811]), 'dynamicTrap': False, 'previousTarget': array([43.96548746, 10.82555956]), 'currentState': array([24.66460543, 10.45381511,  6.12834835]), 'targetState': array([58, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7750661832203063
running average episode reward sum: 0.6890911413831732
{'scaleFactor': 20, 'currentTarget': array([57.49003874,  9.83134059]), 'dynamicTrap': True, 'previousTarget': array([58., 10.]), 'currentState': array([57.21813912,  8.92204474,  0.45808408]), 'targetState': array([58, 10], dtype=int32), 'currentDistance': 0.9490776267841509}
episode index:270
target Thresh 58.26796617182102
target distance 25.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.12817929,  6.30487973]), 'dynamicTrap': False, 'previousTarget': array([52.18225176,  6.22561063]), 'currentState': array([70.84791228, 13.3455897 ,  4.229985  ]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6896277562173685
{'scaleFactor': 20, 'currentTarget': array([46.,  4.]), 'dynamicTrap': False, 'previousTarget': array([46.,  4.]), 'currentState': array([46.95762643,  3.04947331,  3.7467275 ]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 1.3492773514294523}
episode index:271
target Thresh 58.35640505949512
target distance 15.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  21.]), 'dynamicTrap': False, 'previousTarget': array([108.,  21.]), 'currentState': array([110.16677377,   7.21285904,   1.81476229]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 13.956366445842145}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6904508793470239
{'scaleFactor': 20, 'currentTarget': array([108.,  21.]), 'dynamicTrap': False, 'previousTarget': array([108.,  21.]), 'currentState': array([107.48860961,  20.80040679,   6.10507196]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 0.5489604534460187}
episode index:272
target Thresh 58.44440285637678
target distance 42.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.16491512, 10.98917037]), 'dynamicTrap': False, 'previousTarget': array([73., 11.]), 'currentState': array([91.16491224, 10.97842932,  3.69062865]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 49
reward sum = 0.451245269282287
running average episode reward sum: 0.6895746683211457
{'scaleFactor': 20, 'currentTarget': array([51., 11.]), 'dynamicTrap': False, 'previousTarget': array([51., 11.]), 'currentState': array([50.9479005 , 10.38101268,  2.01985742]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 0.6211760334841356}
episode index:273
target Thresh 58.531961762415484
target distance 3.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([11.36487895, 18.86681858,  3.92786556]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 2.312554033497661}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6906711111374919
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([10.27571221, 17.23226675,  4.3253347 ]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.360506676527267}
episode index:274
target Thresh 58.61908396658846
target distance 52.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.56088955, 11.18278993]), 'dynamicTrap': False, 'previousTarget': array([75.70701012, 11.41082867]), 'currentState': array([57.91640946,  7.42853872,  6.01823314]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5117181358027293
running average episode reward sum: 0.6900203730453656
{'scaleFactor': 20, 'currentTarget': array([108.,  17.]), 'dynamicTrap': False, 'previousTarget': array([108.,  17.]), 'currentState': array([107.11987638,  16.47188242,   2.12070382]), 'targetState': array([108,  17], dtype=int32), 'currentDistance': 1.0264140283494374}
episode index:275
target Thresh 58.705771646955334
target distance 58.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.6157249 , 10.61641224]), 'dynamicTrap': False, 'previousTarget': array([51., 10.]), 'currentState': array([69.61289145, 10.95305717,  3.54958928]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4799289049401969
running average episode reward sum: 0.68925917207397
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'dynamicTrap': False, 'previousTarget': array([13., 10.]), 'currentState': array([13.33276963, 10.38182913,  3.24158673]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 0.506487029023167}
episode index:276
target Thresh 58.792026970712655
target distance 34.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.88843132, 11.01668562]), 'dynamicTrap': False, 'previousTarget': array([24.29835603, 10.07280413]), 'currentState': array([6.29299472, 3.65395938, 0.38546777]), 'targetState': array([40, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6646646159033764
running average episode reward sum: 0.6891703830625239
{'scaleFactor': 20, 'currentTarget': array([40., 17.]), 'dynamicTrap': False, 'previousTarget': array([40., 17.]), 'currentState': array([39.6957921 , 16.69643237,  6.05469419]), 'targetState': array([40, 17], dtype=int32), 'currentDistance': 0.4297624396678319}
episode index:277
target Thresh 58.877852094247984
target distance 20.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.40056408,   6.29076362]), 'dynamicTrap': False, 'previousTarget': array([115.56953382,   6.57218647]), 'currentState': array([98.40579086, 15.01934523,  5.99376113]), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8043233427849268
running average episode reward sum: 0.6895846023421008
{'scaleFactor': 20, 'currentTarget': array([117.,   6.]), 'dynamicTrap': False, 'previousTarget': array([117.,   6.]), 'currentState': array([116.77119897,   5.46759099,   0.70316101]), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 0.5794905198146539}
episode index:278
target Thresh 58.96324916319389
target distance 53.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.6768269 , 17.39303677]), 'dynamicTrap': False, 'previousTarget': array([84.28226871, 16.34829399]), 'currentState': array([103.48097236,  14.60093672,   2.89430672]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.597695636512422
running average episode reward sum: 0.6892552512100948
{'scaleFactor': 20, 'currentTarget': array([51., 22.]), 'dynamicTrap': False, 'previousTarget': array([51., 22.]), 'currentState': array([50.94244773, 22.80579026,  3.98020682]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 0.8078429311909718}
episode index:279
target Thresh 59.048220312481554
target distance 28.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.26819108, 16.87938629]), 'dynamicTrap': False, 'previousTarget': array([43.3829006 , 16.87838597]), 'currentState': array([24.85109461,  9.08127386,  1.63197303]), 'targetState': array([53, 21], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 42
reward sum = 0.4857859533835358
running average episode reward sum: 0.6885285751464285
{'scaleFactor': 20, 'currentTarget': array([53., 21.]), 'dynamicTrap': False, 'previousTarget': array([53., 21.]), 'currentState': array([52.69108442, 21.32884889,  0.51178456]), 'targetState': array([53, 21], dtype=int32), 'currentDistance': 0.45118779678730164}
episode index:280
target Thresh 59.132767666394116
target distance 58.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.05133647, 12.43206956]), 'dynamicTrap': True, 'previousTarget': array([75.04739343, 12.37604183]), 'currentState': array([95.      , 11.      ,  5.353104], dtype=float32), 'targetState': array([37, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4395094423545699
running average episode reward sum: 0.6876423860617599
{'scaleFactor': 20, 'currentTarget': array([37., 15.]), 'dynamicTrap': False, 'previousTarget': array([37., 15.]), 'currentState': array([36.3723416 , 15.55817337,  4.43239692]), 'targetState': array([37, 15], dtype=int32), 'currentDistance': 0.839947958366813}
episode index:281
target Thresh 59.21689333861984
target distance 38.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.42868934, 13.38995784]), 'dynamicTrap': False, 'previousTarget': array([66.39689557, 13.34333069]), 'currentState': array([85.05110067,  6.0957275 ,  3.21205759]), 'targetState': array([47, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6459760024925558
running average episode reward sum: 0.6874946329285357
{'scaleFactor': 20, 'currentTarget': array([47., 21.]), 'dynamicTrap': False, 'previousTarget': array([47., 21.]), 'currentState': array([46.93280405, 21.63739553,  2.83324001]), 'targetState': array([47, 21], dtype=int32), 'currentDistance': 0.6409277312279071}
episode index:282
target Thresh 59.3005994323049
target distance 8.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 15.]), 'dynamicTrap': False, 'previousTarget': array([91., 15.]), 'currentState': array([92.22278766, 21.44707705,  3.33171892]), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 6.5620128074444395}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6884596554623571
{'scaleFactor': 20, 'currentTarget': array([91., 15.]), 'dynamicTrap': False, 'previousTarget': array([91., 15.]), 'currentState': array([91.04019827, 15.1520615 ,  4.69205835]), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 0.15728509623877984}
episode index:283
target Thresh 59.383888040106015
target distance 17.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.,   2.]), 'dynamicTrap': False, 'previousTarget': array([104.,   2.]), 'currentState': array([112.35107699,  18.93984698,   5.14107395]), 'targetState': array([104,   2], dtype=int32), 'currentDistance': 18.886474070556346}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6892199456720277
{'scaleFactor': 20, 'currentTarget': array([104.,   2.]), 'dynamicTrap': False, 'previousTarget': array([104.,   2.]), 'currentState': array([104.47113013,   2.40895699,   5.03644482]), 'targetState': array([104,   2], dtype=int32), 'currentDistance': 0.6238665052742184}
episode index:284
target Thresh 59.46676124424271
target distance 17.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.60743163,  4.54254642]), 'dynamicTrap': False, 'previousTarget': array([78.00324289,  5.76756726]), 'currentState': array([91.52378429, 17.86554997,  3.13945794]), 'targetState': array([76,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8680111224999678
running average episode reward sum: 0.689847283134582
{'scaleFactor': 20, 'currentTarget': array([76.,  4.]), 'dynamicTrap': False, 'previousTarget': array([76.,  4.]), 'currentState': array([75.70888047,  3.77766467,  3.84809009]), 'targetState': array([76,  4], dtype=int32), 'currentDistance': 0.3663107661733138}
episode index:285
target Thresh 59.549221116549404
target distance 36.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.38386791, 17.89684311]), 'dynamicTrap': False, 'previousTarget': array([52.4762588 , 18.33860916]), 'currentState': array([70.70831896, 12.74266116,  3.12426186]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6218560325752341
running average episode reward sum: 0.6896095514892696
{'scaleFactor': 20, 'currentTarget': array([36., 22.]), 'dynamicTrap': False, 'previousTarget': array([36., 22.]), 'currentState': array([36.16715939, 21.53344813,  2.49274831]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 0.49559348979242024}
episode index:286
target Thresh 59.63126971852719
target distance 16.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.7437615 ,  6.00615309]), 'dynamicTrap': False, 'previousTarget': array([87.59074408,  6.32117742]), 'currentState': array([72.13343312, 18.50885872,  5.12397265]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.630786336333456
running average episode reward sum: 0.6894045925514444
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([88.03377131,  4.19466525,  6.15724583]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 1.2578401934753116}
episode index:287
target Thresh 59.71290910139541
target distance 15.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  20.]), 'dynamicTrap': False, 'previousTarget': array([112.64636501,  19.62110536]), 'currentState': array([98.59862439,  6.53248869,  0.82707465]), 'targetState': array([113,  20], dtype=int32), 'currentDistance': 19.71733958546278}
done in step count: 18
reward sum = 0.7771411919309719
running average episode reward sum: 0.6897092335215123
{'scaleFactor': 20, 'currentTarget': array([113.,  20.]), 'dynamicTrap': False, 'previousTarget': array([113.,  20.]), 'currentState': array([1.12565698e+02, 2.01363482e+01, 1.06206450e-01]), 'targetState': array([113,  20], dtype=int32), 'currentDistance': 0.4552023843532786}
episode index:288
target Thresh 59.79414130614287
target distance 28.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.07618588,  16.35349609]), 'dynamicTrap': False, 'previousTarget': array([108.55604828,  16.19058177]), 'currentState': array([90.6187812 , 11.72646648,  0.84260624]), 'targetState': array([117,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8062626710267384
running average episode reward sum: 0.6901125326132258
{'scaleFactor': 20, 'currentTarget': array([117.,  18.]), 'dynamicTrap': False, 'previousTarget': array([117.,  18.]), 'currentState': array([116.24887539,  18.75925322,   5.58683758]), 'targetState': array([117,  18], dtype=int32), 'currentDistance': 1.0680138714812162}
episode index:289
target Thresh 59.87496836357894
target distance 3.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.,  9.]), 'dynamicTrap': False, 'previousTarget': array([86.,  9.]), 'currentState': array([85.43490459,  5.9710774 ,  0.79331976]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 3.0811856341303048}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6911124893973181
{'scaleFactor': 20, 'currentTarget': array([86.,  9.]), 'dynamicTrap': False, 'previousTarget': array([86.,  9.]), 'currentState': array([85.30638254,  8.43865526,  1.46749401]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 0.8923077330095162}
episode index:290
target Thresh 59.95539229438424
target distance 42.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.62656157,  6.57496042]), 'dynamicTrap': False, 'previousTarget': array([94.64677133,  5.74224216]), 'currentState': array([7.58785119e+01, 3.41038530e+00, 1.85605288e-02]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 29
reward sum = 0.7050206772875385
running average episode reward sum: 0.6911602838574219
{'scaleFactor': 20, 'currentTarget': array([117.,  10.]), 'dynamicTrap': False, 'previousTarget': array([117.,  10.]), 'currentState': array([116.9356724 ,   9.16200881,   0.98927061]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 0.8404565889601754}
episode index:291
target Thresh 60.03541510916125
target distance 55.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.84599377, 13.74853764]), 'dynamicTrap': False, 'previousTarget': array([46.79172879, 12.87879691]), 'currentState': array([27.99916663, 11.27802106,  6.19382626]), 'targetState': array([82, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5118788167817733
running average episode reward sum: 0.6905463062304505
{'scaleFactor': 20, 'currentTarget': array([82., 18.]), 'dynamicTrap': False, 'previousTarget': array([82., 18.]), 'currentState': array([81.44215938, 17.1945374 ,  0.40339533]), 'targetState': array([82, 18], dtype=int32), 'currentDistance': 0.9797735293304756}
episode index:292
target Thresh 60.1150388084845
target distance 39.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.05071724, 14.14724886]), 'dynamicTrap': False, 'previousTarget': array([49.97375327, 13.97570496]), 'currentState': array([30.08727149, 15.35589832,  5.374066  ]), 'targetState': array([69, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.49296635824253254
running average episode reward sum: 0.6898719719369765
{'scaleFactor': 20, 'currentTarget': array([69., 13.]), 'dynamicTrap': False, 'previousTarget': array([69., 13.]), 'currentState': array([68.57304893, 13.61779061,  0.84202692]), 'targetState': array([69, 13], dtype=int32), 'currentDistance': 0.7509676782073449}
episode index:293
target Thresh 60.19426538295062
target distance 40.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.03025544, 18.76834877]), 'dynamicTrap': False, 'previousTarget': array([30.59715  , 17.8507125]), 'currentState': array([49.59834036, 14.63433706,  2.8217833 ]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6925944736060267
running average episode reward sum: 0.6898812321467351
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'dynamicTrap': False, 'previousTarget': array([10., 23.]), 'currentState': array([10.20068138, 23.81584623,  2.97694699]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.8401655096501126}
episode index:294
target Thresh 60.273096813228086
target distance 32.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.65268719,  8.44320306]), 'dynamicTrap': False, 'previousTarget': array([68.99024152,  8.62469505]), 'currentState': array([50.68158046,  7.3685423 ,  5.12168518]), 'targetState': array([81,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7129514352803579
running average episode reward sum: 0.6899594362251542
{'scaleFactor': 20, 'currentTarget': array([81.37719884,  8.24618633]), 'dynamicTrap': True, 'previousTarget': array([81.,  9.]), 'currentState': array([82.09776347,  8.4488614 ,  1.83096601]), 'targetState': array([81,  9], dtype=int32), 'currentDistance': 0.748525596751143}
episode index:295
target Thresh 60.35153507010679
target distance 24.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.45954303, 13.12289581]), 'dynamicTrap': False, 'previousTarget': array([72.01733854, 13.16738911]), 'currentState': array([90.43462275, 14.12098915,  2.05445528]), 'targetState': array([68, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.2332933373542843
running average episode reward sum: 0.6884166453505906
{'scaleFactor': 20, 'currentTarget': array([68., 13.]), 'dynamicTrap': False, 'previousTarget': array([68., 13.]), 'currentState': array([68.2859685 , 13.34491722,  3.33199789]), 'targetState': array([68, 13], dtype=int32), 'currentDistance': 0.44804673093720243}
episode index:296
target Thresh 60.42958211454721
target distance 8.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 14.]), 'currentState': array([7.23507516, 5.82241476, 6.03933442]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 8.21328256018722}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.689300730887794
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 14.]), 'currentState': array([ 8.50219592, 13.63740189,  1.895344  ]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.6194175736564173}
episode index:297
target Thresh 60.507239897729534
target distance 57.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.8398826 , 14.22761392]), 'dynamicTrap': False, 'previousTarget': array([91.04906478, 13.59993437]), 'currentState': array([109.75927089,  16.02148405,   2.29640153]), 'targetState': array([54, 11], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.601337692210461
running average episode reward sum: 0.6890055529056552
{'scaleFactor': 20, 'currentTarget': array([54., 11.]), 'dynamicTrap': False, 'previousTarget': array([54., 11.]), 'currentState': array([54.2084736 , 10.39678776,  0.53458339]), 'targetState': array([54, 11], dtype=int32), 'currentDistance': 0.6382211560493661}
episode index:298
target Thresh 60.58451036110239
target distance 29.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.05796288, 11.3713887 ]), 'dynamicTrap': True, 'previousTarget': array([41.00719668, 11.45274276]), 'currentState': array([58.       , 22.       ,  2.7802112], dtype=float32), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6745600457589167
running average episode reward sum: 0.688957240172723
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([28.80840752,  4.73483896,  4.11464658]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 0.7594050178544766}
episode index:299
target Thresh 60.66139543643139
target distance 59.0
model initialize at round 299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.89468993, 11.41707812]), 'dynamicTrap': False, 'previousTarget': array([72.66120598, 12.33435143]), 'currentState': array([5.41723953e+01, 1.47383870e+01, 4.21226581e-02]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 60
reward sum = 0.37108402226665294
running average episode reward sum: 0.6878976627797028
{'scaleFactor': 20, 'currentTarget': array([112.,   5.]), 'dynamicTrap': False, 'previousTarget': array([112.,   5.]), 'currentState': array([111.80735625,   4.3296148 ,   0.65229038]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 0.6975155431525236}
episode index:300
target Thresh 60.7378970458474
target distance 17.0
model initialize at round 300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 12.]), 'dynamicTrap': False, 'previousTarget': array([72., 12.]), 'currentState': array([87.85152321, 14.38485352,  2.5434736 ]), 'targetState': array([72, 12], dtype=int32), 'currentDistance': 16.02991935146045}
done in step count: 23
reward sum = 0.6771841007719174
running average episode reward sum: 0.6878620695504409
{'scaleFactor': 20, 'currentTarget': array([72., 12.]), 'dynamicTrap': False, 'previousTarget': array([72., 12.]), 'currentState': array([72.7229023 , 11.91650627,  3.53348766]), 'targetState': array([72, 12], dtype=int32), 'currentDistance': 0.7277080044474695}
episode index:301
target Thresh 60.81401710189466
target distance 57.0
model initialize at round 301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.58354041, 13.93983437]), 'dynamicTrap': True, 'previousTarget': array([79.5709957 , 13.87979038]), 'currentState': array([60.      , 18.      ,  4.871882], dtype=float32), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5503295520987714
running average episode reward sum: 0.6874066638635149
{'scaleFactor': 20, 'currentTarget': array([117.,   6.]), 'dynamicTrap': False, 'previousTarget': array([117.,   6.]), 'currentState': array([116.82991755,   5.62444415,   0.49458562]), 'targetState': array([117,   6], dtype=int32), 'currentDistance': 0.41227446327367656}
episode index:302
target Thresh 60.88975750757854
target distance 46.0
model initialize at round 302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.54495202, 17.89159225]), 'dynamicTrap': False, 'previousTarget': array([61.99527578, 17.43467991]), 'currentState': array([43.54514852, 17.80293433,  0.09654748]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6152119080336553
running average episode reward sum: 0.6871683973426242
{'scaleFactor': 20, 'currentTarget': array([88., 18.]), 'dynamicTrap': False, 'previousTarget': array([88., 18.]), 'currentState': array([87.067937  , 17.91732948,  5.85290674]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 0.9357221040515592}
episode index:303
target Thresh 60.96512015641309
target distance 13.0
model initialize at round 303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'dynamicTrap': False, 'previousTarget': array([17., 17.]), 'currentState': array([3.86661905, 9.60165051, 0.6438818 ]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 15.073860495933586}
done in step count: 11
reward sum = 0.8861108073144371
running average episode reward sum: 0.6878228131648999
{'scaleFactor': 20, 'currentTarget': array([17., 17.]), 'dynamicTrap': False, 'previousTarget': array([17., 17.]), 'currentState': array([17.09068646, 16.31337514,  0.18862443]), 'targetState': array([17, 17], dtype=int32), 'currentDistance': 0.692587704941522}
episode index:304
target Thresh 61.04010693246851
target distance 37.0
model initialize at round 304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.85766702, 11.61818125]), 'dynamicTrap': True, 'previousTarget': array([37.81984861, 11.32164208]), 'currentState': array([18.      , 14.      ,  5.506249], dtype=float32), 'targetState': array([55,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5604657840583306
running average episode reward sum: 0.6874052491350423
{'scaleFactor': 20, 'currentTarget': array([55.,  9.]), 'dynamicTrap': False, 'previousTarget': array([55.,  9.]), 'currentState': array([54.90348282,  8.41024187,  0.46584082]), 'targetState': array([55,  9], dtype=int32), 'currentDistance': 0.5976037337732333}
episode index:305
target Thresh 61.114719710418065
target distance 21.0
model initialize at round 305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 19.]), 'dynamicTrap': False, 'previousTarget': array([28.90990945, 18.89618185]), 'currentState': array([10.65799649, 17.67422445,  5.97301644]), 'targetState': array([30, 19], dtype=int32), 'currentDistance': 19.38738715045953}
done in step count: 20
reward sum = 0.7533017594133601
running average episode reward sum: 0.6876205972078472
{'scaleFactor': 20, 'currentTarget': array([30., 19.]), 'dynamicTrap': False, 'previousTarget': array([30., 19.]), 'currentState': array([29.37005477, 18.56761041,  1.08235749]), 'targetState': array([30, 19], dtype=int32), 'currentDistance': 0.7640626565996279}
episode index:306
target Thresh 61.18896035558512
target distance 9.0
model initialize at round 306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62., 22.]), 'dynamicTrap': False, 'previousTarget': array([62., 22.]), 'currentState': array([67.29397635, 12.78330257,  2.32117677]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 10.628908560447627}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6884784781612419
{'scaleFactor': 20, 'currentTarget': array([62., 22.]), 'dynamicTrap': False, 'previousTarget': array([62., 22.]), 'currentState': array([62.65694669, 21.02133798,  1.7815998 ]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 1.178710437350585}
episode index:307
target Thresh 61.26283072398964
target distance 23.0
model initialize at round 307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.57705983,  3.08680112]), 'dynamicTrap': False, 'previousTarget': array([89.,  3.]), 'currentState': array([107.54683465,   4.18593435,   2.96422648]), 'targetState': array([86,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8681062215049579
running average episode reward sum: 0.68906168512015
{'scaleFactor': 20, 'currentTarget': array([86.,  3.]), 'dynamicTrap': False, 'previousTarget': array([86.,  3.]), 'currentState': array([86.69990192,  3.31236763,  2.71591834]), 'targetState': array([86,  3], dtype=int32), 'currentDistance': 0.7664438888865831}
episode index:308
target Thresh 61.33633266239471
target distance 22.0
model initialize at round 308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.78005291,  6.96985848]), 'dynamicTrap': False, 'previousTarget': array([21.81660336,  6.70226409]), 'currentState': array([3.9652478 , 4.25443984, 6.17462892]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.7743338062153909
running average episode reward sum: 0.6893376466770925
{'scaleFactor': 20, 'currentTarget': array([24.,  7.]), 'dynamicTrap': False, 'previousTarget': array([24.,  7.]), 'currentState': array([24.14915143,  6.73279731,  1.80788797]), 'targetState': array([24,  7], dtype=int32), 'currentDistance': 0.30601213398686866}
episode index:309
target Thresh 61.4094680083526
target distance 17.0
model initialize at round 309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 10.]), 'dynamicTrap': False, 'previousTarget': array([15., 10.]), 'currentState': array([30.78036043,  4.30371157,  3.14719296]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 16.776992497139133}
done in step count: 13
reward sum = 0.8308944971968982
running average episode reward sum: 0.6897942816787693
{'scaleFactor': 20, 'currentTarget': array([16.81518638,  9.57470854]), 'dynamicTrap': True, 'previousTarget': array([15., 10.]), 'currentState': array([17.57167533,  9.17251734,  3.54964041]), 'targetState': array([15, 10], dtype=int32), 'currentDistance': 0.8567574302114742}
episode index:310
target Thresh 61.48223859025077
target distance 10.0
model initialize at round 310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.,  3.]), 'dynamicTrap': False, 'previousTarget': array([45.,  3.]), 'currentState': array([53.77055911,  3.85071188,  2.88330793]), 'targetState': array([45,  3], dtype=int32), 'currentDistance': 8.81172048110649}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6906341394544003
{'scaleFactor': 20, 'currentTarget': array([45.,  3.]), 'dynamicTrap': False, 'previousTarget': array([45.,  3.]), 'currentState': array([45.32189805,  3.36116756,  2.93525325]), 'targetState': array([45,  3], dtype=int32), 'currentDistance': 0.4837978515336701}
episode index:311
target Thresh 61.55464622735757
target distance 3.0
model initialize at round 311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.02205635, 18.03367296]), 'dynamicTrap': True, 'previousTarget': array([21., 18.]), 'currentState': array([24.        , 20.        ,  0.98393625], dtype=float32), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 3.568555792214907}
done in step count: 6
reward sum = 0.921580149401
running average episode reward sum: 0.691374351024742
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'dynamicTrap': False, 'previousTarget': array([21., 18.]), 'currentState': array([21.75258621, 18.24268572,  2.93402615]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.7907479775485794}
episode index:312
target Thresh 61.62669272986769
target distance 7.0
model initialize at round 312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.05092819, 16.09452797,  4.19933033]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 5.0947825145021}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6922654840885607
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 11.]), 'currentState': array([ 1.93727355, 11.34273596,  4.57736868]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.34842868026557106}
episode index:313
target Thresh 61.698379898947444
target distance 16.0
model initialize at round 313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 19.]), 'dynamicTrap': False, 'previousTarget': array([48., 19.]), 'currentState': array([32.84725324, 14.54585075,  6.24994993]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 15.793833602910778}
done in step count: 16
reward sum = 0.7956037983485268
running average episode reward sum: 0.6925945870002167
{'scaleFactor': 20, 'currentTarget': array([48., 19.]), 'dynamicTrap': False, 'previousTarget': array([48., 19.]), 'currentState': array([47.66874815, 19.83721197,  5.82531399]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 0.9003619618369288}
episode index:314
target Thresh 61.7697095267798
target distance 12.0
model initialize at round 314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.,  9.]), 'dynamicTrap': False, 'previousTarget': array([53.,  9.]), 'currentState': array([52.53825579, 20.83826853,  5.32520867]), 'targetState': array([53,  9], dtype=int32), 'currentDistance': 11.847270129045794}
done in step count: 9
reward sum = 0.8940132574836408
running average episode reward sum: 0.6932340113509577
{'scaleFactor': 20, 'currentTarget': array([53.,  9.]), 'dynamicTrap': False, 'previousTarget': array([53.,  9.]), 'currentState': array([53.63013047,  9.77839613,  5.18806092]), 'targetState': array([53,  9], dtype=int32), 'currentDistance': 1.0014813685946629}
episode index:315
target Thresh 61.84068339660916
target distance 22.0
model initialize at round 315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.45668638, 17.55737009]), 'dynamicTrap': False, 'previousTarget': array([67.52454686, 17.26673649]), 'currentState': array([52.51606898,  5.47859876,  6.06972224]), 'targetState': array([73, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7549942237715342
running average episode reward sum: 0.6934294550611495
{'scaleFactor': 20, 'currentTarget': array([73., 21.]), 'dynamicTrap': False, 'previousTarget': array([73., 21.]), 'currentState': array([72.37588943, 20.07227864,  1.25547552]), 'targetState': array([73, 21], dtype=int32), 'currentDistance': 1.1181149005096749}
episode index:316
target Thresh 61.91130328278597
target distance 19.0
model initialize at round 316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.16125679, 14.8820909 ]), 'dynamicTrap': True, 'previousTarget': array([38., 15.]), 'currentState': array([57.       , 12.       ,  2.4272087], dtype=float32), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 19.057929942577054}
done in step count: 31
reward sum = 0.6205947581179618
running average episode reward sum: 0.6931996926102246
{'scaleFactor': 20, 'currentTarget': array([38., 15.]), 'dynamicTrap': False, 'previousTarget': array([38., 15.]), 'currentState': array([37.47161877, 15.83413967,  6.06849541]), 'targetState': array([38, 15], dtype=int32), 'currentDistance': 0.9874085842615672}
episode index:317
target Thresh 61.98157095081106
target distance 60.0
model initialize at round 317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.01516392, 22.22133002]), 'dynamicTrap': True, 'previousTarget': array([79.02495322, 22.00124766]), 'currentState': array([99.       , 23.       ,  4.6112247], dtype=float32), 'targetState': array([39, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.49926079985080807
running average episode reward sum: 0.6925898218782768
{'scaleFactor': 20, 'currentTarget': array([39., 20.]), 'dynamicTrap': False, 'previousTarget': array([39., 20.]), 'currentState': array([39.32209721, 20.74743493,  2.83645308]), 'targetState': array([39, 20], dtype=int32), 'currentDistance': 0.8138830291332447}
episode index:318
target Thresh 62.0514881573798
target distance 40.0
model initialize at round 318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.3418804 ,  8.95031964]), 'dynamicTrap': False, 'previousTarget': array([97.22127294,  9.03319094]), 'currentState': array([117.13478551,  11.8210177 ,   4.38698222]), 'targetState': array([77,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.21240208426576673
running average episode reward sum: 0.6910845311647579
{'scaleFactor': 20, 'currentTarget': array([77.,  6.]), 'dynamicTrap': False, 'previousTarget': array([77.,  6.]), 'currentState': array([77.1995201 ,  6.63181469,  3.47737147]), 'targetState': array([77,  6], dtype=int32), 'currentDistance': 0.6625692952509389}
episode index:319
target Thresh 62.121056650425984
target distance 39.0
model initialize at round 319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.36918727, 21.82723604]), 'dynamicTrap': False, 'previousTarget': array([91.97375327, 21.02429504]), 'currentState': array([72.37004711, 21.64178355,  0.33910751]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7338120522540935
running average episode reward sum: 0.691218054668162
{'scaleFactor': 20, 'currentTarget': array([111.,  22.]), 'dynamicTrap': False, 'previousTarget': array([111.,  22.]), 'currentState': array([110.59297137,  21.68864452,   5.96458786]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 0.5124593077848271}
episode index:320
target Thresh 62.19027816916557
target distance 25.0
model initialize at round 320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.05751536, 19.19388855]), 'dynamicTrap': False, 'previousTarget': array([71., 19.]), 'currentState': array([90.03472029, 20.14850019,  3.18970597]), 'targetState': array([66, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6956557625010944
running average episode reward sum: 0.6912318793031557
{'scaleFactor': 20, 'currentTarget': array([66., 19.]), 'dynamicTrap': False, 'previousTarget': array([66., 19.]), 'currentState': array([66.57648757, 19.12602324,  3.78328334]), 'targetState': array([66, 19], dtype=int32), 'currentDistance': 0.5901014924125006}
episode index:321
target Thresh 62.25915444414012
target distance 39.0
model initialize at round 321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.62779928,  6.22262844]), 'dynamicTrap': False, 'previousTarget': array([47.23256605,  6.95885631]), 'currentState': array([65.47069188,  8.72455036,  3.99935153]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5132025860317041
running average episode reward sum: 0.690678993299207
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([28.25460791,  4.79806125,  3.22146312]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 0.8376914422380389}
episode index:322
target Thresh 62.32768719726012
target distance 29.0
model initialize at round 322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.83774356,  9.57079813]), 'dynamicTrap': False, 'previousTarget': array([54.53574428, 10.64534088]), 'currentState': array([70.68663304, 20.3464616 ,  3.51509714]), 'targetState': array([42,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7015514379410308
running average episode reward sum: 0.6907126541185314
{'scaleFactor': 20, 'currentTarget': array([42.,  2.]), 'dynamicTrap': False, 'previousTarget': array([42.,  2.]), 'currentState': array([42.84426656,  2.8776402 ,  3.50319522]), 'targetState': array([42,  2], dtype=int32), 'currentDistance': 1.217800617248779}
episode index:323
target Thresh 62.39587814184794
target distance 29.0
model initialize at round 323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.59665913, 21.15227797]), 'dynamicTrap': False, 'previousTarget': array([27.04739343, 21.37604183]), 'currentState': array([45.47328374, 18.93421708,  4.40802693]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8261686238355866
running average episode reward sum: 0.6911307280991397
{'scaleFactor': 20, 'currentTarget': array([19.94900048, 22.39160083]), 'dynamicTrap': True, 'previousTarget': array([18., 22.]), 'currentState': array([20.36665165, 22.8553698 ,  3.15148381]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.6241106905191505}
episode index:324
target Thresh 62.463728982680756
target distance 17.0
model initialize at round 324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,   4.]), 'dynamicTrap': False, 'previousTarget': array([113.,   4.]), 'currentState': array([96.63378804, 10.15509619,  5.4330554 ]), 'targetState': array([113,   4], dtype=int32), 'currentDistance': 17.48536825554595}
done in step count: 16
reward sum = 0.7941024655227956
running average episode reward sum: 0.6914475642142894
{'scaleFactor': 20, 'currentTarget': array([113.,   4.]), 'dynamicTrap': False, 'previousTarget': array([113.,   4.]), 'currentState': array([113.13577812,   3.05893289,   0.14316422]), 'targetState': array([113,   4], dtype=int32), 'currentDistance': 0.9508117547315094}
episode index:325
target Thresh 62.531241416033126
target distance 53.0
model initialize at round 325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.58442418, 12.80949733]), 'dynamicTrap': False, 'previousTarget': array([44.57578312, 13.23556944]), 'currentState': array([64.06469143, 17.33930837,  3.79999256]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.48591611167520227
running average episode reward sum: 0.6908170996359487
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'dynamicTrap': False, 'previousTarget': array([11.,  5.]), 'currentState': array([10.50428775,  4.00688042,  0.45906521]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.1099626682754131}
episode index:326
target Thresh 62.5984171297194
target distance 39.0
model initialize at round 326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.78220257, 12.94354576]), 'dynamicTrap': True, 'previousTarget': array([97.76743395, 13.04114369]), 'currentState': array([78.       , 10.       ,  1.9829822], dtype=float32), 'targetState': array([117,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.001328532490005363
running average episode reward sum: 0.6887085719076735
{'scaleFactor': 20, 'currentTarget': array([117.,  16.]), 'dynamicTrap': False, 'previousTarget': array([117.,  16.]), 'currentState': array([117.67807533,  15.41318355,   0.49094716]), 'targetState': array([117,  16], dtype=int32), 'currentDistance': 0.8967383654409581}
episode index:327
target Thresh 62.665257803135916
target distance 8.0
model initialize at round 327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43., 15.]), 'dynamicTrap': False, 'previousTarget': array([43., 15.]), 'currentState': array([34.95124561, 12.23669038,  0.7639389 ]), 'targetState': array([43, 15], dtype=int32), 'currentDistance': 8.509895848234313}
done in step count: 12
reward sum = 0.8301709367249337
running average episode reward sum: 0.6891398596052871
{'scaleFactor': 20, 'currentTarget': array([43., 15.]), 'dynamicTrap': False, 'previousTarget': array([43., 15.]), 'currentState': array([42.43373603, 15.60456411,  0.68238258]), 'targetState': array([43, 15], dtype=int32), 'currentDistance': 0.8283433158409027}
episode index:328
target Thresh 62.73176510730299
target distance 6.0
model initialize at round 328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81., 21.]), 'dynamicTrap': False, 'previousTarget': array([81., 21.]), 'currentState': array([85.7112782 , 21.80295286,  3.01481509]), 'targetState': array([81, 21], dtype=int32), 'currentDistance': 4.7792128643212}
done in step count: 5
reward sum = 0.9120761594009998
running average episode reward sum: 0.6898174775377969
{'scaleFactor': 20, 'currentTarget': array([82.73879839, 21.50271129]), 'dynamicTrap': True, 'previousTarget': array([81., 21.]), 'currentState': array([83.30611798, 21.38264238,  3.64826384]), 'targetState': array([81, 21], dtype=int32), 'currentDistance': 0.5798862505144983}
episode index:329
target Thresh 62.797940704906694
target distance 7.0
model initialize at round 329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  11.]), 'dynamicTrap': False, 'previousTarget': array([115.,  11.]), 'currentState': array([1.10763261e+02, 5.49994017e+00, 9.00850296e-02]), 'targetState': array([115,  11], dtype=int32), 'currentDistance': 6.942666224087889}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6906380185452581
{'scaleFactor': 20, 'currentTarget': array([115.,  11.]), 'dynamicTrap': False, 'previousTarget': array([115.,  11.]), 'currentState': array([115.09282053,  10.26222307,   0.87239362]), 'targetState': array([115,  11], dtype=int32), 'currentDistance': 0.7435929334473538}
episode index:330
target Thresh 62.86378625034042
target distance 31.0
model initialize at round 330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.51111794,  12.62327754]), 'dynamicTrap': False, 'previousTarget': array([104.53624817,  12.61665222]), 'currentState': array([86.96368971,  3.02704064,  1.53330338]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7606283443111451
running average episode reward sum: 0.6908494696805024
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([118.34864711,  19.171777  ,   1.55237786]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 0.8986145646445913}
episode index:331
target Thresh 62.92930338974622
target distance 11.0
model initialize at round 331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,   2.]), 'dynamicTrap': False, 'previousTarget': array([107.,   2.]), 'currentState': array([118.21499592,  12.00414472,   5.33359123]), 'targetState': array([107,   2], dtype=int32), 'currentDistance': 15.028607554894492}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6915479492731151
{'scaleFactor': 20, 'currentTarget': array([107.,   2.]), 'dynamicTrap': False, 'previousTarget': array([107.,   2.]), 'currentState': array([107.57433315,   2.34712374,   3.52411265]), 'targetState': array([107,   2], dtype=int32), 'currentDistance': 0.6710837946003185}
episode index:332
target Thresh 62.99449376105599
target distance 15.0
model initialize at round 332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  5.]), 'dynamicTrap': False, 'previousTarget': array([37.,  5.]), 'currentState': array([22.43668243, 15.30889325,  2.25878128]), 'targetState': array([37,  5], dtype=int32), 'currentDistance': 17.842743579997602}
done in step count: 15
reward sum = 0.8221731943112988
running average episode reward sum: 0.6919402172762328
{'scaleFactor': 20, 'currentTarget': array([37.,  5.]), 'dynamicTrap': False, 'previousTarget': array([37.,  5.]), 'currentState': array([36.75809615,  4.73097516,  5.69978224]), 'targetState': array([37,  5], dtype=int32), 'currentDistance': 0.3617897719534368}
episode index:333
target Thresh 63.05935899403243
target distance 63.0
model initialize at round 333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.16549694, 14.03959479]), 'dynamicTrap': False, 'previousTarget': array([29.95980905, 15.26728946]), 'currentState': array([10.2504381 , 12.19828237,  4.65536267]), 'targetState': array([73, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.41044861506214947
running average episode reward sum: 0.6910974280480469
{'scaleFactor': 20, 'currentTarget': array([73., 18.]), 'dynamicTrap': False, 'previousTarget': array([73., 18.]), 'currentState': array([73.83920792, 17.47122253,  0.5226966 ]), 'targetState': array([73, 18], dtype=int32), 'currentDistance': 0.9919050104851715}
episode index:334
target Thresh 63.12390071030972
target distance 27.0
model initialize at round 334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.63871801,  6.23523148]), 'dynamicTrap': False, 'previousTarget': array([22.4762588 ,  6.33860916]), 'currentState': array([42.12543047,  1.73322778,  4.14189577]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5791883036931506
running average episode reward sum: 0.6907633709604203
{'scaleFactor': 20, 'currentTarget': array([15.,  8.]), 'dynamicTrap': False, 'previousTarget': array([15.,  8.]), 'currentState': array([14.61429242,  8.66112973,  3.9680017 ]), 'targetState': array([15,  8], dtype=int32), 'currentDistance': 0.765416787880952}
episode index:335
target Thresh 63.18812052343414
target distance 56.0
model initialize at round 335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.64427944, 19.32469976]), 'dynamicTrap': False, 'previousTarget': array([43.31144849, 19.48418723]), 'currentState': array([61.3190998 , 22.91655774,  2.18119335]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7097142267028844
running average episode reward sum: 0.6908197723167967
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.24366065, 13.53965008,  4.03955423]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 0.5921087084163708}
episode index:336
target Thresh 63.25202003890436
target distance 9.0
model initialize at round 336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   4.]), 'dynamicTrap': False, 'previousTarget': array([106.,   4.]), 'currentState': array([98.45191408,  5.92178184,  5.90222937]), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 7.7888925070658495}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.691620295277281
{'scaleFactor': 20, 'currentTarget': array([106.,   4.]), 'dynamicTrap': False, 'previousTarget': array([106.,   4.]), 'currentState': array([105.76422926,   4.02771785,   5.96642803]), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 0.2373944438735954}
episode index:337
target Thresh 63.31560085421159
target distance 59.0
model initialize at round 337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.36956217, 14.60137188]), 'dynamicTrap': False, 'previousTarget': array([65.9285661 , 15.68886153]), 'currentState': array([46.49796575, 12.33870627,  5.92429876]), 'targetState': array([105,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.3256732681458485
running average episode reward sum: 0.6905376117650578
{'scaleFactor': 20, 'currentTarget': array([105.,  19.]), 'dynamicTrap': False, 'previousTarget': array([105.,  19.]), 'currentState': array([105.27653569,  18.24635593,   1.23628647]), 'targetState': array([105,  19], dtype=int32), 'currentDistance': 0.8027772863126197}
episode index:338
target Thresh 63.378864558879535
target distance 7.0
model initialize at round 338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([24.82467452, 10.9649958 ,  5.47024686]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 7.0136477383349805}
done in step count: 8
reward sum = 0.8936347443279201
running average episode reward sum: 0.6911367183507889
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([23.73510038,  4.74453374,  4.43347177]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.7902545758373232}
episode index:339
target Thresh 63.4418127345041
target distance 45.0
model initialize at round 339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.55356179,  4.75686129]), 'dynamicTrap': False, 'previousTarget': array([48.95570316,  4.66961979]), 'currentState': array([30.60947277,  6.25128862,  0.2779124 ]), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6844269334055592
running average episode reward sum: 0.6911169836891853
{'scaleFactor': 20, 'currentTarget': array([72.42505037,  2.8904669 ]), 'dynamicTrap': True, 'previousTarget': array([74.,  3.]), 'currentState': array([71.61745697,  2.62168498,  0.10415909]), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 0.8511467691272183}
episode index:340
target Thresh 63.50444695479295
target distance 10.0
model initialize at round 340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([ 7.84700743, 13.97338614,  5.46599132]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 10.689616329121668}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6918511865211848
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([4.01900418, 4.71194671, 4.17745452]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.712200306872873}
episode index:341
target Thresh 63.566768785604864
target distance 10.0
model initialize at round 341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.,   4.]), 'dynamicTrap': False, 'previousTarget': array([104.,   4.]), 'currentState': array([113.24676341,   4.65762787,   3.19104576]), 'targetState': array([104,   4], dtype=int32), 'currentDistance': 9.270119090330404}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6926089024959767
{'scaleFactor': 20, 'currentTarget': array([104.,   4.]), 'dynamicTrap': False, 'previousTarget': array([104.,   4.]), 'currentState': array([104.44027759,   4.64308168,   3.7983683 ]), 'targetState': array([104,   4], dtype=int32), 'currentDistance': 0.7793576868728322}
episode index:342
target Thresh 63.62877978498885
target distance 23.0
model initialize at round 342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.41925386, 17.74395673]), 'dynamicTrap': False, 'previousTarget': array([60.91647717, 16.88782122]), 'currentState': array([77.08832206,  6.69216206,  2.50922275]), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7907069342859094
running average episode reward sum: 0.6928949025886588
{'scaleFactor': 20, 'currentTarget': array([54., 22.]), 'dynamicTrap': False, 'previousTarget': array([54., 22.]), 'currentState': array([54.65337257, 22.24019827,  2.10194768]), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 0.6961256506782842}
episode index:343
target Thresh 63.690481503223126
target distance 27.0
model initialize at round 343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.71410086, 22.29661159]), 'dynamicTrap': True, 'previousTarget': array([77.4762588 , 21.33860916]), 'currentState': array([97.       , 17.       ,  4.8091874], dtype=float32), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8146137614500875
running average episode reward sum: 0.6932487364806978
{'scaleFactor': 20, 'currentTarget': array([70., 23.]), 'dynamicTrap': False, 'previousTarget': array([70., 23.]), 'currentState': array([70.50414703, 23.04564056,  3.14993172]), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 0.5062087391545972}
episode index:344
target Thresh 63.75187548285386
target distance 9.0
model initialize at round 344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'dynamicTrap': False, 'previousTarget': array([26.,  8.]), 'currentState': array([35.00530043,  6.99571639,  5.51734428]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 9.061126940186973}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6939682478224958
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'dynamicTrap': False, 'previousTarget': array([26.,  8.]), 'currentState': array([26.74192513,  7.68390783,  2.56131725]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.8064534433056907}
episode index:345
target Thresh 63.812963258733745
target distance 41.0
model initialize at round 345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.26035012, 11.75929821]), 'dynamicTrap': False, 'previousTarget': array([71.62981184, 12.16979281]), 'currentState': array([53.61346262, 15.50093038,  0.31092888]), 'targetState': array([93,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5650735632093626
running average episode reward sum: 0.6935957198322844
{'scaleFactor': 20, 'currentTarget': array([93.,  8.]), 'dynamicTrap': False, 'previousTarget': array([93.,  8.]), 'currentState': array([92.39931348,  7.43632736,  0.77303137]), 'targetState': array([93,  8], dtype=int32), 'currentDistance': 0.8237421587144058}
episode index:346
target Thresh 63.87374635806036
target distance 50.0
model initialize at round 346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.27558101,  8.34305832]), 'dynamicTrap': False, 'previousTarget': array([55.61161351,  7.9223227 ]), 'currentState': array([37.65249919,  4.4785196 ,  5.61945206]), 'targetState': array([86, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6127224505373842
running average episode reward sum: 0.693362655655642
{'scaleFactor': 20, 'currentTarget': array([86., 14.]), 'dynamicTrap': False, 'previousTarget': array([86., 14.]), 'currentState': array([85.31730258, 13.57026195,  6.26835483]), 'targetState': array([86, 14], dtype=int32), 'currentDistance': 0.8066911155689553}
episode index:347
target Thresh 63.934226300414345
target distance 35.0
model initialize at round 347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.71859534, 12.95626602]), 'dynamicTrap': True, 'previousTarget': array([96.74850544, 13.03626941]), 'currentState': array([78.       , 20.       ,  1.7931943], dtype=float32), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5689394667265162
running average episode reward sum: 0.6930051177564204
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([112.2500774 ,   7.81995192,   5.44446614]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 1.1111728345640675}
episode index:348
target Thresh 63.99440459779741
target distance 22.0
model initialize at round 348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.41143833, 22.44910202]), 'dynamicTrap': False, 'previousTarget': array([ 9.20413153, 21.83486126]), 'currentState': array([26.04257591, 15.17718946,  3.37745482]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5969577357769376
running average episode reward sum: 0.6927299103581983
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.31740293, 22.85143107,  2.3260576 ]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.35045306091455514}
episode index:349
target Thresh 64.05428275467014
target distance 50.0
model initialize at round 349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.17801207, 13.21468148]), 'dynamicTrap': False, 'previousTarget': array([83.04848294, 13.09551454]), 'currentState': array([64.1156267 ,  7.16278394,  0.85267603]), 'targetState': array([114,  23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6016816237560383
running average episode reward sum: 0.6924697723964779
{'scaleFactor': 20, 'currentTarget': array([114.,  23.]), 'dynamicTrap': False, 'previousTarget': array([114.,  23.]), 'currentState': array([113.20729287,  22.44891553,   0.12122796]), 'targetState': array([114,  23], dtype=int32), 'currentDistance': 0.9654422232292832}
episode index:350
target Thresh 64.11386226798955
target distance 29.0
model initialize at round 350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.07714458,  8.34827254]), 'dynamicTrap': False, 'previousTarget': array([11.,  8.]), 'currentState': array([30.05857861,  9.2098375 ,  3.2341131 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.7893762755042207
running average episode reward sum: 0.6927458592999188
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'dynamicTrap': False, 'previousTarget': array([2., 8.]), 'currentState': array([2.71080256, 8.39116127, 2.85192781]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.811324479662664}
episode index:351
target Thresh 64.1731446272466
target distance 25.0
model initialize at round 351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.86058335, 10.08047546]), 'dynamicTrap': False, 'previousTarget': array([82.92324388,  9.89833465]), 'currentState': array([67.05012786, 22.32912834,  0.40965295]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7678609095474703
running average episode reward sum: 0.6929592543290312
{'scaleFactor': 20, 'currentTarget': array([92.,  3.]), 'dynamicTrap': False, 'previousTarget': array([92.,  3.]), 'currentState': array([91.01414221,  2.20669598,  6.1503337 ]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 1.265403827538481}
episode index:352
target Thresh 64.23213131450333
target distance 7.0
model initialize at round 352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.38463281, 18.82498912]), 'dynamicTrap': True, 'previousTarget': array([81., 19.]), 'currentState': array([74.       , 19.       ,  1.0979203], dtype=float32), 'targetState': array([81, 19], dtype=int32), 'currentDistance': 5.3874761574503385}
done in step count: 5
reward sum = 0.9310900498999999
running average episode reward sum: 0.6936338458179008
{'scaleFactor': 20, 'currentTarget': array([81., 19.]), 'dynamicTrap': False, 'previousTarget': array([81., 19.]), 'currentState': array([80.15513571, 18.17408775,  6.1841402 ]), 'targetState': array([81, 19], dtype=int32), 'currentDistance': 1.1814934230410377}
episode index:353
target Thresh 64.29082380443003
target distance 24.0
model initialize at round 353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.43638058, 16.89838874]), 'dynamicTrap': False, 'previousTarget': array([67.06908484, 16.6609096 ]), 'currentState': array([86.42764289, 16.30726172,  2.83190417]), 'targetState': array([63, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7639666285166967
running average episode reward sum: 0.693832525995016
{'scaleFactor': 20, 'currentTarget': array([63.35476261, 16.97454427]), 'dynamicTrap': True, 'previousTarget': array([63., 17.]), 'currentState': array([64.08200321, 16.4567802 ,  3.19489751]), 'targetState': array([63, 17], dtype=int32), 'currentDistance': 0.8927253340309588}
episode index:354
target Thresh 64.349223564342
target distance 9.0
model initialize at round 354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 17.]), 'dynamicTrap': False, 'previousTarget': array([38., 17.]), 'currentState': array([30.56834544, 16.62811938,  0.77718323]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 7.440953207297458}
done in step count: 4
reward sum = 0.9509900498999999
running average episode reward sum: 0.6945569133862977
{'scaleFactor': 20, 'currentTarget': array([36.70534908, 17.24320362]), 'dynamicTrap': True, 'previousTarget': array([38., 17.]), 'currentState': array([36.00071281, 17.83494976,  5.98285818]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 0.9201498563283653}
episode index:355
target Thresh 64.40733205423624
target distance 52.0
model initialize at round 355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.4774297 , 17.33782489]), 'dynamicTrap': False, 'previousTarget': array([38.35987106, 17.22305213]), 'currentState': array([58.10715458, 21.16847775,  1.10531445]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5290261796630069
running average episode reward sum: 0.6940919394151648
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 11.]), 'currentState': array([ 6.9943457 , 11.36245664,  3.42499525]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 1.0583469086092054}
episode index:356
target Thresh 64.46515072682806
target distance 1.0
model initialize at round 356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  3.]), 'dynamicTrap': False, 'previousTarget': array([96.,  3.]), 'currentState': array([97.12854747,  2.05837981,  3.07403022]), 'targetState': array([96,  3], dtype=int32), 'currentDistance': 1.4697850062438995}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6949208135344501
{'scaleFactor': 20, 'currentTarget': array([96.,  3.]), 'dynamicTrap': False, 'previousTarget': array([96.,  3.]), 'currentState': array([96.40270265,  2.30127269,  2.55815363]), 'targetState': array([96,  3], dtype=int32), 'currentDistance': 0.8064671631421568}
episode index:357
target Thresh 64.52268102758728
target distance 64.0
model initialize at round 357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.02250946,  9.51593226]), 'dynamicTrap': False, 'previousTarget': array([48.23975932,  8.91246239]), 'currentState': array([66.72412934, 12.95775939,  3.37815404]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4263424675063666
running average episode reward sum: 0.6941705946907962
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([4.98478327, 2.89593094, 3.01699648]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.331349063701121}
episode index:358
target Thresh 64.57992439477441
target distance 56.0
model initialize at round 358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.9744548 ,  4.98947758]), 'dynamicTrap': True, 'previousTarget': array([63.97136265,  4.93010557]), 'currentState': array([44.       ,  6.       ,  3.9831693], dtype=float32), 'targetState': array([100,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5351003149037449
running average episode reward sum: 0.6937275019894396
{'scaleFactor': 20, 'currentTarget': array([100.,   3.]), 'dynamicTrap': False, 'previousTarget': array([100.,   3.]), 'currentState': array([99.04943273,  2.6540538 ,  6.12977897]), 'targetState': array([100,   3], dtype=int32), 'currentDistance': 1.0115616189198307}
episode index:359
target Thresh 64.63688225947662
target distance 37.0
model initialize at round 359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.38872186, 12.41037522]), 'dynamicTrap': False, 'previousTarget': array([52.46521464, 11.51410217]), 'currentState': array([70.0908461 ,  5.32302388,  3.1828655 ]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6105261310377513
running average episode reward sum: 0.6934963870701293
{'scaleFactor': 20, 'currentTarget': array([34., 19.]), 'dynamicTrap': False, 'previousTarget': array([34., 19.]), 'currentState': array([34.67357791, 18.35577568,  3.85177794]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 0.9320580384220313}
episode index:360
target Thresh 64.69355604564348
target distance 43.0
model initialize at round 360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.47011576, 14.57324747]), 'dynamicTrap': True, 'previousTarget': array([94.48016054, 14.53026989]), 'currentState': array([75.       , 10.       ,  5.9405737], dtype=float32), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.43514444601083835
running average episode reward sum: 0.6927807307237046
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([117.1404701 ,  19.22929629,   0.76509554]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 1.1544591131844695}
episode index:361
target Thresh 64.74994717012261
target distance 26.0
model initialize at round 361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.84600364, 10.1096129 ]), 'dynamicTrap': False, 'previousTarget': array([94.66691212, 10.82041841]), 'currentState': array([7.69267747e+01, 1.65951951e+01, 2.23800500e-02]), 'targetState': array([102,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6932428236074549
{'scaleFactor': 20, 'currentTarget': array([102.,   8.]), 'dynamicTrap': False, 'previousTarget': array([102.,   8.]), 'currentState': array([101.10078006,   7.28996601,   5.70496666]), 'targetState': array([102,   8], dtype=int32), 'currentDistance': 1.1457507399763855}
episode index:362
target Thresh 64.80605704269504
target distance 54.0
model initialize at round 362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.22558644, 15.99542459]), 'dynamicTrap': True, 'previousTarget': array([37.21593075, 15.93097322]), 'currentState': array([57.      , 13.      ,  4.325237], dtype=float32), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5262707789563792
running average episode reward sum: 0.6927828455230167
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.73741054, 21.88382544,  4.77136953]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 1.1510524408309804}
episode index:363
target Thresh 64.86188706611053
target distance 6.0
model initialize at round 363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 20.]), 'dynamicTrap': False, 'previousTarget': array([86., 20.]), 'currentState': array([79.82188757, 23.20133067,  1.58676922]), 'targetState': array([86, 20], dtype=int32), 'currentDistance': 6.95827502063004}
done in step count: 28
reward sum = 0.5524406466027794
running average episode reward sum: 0.6923972900314777
{'scaleFactor': 20, 'currentTarget': array([86., 20.]), 'dynamicTrap': False, 'previousTarget': array([86., 20.]), 'currentState': array([85.05061312, 19.02818988,  0.5126684 ]), 'targetState': array([86, 20], dtype=int32), 'currentDistance': 1.3585839589217792}
episode index:364
target Thresh 64.91743863612255
target distance 21.0
model initialize at round 364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.63565586, 18.21340751]), 'dynamicTrap': False, 'previousTarget': array([52.58173352, 16.83071558]), 'currentState': array([65.54946102,  3.8465698 ,  2.24088526]), 'targetState': array([47, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6340411027730919
running average episode reward sum: 0.6922374100663862
{'scaleFactor': 20, 'currentTarget': array([47., 23.]), 'dynamicTrap': False, 'previousTarget': array([47., 23.]), 'currentState': array([47.870496  , 22.84471859,  3.60356303]), 'targetState': array([47, 23], dtype=int32), 'currentDistance': 0.8842373009483874}
episode index:365
target Thresh 64.97271314152326
target distance 6.0
model initialize at round 365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'dynamicTrap': False, 'previousTarget': array([23.,  5.]), 'currentState': array([18.01542586,  4.58615283,  0.39847064]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 5.001724586484037}
done in step count: 20
reward sum = 0.7717650424549473
running average episode reward sum: 0.6924546986794696
{'scaleFactor': 20, 'currentTarget': array([23.,  5.]), 'dynamicTrap': False, 'previousTarget': array([23.,  5.]), 'currentState': array([22.0177079 ,  5.77308838,  0.71597268]), 'targetState': array([23,  5], dtype=int32), 'currentDistance': 1.2500253642200063}
episode index:366
target Thresh 65.02771196417818
target distance 26.0
model initialize at round 366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.75316835, 13.21349916]), 'dynamicTrap': False, 'previousTarget': array([10.11145618, 14.05572809]), 'currentState': array([26.81276069, 21.80716686,  2.91934109]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7922157880450359
running average episode reward sum: 0.6927265272608472
{'scaleFactor': 20, 'currentTarget': array([ 2., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 10.]), 'currentState': array([ 2.23194971, 10.93477701,  4.73134906]), 'targetState': array([ 2, 10], dtype=int32), 'currentDistance': 0.9631244590583679}
episode index:367
target Thresh 65.08243647906072
target distance 19.0
model initialize at round 367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67., 23.]), 'dynamicTrap': False, 'previousTarget': array([67., 23.]), 'currentState': array([6.30211874e+01, 5.39420277e+00, 6.89768791e-03]), 'targetState': array([67, 23], dtype=int32), 'currentDistance': 18.049793510180557}
done in step count: 26
reward sum = 0.7504401558051551
running average episode reward sum: 0.6928833577731959
{'scaleFactor': 20, 'currentTarget': array([67., 23.]), 'dynamicTrap': False, 'previousTarget': array([67., 23.]), 'currentState': array([67.92944733, 22.01013467,  1.12997513]), 'targetState': array([67, 23], dtype=int32), 'currentDistance': 1.3578312478537453}
episode index:368
target Thresh 65.13688805428663
target distance 21.0
model initialize at round 368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.62400028,  21.45812482]), 'dynamicTrap': True, 'previousTarget': array([107.63241055,  21.45612429]), 'currentState': array([103.       ,   2.       ,   1.3405337], dtype=float32), 'targetState': array([108,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8329431933839267
running average episode reward sum: 0.6932629237233604
{'scaleFactor': 20, 'currentTarget': array([108.,  23.]), 'dynamicTrap': False, 'previousTarget': array([108.,  23.]), 'currentState': array([108.90346683,  22.23122377,   1.9116952 ]), 'targetState': array([108,  23], dtype=int32), 'currentDistance': 1.1862837810680589}
episode index:369
target Thresh 65.1910680511481
target distance 42.0
model initialize at round 369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.67667274,  8.06038846]), 'dynamicTrap': False, 'previousTarget': array([33.00566653,  8.47605556]), 'currentState': array([51.65605394,  7.15246399,  2.78540194]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.561890098849701
running average episode reward sum: 0.6929078620345127
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'dynamicTrap': False, 'previousTarget': array([11.,  9.]), 'currentState': array([11.97702607,  8.51277563,  3.10339069]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 1.091772656101814}
episode index:370
target Thresh 65.24497782414787
target distance 60.0
model initialize at round 370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.15599772, 16.13549266]), 'dynamicTrap': False, 'previousTarget': array([44.38838649, 15.0776773 ]), 'currentState': array([63.65769621, 20.57213061,  2.7951827 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.15913513281063485
running average episode reward sum: 0.6914691215244754
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.01517149, 6.8850117 , 0.56521417]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9915187858055051}
episode index:371
target Thresh 65.2986187210331
target distance 14.0
model initialize at round 371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.,  3.]), 'dynamicTrap': False, 'previousTarget': array([81.,  3.]), 'currentState': array([82.17530285, 15.24270159,  3.84745753]), 'targetState': array([81,  3], dtype=int32), 'currentDistance': 12.298986913890584}
done in step count: 20
reward sum = 0.7168821528375193
running average episode reward sum: 0.6915374361247792
{'scaleFactor': 20, 'currentTarget': array([81.,  3.]), 'dynamicTrap': False, 'previousTarget': array([81.,  3.]), 'currentState': array([80.52018867,  3.96207052,  4.91474597]), 'targetState': array([81,  3], dtype=int32), 'currentDistance': 1.0750807457152318}
episode index:372
target Thresh 65.35199208282899
target distance 39.0
model initialize at round 372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.08067362, 15.69597428]), 'dynamicTrap': False, 'previousTarget': array([91.99342862, 16.48734798]), 'currentState': array([72.08325544, 15.37462408,  5.70578301]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4638714759812146
running average episode reward sum: 0.6909270716203728
{'scaleFactor': 20, 'currentTarget': array([111.,  16.]), 'dynamicTrap': False, 'previousTarget': array([111.,  16.]), 'currentState': array([111.97503656,  15.91836293,   3.08943197]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 0.9784482114362218}
episode index:373
target Thresh 65.40509924387236
target distance 19.0
model initialize at round 373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([90.08117057, 23.02902512,  5.61659855]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 19.92827988101359}
done in step count: 17
reward sum = 0.7983318410564854
running average episode reward sum: 0.6912142501482769
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([96.92389495,  4.94251031,  5.04970453]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 1.3198134586086034}
episode index:374
target Thresh 65.45794153184501
target distance 9.0
model initialize at round 374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.,  12.]), 'dynamicTrap': False, 'previousTarget': array([100.,  12.]), 'currentState': array([92.61598305,  5.47009242,  1.29308909]), 'targetState': array([100,  12], dtype=int32), 'currentDistance': 9.85714965899533}
done in step count: 7
reward sum = 0.9127563978069899
running average episode reward sum: 0.6918050292087002
{'scaleFactor': 20, 'currentTarget': array([100.,  12.]), 'dynamicTrap': False, 'previousTarget': array([100.,  12.]), 'currentState': array([99.40047574, 12.51890907,  0.49008147]), 'targetState': array([100,  12], dtype=int32), 'currentDistance': 0.7929034991998594}
episode index:375
target Thresh 65.5105202678069
target distance 24.0
model initialize at round 375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.46753958,  6.5150377 ]), 'dynamicTrap': False, 'previousTarget': array([72.04003392,  7.4000212 ]), 'currentState': array([87.29084199, 17.330605  ,  2.79643881]), 'targetState': array([65,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7639420574423943
running average episode reward sum: 0.6919968830071941
{'scaleFactor': 20, 'currentTarget': array([65.,  3.]), 'dynamicTrap': False, 'previousTarget': array([65.,  3.]), 'currentState': array([65.91695712,  3.90470001,  3.5553369 ]), 'targetState': array([65,  3], dtype=int32), 'currentDistance': 1.2881352725262125}
episode index:376
target Thresh 65.56283676622914
target distance 24.0
model initialize at round 376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.76791252,  7.48878959]), 'dynamicTrap': True, 'previousTarget': array([82.8,  7.6]), 'currentState': array([102.       ,   2.       ,   1.8480915], dtype=float32), 'targetState': array([78,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7078849860295093
running average episode reward sum: 0.6920390265165371
{'scaleFactor': 20, 'currentTarget': array([78.,  9.]), 'dynamicTrap': False, 'previousTarget': array([78.,  9.]), 'currentState': array([78.34120631,  8.86338033,  3.64462725]), 'targetState': array([78,  9], dtype=int32), 'currentDistance': 0.36754140219003406}
episode index:377
target Thresh 65.61489233502695
target distance 7.0
model initialize at round 377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([27.6393159 , 13.10590899,  5.09109092]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 5.625223202903999}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6927751640125251
{'scaleFactor': 20, 'currentTarget': array([30.,  8.]), 'dynamicTrap': False, 'previousTarget': array([30.,  8.]), 'currentState': array([30.47896887,  8.94843175,  5.69283256]), 'targetState': array([30,  8], dtype=int32), 'currentDistance': 1.0625130417430968}
episode index:378
target Thresh 65.66668827559224
target distance 18.0
model initialize at round 378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.97197033, 21.65275199]), 'dynamicTrap': False, 'previousTarget': array([23.80368799, 20.36442559]), 'currentState': array([12.83949804,  5.75296846,  0.19385242]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4805715199848716
running average episode reward sum: 0.6922152599385736
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([26.81882413, 22.448406  ,  2.50400272]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.9872835922067369}
episode index:379
target Thresh 65.71822588282622
target distance 24.0
model initialize at round 379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.9326351 ,  5.65442953]), 'dynamicTrap': False, 'previousTarget': array([74.27341645,  5.97753117]), 'currentState': array([91.36771712, 13.40992766,  3.90554357]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6984474069496036
running average episode reward sum: 0.6922316603254447
{'scaleFactor': 20, 'currentTarget': array([69.,  4.]), 'dynamicTrap': False, 'previousTarget': array([69.,  4.]), 'currentState': array([69.97781279,  4.98077485,  3.09210107]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 1.3849321847450158}
episode index:380
target Thresh 65.76950644517176
target distance 4.0
model initialize at round 380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([ 4.21520336, 11.0813332 ,  2.86835563]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 4.305978236862586}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6929108162035931
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([ 5.05495973, 14.40683054,  0.96084844]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 1.11577377955052}
episode index:381
target Thresh 65.8205312446456
target distance 44.0
model initialize at round 381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.99486695, 19.45309564]), 'dynamicTrap': True, 'previousTarget': array([37.99483671, 19.45442811]), 'currentState': array([18.       , 19.       ,  3.0986166], dtype=float32), 'targetState': array([62, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4657358910153021
running average episode reward sum: 0.6923161174465557
{'scaleFactor': 20, 'currentTarget': array([62., 20.]), 'dynamicTrap': False, 'previousTarget': array([62., 20.]), 'currentState': array([61.00459038, 19.60652989,  5.82924375]), 'targetState': array([62, 20], dtype=int32), 'currentDistance': 1.0703546310180896}
episode index:382
target Thresh 65.87130155687036
target distance 14.0
model initialize at round 382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 20.]), 'dynamicTrap': False, 'previousTarget': array([34., 20.]), 'currentState': array([48.00174983,  7.92263737,  3.72500372]), 'targetState': array([34, 20], dtype=int32), 'currentDistance': 18.49085413395959}
done in step count: 19
reward sum = 0.7624667521766448
running average episode reward sum: 0.6924992783727439
{'scaleFactor': 20, 'currentTarget': array([34., 20.]), 'dynamicTrap': False, 'previousTarget': array([34., 20.]), 'currentState': array([34.90937768, 20.37026161,  1.91289483]), 'targetState': array([34, 20], dtype=int32), 'currentDistance': 0.9818662921848897}
episode index:383
target Thresh 65.92181865110652
target distance 46.0
model initialize at round 383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.28077794,  7.58608903]), 'dynamicTrap': False, 'previousTarget': array([98.2957649,  7.5731765]), 'currentState': array([117.98304585,  11.02420472,   4.31341576]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5795103457808641
running average episode reward sum: 0.6922050363607859
{'scaleFactor': 20, 'currentTarget': array([72.03509253,  3.13807231]), 'dynamicTrap': True, 'previousTarget': array([72.,  3.]), 'currentState': array([73.02162116,  2.54736944,  1.28695639]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 1.1498559100859078}
episode index:384
target Thresh 65.97208379028405
target distance 65.0
model initialize at round 384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.99342291, 16.02739884]), 'dynamicTrap': False, 'previousTarget': array([50.44836438, 15.78887848]), 'currentState': array([68.4933085 , 20.47199799,  1.82800841]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.49819747682920545
running average episode reward sum: 0.6917011206217428
{'scaleFactor': 20, 'currentTarget': array([5., 6.]), 'dynamicTrap': False, 'previousTarget': array([5., 6.]), 'currentState': array([5.87673448, 6.78199812, 3.36202673]), 'targetState': array([5, 6], dtype=int32), 'currentDistance': 1.1748124987896178}
episode index:385
target Thresh 66.02209823103405
target distance 35.0
model initialize at round 385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.64111758,  9.13152257]), 'dynamicTrap': False, 'previousTarget': array([83.6170994 ,  9.12161403]), 'currentState': array([102.02417215,  17.00954926,   0.27523391]), 'targetState': array([67,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.6097692799199712
running average episode reward sum: 0.691488861967075
{'scaleFactor': 20, 'currentTarget': array([67.,  2.]), 'dynamicTrap': False, 'previousTarget': array([67.,  2.]), 'currentState': array([66.61378741,  2.95662065,  3.92653025]), 'targetState': array([67,  2], dtype=int32), 'currentDistance': 1.0316410417707917}
episode index:386
target Thresh 66.07186322372013
target distance 59.0
model initialize at round 386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.92968575, 21.07508641]), 'dynamicTrap': False, 'previousTarget': array([49., 21.]), 'currentState': array([28.92972268, 21.1135229 ,  1.16610658]), 'targetState': array([88, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5465460315300044
running average episode reward sum: 0.6911143326894599
{'scaleFactor': 20, 'currentTarget': array([88., 21.]), 'dynamicTrap': False, 'previousTarget': array([88., 21.]), 'currentState': array([87.59113331, 20.7302283 ,  1.79124812]), 'targetState': array([88, 21], dtype=int32), 'currentDistance': 0.4898456316576369}
episode index:387
target Thresh 66.12138001246973
target distance 33.0
model initialize at round 387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.62634111,  7.91085533]), 'dynamicTrap': False, 'previousTarget': array([43.08213587,  8.18928508]), 'currentState': array([61.56524422,  9.47295092,  2.48363447]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6849901762740982
running average episode reward sum: 0.6910985487811728
{'scaleFactor': 20, 'currentTarget': array([30.,  7.]), 'dynamicTrap': False, 'previousTarget': array([30.,  7.]), 'currentState': array([30.58375899,  7.13042291,  3.52411706]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 0.5981510598577573}
episode index:388
target Thresh 66.17064983520513
target distance 64.0
model initialize at round 388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.27275486, 20.28029607]), 'dynamicTrap': False, 'previousTarget': array([50.00244096, 21.31246186]), 'currentState': array([69.25698004, 19.48610234,  3.25190377]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.5235031377641198
running average episode reward sum: 0.6906677122489954
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.37868739, 22.90193029,  4.01899758]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.9782036551102891}
episode index:389
target Thresh 66.21967392367445
target distance 45.0
model initialize at round 389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.71998769, 11.06567143]), 'dynamicTrap': False, 'previousTarget': array([68.57201987, 11.74906181]), 'currentState': array([86.9766907 ,  5.66387644,  3.04880238]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6248523452865877
running average episode reward sum: 0.6904989548978097
{'scaleFactor': 20, 'currentTarget': array([43., 18.]), 'dynamicTrap': False, 'previousTarget': array([43., 18.]), 'currentState': array([43.40003134, 18.51741703,  1.89882833]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 0.654022519442404}
episode index:390
target Thresh 66.26845350348248
target distance 18.0
model initialize at round 390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.91251906,  6.27477947]), 'dynamicTrap': False, 'previousTarget': array([47.27881227,  7.21295565]), 'currentState': array([34.93241899, 21.49046224,  3.65764952]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6907626769662133
{'scaleFactor': 20, 'currentTarget': array([49.,  5.]), 'dynamicTrap': False, 'previousTarget': array([49.,  5.]), 'currentState': array([48.43613353,  4.50917712,  5.83949837]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 0.7475643751097661}
episode index:391
target Thresh 66.31698979412124
target distance 21.0
model initialize at round 391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.88809571, 11.3346904 ]), 'dynamicTrap': False, 'previousTarget': array([13.6170994 , 10.87838597]), 'currentState': array([31.75127726,  4.68785792,  2.6046049 ]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6911508925693197
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([10.29154154, 12.70934857,  4.13048193]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.0025411638397121}
episode index:392
target Thresh 66.36528400900053
target distance 44.0
model initialize at round 392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.84588742, 12.31300536]), 'dynamicTrap': False, 'previousTarget': array([85.51093912, 12.57265691]), 'currentState': array([68.55614576,  4.22070346,  5.17208623]), 'targetState': array([111,  23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 37
reward sum = 0.6894490858690777
running average episode reward sum: 0.6911465622723725
{'scaleFactor': 20, 'currentTarget': array([111.,  23.]), 'dynamicTrap': False, 'previousTarget': array([111.,  23.]), 'currentState': array([111.02575262,  23.76632446,   3.28416297]), 'targetState': array([111,  23], dtype=int32), 'currentDistance': 0.7667570511728193}
episode index:393
target Thresh 66.41333735547822
target distance 13.0
model initialize at round 393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 10.]), 'dynamicTrap': False, 'previousTarget': array([48., 10.]), 'currentState': array([35.25475433,  5.30313814,  5.81495297]), 'targetState': array([48, 10], dtype=int32), 'currentDistance': 13.583143910376801}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6917343747905338
{'scaleFactor': 20, 'currentTarget': array([48., 10.]), 'dynamicTrap': False, 'previousTarget': array([48., 10.]), 'currentState': array([47.44692149, 10.6430943 ,  0.18113571]), 'targetState': array([48, 10], dtype=int32), 'currentDistance': 0.8482134891101332}
episode index:394
target Thresh 66.46115103489052
target distance 56.0
model initialize at round 394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.04667101, 10.6344736 ]), 'dynamicTrap': True, 'previousTarget': array([75.050826, 10.575059]), 'currentState': array([95.     , 12.     ,  4.13916], dtype=float32), 'targetState': array([39,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 68
reward sum = 0.2597916297806444
running average episode reward sum: 0.6906408488537998
{'scaleFactor': 20, 'currentTarget': array([39.72757943,  8.86829714]), 'dynamicTrap': True, 'previousTarget': array([39.,  8.]), 'currentState': array([38.79688451,  9.24540398,  3.92627686]), 'targetState': array([39,  8], dtype=int32), 'currentDistance': 1.004192512342589}
episode index:395
target Thresh 66.50872624258186
target distance 58.0
model initialize at round 395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.48631715, 12.31922972]), 'dynamicTrap': False, 'previousTarget': array([73.48420846, 12.37422914]), 'currentState': array([92.99619184,  7.91868549,  3.66172338]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.46131120069735704
running average episode reward sum: 0.6900617335806776
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([34.7026045 , 21.44367709,  3.21155527]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 0.5341286775392964}
episode index:396
target Thresh 66.55606416793493
target distance 13.0
model initialize at round 396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  3.]), 'dynamicTrap': False, 'previousTarget': array([89.,  3.]), 'currentState': array([82.66519152, 14.45406717,  6.12872744]), 'targetState': array([89,  3], dtype=int32), 'currentDistance': 13.089134926782105}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6906713144731872
{'scaleFactor': 20, 'currentTarget': array([89.,  3.]), 'dynamicTrap': False, 'previousTarget': array([89.,  3.]), 'currentState': array([88.40611174,  3.30366792,  5.55407199]), 'targetState': array([89,  3], dtype=int32), 'currentDistance': 0.667021341429687}
episode index:397
target Thresh 66.60316599440033
target distance 39.0
model initialize at round 397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.00983206,  6.57789046]), 'dynamicTrap': False, 'previousTarget': array([88.48782391,  6.50280987]), 'currentState': array([70.62752161, 11.5100342 ,  5.61535496]), 'targetState': array([108,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5755882070594746
running average episode reward sum: 0.6903821609369718
{'scaleFactor': 20, 'currentTarget': array([108.,   2.]), 'dynamicTrap': False, 'previousTarget': array([108.,   2.]), 'currentState': array([107.16682124,   2.60477815,   6.23614738]), 'targetState': array([108,   2], dtype=int32), 'currentDistance': 1.0295355531119683}
episode index:398
target Thresh 66.65003289952617
target distance 21.0
model initialize at round 398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.10769737,  8.38236586]), 'dynamicTrap': False, 'previousTarget': array([25.6897547 ,  8.88009345]), 'currentState': array([ 8.72442719, 16.25988943,  5.5295437 ]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6908958353563245
{'scaleFactor': 20, 'currentTarget': array([28.,  8.]), 'dynamicTrap': False, 'previousTarget': array([28.,  8.]), 'currentState': array([27.03864767,  8.70327351,  0.23012851]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 1.1911305287394909}
episode index:399
target Thresh 66.69666605498752
target distance 58.0
model initialize at round 399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.36767657,  5.92443794]), 'dynamicTrap': False, 'previousTarget': array([67.18757742,  5.73274794]), 'currentState': array([87.19494076,  3.30153974,  1.97079807]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.32696990014360555
running average episode reward sum: 0.6899860205182926
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([29.71602283, 11.09545702,  3.3142206 ]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.7223577587444069}
episode index:400
target Thresh 66.74306662661569
target distance 4.0
model initialize at round 400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.,  3.]), 'dynamicTrap': False, 'previousTarget': array([66.,  3.]), 'currentState': array([68.66979865,  1.8809358 ,  3.13958251]), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 2.8948453307585207}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6907341850556535
{'scaleFactor': 20, 'currentTarget': array([66.,  3.]), 'dynamicTrap': False, 'previousTarget': array([66.,  3.]), 'currentState': array([66.84149553,  2.57587236,  2.40983796]), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 0.9423369802342041}
episode index:401
target Thresh 66.7892357744274
target distance 21.0
model initialize at round 401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.1836989 , 15.00457771]), 'dynamicTrap': False, 'previousTarget': array([91.09009055, 15.10381815]), 'currentState': array([110.17749191,  15.50281558,   3.24314237]), 'targetState': array([90, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6911988289311344
{'scaleFactor': 20, 'currentTarget': array([90., 15.]), 'dynamicTrap': False, 'previousTarget': array([90., 15.]), 'currentState': array([90.952017  , 15.52035457,  3.87913737]), 'targetState': array([90, 15], dtype=int32), 'currentDistance': 1.0849448073474688}
episode index:402
target Thresh 66.83517465265373
target distance 28.0
model initialize at round 402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.22922367,  7.40700775]), 'dynamicTrap': False, 'previousTarget': array([75.6170994 ,  8.12161403]), 'currentState': array([93.9916192 , 14.33322704,  3.74072421]), 'targetState': array([66,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7572544609318645
running average episode reward sum: 0.69136273868796
{'scaleFactor': 20, 'currentTarget': array([67.62451991,  3.9611196 ]), 'dynamicTrap': True, 'previousTarget': array([66.,  4.]), 'currentState': array([67.80615678,  3.74543484,  2.80704642]), 'targetState': array([66,  4], dtype=int32), 'currentDistance': 0.28197849368472855}
episode index:403
target Thresh 66.88088440976907
target distance 11.0
model initialize at round 403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76., 13.]), 'dynamicTrap': False, 'previousTarget': array([76., 13.]), 'currentState': array([66.29694282, 20.48691343,  5.50377752]), 'targetState': array([76, 13], dtype=int32), 'currentDistance': 12.25574115909309}
done in step count: 8
reward sum = 0.9135172474836408
running average episode reward sum: 0.6919126260859692
{'scaleFactor': 20, 'currentTarget': array([75.70447629, 13.13084884]), 'dynamicTrap': True, 'previousTarget': array([76., 13.]), 'currentState': array([74.9919832 , 13.30083697,  5.89651944]), 'targetState': array([76, 13], dtype=int32), 'currentDistance': 0.7324905216507027}
episode index:404
target Thresh 66.92636618851968
target distance 66.0
model initialize at round 404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.49548919, 16.31153985]), 'dynamicTrap': False, 'previousTarget': array([72.0206292 , 15.90815322]), 'currentState': array([90.48110515, 15.55315025,  3.76513398]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4851559953614135
running average episode reward sum: 0.6914021158866492
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([26.39749997, 17.60250103,  2.55372202]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.5621491379335831}
episode index:405
target Thresh 66.97162112595241
target distance 6.0
model initialize at round 405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.41196233,  8.45261277]), 'dynamicTrap': True, 'previousTarget': array([19.,  8.]), 'currentState': array([25.       ,  8.       ,  2.5537398], dtype=float32), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 4.610308882130648}
done in step count: 7
reward sum = 0.9023643479069899
running average episode reward sum: 0.6919217272955663
{'scaleFactor': 20, 'currentTarget': array([19.,  8.]), 'dynamicTrap': False, 'previousTarget': array([19.,  8.]), 'currentState': array([19.85835877,  7.44280698,  3.36979184]), 'targetState': array([19,  8], dtype=int32), 'currentDistance': 1.0233493255772257}
episode index:406
target Thresh 67.01665035344307
target distance 27.0
model initialize at round 406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.18524971, 16.18054373]), 'dynamicTrap': False, 'previousTarget': array([36.9246568 , 16.65626539]), 'currentState': array([20.76071845,  4.76849865,  6.20747089]), 'targetState': array([47, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7526156635160027
running average episode reward sum: 0.6920708524459851
{'scaleFactor': 20, 'currentTarget': array([47., 23.]), 'dynamicTrap': False, 'previousTarget': array([47., 23.]), 'currentState': array([47.1519095 , 22.58793672,  2.24580626]), 'targetState': array([47, 23], dtype=int32), 'currentDistance': 0.43917268089417627}
episode index:407
target Thresh 67.06145499672469
target distance 14.0
model initialize at round 407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81., 11.]), 'dynamicTrap': False, 'previousTarget': array([81., 11.]), 'currentState': array([68.73569989, 15.63389781,  5.98160864]), 'targetState': array([81, 11], dtype=int32), 'currentDistance': 13.110532641113394}
done in step count: 12
reward sum = 0.8486267626788401
running average episode reward sum: 0.6924545679122421
{'scaleFactor': 20, 'currentTarget': array([81., 11.]), 'dynamicTrap': False, 'previousTarget': array([81., 11.]), 'currentState': array([81.18098789, 10.1235252 ,  0.52612974]), 'targetState': array([81, 11], dtype=int32), 'currentDistance': 0.8949663056226478}
episode index:408
target Thresh 67.10603617591566
target distance 12.0
model initialize at round 408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([32.44451933, 16.33850688,  3.90998018]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 13.114591270127537}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6930404133401021
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([27.03334861,  4.58941946,  5.42969153]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 1.132179410598081}
episode index:409
target Thresh 67.15039500554782
target distance 53.0
model initialize at round 409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.25883191, 14.77435748]), 'dynamicTrap': False, 'previousTarget': array([38.95569367, 13.89144891]), 'currentState': array([57.11223469, 21.44887644,  3.10295987]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.6931183155903894
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'dynamicTrap': False, 'previousTarget': array([5., 3.]), 'currentState': array([4.41467648, 3.76033603, 4.5942909 ]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.9595386935872746}
episode index:410
target Thresh 67.1945325945942
target distance 42.0
model initialize at round 410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.19367707,  5.02539484]), 'dynamicTrap': False, 'previousTarget': array([77.,  5.]), 'currentState': array([57.19369063,  5.04868609,  0.15297163]), 'targetState': array([99,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6933244057213109
{'scaleFactor': 20, 'currentTarget': array([99.,  5.]), 'dynamicTrap': False, 'previousTarget': array([99.,  5.]), 'currentState': array([98.03270999,  4.41540365,  0.34552237]), 'targetState': array([99,  5], dtype=int32), 'currentDistance': 1.1302224793205309}
episode index:411
target Thresh 67.23845004649681
target distance 42.0
model initialize at round 411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.75865044,  5.88505501]), 'dynamicTrap': False, 'previousTarget': array([55.97736275,  5.95130299]), 'currentState': array([37.78892243,  4.78507162,  5.75530184]), 'targetState': array([78,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5370610810354963
running average episode reward sum: 0.6929451258070249
{'scaleFactor': 20, 'currentTarget': array([78.,  7.]), 'dynamicTrap': False, 'previousTarget': array([78.,  7.]), 'currentState': array([78.42341119,  7.12182311,  5.27068286]), 'targetState': array([78,  7], dtype=int32), 'currentDistance': 0.44058813123912793}
episode index:412
target Thresh 67.28214845919426
target distance 27.0
model initialize at round 412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.34264813, 11.23104796]), 'dynamicTrap': False, 'previousTarget': array([92.17596225, 10.68176659]), 'currentState': array([72.85586788,  6.72933766,  0.73509681]), 'targetState': array([100,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.48957607272998566
running average episode reward sum: 0.6924527067923105
{'scaleFactor': 20, 'currentTarget': array([100.,  13.]), 'dynamicTrap': False, 'previousTarget': array([100.,  13.]), 'currentState': array([100.89503005,  12.0004546 ,   1.53956023]), 'targetState': array([100,  13], dtype=int32), 'currentDistance': 1.3417040693958089}
episode index:413
target Thresh 67.32562892514913
target distance 58.0
model initialize at round 413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.37947898, 10.45065787]), 'dynamicTrap': False, 'previousTarget': array([70.76347807, 10.0667466 ]), 'currentState': array([52.60523115,  7.45413943,  1.02251595]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5095997482038435
running average episode reward sum: 0.692011032979295
{'scaleFactor': 20, 'currentTarget': array([109.,  16.]), 'dynamicTrap': False, 'previousTarget': array([109.,  16.]), 'currentState': array([109.44919417,  15.61729588,   6.07177235]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 0.5901168044054912}
episode index:414
target Thresh 67.36889253137535
target distance 46.0
model initialize at round 414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.44332988, 10.30082368]), 'dynamicTrap': False, 'previousTarget': array([72.75381194,  9.4391401 ]), 'currentState': array([90.78415385,  5.2084246 ,  1.4028126 ]), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6712188982095573
running average episode reward sum: 0.6919609314497293
{'scaleFactor': 20, 'currentTarget': array([46., 17.]), 'dynamicTrap': False, 'previousTarget': array([46., 17.]), 'currentState': array([46.31048573, 17.70777369,  3.72772922]), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 0.7728809627966102}
episode index:415
target Thresh 67.41194035946532
target distance 58.0
model initialize at round 415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.00426639,  5.41308272]), 'dynamicTrap': True, 'previousTarget': array([42.00297199,  5.34477635]), 'currentState': array([62.     ,  5.     ,  4.66353], dtype=float32), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.521434314578849
running average episode reward sum: 0.6915510116976359
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'dynamicTrap': False, 'previousTarget': array([4., 6.]), 'currentState': array([3.2512745 , 6.12256035, 4.67048097]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.7586902593842536}
episode index:416
target Thresh 67.45477348561697
target distance 21.0
model initialize at round 416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.79441742,  5.20550262]), 'dynamicTrap': False, 'previousTarget': array([87.54489741,  6.41603543]), 'currentState': array([72.88806097, 17.32935615,  4.62131333]), 'targetState': array([93,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.763200058481748
running average episode reward sum: 0.6917228319537129
{'scaleFactor': 20, 'currentTarget': array([93.,  2.]), 'dynamicTrap': False, 'previousTarget': array([93.,  2.]), 'currentState': array([92.22165741,  2.60075822,  5.24035543]), 'targetState': array([93,  2], dtype=int32), 'currentDistance': 0.9832230795386278}
episode index:417
target Thresh 67.49739298066069
target distance 29.0
model initialize at round 417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.08980581, 21.25595397]), 'dynamicTrap': False, 'previousTarget': array([43.98811999, 21.31075448]), 'currentState': array([24.09805252, 21.83023646,  5.33716221]), 'targetState': array([53, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6921049729564429
{'scaleFactor': 20, 'currentTarget': array([53., 21.]), 'dynamicTrap': False, 'previousTarget': array([53., 21.]), 'currentState': array([53.63320403, 20.46903143,  0.13207572]), 'targetState': array([53, 21], dtype=int32), 'currentDistance': 0.8263624885301184}
episode index:418
target Thresh 67.53979991008609
target distance 15.0
model initialize at round 418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.05066249,  11.19313252]), 'dynamicTrap': True, 'previousTarget': array([117.,  11.]), 'currentState': array([102.      ,   3.      ,   4.356849], dtype=float32), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 17.136214924436864}
done in step count: 15
reward sum = 0.8110484045412885
running average episode reward sum: 0.6923888474948315
{'scaleFactor': 20, 'currentTarget': array([117.,  11.]), 'dynamicTrap': False, 'previousTarget': array([117.,  11.]), 'currentState': array([117.72266043,  10.55689209,   1.08182639]), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 0.8476925863628952}
episode index:419
target Thresh 67.58199533406861
target distance 51.0
model initialize at round 419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.37418312,  9.37715119]), 'dynamicTrap': False, 'previousTarget': array([87.01536099,  9.21628867]), 'currentState': array([105.35223909,  10.31378331,   2.18408382]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6784114392128732
running average episode reward sum: 0.6923555679513032
{'scaleFactor': 20, 'currentTarget': array([56.,  8.]), 'dynamicTrap': False, 'previousTarget': array([56.,  8.]), 'currentState': array([56.60411783,  8.03134997,  3.2651279 ]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 0.6049307162223878}
episode index:420
target Thresh 67.62398030749604
target distance 13.0
model initialize at round 420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.16685698, 17.09838219]), 'dynamicTrap': True, 'previousTarget': array([47., 17.]), 'currentState': array([34.      , 12.      ,  3.586614], dtype=float32), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 14.119476753529232}
done in step count: 19
reward sum = 0.7776294613607503
running average episode reward sum: 0.6925581187670025
{'scaleFactor': 20, 'currentTarget': array([47., 17.]), 'dynamicTrap': False, 'previousTarget': array([47., 17.]), 'currentState': array([47.23668106, 17.71807075,  4.34505577]), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 0.7560711148927086}
episode index:421
target Thresh 67.6657558799949
target distance 40.0
model initialize at round 421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.73236746, 14.0593567 ]), 'dynamicTrap': False, 'previousTarget': array([90.06555009, 13.41886371]), 'currentState': array([72.98809556, 23.28653476,  6.18904168]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6089807977910426
running average episode reward sum: 0.6923600682433628
{'scaleFactor': 20, 'currentTarget': array([112.,   3.]), 'dynamicTrap': False, 'previousTarget': array([112.,   3.]), 'currentState': array([111.43175432,   2.8319429 ,   4.98444974]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 0.5925760194466985}
episode index:422
target Thresh 67.7073230959567
target distance 22.0
model initialize at round 422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.38796529,  7.15904921]), 'dynamicTrap': False, 'previousTarget': array([49.78146944,  7.82541376]), 'currentState': array([65.25426613, 17.9074396 ,  2.29923749]), 'targetState': array([45,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7697332055993098
running average episode reward sum: 0.6925429834616984
{'scaleFactor': 20, 'currentTarget': array([45.,  5.]), 'dynamicTrap': False, 'previousTarget': array([45.,  5.]), 'currentState': array([45.0400915 ,  5.72939352,  3.15668984]), 'targetState': array([45,  5], dtype=int32), 'currentDistance': 0.73049451024852}
episode index:423
target Thresh 67.74868299456398
target distance 52.0
model initialize at round 423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.81385568, 16.93641316]), 'dynamicTrap': False, 'previousTarget': array([72.21647182, 17.45678697]), 'currentState': array([54.58763059, 22.44570246,  5.01971156]), 'targetState': array([105,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6173011945584483
running average episode reward sum: 0.6923655264123982
{'scaleFactor': 20, 'currentTarget': array([105.,   8.]), 'dynamicTrap': False, 'previousTarget': array([105.,   8.]), 'currentState': array([104.75632784,   7.44875705,   0.79583161]), 'targetState': array([105,   8], dtype=int32), 'currentDistance': 0.6026980248994256}
episode index:424
target Thresh 67.78983660981636
target distance 16.0
model initialize at round 424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([91.72573718,  1.82839511,  0.75808769]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 16.447787251362055}
done in step count: 16
reward sum = 0.7963624934100049
running average episode reward sum: 0.692610225158275
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([105.51398819,  10.85897382,   0.10788736]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 0.9869364220134708}
episode index:425
target Thresh 67.83078497055638
target distance 18.0
model initialize at round 425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 19.]), 'dynamicTrap': False, 'previousTarget': array([91., 19.]), 'currentState': array([71.03813681, 19.8989942 ,  3.52371836]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 19.982096304259517}
done in step count: 17
reward sum = 0.8048987883108468
running average episode reward sum: 0.6928738133346893
{'scaleFactor': 20, 'currentTarget': array([91., 19.]), 'dynamicTrap': False, 'previousTarget': array([91., 19.]), 'currentState': array([90.56235829, 19.56348633,  0.39335988]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 0.7134753752730575}
episode index:426
target Thresh 67.87152910049518
target distance 12.0
model initialize at round 426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42., 22.]), 'dynamicTrap': False, 'previousTarget': array([42., 22.]), 'currentState': array([31.48130085, 13.2088846 ,  0.51947087]), 'targetState': array([42, 22], dtype=int32), 'currentDistance': 13.708637488434167}
done in step count: 11
reward sum = 0.8488969283912048
running average episode reward sum: 0.6932392070467654
{'scaleFactor': 20, 'currentTarget': array([40.63859303, 20.6335794 ]), 'dynamicTrap': True, 'previousTarget': array([40.71383572, 20.81265058]), 'currentState': array([40.287643  , 19.7286304 ,  0.51440358]), 'targetState': array([42, 22], dtype=int32), 'currentDistance': 0.9706176488305202}
episode index:427
target Thresh 67.91207001823814
target distance 62.0
model initialize at round 427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.02964608, 19.16357642]), 'dynamicTrap': False, 'previousTarget': array([40.95850618, 19.28764556]), 'currentState': array([21.07516294, 17.81502057,  5.19934149]), 'targetState': array([83, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.40469747313640503
running average episode reward sum: 0.6925650441170684
{'scaleFactor': 20, 'currentTarget': array([83., 22.]), 'dynamicTrap': False, 'previousTarget': array([83., 22.]), 'currentState': array([83.30746175, 21.63234714,  1.63259517]), 'targetState': array([83, 22], dtype=int32), 'currentDistance': 0.4792716904497578}
episode index:428
target Thresh 67.95240873731031
target distance 56.0
model initialize at round 428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.41882175,  7.92509487]), 'dynamicTrap': False, 'previousTarget': array([71.40285  ,  6.8507125]), 'currentState': array([52.91477647,  3.49878514,  0.14184439]), 'targetState': array([108,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5151879851684091
running average episode reward sum: 0.6921515777791927
{'scaleFactor': 20, 'currentTarget': array([108.,  16.]), 'dynamicTrap': False, 'previousTarget': array([108.,  16.]), 'currentState': array([107.50663105,  15.71545075,   6.20786693]), 'targetState': array([108,  16], dtype=int32), 'currentDistance': 0.5695447258761078}
episode index:429
target Thresh 67.99254626618176
target distance 36.0
model initialize at round 429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.44115038,  6.3051441 ]), 'dynamicTrap': True, 'previousTarget': array([72.63230779,  7.18260682]), 'currentState': array([53.       , 11.       ,  2.0565019], dtype=float32), 'targetState': array([89,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6524106118420364
running average episode reward sum: 0.692059156928176
{'scaleFactor': 20, 'currentTarget': array([89.,  4.]), 'dynamicTrap': False, 'previousTarget': array([89.,  4.]), 'currentState': array([88.58332305,  3.45893959,  0.20179661]), 'targetState': array([89,  4], dtype=int32), 'currentDistance': 0.6829099900661286}
episode index:430
target Thresh 68.03248360829281
target distance 11.0
model initialize at round 430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([15.03350132, 11.32825566,  1.91955918]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 9.671802359897871}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6926378599269528
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([15.20024366, 20.5855895 ,  1.87151312]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.4602538289829344}
episode index:431
target Thresh 68.0722217620791
target distance 22.0
model initialize at round 431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.41511863, 22.99494092]), 'dynamicTrap': False, 'previousTarget': array([25., 23.]), 'currentState': array([43.41363355, 22.75121773,  2.28715086]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7855878878118469
running average episode reward sum: 0.6928530220285383
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'dynamicTrap': False, 'previousTarget': array([23., 23.]), 'currentState': array([23.33580756, 23.37982193,  3.2868008 ]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.5069826635814584}
episode index:432
target Thresh 68.11176172099653
target distance 13.0
model initialize at round 432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([14.64242774,  8.63282264,  0.79005879]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 13.854555581767375}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6933839496784214
{'scaleFactor': 20, 'currentTarget': array([11., 22.]), 'dynamicTrap': False, 'previousTarget': array([11., 22.]), 'currentState': array([11.12993328, 21.98919308,  1.64643967]), 'targetState': array([11, 22], dtype=int32), 'currentDistance': 0.1303819302589802}
episode index:433
target Thresh 68.15110447354613
target distance 32.0
model initialize at round 433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.13711732, 12.9122143 ]), 'dynamicTrap': False, 'previousTarget': array([46.6768533 , 12.98362332]), 'currentState': array([63.11734648, 21.67071642,  2.86572957]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6536039813373393
running average episode reward sum: 0.693292290765193
{'scaleFactor': 20, 'currentTarget': array([33.,  7.]), 'dynamicTrap': False, 'previousTarget': array([33.,  7.]), 'currentState': array([32.80049884,  7.56243173,  4.92177038]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 0.5967664278216226}
episode index:434
target Thresh 68.19025100329878
target distance 47.0
model initialize at round 434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.93647107,  9.1041347 ]), 'dynamicTrap': False, 'previousTarget': array([51.2835766 ,  8.35598696]), 'currentState': array([69.71459424,  6.13330249,  3.33896041]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5740348460883566
running average episode reward sum: 0.693018135719959
{'scaleFactor': 20, 'currentTarget': array([24., 13.]), 'dynamicTrap': False, 'previousTarget': array([24., 13.]), 'currentState': array([23.61701202, 13.85377241,  5.65361171]), 'targetState': array([24, 13], dtype=int32), 'currentDistance': 0.935738807066189}
episode index:435
target Thresh 68.22920228891977
target distance 63.0
model initialize at round 435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.43735526, 19.36791088]), 'dynamicTrap': False, 'previousTarget': array([45.75271057, 19.86464912]), 'currentState': array([27.66804202, 22.39681401,  5.08929831]), 'targetState': array([89, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5191746325061767
running average episode reward sum: 0.6926194120887347
{'scaleFactor': 20, 'currentTarget': array([89., 13.]), 'dynamicTrap': False, 'previousTarget': array([89., 13.]), 'currentState': array([88.95383718, 12.06927109,  5.98089303]), 'targetState': array([89, 13], dtype=int32), 'currentDistance': 0.9318730167589495}
episode index:436
target Thresh 68.26795930419323
target distance 61.0
model initialize at round 436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.2491636 , 12.78714853]), 'dynamicTrap': False, 'previousTarget': array([71.62388913, 12.86043721]), 'currentState': array([53.66283569,  8.74045124,  0.85516136]), 'targetState': array([113,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5426307649222805
running average episode reward sum: 0.6922761886398413
{'scaleFactor': 20, 'currentTarget': array([113.,  21.]), 'dynamicTrap': False, 'previousTarget': array([113.,  21.]), 'currentState': array([113.02501051,  20.38998614,   6.111998  ]), 'targetState': array([113,  21], dtype=int32), 'currentDistance': 0.6105263575796812}
episode index:437
target Thresh 68.30652301804659
target distance 53.0
model initialize at round 437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.38460502,  7.68349814]), 'dynamicTrap': False, 'previousTarget': array([65.05671759,  7.50515339]), 'currentState': array([83.33034726,  6.21130026,  4.02570844]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4360027575687056
running average episode reward sum: 0.6916910894821446
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([32.16174804, 10.76065308,  3.49114882]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 0.7776602927174471}
episode index:438
target Thresh 68.34489439457471
target distance 27.0
model initialize at round 438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.04632248, 12.05237159]), 'dynamicTrap': False, 'previousTarget': array([49.98629668, 11.74023321]), 'currentState': array([31.04709622, 12.22829502,  6.1380499 ]), 'targetState': array([57, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7968498004277383
running average episode reward sum: 0.6919306309649365
{'scaleFactor': 20, 'currentTarget': array([57., 12.]), 'dynamicTrap': False, 'previousTarget': array([57., 12.]), 'currentState': array([56.12922248, 11.86852689,  1.43366333]), 'targetState': array([57, 12], dtype=int32), 'currentDistance': 0.8806467315819808}
episode index:439
target Thresh 68.38307439306398
target distance 58.0
model initialize at round 439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.1267292 ,  9.75208811]), 'dynamicTrap': True, 'previousTarget': array([63.18757742,  9.26725206]), 'currentState': array([83.      , 12.      ,  5.619603], dtype=float32), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5207172345651977
running average episode reward sum: 0.6915415096094825
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'dynamicTrap': False, 'previousTarget': array([25.,  4.]), 'currentState': array([25.60168192,  4.9881962 ,  3.63575611]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 1.156958454487312}
episode index:440
target Thresh 68.42106396801636
target distance 15.0
model initialize at round 440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63., 10.]), 'dynamicTrap': False, 'previousTarget': array([63., 10.]), 'currentState': array([49.08600451, 11.28568359,  6.14257819]), 'targetState': array([63, 10], dtype=int32), 'currentDistance': 13.97326922432946}
done in step count: 18
reward sum = 0.7359768054330144
running average episode reward sum: 0.6916422699174725
{'scaleFactor': 20, 'currentTarget': array([63., 10.]), 'dynamicTrap': False, 'previousTarget': array([63., 10.]), 'currentState': array([62.43747252, 10.53941265,  4.94728659]), 'targetState': array([63, 10], dtype=int32), 'currentDistance': 0.7793607466605905}
episode index:441
target Thresh 68.45886406917322
target distance 7.0
model initialize at round 441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'dynamicTrap': False, 'previousTarget': array([11., 17.]), 'currentState': array([ 5.80051498, 14.62259852,  0.12794787]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 5.71722679935046}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6922507625420936
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'dynamicTrap': False, 'previousTarget': array([11., 17.]), 'currentState': array([10.84687535, 16.37100719,  0.58180363]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.6473631972023003}
episode index:442
target Thresh 68.49647564153902
target distance 24.0
model initialize at round 442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.5593053 , 13.77057105]), 'dynamicTrap': False, 'previousTarget': array([62.27557802, 14.07742051]), 'currentState': array([4.58415900e+01, 2.79250228e+00, 1.91229185e-03]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6492907704555859
running average episode reward sum: 0.6921537873906568
{'scaleFactor': 20, 'currentTarget': array([69., 18.]), 'dynamicTrap': False, 'previousTarget': array([69., 18.]), 'currentState': array([69.81943604, 18.34071231,  2.33275144]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 0.8874459469897222}
episode index:443
target Thresh 68.53389962540506
target distance 35.0
model initialize at round 443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.30854144, 12.39440949]), 'dynamicTrap': False, 'previousTarget': array([32.42293137, 12.54183726]), 'currentState': array([48.54152876,  2.24432831,  2.65981281]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7069444871805649
running average episode reward sum: 0.692187099777571
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'dynamicTrap': False, 'previousTarget': array([15., 22.]), 'currentState': array([14.39794711, 22.19903412,  3.55293053]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.6340995661531279}
episode index:444
target Thresh 68.57113695637288
target distance 8.0
model initialize at round 444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([17.01012042, 21.17507573,  3.14015159]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 6.1239162688554885}
done in step count: 8
reward sum = 0.89334070442792
running average episode reward sum: 0.692639130349819
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([10.47518208, 19.00442655,  5.76922519]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 1.125433398521808}
episode index:445
target Thresh 68.60818856537767
target distance 26.0
model initialize at round 445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.69692347,  2.67064539]), 'dynamicTrap': False, 'previousTarget': array([63.01477651,  2.76866244]), 'currentState': array([81.64793402,  1.27165363,  2.76469898]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8084505095459952
running average episode reward sum: 0.6928987971193172
{'scaleFactor': 20, 'currentTarget': array([57.,  3.]), 'dynamicTrap': False, 'previousTarget': array([57.,  3.]), 'currentState': array([56.37171803,  3.37706385,  4.75186769]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 0.7327450942715122}
episode index:446
target Thresh 68.64505537871163
target distance 63.0
model initialize at round 446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.0174157 ,  7.41358367]), 'dynamicTrap': False, 'previousTarget': array([89.29806458,  8.44002047]), 'currentState': array([108.63052419,   3.49874331,   3.46107388]), 'targetState': array([46, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5290894093976517
running average episode reward sum: 0.6925323331646827
{'scaleFactor': 20, 'currentTarget': array([46., 16.]), 'dynamicTrap': False, 'previousTarget': array([46., 16.]), 'currentState': array([46.70391484, 15.53122351,  1.71504992]), 'targetState': array([46, 16], dtype=int32), 'currentDistance': 0.8457230606377522}
episode index:447
target Thresh 68.68173831804697
target distance 54.0
model initialize at round 447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.31561561, 20.66352863]), 'dynamicTrap': False, 'previousTarget': array([41.12232531, 19.79136948]), 'currentState': array([60.12250422, 23.4361021 ,  3.12928379]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4906108538186994
running average episode reward sum: 0.6920816155768568
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 7.27925626, 16.32422437,  4.55098097]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.4279082844250947}
episode index:448
target Thresh 68.7182383004591
target distance 43.0
model initialize at round 448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.34890253, 21.44720754]), 'dynamicTrap': False, 'previousTarget': array([21.97840172, 20.92922799]), 'currentState': array([ 3.3554181 , 20.93673699,  5.87791688]), 'targetState': array([45, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6558097628962609
running average episode reward sum: 0.692000831940597
{'scaleFactor': 20, 'currentTarget': array([45., 22.]), 'dynamicTrap': False, 'previousTarget': array([45., 22.]), 'currentState': array([44.58246089, 21.46978841,  0.20571715]), 'targetState': array([45, 22], dtype=int32), 'currentDistance': 0.6748801693698031}
episode index:449
target Thresh 68.7545562384495
target distance 9.0
model initialize at round 449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76., 16.]), 'dynamicTrap': False, 'previousTarget': array([76., 16.]), 'currentState': array([68.20506529, 19.79285804,  0.18504923]), 'targetState': array([76, 16], dtype=int32), 'currentDistance': 8.668724195502666}
done in step count: 6
reward sum = 0.931874189301
running average episode reward sum: 0.6925338838458424
{'scaleFactor': 20, 'currentTarget': array([76., 16.]), 'dynamicTrap': False, 'previousTarget': array([76., 16.]), 'currentState': array([75.79042331, 16.48050803,  0.56397031]), 'targetState': array([76, 16], dtype=int32), 'currentDistance': 0.5242235704115572}
episode index:450
target Thresh 68.79069303996847
target distance 10.0
model initialize at round 450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'dynamicTrap': False, 'previousTarget': array([12., 18.]), 'currentState': array([ 3.58231599, 19.42670758,  0.66239899]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 8.537733812487618}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6931069573847651
{'scaleFactor': 20, 'currentTarget': array([12., 18.]), 'dynamicTrap': False, 'previousTarget': array([12., 18.]), 'currentState': array([11.96480017, 17.81083883,  5.72380023]), 'targetState': array([12, 18], dtype=int32), 'currentDistance': 0.19240835426714187}
episode index:451
target Thresh 68.82664960843798
target distance 10.0
model initialize at round 451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77., 17.]), 'dynamicTrap': False, 'previousTarget': array([77., 17.]), 'currentState': array([85.15019477, 16.69807071,  3.04570717]), 'targetState': array([77, 17], dtype=int32), 'currentDistance': 8.1557854376834}
done in step count: 6
reward sum = 0.921779149401
running average episode reward sum: 0.6936128693140046
{'scaleFactor': 20, 'currentTarget': array([77., 17.]), 'dynamicTrap': False, 'previousTarget': array([77., 17.]), 'currentState': array([77.57122205, 17.57783711,  4.09315066]), 'targetState': array([77, 17], dtype=int32), 'currentDistance': 0.8125209890973707}
episode index:452
target Thresh 68.86242684277407
target distance 18.0
model initialize at round 452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([106.59391102,  10.41453951,   3.45164633]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 18.4082303059571}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6940384145731704
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([89.34189067,  5.70546773,  3.83369043]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 0.7839476730087966}
episode index:453
target Thresh 68.89802563740949
target distance 34.0
model initialize at round 453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.26630807, 19.23110409]), 'dynamicTrap': False, 'previousTarget': array([92.53165663, 18.58078667]), 'currentState': array([110.84441775,  15.14483175,   1.35086775]), 'targetState': array([78, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7429107700376435
running average episode reward sum: 0.6941460629332243
{'scaleFactor': 20, 'currentTarget': array([78., 22.]), 'dynamicTrap': False, 'previousTarget': array([78., 22.]), 'currentState': array([77.9041402 , 22.21703623,  4.12879901]), 'targetState': array([78, 22], dtype=int32), 'currentDistance': 0.23726320423753902}
episode index:454
target Thresh 68.93344688231595
target distance 66.0
model initialize at round 454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.46685243,  9.11609902]), 'dynamicTrap': False, 'previousTarget': array([40.96336993, 10.20990121]), 'currentState': array([20.53615503,  7.4525783 ,  5.39694774]), 'targetState': array([87, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4832649921926408
running average episode reward sum: 0.6936825880524758
{'scaleFactor': 20, 'currentTarget': array([87., 13.]), 'dynamicTrap': False, 'previousTarget': array([87., 13.]), 'currentState': array([86.19534126, 13.21754693,  0.53044048]), 'targetState': array([87, 13], dtype=int32), 'currentDistance': 0.8335480525059579}
episode index:455
target Thresh 68.9686914630264
target distance 9.0
model initialize at round 455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'dynamicTrap': False, 'previousTarget': array([18., 13.]), 'currentState': array([10.00581541, 23.44882062,  0.19604862]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 13.156171165967134}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6942053572626831
{'scaleFactor': 20, 'currentTarget': array([18., 13.]), 'dynamicTrap': False, 'previousTarget': array([18., 13.]), 'currentState': array([18.3466938 , 13.70051514,  5.11070107]), 'targetState': array([18, 13], dtype=int32), 'currentDistance': 0.7816124633568695}
episode index:456
target Thresh 69.00376026065724
target distance 38.0
model initialize at round 456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.54148885, 12.19109976]), 'dynamicTrap': False, 'previousTarget': array([31.33093623, 12.37675141]), 'currentState': array([49.17940305, 15.97954613,  2.40174687]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6374361486218345
running average episode reward sum: 0.694081135799574
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'dynamicTrap': False, 'previousTarget': array([13.,  9.]), 'currentState': array([13.061396  ,  9.15447367,  3.19462332]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.1662275055443022}
episode index:457
target Thresh 69.03865415193019
target distance 56.0
model initialize at round 457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.81770041, 12.80071915]), 'dynamicTrap': False, 'previousTarget': array([72.68855151, 13.51581277]), 'currentState': array([54.22371907,  8.79124346,  0.16694289]), 'targetState': array([109,  20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 99
reward sum = -0.6141666587267708
running average episode reward sum: 0.6912246995669836
{'scaleFactor': 20, 'currentTarget': array([75.37394943, 12.61070278]), 'dynamicTrap': True, 'previousTarget': array([75.3754473 , 12.60470002]), 'currentState': array([55.9697081,  7.765559 ,  5.1094102]), 'targetState': array([109,  20], dtype=int32), 'currentDistance': 20.0}
episode index:458
target Thresh 69.07337400919438
target distance 64.0
model initialize at round 458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.44373868, 16.93400822]), 'dynamicTrap': False, 'previousTarget': array([44.84555753, 17.51930531]), 'currentState': array([26.57682123, 19.23739509,  5.65201026]), 'targetState': array([89, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5313534108841281
running average episode reward sum: 0.690876396105801
{'scaleFactor': 20, 'currentTarget': array([89., 12.]), 'dynamicTrap': False, 'previousTarget': array([89., 12.]), 'currentState': array([88.48379589, 11.0472297 ,  0.95222397]), 'targetState': array([89, 12], dtype=int32), 'currentDistance': 1.0836225908299004}
episode index:459
target Thresh 69.10792070044803
target distance 60.0
model initialize at round 459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.04934792, 17.29132236]), 'dynamicTrap': False, 'previousTarget': array([68.59715  , 17.1492875]), 'currentState': array([86.3889873 , 22.38821832,  3.12296137]), 'targetState': array([28,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6272125961957983
running average episode reward sum: 0.6907379965407793
{'scaleFactor': 20, 'currentTarget': array([28.,  7.]), 'dynamicTrap': False, 'previousTarget': array([28.,  7.]), 'currentState': array([27.56142211,  6.76567314,  4.93904282]), 'targetState': array([28,  7], dtype=int32), 'currentDistance': 0.4972520940271865}
episode index:460
target Thresh 69.14229508936023
target distance 7.0
model initialize at round 460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  23.]), 'dynamicTrap': False, 'previousTarget': array([115.,  23.]), 'currentState': array([112.2810022 ,  16.2108407 ,   1.89782059]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 7.313387252185904}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6913233718411247
{'scaleFactor': 20, 'currentTarget': array([115.,  23.]), 'dynamicTrap': False, 'previousTarget': array([115.,  23.]), 'currentState': array([115.5250935 ,  22.08129177,   0.41480723]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 1.0581814580979492}
episode index:461
target Thresh 69.17649803529248
target distance 13.0
model initialize at round 461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85., 20.]), 'dynamicTrap': False, 'previousTarget': array([85., 20.]), 'currentState': array([73.23163161, 14.83927513,  0.23832827]), 'targetState': array([85, 20], dtype=int32), 'currentDistance': 12.850197495632047}
done in step count: 18
reward sum = 0.7229625642190658
running average episode reward sum: 0.6913918549415099
{'scaleFactor': 20, 'currentTarget': array([85., 20.]), 'dynamicTrap': False, 'previousTarget': array([85., 20.]), 'currentState': array([84.89677622, 19.68375019,  1.25764054]), 'targetState': array([85, 20], dtype=int32), 'currentDistance': 0.3326696421506942}
episode index:462
target Thresh 69.21053039332024
target distance 9.0
model initialize at round 462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'dynamicTrap': False, 'previousTarget': array([13.,  9.]), 'currentState': array([3.21327143, 8.66612349, 5.04152369]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 9.792422046543198}
done in step count: 11
reward sum = 0.8769756348396007
running average episode reward sum: 0.6917926838397779
{'scaleFactor': 20, 'currentTarget': array([13.,  9.]), 'dynamicTrap': False, 'previousTarget': array([13.,  9.]), 'currentState': array([13.20918929,  9.00618298,  3.85162102]), 'targetState': array([13,  9], dtype=int32), 'currentDistance': 0.20928064677015631}
episode index:463
target Thresh 69.2443930142542
target distance 57.0
model initialize at round 463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.89838425, 20.61953187]), 'dynamicTrap': False, 'previousTarget': array([72.98769988, 20.7013228 ]), 'currentState': array([54.91383306, 19.83358411,  6.15916125]), 'targetState': array([110,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.46712148848965385
running average episode reward sum: 0.6913084786773854
{'scaleFactor': 20, 'currentTarget': array([110.,  22.]), 'dynamicTrap': False, 'previousTarget': array([110.,  22.]), 'currentState': array([110.52009075,  21.21152142,   1.62156337]), 'targetState': array([110,  22], dtype=int32), 'currentDistance': 0.9445596113167019}
episode index:464
target Thresh 69.27808674466168
target distance 69.0
model initialize at round 464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.50967504, 17.51366153]), 'dynamicTrap': True, 'previousTarget': array([75.51694593, 17.48219036]), 'currentState': array([95.       , 22.       ,  5.9983687], dtype=float32), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.449353937177269
running average episode reward sum: 0.6907881463300733
{'scaleFactor': 20, 'currentTarget': array([26.,  6.]), 'dynamicTrap': False, 'previousTarget': array([26.,  6.]), 'currentState': array([26.81925805,  6.88631483,  3.83941785]), 'targetState': array([26,  6], dtype=int32), 'currentDistance': 1.2069539057333565}
episode index:465
target Thresh 69.31161242688765
target distance 39.0
model initialize at round 465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.41646192,  9.17433953]), 'dynamicTrap': False, 'previousTarget': array([32.02633404,  8.32455532]), 'currentState': array([50.57279159,  3.4267284 ,  2.87173212]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.675441211237878
running average episode reward sum: 0.6907552129929656
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([12.65353465, 15.97715251,  3.29656773]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.1755571295109655}
episode index:466
target Thresh 69.34497089907596
target distance 19.0
model initialize at round 466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.50686252, 22.26256666]), 'dynamicTrap': False, 'previousTarget': array([83.67985983, 20.90977806]), 'currentState': array([73.38921992,  5.63734417,  0.3274138 ]), 'targetState': array([85, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6504560564025086
running average episode reward sum: 0.6906689192957697
{'scaleFactor': 20, 'currentTarget': array([85., 23.]), 'dynamicTrap': False, 'previousTarget': array([85., 23.]), 'currentState': array([85.18689084, 23.42475251,  2.16612091]), 'targetState': array([85, 23], dtype=int32), 'currentDistance': 0.46405051070509895}
episode index:467
target Thresh 69.37816299519011
target distance 37.0
model initialize at round 467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.42063225, 13.28789107]), 'dynamicTrap': False, 'previousTarget': array([96.11585905, 13.14963686]), 'currentState': array([114.29849004,  11.08090907,   2.22157645]), 'targetState': array([79, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7022726607415182
running average episode reward sum: 0.6906937136150982
{'scaleFactor': 20, 'currentTarget': array([79., 15.]), 'dynamicTrap': False, 'previousTarget': array([79., 15.]), 'currentState': array([78.7812806 , 15.34923408,  4.01749929]), 'targetState': array([79, 15], dtype=int32), 'currentDistance': 0.4120711306767621}
episode index:468
target Thresh 69.41118954503428
target distance 19.0
model initialize at round 468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.12422169, 19.84368036]), 'dynamicTrap': True, 'previousTarget': array([57., 20.]), 'currentState': array([76.       , 23.       ,  2.1432755], dtype=float32), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 19.137851516028828}
done in step count: 19
reward sum = 0.7782886306106622
running average episode reward sum: 0.6908804831609311
{'scaleFactor': 20, 'currentTarget': array([57., 20.]), 'dynamicTrap': False, 'previousTarget': array([57., 20.]), 'currentState': array([57.1658913 , 19.92387985,  1.07865214]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 0.18252178694446608}
episode index:469
target Thresh 69.4440513742739
target distance 36.0
model initialize at round 469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.69295178, 19.55582038]), 'dynamicTrap': False, 'previousTarget': array([47.93091516, 19.6609096 ]), 'currentState': array([29.7940722 , 17.54719107,  6.20364469]), 'targetState': array([64, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.25835682307425667
running average episode reward sum: 0.6899602200543637
{'scaleFactor': 20, 'currentTarget': array([64., 21.]), 'dynamicTrap': False, 'previousTarget': array([64., 21.]), 'currentState': array([64.03355439, 20.71934508,  6.23104462]), 'targetState': array([64, 21], dtype=int32), 'currentDistance': 0.28265364632156326}
episode index:470
target Thresh 69.4767493044564
target distance 55.0
model initialize at round 470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.1388696 , 10.21574977]), 'dynamicTrap': False, 'previousTarget': array([27.73749166, 11.22977136]), 'currentState': array([8.50716315, 6.39526525, 6.02849722]), 'targetState': array([63, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3161279252469347
running average episode reward sum: 0.6891665209146451
{'scaleFactor': 20, 'currentTarget': array([56.47523842, 18.08296632]), 'dynamicTrap': True, 'previousTarget': array([56.60061039, 18.0970555 ]), 'currentState': array([55.52988971, 18.39560403,  6.01140789]), 'targetState': array([63, 17], dtype=int32), 'currentDistance': 0.9957040262914887}
episode index:471
target Thresh 69.50928415303179
target distance 42.0
model initialize at round 471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.62074274,  4.33010924]), 'dynamicTrap': False, 'previousTarget': array([84.90990945,  3.89618185]), 'currentState': array([66.6875493 ,  2.69676927,  0.12670827]), 'targetState': array([107,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6332743754675091
running average episode reward sum: 0.6890481053522572
{'scaleFactor': 20, 'currentTarget': array([107.,   6.]), 'dynamicTrap': False, 'previousTarget': array([107.,   6.]), 'currentState': array([106.6131096 ,   5.00420782,   1.36746556]), 'targetState': array([107,   6], dtype=int32), 'currentDistance': 1.0683099994140406}
episode index:472
target Thresh 69.54165673337292
target distance 47.0
model initialize at round 472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.44181903,  9.79478985]), 'dynamicTrap': False, 'previousTarget': array([32.04061834, 10.72599692]), 'currentState': array([52.43343591, 10.37380155,  3.96286678]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5808165142122309
running average episode reward sum: 0.6888192859206715
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([4.1863096 , 8.39101813, 5.22846954]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 1.0163419661132278}
episode index:473
target Thresh 69.57386785479602
target distance 61.0
model initialize at round 473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.58376089,  9.28871542]), 'dynamicTrap': False, 'previousTarget': array([50., 10.]), 'currentState': array([31.58701649,  8.92786475,  6.20170653]), 'targetState': array([91, 10], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6171685251201263
running average episode reward sum: 0.6886681239780543
{'scaleFactor': 20, 'currentTarget': array([91., 10.]), 'dynamicTrap': False, 'previousTarget': array([91., 10.]), 'currentState': array([90.84427   ,  9.52035003,  1.11671981]), 'targetState': array([91, 10], dtype=int32), 'currentDistance': 0.504297459552758}
episode index:474
target Thresh 69.6059183225808
target distance 62.0
model initialize at round 474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.29224047,  8.55544204]), 'dynamicTrap': False, 'previousTarget': array([58.96582764,  8.1400556 ]), 'currentState': array([76.30959145,  2.36348261,  3.16031966]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.538436025522959
running average episode reward sum: 0.6883518458760437
{'scaleFactor': 20, 'currentTarget': array([16.75735853, 20.6386056 ]), 'dynamicTrap': True, 'previousTarget': array([16., 22.]), 'currentState': array([17.63248565, 20.46581873,  1.9059173 ]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8920217341829454}
episode index:475
target Thresh 69.63780893799061
target distance 20.0
model initialize at round 475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 16.]), 'dynamicTrap': False, 'previousTarget': array([30.40285  , 15.8507125]), 'currentState': array([12.68806712, 10.27085844,  0.30341309]), 'targetState': array([31, 16], dtype=int32), 'currentDistance': 19.187234009373668}
done in step count: 21
reward sum = 0.7175335326688925
running average episode reward sum: 0.6884131519407345
{'scaleFactor': 20, 'currentTarget': array([31., 16.]), 'dynamicTrap': False, 'previousTarget': array([31., 16.]), 'currentState': array([31.67680149, 15.43413514,  0.60720036]), 'targetState': array([31, 16], dtype=int32), 'currentDistance': 0.8821923260285901}
episode index:476
target Thresh 69.66954049829252
target distance 66.0
model initialize at round 476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.65776535,  5.62278062]), 'dynamicTrap': False, 'previousTarget': array([57.4353175 ,  6.15008417]), 'currentState': array([76.16038051,  1.19017394,  3.1453886 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5727569737441394
running average episode reward sum: 0.6881706861583518
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([10.97008874, 16.49875821,  3.37807686]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.4996543144097149}
episode index:477
target Thresh 69.70111379677716
target distance 51.0
model initialize at round 477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.66604019,  7.49890985]), 'dynamicTrap': False, 'previousTarget': array([50.86301209,  6.66317505]), 'currentState': array([3.18824430e+01, 1.04330675e+01, 8.86034966e-03]), 'targetState': array([82,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.624386383455389
running average episode reward sum: 0.688037246194538
{'scaleFactor': 20, 'currentTarget': array([82.,  3.]), 'dynamicTrap': False, 'previousTarget': array([82.,  3.]), 'currentState': array([82.99642239,  2.18743197,  1.45950302]), 'targetState': array([82,  3], dtype=int32), 'currentDistance': 1.2857388456785048}
episode index:478
target Thresh 69.73252962277867
target distance 45.0
model initialize at round 478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.33950703,  8.66947083]), 'dynamicTrap': True, 'previousTarget': array([54.4762588 ,  9.33860916]), 'currentState': array([74.        ,  5.        ,  0.90803164], dtype=float32), 'targetState': array([29, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6332633364103183
running average episode reward sum: 0.687922895652191
{'scaleFactor': 20, 'currentTarget': array([29., 15.]), 'dynamicTrap': False, 'previousTarget': array([29., 15.]), 'currentState': array([29.28219218, 15.24022953,  2.49866155]), 'targetState': array([29, 15], dtype=int32), 'currentDistance': 0.3705976963898523}
episode index:479
target Thresh 69.7637887616943
target distance 33.0
model initialize at round 479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.37144869, 15.21292499]), 'dynamicTrap': False, 'previousTarget': array([34.99082358, 15.39421747]), 'currentState': array([16.3748006 , 15.57907431,  0.71289032]), 'targetState': array([48, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5281931639984748
running average episode reward sum: 0.6875901253779124
{'scaleFactor': 20, 'currentTarget': array([48., 15.]), 'dynamicTrap': False, 'previousTarget': array([48., 15.]), 'currentState': array([47.51946454, 14.78135888,  6.0195657 ]), 'targetState': array([48, 15], dtype=int32), 'currentDistance': 0.5279377492014591}
episode index:480
target Thresh 69.79489199500418
target distance 48.0
model initialize at round 480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.84653819, 10.4039962 ]), 'dynamicTrap': False, 'previousTarget': array([49.46153846,  9.69230769]), 'currentState': array([32.35376304,  2.78708953,  5.79982251]), 'targetState': array([79, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6799907564475173
running average episode reward sum: 0.687574326274107
{'scaleFactor': 20, 'currentTarget': array([79., 22.]), 'dynamicTrap': False, 'previousTarget': array([79., 22.]), 'currentState': array([78.99605016, 22.6636634 ,  0.31338169]), 'targetState': array([79, 22], dtype=int32), 'currentDistance': 0.6636751562688918}
episode index:481
target Thresh 69.82584010029075
target distance 40.0
model initialize at round 481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.99254027, 16.45380077]), 'dynamicTrap': True, 'previousTarget': array([95.99375293, 16.50015618]), 'currentState': array([76.     , 17.     ,  0.46438], dtype=float32), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.42343791484688775
running average episode reward sum: 0.6870263254205236
{'scaleFactor': 20, 'currentTarget': array([116.,  16.]), 'dynamicTrap': False, 'previousTarget': array([116.,  16.]), 'currentState': array([115.7821541 ,  15.87052471,   0.20459641]), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 0.2534180089707068}
episode index:482
target Thresh 69.85663385125827
target distance 19.0
model initialize at round 482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.38666346, 14.6978981 ]), 'dynamicTrap': False, 'previousTarget': array([88.69765531, 14.39288577]), 'currentState': array([74.37913277,  1.47768545,  0.64787978]), 'targetState': array([92, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7362588413202265
running average episode reward sum: 0.6871282560952642
{'scaleFactor': 20, 'currentTarget': array([92., 17.]), 'dynamicTrap': False, 'previousTarget': array([92., 17.]), 'currentState': array([91.89349586, 16.51966432,  1.72156175]), 'targetState': array([92, 17], dtype=int32), 'currentDistance': 0.49200152165883887}
episode index:483
target Thresh 69.88727401775208
target distance 57.0
model initialize at round 483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.98954558, 21.35341859]), 'dynamicTrap': True, 'previousTarget': array([34.98769988, 21.2986772 ]), 'currentState': array([15.      , 22.      ,  5.292453], dtype=float32), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6057881630720394
running average episode reward sum: 0.6869601980518277
{'scaleFactor': 20, 'currentTarget': array([72., 20.]), 'dynamicTrap': False, 'previousTarget': array([72., 20.]), 'currentState': array([71.46929091, 19.85426572,  0.39790345]), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 0.5503549953374117}
episode index:484
target Thresh 69.91776136577796
target distance 4.0
model initialize at round 484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.0368424 , 5.02792686, 4.69183363]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 3.1774224813817167}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6875646100146074
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.41282304, 2.32578718, 6.16039499]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.6715013591598532}
episode index:485
target Thresh 69.94809665752123
target distance 10.0
model initialize at round 485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([109.02022486,  10.81819283,   4.9569838 ]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 11.892742998686431}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6880676979526577
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([117.23656843,   2.07973244,   0.26003352]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 0.24964351618091818}
episode index:486
target Thresh 69.97828065136572
target distance 55.0
model initialize at round 486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.43876856, 18.75863875]), 'dynamicTrap': False, 'previousTarget': array([58.91786413, 19.81071492]), 'currentState': array([38.57952019, 16.39004118,  5.46692133]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7078651747004917
running average episode reward sum: 0.6881083498556306
{'scaleFactor': 20, 'currentTarget': array([94., 23.]), 'dynamicTrap': False, 'previousTarget': array([94., 23.]), 'currentState': array([93.41113932, 23.28262786,  1.48859179]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 0.6531733400905956}
episode index:487
target Thresh 70.00831410191286
target distance 32.0
model initialize at round 487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.72739733,  3.30049696]), 'dynamicTrap': False, 'previousTarget': array([66.84555753,  3.51930531]), 'currentState': array([48.88576819,  5.81241912,  0.21106612]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7460599780715755
running average episode reward sum: 0.6882271031921386
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([78.78360043,  1.76071045,  5.95757979]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 0.32262712668989435}
episode index:488
target Thresh 70.03819776000047
target distance 46.0
model initialize at round 488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.24257142,  9.54807878]), 'dynamicTrap': True, 'previousTarget': array([57.24618806,  9.5608599 ]), 'currentState': array([38.      , 15.      ,  2.604154], dtype=float32), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.489321845263048
running average episode reward sum: 0.6878203439734697
{'scaleFactor': 20, 'currentTarget': array([84.,  2.]), 'dynamicTrap': False, 'previousTarget': array([84.,  2.]), 'currentState': array([84.03952649,  1.91021711,  1.18740497]), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 0.0980984735251402}
episode index:489
target Thresh 70.06793237272159
target distance 8.0
model initialize at round 489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.,  9.]), 'dynamicTrap': False, 'previousTarget': array([64.,  9.]), 'currentState': array([70.21978102,  9.8692249 ,  2.92826849]), 'targetState': array([64,  9], dtype=int32), 'currentDistance': 6.280225140421652}
done in step count: 3
reward sum = 0.9605960100000001
running average episode reward sum: 0.6883770290061769
{'scaleFactor': 20, 'currentTarget': array([65.33226789,  9.93704743]), 'dynamicTrap': True, 'previousTarget': array([64.,  9.]), 'currentState': array([66.28224427, 10.07172166,  2.7645618 ]), 'targetState': array([64,  9], dtype=int32), 'currentDistance': 0.9594749973098933}
episode index:490
target Thresh 70.09751868344306
target distance 14.0
model initialize at round 490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([47.40720898,  7.19404484,  2.32911706]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 17.830628770194647}
done in step count: 14
reward sum = 0.8219447483767827
running average episode reward sum: 0.6886490610211884
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([35.83036111, 19.21565017,  3.28110658]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 1.1422365026004866}
episode index:491
target Thresh 70.1269574318242
target distance 13.0
model initialize at round 491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([93.02397631,  7.39466594,  1.24001741]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 11.605358829003203}
done in step count: 14
reward sum = 0.8310799208516191
running average episode reward sum: 0.6889385546387299
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([92.32138161, 18.49444297,  4.65351642]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 0.8462333162281285}
episode index:492
target Thresh 70.15624935383522
target distance 37.0
model initialize at round 492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.71535211,  8.76653251]), 'dynamicTrap': False, 'previousTarget': array([29.0073006 ,  9.54034323]), 'currentState': array([48.66112017,  7.29468458,  3.60341334]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5813533484829005
running average episode reward sum: 0.6887203290684341
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([12.80556684,  9.25038718,  2.24949535]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 1.100389614760765}
episode index:493
target Thresh 70.18539518177575
target distance 59.0
model initialize at round 493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.48222299,  9.04378477]), 'dynamicTrap': False, 'previousTarget': array([69.95419404,  9.64717329]), 'currentState': array([51.51183247, 10.13167385,  5.21960139]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = -0.04600854785771241
running average episode reward sum: 0.6872330236495552
{'scaleFactor': 20, 'currentTarget': array([109.,   7.]), 'dynamicTrap': False, 'previousTarget': array([109.,   7.]), 'currentState': array([108.8715698 ,   6.74283573,   6.23620346]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 0.2874504805411589}
episode index:494
target Thresh 70.21439564429298
target distance 2.0
model initialize at round 494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,  17.]), 'dynamicTrap': False, 'previousTarget': array([105.,  17.]), 'currentState': array([107.85635079,  18.01515958,   0.590635  ]), 'targetState': array([105,  17], dtype=int32), 'currentDistance': 3.031383977861014}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6878048741068289
{'scaleFactor': 20, 'currentTarget': array([105.,  17.]), 'dynamicTrap': False, 'previousTarget': array([105.,  17.]), 'currentState': array([105.3039455 ,  17.93066618,   3.69509297]), 'targetState': array([105,  17], dtype=int32), 'currentDistance': 0.979041575194516}
episode index:495
target Thresh 70.24325146639997
target distance 18.0
model initialize at round 495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.90114073,  5.69760504]), 'dynamicTrap': False, 'previousTarget': array([56.21295565,  6.72118773]), 'currentState': array([70.71605917, 17.940495  ,  2.82151031]), 'targetState': array([54,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7368130719519455
running average episode reward sum: 0.687903680957323
{'scaleFactor': 20, 'currentTarget': array([54.,  5.]), 'dynamicTrap': False, 'previousTarget': array([54.,  5.]), 'currentState': array([53.26365923,  5.42306445,  3.48784878]), 'targetState': array([54,  5], dtype=int32), 'currentDistance': 0.8492239211624453}
episode index:496
target Thresh 70.27196336949379
target distance 20.0
model initialize at round 496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.02068761, 22.95430052]), 'dynamicTrap': False, 'previousTarget': array([97.79270645, 21.2384301 ]), 'currentState': array([105.26868987,   4.73424402,   1.363801  ]), 'targetState': array([97, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7457187159457463
running average episode reward sum: 0.6880200089955292
{'scaleFactor': 20, 'currentTarget': array([97., 23.]), 'dynamicTrap': False, 'previousTarget': array([97., 23.]), 'currentState': array([96.5450437 , 23.58279148,  3.7794268 ]), 'targetState': array([97, 23], dtype=int32), 'currentDistance': 0.73934507694889}
episode index:497
target Thresh 70.30053207137352
target distance 26.0
model initialize at round 497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.03514668, 12.30520096]), 'dynamicTrap': False, 'previousTarget': array([11.84081231, 12.38116355]), 'currentState': array([31.54708894,  4.73499662,  1.40244585]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6883310997272327
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 16.]), 'currentState': array([ 3.8495977 , 16.20323856,  4.15526423]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.2528374264671526}
episode index:498
target Thresh 70.32895828625819
target distance 38.0
model initialize at round 498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.18867274, 10.78109088]), 'dynamicTrap': False, 'previousTarget': array([47.00692161, 11.47386636]), 'currentState': array([66.18705097, 10.52639831,  3.19826078]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6395327815819237
running average episode reward sum: 0.6882333075065007
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([28.82299507, 11.56452937,  4.46643706]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.591628394268279}
episode index:499
target Thresh 70.35724272480464
target distance 10.0
model initialize at round 499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  19.]), 'dynamicTrap': False, 'previousTarget': array([110.,  19.]), 'currentState': array([117.06943227,  10.77020155,   2.03663683]), 'targetState': array([110,  19], dtype=int32), 'currentDistance': 10.849260582812176}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6887398011902897
{'scaleFactor': 20, 'currentTarget': array([110.,  19.]), 'dynamicTrap': False, 'previousTarget': array([110.,  19.]), 'currentState': array([110.48509246,  19.49198689,   3.40352101]), 'targetState': array([110,  19], dtype=int32), 'currentDistance': 0.6909166367632485}
episode index:500
target Thresh 70.38538609412532
target distance 40.0
model initialize at round 500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.36404027, 17.09403635]), 'dynamicTrap': False, 'previousTarget': array([88.28410786, 16.30312966]), 'currentState': array([68.90635339, 12.46819361,  0.6244936 ]), 'targetState': array([109,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6351302828802431
running average episode reward sum: 0.6886327961637227
{'scaleFactor': 20, 'currentTarget': array([109.,  22.]), 'dynamicTrap': False, 'previousTarget': array([109.,  22.]), 'currentState': array([109.24272201,  22.90240567,   0.24479332]), 'targetState': array([109,  22], dtype=int32), 'currentDistance': 0.9344784521732608}
episode index:501
target Thresh 70.41338909780593
target distance 47.0
model initialize at round 501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.49454454, 16.86616485]), 'dynamicTrap': False, 'previousTarget': array([31.21819644, 15.94622606]), 'currentState': array([51.36588185, 14.60122478,  2.30559707]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.11593267821675707
running average episode reward sum: 0.6874919592753822
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 20.]), 'currentState': array([ 3.68506866, 19.97994355,  5.9990641 ]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.31556934214281035}
episode index:502
target Thresh 70.441252435923
target distance 41.0
model initialize at round 502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.31655356, 15.4229754 ]), 'dynamicTrap': False, 'previousTarget': array([35.37018816, 15.83020719]), 'currentState': array([53.82755508, 11.02742999,  2.85263515]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.687687160431509
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'dynamicTrap': False, 'previousTarget': array([14., 20.]), 'currentState': array([13.26327261, 20.98407688,  3.36494313]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 1.229298397375928}
episode index:503
target Thresh 70.46897680506147
target distance 18.0
model initialize at round 503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.99193583,   9.14940149]), 'dynamicTrap': False, 'previousTarget': array([101.45973695,   8.26752934]), 'currentState': array([115.85794801,  23.56237156,   2.64331281]), 'targetState': array([98,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6763354810854847
running average episode reward sum: 0.6876646372582034
{'scaleFactor': 20, 'currentTarget': array([98.,  5.]), 'dynamicTrap': False, 'previousTarget': array([98.,  5.]), 'currentState': array([97.72936859,  4.93845251,  5.715778  ]), 'targetState': array([98,  5], dtype=int32), 'currentDistance': 0.27754180552283453}
episode index:504
target Thresh 70.49656289833197
target distance 64.0
model initialize at round 504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.9715696 , 13.93397575]), 'dynamicTrap': True, 'previousTarget': array([73.93924283, 13.44224665]), 'currentState': array([54.      , 15.      ,  3.392036], dtype=float32), 'targetState': array([118,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3819527532792592
running average episode reward sum: 0.6870592671909184
{'scaleFactor': 20, 'currentTarget': array([118.,  10.]), 'dynamicTrap': False, 'previousTarget': array([118.,  10.]), 'currentState': array([117.60115823,   9.72145541,   1.81417268]), 'targetState': array([118,  10], dtype=int32), 'currentDistance': 0.48647902365636386}
episode index:505
target Thresh 70.52401140538831
target distance 8.0
model initialize at round 505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,  15.]), 'dynamicTrap': False, 'previousTarget': array([114.,  15.]), 'currentState': array([107.92325724,  21.30721535,   5.17497946]), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 8.758297100846884}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6875808695282881
{'scaleFactor': 20, 'currentTarget': array([114.,  15.]), 'dynamicTrap': False, 'previousTarget': array([114.,  15.]), 'currentState': array([113.05100262,  14.67478023,   5.05525533]), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 1.0031769176047318}
episode index:506
target Thresh 70.55132301244458
target distance 64.0
model initialize at round 506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.88890359, 12.73159609]), 'dynamicTrap': False, 'previousTarget': array([94.19486842, 12.21490337]), 'currentState': array([112.64701722,  15.83271618,   3.59987795]), 'targetState': array([50,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.43414615492954206
running average episode reward sum: 0.6870809982963378
{'scaleFactor': 20, 'currentTarget': array([50.,  6.]), 'dynamicTrap': False, 'previousTarget': array([50.,  6.]), 'currentState': array([50.14903881,  5.88819994,  2.30145148]), 'targetState': array([50,  6], dtype=int32), 'currentDistance': 0.18631108189056134}
episode index:507
target Thresh 70.57849840229235
target distance 16.0
model initialize at round 507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 23.]), 'dynamicTrap': False, 'previousTarget': array([30., 23.]), 'currentState': array([32.67385477,  8.04851978,  1.47540092]), 'targetState': array([30, 23], dtype=int32), 'currentDistance': 15.188688559965332}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6875449032099827
{'scaleFactor': 20, 'currentTarget': array([30., 23.]), 'dynamicTrap': False, 'previousTarget': array([30., 23.]), 'currentState': array([29.76618891, 22.64950416,  1.2021286 ]), 'targetState': array([30, 23], dtype=int32), 'currentDistance': 0.4213252439319385}
episode index:508
target Thresh 70.60553825431784
target distance 32.0
model initialize at round 508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.33407113, 16.58817977]), 'dynamicTrap': False, 'previousTarget': array([67.52933154, 17.52754094]), 'currentState': array([48.00884943,  8.57654762,  5.02445626]), 'targetState': array([81, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6429106133984862
running average episode reward sum: 0.6874572130531822
{'scaleFactor': 20, 'currentTarget': array([81., 23.]), 'dynamicTrap': False, 'previousTarget': array([81., 23.]), 'currentState': array([80.29663197, 22.5342691 ,  5.39430721]), 'targetState': array([81, 23], dtype=int32), 'currentDistance': 0.843582750339239}
episode index:509
target Thresh 70.6324432445187
target distance 24.0
model initialize at round 509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.37579218,  6.41005201]), 'dynamicTrap': False, 'previousTarget': array([37.57960839,  6.07908508]), 'currentState': array([19.86280313,  2.02333817,  6.19894463]), 'targetState': array([42,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7045962082419183
running average episode reward sum: 0.6874908189261013
{'scaleFactor': 20, 'currentTarget': array([42.,  7.]), 'dynamicTrap': False, 'previousTarget': array([42.,  7.]), 'currentState': array([42.42359667,  7.56059129,  3.08198861]), 'targetState': array([42,  7], dtype=int32), 'currentDistance': 0.7026355657424269}
episode index:510
target Thresh 70.65921404552113
target distance 29.0
model initialize at round 510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.17971752, 12.81661788]), 'dynamicTrap': False, 'previousTarget': array([16.74981351, 12.18111808]), 'currentState': array([33.37898496,  4.52284525,  3.65478396]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6902416447611619
running average episode reward sum: 0.6874962021469135
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.25235123, 17.41867794,  2.63254454]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.4888479957810765}
episode index:511
target Thresh 70.68585132659652
target distance 55.0
model initialize at round 511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.23322866,  3.20435699]), 'dynamicTrap': False, 'previousTarget': array([53.01321004,  3.27320764]), 'currentState': array([71.22010854,  3.92867237,  2.73757464]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5787005432969217
running average episode reward sum: 0.6872837106257221
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'dynamicTrap': False, 'previousTarget': array([18.,  2.]), 'currentState': array([18.79050725,  2.54581495,  2.35381484]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.9606329513219485}
episode index:512
target Thresh 70.7123557536783
target distance 41.0
model initialize at round 512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.75870262, 10.9518272 ]), 'dynamicTrap': False, 'previousTarget': array([51.06461275,  9.95512279]), 'currentState': array([30.83563664, 17.42620547,  1.97167563]), 'targetState': array([73,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7151623390917394
running average episode reward sum: 0.6873380549307242
{'scaleFactor': 20, 'currentTarget': array([73.,  3.]), 'dynamicTrap': False, 'previousTarget': array([73.,  3.]), 'currentState': array([72.09379829,  3.32926189,  5.06792739]), 'targetState': array([73,  3], dtype=int32), 'currentDistance': 0.9641654037339549}
episode index:513
target Thresh 70.73872798937852
target distance 66.0
model initialize at round 513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.25435274,  8.17953052]), 'dynamicTrap': True, 'previousTarget': array([83.3226018 ,  8.57770876]), 'currentState': array([103.        ,   5.        ,   0.11664411], dtype=float32), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.04722565867543688
running average episode reward sum: 0.6860927000741963
{'scaleFactor': 20, 'currentTarget': array([37., 17.]), 'dynamicTrap': False, 'previousTarget': array([37., 17.]), 'currentState': array([36.81911437, 17.16673486,  4.22815673]), 'targetState': array([37, 17], dtype=int32), 'currentDistance': 0.24600838681776915}
episode index:514
target Thresh 70.76496869300445
target distance 7.0
model initialize at round 514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  12.]), 'dynamicTrap': False, 'previousTarget': array([113.,  12.]), 'currentState': array([116.36205001,  17.33771684,   3.94788539]), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 6.308296232290609}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6866445569672561
{'scaleFactor': 20, 'currentTarget': array([113.,  12.]), 'dynamicTrap': False, 'previousTarget': array([113.,  12.]), 'currentState': array([113.33144213,  12.96539337,   5.17182086]), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 1.0207047786798509}
episode index:515
target Thresh 70.79107852057506
target distance 28.0
model initialize at round 515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.81389551, 20.7220479 ]), 'dynamicTrap': True, 'previousTarget': array([78.79898987, 20.82842712]), 'currentState': array([59.       , 18.       ,  2.5125315], dtype=float32), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8154699406999995
running average episode reward sum: 0.6868942185636374
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([86.67439428, 22.06185889,  6.25710161]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 0.3314296408435839}
episode index:516
target Thresh 70.81705812483737
target distance 58.0
model initialize at round 516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.492169  ,  8.64849547]), 'dynamicTrap': False, 'previousTarget': array([48.76347807,  9.9332534 ]), 'currentState': array([29.64401832, 11.10835615,  5.11334631]), 'targetState': array([87,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5150662069408876
running average episode reward sum: 0.6865618626417366
{'scaleFactor': 20, 'currentTarget': array([87.,  4.]), 'dynamicTrap': False, 'previousTarget': array([87.,  4.]), 'currentState': array([86.05473959,  4.10124961,  1.73054909]), 'targetState': array([87,  4], dtype=int32), 'currentDistance': 0.9506675168275082}
episode index:517
target Thresh 70.84290815528286
target distance 14.0
model initialize at round 517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([64.77261392, 12.5896958 ,  6.22356844]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 14.839780323498092}
done in step count: 12
reward sum = 0.8674486992412929
running average episode reward sum: 0.6869110650289943
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([50.74103621, 13.79924603,  4.62581043]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 0.7677478929364733}
episode index:518
target Thresh 70.86862925816364
target distance 31.0
model initialize at round 518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.68623211, 12.6510424 ]), 'dynamicTrap': False, 'previousTarget': array([89.94797495, 11.54875884]), 'currentState': array([107.40029477,  23.63467157,   2.34064439]), 'targetState': array([76,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4350686952881581
running average episode reward sum: 0.6864258196152355
{'scaleFactor': 20, 'currentTarget': array([76.,  3.]), 'dynamicTrap': False, 'previousTarget': array([76.,  3.]), 'currentState': array([75.75526995,  3.58584787,  5.01283788]), 'targetState': array([76,  3], dtype=int32), 'currentDistance': 0.6349098565100849}
episode index:519
target Thresh 70.89422207650861
target distance 27.0
model initialize at round 519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.01515694,  8.59246032]), 'dynamicTrap': False, 'previousTarget': array([84.24490937,  8.79365671]), 'currentState': array([100.36847066,  18.53542279,   3.77582324]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7893762755042207
running average episode reward sum: 0.6866238012611758
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([75.46431923,  3.49880459,  4.79103832]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 0.6832197164276497}
episode index:520
target Thresh 70.91968725013956
target distance 44.0
model initialize at round 520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.46878861,  9.5189286 ]), 'dynamicTrap': False, 'previousTarget': array([50.1278949 , 10.25819376]), 'currentState': array([69.25234261,  6.58447957,  3.29215419]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5964371920362457
running average episode reward sum: 0.686450698364391
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([26.47436406, 13.71424523,  2.73008071]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.8574190929728245}
episode index:521
target Thresh 70.94502541568718
target distance 30.0
model initialize at round 521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.65305724, 15.10448913]), 'dynamicTrap': False, 'previousTarget': array([99.43046618, 14.42781353]), 'currentState': array([116.19978746,   7.61991846,   3.35609317]), 'targetState': array([88, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5391477487460149
running average episode reward sum: 0.6841028086189688
{'scaleFactor': 20, 'currentTarget': array([93.78010443,  5.83423149]), 'dynamicTrap': True, 'previousTarget': array([93.92176584,  5.95632085]), 'currentState': array([102.98380644,   9.29051274,   3.44436362]), 'targetState': array([88, 19], dtype=int32), 'currentDistance': 9.831277171063865}
episode index:522
target Thresh 70.9702372066069
target distance 17.0
model initialize at round 522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.,  5.]), 'dynamicTrap': False, 'previousTarget': array([67.,  5.]), 'currentState': array([70.6786297 , 20.28382138,  3.62792635]), 'targetState': array([67,  5], dtype=int32), 'currentDistance': 15.720289831055236}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6845414595537005
{'scaleFactor': 20, 'currentTarget': array([67.,  5.]), 'dynamicTrap': False, 'previousTarget': array([67.,  5.]), 'currentState': array([66.09114593,  5.58974451,  4.2778075 ]), 'targetState': array([67,  5], dtype=int32), 'currentDistance': 1.0834271162007563}
episode index:523
target Thresh 70.99532325319481
target distance 40.0
model initialize at round 523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.01530639, 10.13423605]), 'dynamicTrap': False, 'previousTarget': array([86.28410786,  9.30312966]), 'currentState': array([66.53220084,  5.6166484 ,  0.84246659]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4835895006186789
running average episode reward sum: 0.6841579634488626
{'scaleFactor': 20, 'currentTarget': array([107.,  15.]), 'dynamicTrap': False, 'previousTarget': array([107.,  15.]), 'currentState': array([107.05589712,  14.82164831,   5.52340308]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 0.18690589530814425}
episode index:524
target Thresh 71.0202841826034
target distance 40.0
model initialize at round 524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.1362484 , 12.29835305]), 'dynamicTrap': False, 'previousTarget': array([51.0992562 , 11.99007438]), 'currentState': array([69.0487927 , 10.43004382,  2.84711844]), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5561031761825149
running average episode reward sum: 0.6839140495683552
{'scaleFactor': 20, 'currentTarget': array([31., 14.]), 'dynamicTrap': False, 'previousTarget': array([31., 14.]), 'currentState': array([31.45781994, 13.73480825,  0.73768495]), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 0.5290801114874387}
episode index:525
target Thresh 71.0451206188572
target distance 21.0
model initialize at round 525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.01453486, 23.04173265]), 'dynamicTrap': False, 'previousTarget': array([ 9.02263725, 22.95130299]), 'currentState': array([30.01024481, 23.45595932,  1.81426126]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6842989750857464
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.10957727, 23.84799266,  2.06245147]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.8550431150761791}
episode index:526
target Thresh 71.0698331828684
target distance 49.0
model initialize at round 526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.76869134,  4.98936788]), 'dynamicTrap': False, 'previousTarget': array([69.89668285,  4.96972624]), 'currentState': array([51.88812259,  7.17179453,  0.41056996]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7249063659205146
running average episode reward sum: 0.6843760289582982
{'scaleFactor': 20, 'currentTarget': array([99.,  2.]), 'dynamicTrap': False, 'previousTarget': array([97.58085807,  1.6120498 ]), 'currentState': array([98.06152777,  1.68030204,  1.01322045]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 0.991431746013074}
episode index:527
target Thresh 71.0944224924524
target distance 27.0
model initialize at round 527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.95183101,  9.97793847]), 'dynamicTrap': False, 'previousTarget': array([23.48314552, 10.28714138]), 'currentState': array([ 5.03048699, 18.85630705,  5.01922941]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6845678890186181
{'scaleFactor': 20, 'currentTarget': array([33.,  5.]), 'dynamicTrap': False, 'previousTarget': array([33.,  5.]), 'currentState': array([33.51806292,  4.53712886,  2.454463  ]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 0.6947221652223153}
episode index:528
target Thresh 71.1188891623432
target distance 9.0
model initialize at round 528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 18.]), 'currentState': array([12.66638362,  9.23569837,  1.15051096]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 13.805242575627187}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6850181287263861
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.33210103, 18.25229868,  2.33866286]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.41706800361768986}
episode index:529
target Thresh 71.14323380420883
target distance 22.0
model initialize at round 529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.33900674, 16.45357406]), 'dynamicTrap': False, 'previousTarget': array([89.0585156 , 16.93592685]), 'currentState': array([71.04542941, 21.72214823,  0.11348933]), 'targetState': array([92, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5904639419483141
running average episode reward sum: 0.6848397246003898
{'scaleFactor': 20, 'currentTarget': array([92., 16.]), 'dynamicTrap': False, 'previousTarget': array([92., 16.]), 'currentState': array([91.63100978, 16.13826757,  5.22949223]), 'targetState': array([92, 16], dtype=int32), 'currentDistance': 0.39404530933140225}
episode index:530
target Thresh 71.16745702666661
target distance 40.0
model initialize at round 530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.99811608, 16.15823919]), 'dynamicTrap': False, 'previousTarget': array([54., 17.]), 'currentState': array([73.98042222, 15.31714386,  3.69075584]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5160579787268775
running average episode reward sum: 0.6845218682051478
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([33.66018457, 17.61553112,  2.31493158]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 0.7031024761890629}
episode index:531
target Thresh 71.19155943529837
target distance 9.0
model initialize at round 531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  18.]), 'dynamicTrap': False, 'previousTarget': array([115.,  18.]), 'currentState': array([112.25239946,  10.50686037,   0.81612551]), 'targetState': array([115,  18], dtype=int32), 'currentDistance': 7.981005593145839}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6849696554724839
{'scaleFactor': 20, 'currentTarget': array([115.,  18.]), 'dynamicTrap': False, 'previousTarget': array([115.,  18.]), 'currentState': array([114.09908587,  17.83740908,   5.91677621]), 'targetState': array([115,  18], dtype=int32), 'currentDistance': 0.915468232260081}
episode index:532
target Thresh 71.21554163266558
target distance 23.0
model initialize at round 532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.68321428, 22.78778434]), 'dynamicTrap': False, 'previousTarget': array([29., 23.]), 'currentState': array([ 8.72402639, 21.51074909,  5.53915715]), 'targetState': array([32, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7535139332300629
running average episode reward sum: 0.6850982563688396
{'scaleFactor': 20, 'currentTarget': array([32., 23.]), 'dynamicTrap': False, 'previousTarget': array([32., 23.]), 'currentState': array([32.64919451, 22.26883608,  2.20190066]), 'targetState': array([32, 23], dtype=int32), 'currentDistance': 0.977780233905473}
episode index:533
target Thresh 71.2394042183244
target distance 45.0
model initialize at round 533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.69062983,  8.15810508]), 'dynamicTrap': False, 'previousTarget': array([45.23767062,  8.07414013]), 'currentState': array([63.43271684,  4.95654292,  2.1593132 ]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6261352696312621
running average episode reward sum: 0.6849878387906794
{'scaleFactor': 20, 'currentTarget': array([20., 12.]), 'dynamicTrap': False, 'previousTarget': array([20., 12.]), 'currentState': array([20.48231434, 11.47991353,  2.50202733]), 'targetState': array([20, 12], dtype=int32), 'currentDistance': 0.7093074489543111}
episode index:534
target Thresh 71.26314778884073
target distance 63.0
model initialize at round 534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.13615739,  9.9399173 ]), 'dynamicTrap': False, 'previousTarget': array([71.61538462,  9.92307692]), 'currentState': array([89.48085095,  4.86223761,  2.22202921]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5440762192661803
running average episode reward sum: 0.6847244525859606
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.30161555, 20.55673682,  3.25119916]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.5361475415802606}
episode index:535
target Thresh 71.28677293780508
target distance 23.0
model initialize at round 535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.75335304, 14.49094805]), 'dynamicTrap': False, 'previousTarget': array([37.64765455, 13.95156206]), 'currentState': array([57.83662671, 20.47665319,  2.04124828]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.824849075941703
running average episode reward sum: 0.6849858791220721
{'scaleFactor': 20, 'currentTarget': array([34., 13.]), 'dynamicTrap': False, 'previousTarget': array([35.46581171, 14.16381193]), 'currentState': array([34.63141679, 13.84829198,  4.92053901]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 1.0574906428134354}
episode index:536
target Thresh 71.31028025584739
target distance 19.0
model initialize at round 536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.8593297 , 10.37283878]), 'dynamicTrap': False, 'previousTarget': array([71.30163555, 10.68507134]), 'currentState': array([89.2068371 , 18.33330185,  3.82585764]), 'targetState': array([70, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7271051179345159
running average episode reward sum: 0.6850643134587805
{'scaleFactor': 20, 'currentTarget': array([70., 10.]), 'dynamicTrap': False, 'previousTarget': array([70., 10.]), 'currentState': array([70.47891309, 10.10214481,  0.26715841]), 'targetState': array([70, 10], dtype=int32), 'currentDistance': 0.489684912628623}
episode index:537
target Thresh 71.33367033065184
target distance 26.0
model initialize at round 537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.52729161, 11.00016356]), 'dynamicTrap': False, 'previousTarget': array([83.11558017, 11.11828302]), 'currentState': array([65.74256866, 17.86559756,  0.87032121]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7388183684121842
running average episode reward sum: 0.6851642280590656
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([89.40039547,  8.38612033,  0.53401024]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 0.8581222760260149}
episode index:538
target Thresh 71.35694374697152
target distance 56.0
model initialize at round 538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.21876283, 19.09920763]), 'dynamicTrap': False, 'previousTarget': array([46.518056  , 18.47740586]), 'currentState': array([66.64658789, 23.84890376,  2.14393915]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4686800912557688
running average episode reward sum: 0.6847625877310447
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'dynamicTrap': False, 'previousTarget': array([10., 10.]), 'currentState': array([10.28190248,  9.18529284,  3.59597524]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.8621002101135832}
episode index:539
target Thresh 71.38010108664304
target distance 9.0
model initialize at round 539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45., 21.]), 'dynamicTrap': False, 'previousTarget': array([45., 21.]), 'currentState': array([34.32401994, 19.6726832 ,  3.81151235]), 'targetState': array([45, 21], dtype=int32), 'currentDistance': 10.75817457258153}
done in step count: 13
reward sum = 0.8204481709816087
running average episode reward sum: 0.6850138573296568
{'scaleFactor': 20, 'currentTarget': array([45., 21.]), 'dynamicTrap': False, 'previousTarget': array([45., 21.]), 'currentState': array([44.62223486, 20.36180047,  2.1821074 ]), 'targetState': array([45, 21], dtype=int32), 'currentDistance': 0.7416233096358724}
episode index:540
target Thresh 71.40314292860111
target distance 16.0
model initialize at round 540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.,  7.]), 'dynamicTrap': False, 'previousTarget': array([65.,  7.]), 'currentState': array([49.79110293, 11.71872016,  6.27552748]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 15.924097153550292}
done in step count: 21
reward sum = 0.7369935315892368
running average episode reward sum: 0.6851099380584176
{'scaleFactor': 20, 'currentTarget': array([65.,  7.]), 'dynamicTrap': False, 'previousTarget': array([65.,  7.]), 'currentState': array([64.9104984 ,  7.34934123,  2.41351497]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 0.36062422735112326}
episode index:541
target Thresh 71.42606984889298
target distance 43.0
model initialize at round 541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.8713407 , 12.47393618]), 'dynamicTrap': False, 'previousTarget': array([50., 12.]), 'currentState': array([68.86664678, 12.90722004,  3.47656262]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7002184524220523
running average episode reward sum: 0.6851378135461734
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'dynamicTrap': False, 'previousTarget': array([27., 12.]), 'currentState': array([27.85304926, 11.12763157,  6.02470203]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.220131021826644}
episode index:542
target Thresh 71.44888242069284
target distance 63.0
model initialize at round 542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.36516043, 19.03223412]), 'dynamicTrap': False, 'previousTarget': array([47.06269215, 19.41767398]), 'currentState': array([65.31164118, 20.49439211,  4.22512484]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3734189512790139
running average episode reward sum: 0.6845637456598619
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 16.]), 'currentState': array([ 4.60681265, 16.63371274,  3.55489664]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.8773901282637863}
episode index:543
target Thresh 71.47158121431619
target distance 8.0
model initialize at round 543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.,  3.]), 'dynamicTrap': False, 'previousTarget': array([59.,  3.]), 'currentState': array([65.69799926,  9.40730806,  4.21310568]), 'targetState': array([59,  3], dtype=int32), 'currentDistance': 9.269131057253064}
done in step count: 5
reward sum = 0.9414801494009999
running average episode reward sum: 0.6850360184608567
{'scaleFactor': 20, 'currentTarget': array([60.24770232,  4.12017339]), 'dynamicTrap': True, 'previousTarget': array([59.,  3.]), 'currentState': array([60.00834254,  4.76153122,  2.88318485]), 'targetState': array([59,  3], dtype=int32), 'currentDistance': 0.6845677257126743}
episode index:544
target Thresh 71.49416679723404
target distance 64.0
model initialize at round 544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.18155396, 21.32236602]), 'dynamicTrap': False, 'previousTarget': array([47.91268452, 20.13318583]), 'currentState': array([28.32747644, 23.73392274,  0.53922129]), 'targetState': array([92, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6409647264352821
running average episode reward sum: 0.6849551537048465
{'scaleFactor': 20, 'currentTarget': array([92., 16.]), 'dynamicTrap': False, 'previousTarget': array([92., 16.]), 'currentState': array([91.58074842, 16.1760054 ,  0.47675266]), 'targetState': array([92, 16], dtype=int32), 'currentDistance': 0.4546974695995276}
episode index:545
target Thresh 71.51663973408714
target distance 58.0
model initialize at round 545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.75439672,  8.91916722]), 'dynamicTrap': False, 'previousTarget': array([79.81242258,  8.73274794]), 'currentState': array([61.94804592,  6.14275444,  5.97681347]), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.37327037900378995
running average episode reward sum: 0.6843843024691302
{'scaleFactor': 20, 'currentTarget': array([118.,  14.]), 'dynamicTrap': False, 'previousTarget': array([118.,  14.]), 'currentState': array([117.91800846,  14.74430746,   5.71695072]), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 0.7488098602027565}
episode index:546
target Thresh 71.53900058670007
target distance 14.0
model initialize at round 546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([26.69858543,  1.17573987,  6.11794257]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 16.524809925072493}
done in step count: 14
reward sum = 0.8221378378777827
running average episode reward sum: 0.684636137085965
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([34.38212357, 15.33597666,  1.55456797]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 0.7661236270091909}
episode index:547
target Thresh 71.56124991409534
target distance 20.0
model initialize at round 547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.9007438 ,  5.00992562]), 'currentState': array([8.96631941, 8.34627129, 6.22142572]), 'targetState': array([28,  5], dtype=int32), 'currentDistance': 19.32559257708574}
done in step count: 21
reward sum = 0.736585853800807
running average episode reward sum: 0.6847309358390944
{'scaleFactor': 20, 'currentTarget': array([28.,  5.]), 'dynamicTrap': False, 'previousTarget': array([28.,  5.]), 'currentState': array([28.43889933,  5.54096939,  2.47980987]), 'targetState': array([28,  5], dtype=int32), 'currentDistance': 0.6966207718982628}
episode index:548
target Thresh 71.58338827250728
target distance 62.0
model initialize at round 548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.37756449, 10.17631519]), 'dynamicTrap': False, 'previousTarget': array([62.56082452, 10.70302633]), 'currentState': array([80.72653198,  5.11494611,  4.69710958]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.47899292972308827
running average episode reward sum: 0.6843561853725807
{'scaleFactor': 20, 'currentTarget': array([18.66515354, 21.37909684]), 'dynamicTrap': True, 'previousTarget': array([20., 21.]), 'currentState': array([18.00045625, 22.10292046,  5.42336133]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.9827223022160622}
episode index:549
target Thresh 71.60541621539599
target distance 36.0
model initialize at round 549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.36941144, 14.17361752]), 'dynamicTrap': False, 'previousTarget': array([52.19015454, 14.24863257]), 'currentState': array([70.14444782, 17.16492761,  2.39824998]), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6607261990297588
running average episode reward sum: 0.6843132217610483
{'scaleFactor': 20, 'currentTarget': array([38.22075366, 12.13168883]), 'dynamicTrap': True, 'previousTarget': array([38.08243284, 12.15857562]), 'currentState': array([39.10966505, 12.87839719,  3.59079959]), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 1.16092068649667}
episode index:550
target Thresh 71.6273342934612
target distance 43.0
model initialize at round 550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.27069118,  9.50463436]), 'dynamicTrap': False, 'previousTarget': array([75.98257137,  9.19172095]), 'currentState': array([93.22594359,  3.12510413,  3.90891552]), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6069378750036575
running average episode reward sum: 0.6841727946344468
{'scaleFactor': 20, 'currentTarget': array([52., 17.]), 'dynamicTrap': False, 'previousTarget': array([52., 17.]), 'currentState': array([52.13619164, 17.34825981,  4.9175397 ]), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 0.373942590204941}
episode index:551
target Thresh 71.64914305465601
target distance 4.0
model initialize at round 551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 17.]), 'dynamicTrap': False, 'previousTarget': array([35., 17.]), 'currentState': array([32.44062878, 19.37573575,  5.98729253]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 3.492062623489153}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.684708894644167
{'scaleFactor': 20, 'currentTarget': array([35., 17.]), 'dynamicTrap': False, 'previousTarget': array([35., 17.]), 'currentState': array([34.85001818, 17.17272682,  4.74123806]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 0.22875554388507183}
episode index:552
target Thresh 71.67084304420058
target distance 60.0
model initialize at round 552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.18770124, 15.04529276]), 'dynamicTrap': False, 'previousTarget': array([53., 15.]), 'currentState': array([33.18771418, 15.0680459 ,  0.44876466]), 'targetState': array([93, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5778937806212081
running average episode reward sum: 0.6845157389226064
{'scaleFactor': 20, 'currentTarget': array([93., 15.]), 'dynamicTrap': False, 'previousTarget': array([93., 15.]), 'currentState': array([92.14246482, 14.12956257,  4.9965316 ]), 'targetState': array([93, 15], dtype=int32), 'currentDistance': 1.221895211010274}
episode index:553
target Thresh 71.69243480459578
target distance 63.0
model initialize at round 553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.43709524, 18.84154387]), 'dynamicTrap': True, 'previousTarget': array([45.54387571, 18.36758945]), 'currentState': array([65.       , 23.       ,  5.4674993], dtype=float32), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4451525229577329
running average episode reward sum: 0.6840836753558829
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'dynamicTrap': False, 'previousTarget': array([2., 8.]), 'currentState': array([2.69109185, 7.87381472, 5.44348643]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.7025173792126055}
episode index:554
target Thresh 71.71391887563672
target distance 18.0
model initialize at round 554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.,  3.]), 'dynamicTrap': False, 'previousTarget': array([42.,  3.]), 'currentState': array([44.48357214, 20.16722536,  4.83058355]), 'targetState': array([42,  3], dtype=int32), 'currentDistance': 17.34594353875478}
done in step count: 96
reward sum = -0.17607660788808482
running average episode reward sum: 0.6825338370076955
{'scaleFactor': 20, 'currentTarget': array([42.,  3.]), 'dynamicTrap': False, 'previousTarget': array([42.,  3.]), 'currentState': array([42.30455948,  3.65819274,  6.226112  ]), 'targetState': array([42,  3], dtype=int32), 'currentDistance': 0.7252407562414883}
episode index:555
target Thresh 71.73529579442634
target distance 13.0
model initialize at round 555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96., 10.]), 'dynamicTrap': False, 'previousTarget': array([96., 10.]), 'currentState': array([107.2151931 ,  14.27452545,   3.50563949]), 'targetState': array([96, 10], dtype=int32), 'currentDistance': 12.002171637069054}
done in step count: 13
reward sum = 0.821473960751619
running average episode reward sum: 0.6827837293165875
{'scaleFactor': 20, 'currentTarget': array([97.40390955,  9.72091508]), 'dynamicTrap': True, 'previousTarget': array([96., 10.]), 'currentState': array([97.99953226, 10.35016563,  3.59677112]), 'targetState': array([96, 10], dtype=int32), 'currentDistance': 0.8664425431809678}
episode index:556
target Thresh 71.75656609538869
target distance 61.0
model initialize at round 556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.56474091, 12.00404064]), 'dynamicTrap': False, 'previousTarget': array([67.09605178, 12.0422346 ]), 'currentState': array([85.46310204, 14.01779948,  4.02638173]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.41000661914604103
running average episode reward sum: 0.6808218076855954
{'scaleFactor': 20, 'currentTarget': array([27.72816579,  7.98493725]), 'dynamicTrap': True, 'previousTarget': array([27.57486687,  8.1129303 ]), 'currentState': array([44.84545614,  1.49359586,  4.48990241]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 18.306805888311033}
episode index:557
target Thresh 71.77773031028242
target distance 31.0
model initialize at round 557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.19319171, 17.87807471]), 'dynamicTrap': False, 'previousTarget': array([46.65136197, 17.21988205]), 'currentState': array([29.50634124, 10.75054923,  1.47225445]), 'targetState': array([59, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6603418930303045
running average episode reward sum: 0.6807851053295824
{'scaleFactor': 20, 'currentTarget': array([59., 22.]), 'dynamicTrap': False, 'previousTarget': array([59., 22.]), 'currentState': array([59.56903264, 21.72992054,  5.6125746 ]), 'targetState': array([59, 22], dtype=int32), 'currentDistance': 0.6298738386577384}
episode index:558
target Thresh 71.79878896821398
target distance 17.0
model initialize at round 558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62., 22.]), 'dynamicTrap': False, 'previousTarget': array([62., 22.]), 'currentState': array([70.31951679,  4.90854356,  2.18596172]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 19.00874122551048}
done in step count: 15
reward sum = 0.8244220924014622
running average episode reward sum: 0.6810420587948272
{'scaleFactor': 20, 'currentTarget': array([62., 22.]), 'dynamicTrap': False, 'previousTarget': array([62., 22.]), 'currentState': array([61.81926292, 22.4976736 ,  1.89998986]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 0.5294760670648695}
episode index:559
target Thresh 71.81974259565096
target distance 15.0
model initialize at round 559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.86559697, 17.28611999]), 'dynamicTrap': True, 'previousTarget': array([11., 18.]), 'currentState': array([26.       , 15.       ,  2.8615813], dtype=float32), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 13.33187486676505}
done in step count: 16
reward sum = 0.775315445227364
running average episode reward sum: 0.6812104041277424
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([11.04630194, 18.63004421,  3.07609499]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.6317432881227876}
episode index:560
target Thresh 71.84059171643509
target distance 60.0
model initialize at round 560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.98520944, 10.17460545]), 'dynamicTrap': False, 'previousTarget': array([97.59715  ,  9.8507125]), 'currentState': array([115.37876642,   5.28687106,   1.95645928]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4882401966001507
running average episode reward sum: 0.6808664287132548
{'scaleFactor': 20, 'currentTarget': array([58.50753797, 19.10625561]), 'dynamicTrap': True, 'previousTarget': array([57., 20.]), 'currentState': array([58.57304346, 19.41661892,  3.47608825]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 0.31720080880092516}
episode index:561
target Thresh 71.8613368517955
target distance 57.0
model initialize at round 561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.64867799,  9.33689815]), 'dynamicTrap': False, 'previousTarget': array([27.89010906,  8.90630431]), 'currentState': array([ 9.79750385, 11.77223905,  0.77241538]), 'targetState': array([65,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.44196211596008306
running average episode reward sum: 0.6804413320713452
{'scaleFactor': 20, 'currentTarget': array([65.,  5.]), 'dynamicTrap': False, 'previousTarget': array([65.,  5.]), 'currentState': array([64.62781905,  5.75381085,  5.04964341]), 'targetState': array([65,  5], dtype=int32), 'currentDistance': 0.8406839259770744}
episode index:562
target Thresh 71.88197852036164
target distance 57.0
model initialize at round 562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.48897767,  8.5879096 ]), 'dynamicTrap': False, 'previousTarget': array([49.14913089,  8.56217397]), 'currentState': array([67.32391823, 11.15211179,  2.0410862 ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5313024249385145
running average episode reward sum: 0.6801764317034361
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'dynamicTrap': False, 'previousTarget': array([12.,  4.]), 'currentState': array([11.84843175,  4.58127936,  2.5225191 ]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.60071509500921}
episode index:563
target Thresh 71.90251723817633
target distance 34.0
model initialize at round 563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.66153776,  9.8901771 ]), 'dynamicTrap': False, 'previousTarget': array([25.14019333,  9.65640235]), 'currentState': array([42.39408935,  2.88364365,  2.20392132]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6352080098411483
running average episode reward sum: 0.6800967004589995
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([10.73746278, 14.67663821,  2.12929797]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.805241707298987}
episode index:564
target Thresh 71.92295351870854
target distance 10.0
model initialize at round 564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.,  8.]), 'dynamicTrap': False, 'previousTarget': array([40.,  8.]), 'currentState': array([49.06522237,  7.43127947,  3.43563986]), 'targetState': array([40,  8], dtype=int32), 'currentDistance': 9.08304462333899}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6805761577146472
{'scaleFactor': 20, 'currentTarget': array([40.,  8.]), 'dynamicTrap': False, 'previousTarget': array([40.,  8.]), 'currentState': array([39.88216002,  7.26702918,  2.81210545]), 'targetState': array([40,  8], dtype=int32), 'currentDistance': 0.7423829737060117}
episode index:565
target Thresh 71.94328787286639
target distance 9.0
model initialize at round 565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'dynamicTrap': False, 'previousTarget': array([5., 7.]), 'currentState': array([11.33430018, 16.24048322,  4.00820971]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 11.20311960346768}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6810204849057998
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'dynamicTrap': False, 'previousTarget': array([5., 7.]), 'currentState': array([5.06090999, 6.36405832, 5.75083502]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.6388519797683041}
episode index:566
target Thresh 71.96352080900976
target distance 48.0
model initialize at round 566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.74433408, 22.14315079]), 'dynamicTrap': False, 'previousTarget': array([68.03894843, 21.75243428]), 'currentState': array([86.68042591, 23.74072473,  3.48200011]), 'targetState': array([40, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.68146449448265
running average episode reward sum: 0.6810212679914732
{'scaleFactor': 20, 'currentTarget': array([40., 20.]), 'dynamicTrap': False, 'previousTarget': array([40., 20.]), 'currentState': array([39.27180111, 20.86605588,  3.08090586]), 'targetState': array([40, 20], dtype=int32), 'currentDistance': 1.1315150972574182}
episode index:567
target Thresh 71.98365283296313
target distance 27.0
model initialize at round 567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.64323337,  8.05796942]), 'dynamicTrap': False, 'previousTarget': array([93.02633404,  7.67544468]), 'currentState': array([111.21220076,  15.48719886,   3.06793642]), 'targetState': array([85,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 18
reward sum = 0.8251931079710176
running average episode reward sum: 0.681275091653409
{'scaleFactor': 20, 'currentTarget': array([85.,  5.]), 'dynamicTrap': False, 'previousTarget': array([85.,  5.]), 'currentState': array([85.07381479,  5.60066285,  4.64745714]), 'targetState': array([85,  5], dtype=int32), 'currentDistance': 0.6051813610142792}
episode index:568
target Thresh 72.00368444802814
target distance 42.0
model initialize at round 568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.38589417, 11.25064449]), 'dynamicTrap': False, 'previousTarget': array([48.09009055, 12.10381815]), 'currentState': array([67.35178268, 12.41824722,  3.35862577]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 36
reward sum = 0.6128619027480142
running average episode reward sum: 0.6811548575780042
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'dynamicTrap': False, 'previousTarget': array([26., 10.]), 'currentState': array([25.27884378, 10.56544296,  4.1683526 ]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.9164016744271866}
episode index:569
target Thresh 72.0236161549962
target distance 17.0
model initialize at round 569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'dynamicTrap': False, 'previousTarget': array([17.20859686,  7.13497444]), 'currentState': array([32.23281476, 17.62998448,  4.13121998]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 18.575123571228808}
done in step count: 12
reward sum = 0.8678367712927801
running average episode reward sum: 0.6814823697073283
{'scaleFactor': 20, 'currentTarget': array([17.,  7.]), 'dynamicTrap': False, 'previousTarget': array([17.,  7.]), 'currentState': array([17.09891749,  6.45183379,  2.50594844]), 'targetState': array([17,  7], dtype=int32), 'currentDistance': 0.55701962477068}
episode index:570
target Thresh 72.04344845216106
target distance 2.0
model initialize at round 570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.81403648, 20.93195849]), 'dynamicTrap': True, 'previousTarget': array([94., 21.]), 'currentState': array([96.       , 22.       ,  0.3991993], dtype=float32), 'targetState': array([94, 21], dtype=int32), 'currentDistance': 2.432930164400271}
done in step count: 11
reward sum = 0.8368184036597164
running average episode reward sum: 0.6817544117983133
{'scaleFactor': 20, 'currentTarget': array([94., 21.]), 'dynamicTrap': False, 'previousTarget': array([94., 21.]), 'currentState': array([94.40882655, 21.51832391,  2.40709262]), 'targetState': array([94, 21], dtype=int32), 'currentDistance': 0.660150604595487}
episode index:571
target Thresh 72.06318183533112
target distance 39.0
model initialize at round 571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.35170212, 22.18269299]), 'dynamicTrap': False, 'previousTarget': array([93.02624673, 22.02429504]), 'currentState': array([111.3295526 ,  21.24168805,   1.99610007]), 'targetState': array([74, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7331352483127386
running average episode reward sum: 0.6818442384355763
{'scaleFactor': 20, 'currentTarget': array([74., 23.]), 'dynamicTrap': False, 'previousTarget': array([74., 23.]), 'currentState': array([74.96994977, 23.18686766,  4.22339731]), 'targetState': array([74, 23], dtype=int32), 'currentDistance': 0.9877864523914776}
episode index:572
target Thresh 72.08281679784204
target distance 39.0
model initialize at round 572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.62183637,  6.9714221 ]), 'dynamicTrap': False, 'previousTarget': array([34.23256605,  6.95885631]), 'currentState': array([52.34342714, 10.29690814,  3.97424531]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.700015650972177
running average episode reward sum: 0.6818759511974203
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'dynamicTrap': False, 'previousTarget': array([15.,  4.]), 'currentState': array([15.54152777,  4.4633467 ,  4.13017805]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7127008355975841}
episode index:573
target Thresh 72.10235383056887
target distance 69.0
model initialize at round 573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.0392464 , 11.82598298]), 'dynamicTrap': False, 'previousTarget': array([51.07518824, 12.73259233]), 'currentState': array([71.93318222,  9.76896634,  4.46920764]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5587995936839921
running average episode reward sum: 0.6816615324561077
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.15704494, 16.92640624,  3.87177782]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.846161491800952}
episode index:574
target Thresh 72.12179342193848
target distance 19.0
model initialize at round 574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.79690426,  8.5637766 ]), 'dynamicTrap': False, 'previousTarget': array([70.09022194,  9.32014017]), 'currentState': array([85.1241292 , 20.11460886,  3.69382191]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6820021576570519
{'scaleFactor': 20, 'currentTarget': array([68.,  8.]), 'dynamicTrap': False, 'previousTarget': array([68.,  8.]), 'currentState': array([67.54585198,  7.9273387 ,  5.6935935 ]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 0.4599240062661605}
episode index:575
target Thresh 72.14113605794164
target distance 52.0
model initialize at round 575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.21478447, 17.00814278]), 'dynamicTrap': False, 'previousTarget': array([67.09181942, 16.91424813]), 'currentState': array([85.11745075, 15.03738785,  3.29748136]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5725173115728224
running average episode reward sum: 0.6818120797992667
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([35.48310352, 19.2324392 ,  0.7591747 ]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 0.9069391323664279}
episode index:576
target Thresh 72.16038222214526
target distance 12.0
model initialize at round 576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'dynamicTrap': False, 'previousTarget': array([21.,  2.]), 'currentState': array([27.94967923, 12.58497865,  4.3295399 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 12.662535858515962}
done in step count: 12
reward sum = 0.8391130196987702
running average episode reward sum: 0.6820846984126107
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'dynamicTrap': False, 'previousTarget': array([21.,  2.]), 'currentState': array([20.2983159 ,  1.64096252,  4.56841911]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.7882058702419341}
episode index:577
target Thresh 72.17953239570444
target distance 31.0
model initialize at round 577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.72863441, 16.67050843]), 'dynamicTrap': False, 'previousTarget': array([18.01039771, 16.35517412]), 'currentState': array([36.68130176, 18.0456687 ,  3.48116207]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7254317934777137
running average episode reward sum: 0.6821596933867717
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.87964547, 15.99620175,  2.59583556]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.12041444687469716}
episode index:578
target Thresh 72.19858705737457
target distance 20.0
model initialize at round 578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.,  3.]), 'dynamicTrap': False, 'previousTarget': array([94.0992562 ,  3.00992562]), 'currentState': array([112.42388481,   4.37009181,   2.53611636]), 'targetState': array([94,  3], dtype=int32), 'currentDistance': 18.474757999694074}
done in step count: 18
reward sum = 0.7808831991177188
running average episode reward sum: 0.6823302003051327
{'scaleFactor': 20, 'currentTarget': array([91.89478235,  2.68207216]), 'dynamicTrap': True, 'previousTarget': array([92.01303303,  2.83656332]), 'currentState': array([92.85497863,  1.73469472,  5.61084251]), 'targetState': array([94,  3], dtype=int32), 'currentDistance': 1.3488887714492468}
episode index:579
target Thresh 72.21754668352311
target distance 32.0
model initialize at round 579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.04888319, 17.87731526]), 'dynamicTrap': False, 'previousTarget': array([86.72658355, 17.02246883]), 'currentState': array([69.14220864, 11.35522532,  1.72007905]), 'targetState': array([100,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.432504872021219
running average episode reward sum: 0.6818994669805053
{'scaleFactor': 20, 'currentTarget': array([100.,  22.]), 'dynamicTrap': False, 'previousTarget': array([100.,  22.]), 'currentState': array([99.40186711, 21.39834464,  1.91462514]), 'targetState': array([100,  22], dtype=int32), 'currentDistance': 0.8483820573832505}
episode index:580
target Thresh 72.23641174814175
target distance 2.0
model initialize at round 580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  18.]), 'dynamicTrap': False, 'previousTarget': array([101.,  18.]), 'currentState': array([103.86632469,  18.44731775,   2.03501167]), 'targetState': array([101,  18], dtype=int32), 'currentDistance': 2.9010188599617237}
done in step count: 3
reward sum = 0.95069601
running average episode reward sum: 0.6823621116328624
{'scaleFactor': 20, 'currentTarget': array([102.35899728,  18.12565248]), 'dynamicTrap': True, 'previousTarget': array([101.,  18.]), 'currentState': array([102.01862403,  18.7119485 ,   3.93211986]), 'targetState': array([101,  18], dtype=int32), 'currentDistance': 0.6779358256419296}
episode index:581
target Thresh 72.25518272285807
target distance 22.0
model initialize at round 581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.87847995, 13.99424918]), 'dynamicTrap': False, 'previousTarget': array([21.9793708 , 13.90815322]), 'currentState': array([ 3.90083806, 13.04882621,  0.28888315]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.739693486424013
running average episode reward sum: 0.6824606191496857
{'scaleFactor': 20, 'currentTarget': array([24., 14.]), 'dynamicTrap': False, 'previousTarget': array([24., 14.]), 'currentState': array([23.09896612, 13.36061516,  5.61316561]), 'targetState': array([24, 14], dtype=int32), 'currentDistance': 1.104841623750812}
episode index:582
target Thresh 72.27386007694741
target distance 68.0
model initialize at round 582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.24823648,  7.51560833]), 'dynamicTrap': False, 'previousTarget': array([57.73778052,  8.38209074]), 'currentState': array([76.40367779,  1.76503722,  3.26242626]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.029035303133314372
running average episode reward sum: 0.6813398210090058
{'scaleFactor': 20, 'currentTarget': array([ 9., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 22.]), 'currentState': array([ 9.8732775 , 22.81056608,  4.03004499]), 'targetState': array([ 9, 22], dtype=int32), 'currentDistance': 1.191482676255416}
episode index:583
target Thresh 72.29244427734461
target distance 42.0
model initialize at round 583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.28735544,  6.03189557]), 'dynamicTrap': False, 'previousTarget': array([35.02263725,  6.04869701]), 'currentState': array([53.26153409,  7.04786173,  4.02603447]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6398342250685062
running average episode reward sum: 0.6812687497830803
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([12.57004284,  5.77816718,  4.71951584]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.8890485430058526}
episode index:584
target Thresh 72.31093578865566
target distance 38.0
model initialize at round 584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.58571572, 10.77815689]), 'dynamicTrap': False, 'previousTarget': array([54.23313766,  9.91410718]), 'currentState': array([71.44042913,  4.10734118,  3.04447889]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6631468269773778
running average episode reward sum: 0.6812377721372586
{'scaleFactor': 20, 'currentTarget': array([35., 17.]), 'dynamicTrap': False, 'previousTarget': array([35., 17.]), 'currentState': array([35.675466  , 17.6518376 ,  2.36934851]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 0.9386940816907442}
episode index:585
target Thresh 72.32933507316926
target distance 52.0
model initialize at round 585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.43387123, 19.43097501]), 'dynamicTrap': False, 'previousTarget': array([71.94108971, 19.53392998]), 'currentState': array([53.50413999, 17.755919  ,  0.75440067]), 'targetState': array([104,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5311045620924115
running average episode reward sum: 0.6809815721201172
{'scaleFactor': 20, 'currentTarget': array([104.,  22.]), 'dynamicTrap': False, 'previousTarget': array([104.,  22.]), 'currentState': array([104.5667607 ,  21.5874556 ,   6.24264198]), 'targetState': array([104,  22], dtype=int32), 'currentDistance': 0.7010068321801946}
episode index:586
target Thresh 72.34764259086852
target distance 59.0
model initialize at round 586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.74521268, 15.29975024]), 'dynamicTrap': False, 'previousTarget': array([91.28122885, 14.3421646 ]), 'currentState': array([109.52097328,  12.31323177,   2.15500683]), 'targetState': array([52, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.423870205135636
running average episode reward sum: 0.6805435629770432
{'scaleFactor': 20, 'currentTarget': array([52., 21.]), 'dynamicTrap': False, 'previousTarget': array([52., 21.]), 'currentState': array([52.34554362, 20.84011801,  2.59430926]), 'targetState': array([52, 21], dtype=int32), 'currentDistance': 0.38073960305288773}
episode index:587
target Thresh 72.36585879944232
target distance 58.0
model initialize at round 587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.28126489, 20.31334222]), 'dynamicTrap': False, 'previousTarget': array([95.07390462, 20.28223316]), 'currentState': array([113.19838263,  22.13224869,   3.84955561]), 'targetState': array([57, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5579249317218526
running average episode reward sum: 0.6803350278898745
{'scaleFactor': 20, 'currentTarget': array([57., 17.]), 'dynamicTrap': False, 'previousTarget': array([57., 17.]), 'currentState': array([57.74409379, 17.17414748,  6.19509883]), 'targetState': array([57, 17], dtype=int32), 'currentDistance': 0.7642008349599136}
episode index:588
target Thresh 72.38398415429684
target distance 14.0
model initialize at round 588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  17.]), 'dynamicTrap': False, 'previousTarget': array([116.,  17.]), 'currentState': array([105.73047142,   4.66354888,   0.72523642]), 'targetState': array([116,  17], dtype=int32), 'currentDistance': 16.051518422490258}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6807309229995414
{'scaleFactor': 20, 'currentTarget': array([116.,  17.]), 'dynamicTrap': False, 'previousTarget': array([116.,  17.]), 'currentState': array([116.87778225,  16.16471576,   0.34320092]), 'targetState': array([116,  17], dtype=int32), 'currentDistance': 1.211693628949883}
episode index:589
target Thresh 72.40201910856686
target distance 15.0
model initialize at round 589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.07214241, 16.94242168]), 'dynamicTrap': False, 'previousTarget': array([93.37889464, 16.64636501]), 'currentState': array([108.70384163,   4.46644559,   2.13330531]), 'targetState': array([93, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7879979810441006
running average episode reward sum: 0.6809127315724983
{'scaleFactor': 20, 'currentTarget': array([93., 17.]), 'dynamicTrap': False, 'previousTarget': array([93., 17.]), 'currentState': array([93.73320804, 17.49523117,  1.42529888]), 'targetState': array([93, 17], dtype=int32), 'currentDistance': 0.884786949569955}
episode index:590
target Thresh 72.41996411312722
target distance 21.0
model initialize at round 590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.94843089,  5.05640417]), 'dynamicTrap': False, 'previousTarget': array([28.3829006 ,  5.12161403]), 'currentState': array([11.16730013, 14.21235246,  6.07751352]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6812013018593382
{'scaleFactor': 20, 'currentTarget': array([31.,  4.]), 'dynamicTrap': False, 'previousTarget': array([31.,  4.]), 'currentState': array([31.66683336,  3.86614836,  2.8944079 ]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 0.6801345418337907}
episode index:591
target Thresh 72.43781961660395
target distance 22.0
model initialize at round 591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.757697  ,  15.91756599]), 'dynamicTrap': False, 'previousTarget': array([109.50265712,  15.56757793]), 'currentState': array([89.51347199, 21.36364751,  0.90350223]), 'targetState': array([112,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7796408140960762
running average episode reward sum: 0.6813675848191976
{'scaleFactor': 20, 'currentTarget': array([112.,  15.]), 'dynamicTrap': False, 'previousTarget': array([112.,  15.]), 'currentState': array([112.43735788,  14.5911376 ,   6.01143796]), 'targetState': array([112,  15], dtype=int32), 'currentDistance': 0.5987072583952666}
episode index:592
target Thresh 72.45558606538556
target distance 12.0
model initialize at round 592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.81001234, 5.93858709]), 'dynamicTrap': True, 'previousTarget': array([10.,  6.]), 'currentState': array([22.       , 17.       ,  0.4116454], dtype=float32), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 16.4605788117957}
done in step count: 19
reward sum = 0.742777048848663
running average episode reward sum: 0.6814711420941209
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'dynamicTrap': False, 'previousTarget': array([10.,  6.]), 'currentState': array([9.42490635, 5.86554183, 5.51421827]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.5906028297667062}
episode index:593
target Thresh 72.47326390363419
target distance 10.0
model initialize at round 593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,   6.]), 'dynamicTrap': False, 'previousTarget': array([108.,   6.]), 'currentState': array([107.44330251,  14.14830214,   4.17292461]), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 8.16729697045834}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6819088676956475
{'scaleFactor': 20, 'currentTarget': array([108.,   6.]), 'dynamicTrap': False, 'previousTarget': array([108.,   6.]), 'currentState': array([107.66825621,   5.30385421,   3.08031711]), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 0.7711503741370039}
episode index:594
target Thresh 72.49085357329673
target distance 60.0
model initialize at round 594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.10417183,  9.00250551]), 'dynamicTrap': False, 'previousTarget': array([66.5464628 ,  9.23506694]), 'currentState': array([48.61871424,  4.49507359,  0.68621021]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5707383396538308
running average episode reward sum: 0.6817220264720478
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([106.78975583,  17.84228767,   1.68990785]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 0.26282273126841377}
episode index:595
target Thresh 72.50835551411583
target distance 60.0
model initialize at round 595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.67800335,  2.71934356]), 'dynamicTrap': False, 'previousTarget': array([95.,  2.]), 'currentState': array([113.6745453 ,   3.09124445,   2.55606139]), 'targetState': array([55,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6463747443315229
running average episode reward sum: 0.6816627189516778
{'scaleFactor': 20, 'currentTarget': array([55.,  2.]), 'dynamicTrap': False, 'previousTarget': array([55.,  2.]), 'currentState': array([54.42184334,  1.85276836,  5.49193355]), 'targetState': array([55,  2], dtype=int32), 'currentDistance': 0.5966089799946447}
episode index:596
target Thresh 72.52577016364094
target distance 68.0
model initialize at round 596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.55424938, 18.21651244]), 'dynamicTrap': False, 'previousTarget': array([90.21281729, 18.09012019]), 'currentState': array([108.31820469,  21.28018184,   2.34755445]), 'targetState': array([42, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3781025658786556
running average episode reward sum: 0.6811542429833812
{'scaleFactor': 20, 'currentTarget': array([42., 11.]), 'dynamicTrap': False, 'previousTarget': array([42., 11.]), 'currentState': array([42.19360286, 10.72154369,  3.51877586]), 'targetState': array([42, 11], dtype=int32), 'currentDistance': 0.3391459621161206}
episode index:597
target Thresh 72.54309795723918
target distance 17.0
model initialize at round 597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,   7.]), 'dynamicTrap': False, 'previousTarget': array([103.,   7.]), 'currentState': array([87.64855168,  8.59758865,  5.05744136]), 'targetState': array([103,   7], dtype=int32), 'currentDistance': 15.434353077494194}
done in step count: 12
reward sum = 0.8581395162440494
running average episode reward sum: 0.6814502049788004
{'scaleFactor': 20, 'currentTarget': array([103.,   7.]), 'dynamicTrap': False, 'previousTarget': array([103.,   7.]), 'currentState': array([102.88885445,   6.34914923,   1.89766854]), 'targetState': array([103,   7], dtype=int32), 'currentDistance': 0.6602727158548852}
episode index:598
target Thresh 72.56033932810632
target distance 12.0
model initialize at round 598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 15.]), 'dynamicTrap': False, 'previousTarget': array([86., 15.]), 'currentState': array([73.98857951,  7.19946359,  1.547859  ]), 'targetState': array([86, 15], dtype=int32), 'currentDistance': 14.322101465278577}
done in step count: 13
reward sum = 0.8398570619766187
running average episode reward sum: 0.6817146571607666
{'scaleFactor': 20, 'currentTarget': array([86., 15.]), 'dynamicTrap': False, 'previousTarget': array([86., 15.]), 'currentState': array([86.75288709, 14.36489773,  2.03761697]), 'targetState': array([86, 15], dtype=int32), 'currentDistance': 0.9849841954489148}
episode index:599
target Thresh 72.5774947072775
target distance 28.0
model initialize at round 599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.6793277 , 15.09321668]), 'dynamicTrap': False, 'previousTarget': array([42.83483823, 15.72672794]), 'currentState': array([24.23410945,  7.3618569 ,  5.84051263]), 'targetState': array([52, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7567614810121628
running average episode reward sum: 0.6818397352005189
{'scaleFactor': 20, 'currentTarget': array([52., 19.]), 'dynamicTrap': False, 'previousTarget': array([52., 19.]), 'currentState': array([51.85714403, 19.08200531,  1.22414473]), 'targetState': array([52, 19], dtype=int32), 'currentDistance': 0.1647200665077926}
episode index:600
target Thresh 72.5945645236381
target distance 32.0
model initialize at round 600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.02940707, 12.40503714]), 'dynamicTrap': False, 'previousTarget': array([52.00975848, 12.37530495]), 'currentState': array([70.01311755, 13.21207836,  3.1101273 ]), 'targetState': array([40, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.4805998649114942
running average episode reward sum: 0.6799055594931778
{'scaleFactor': 20, 'currentTarget': array([44.29179688, 14.5391117 ]), 'dynamicTrap': True, 'previousTarget': array([44.10379481, 14.47090218]), 'currentState': array([41.41416504, 19.78110639,  3.96183298]), 'targetState': array([40, 12], dtype=int32), 'currentDistance': 5.979905793722003}
episode index:601
target Thresh 72.61154920393444
target distance 57.0
model initialize at round 601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.30453899, 13.94257819]), 'dynamicTrap': False, 'previousTarget': array([68.24474069, 12.88074853]), 'currentState': array([86.94857722, 17.69914073,  2.0494903 ]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6589834571718868
running average episode reward sum: 0.6798708051703849
{'scaleFactor': 20, 'currentTarget': array([31.,  7.]), 'dynamicTrap': False, 'previousTarget': array([31.,  7.]), 'currentState': array([30.48845564,  7.27365582,  0.1253834 ]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 0.5801423450878437}
episode index:602
target Thresh 72.62844917278439
target distance 14.0
model initialize at round 602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,   7.]), 'dynamicTrap': False, 'previousTarget': array([112.,   7.]), 'currentState': array([118.10504457,  19.28501152,   3.82410789]), 'targetState': array([112,   7], dtype=int32), 'currentDistance': 13.718348196621076}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6802735811061353
{'scaleFactor': 20, 'currentTarget': array([112.,   7.]), 'dynamicTrap': False, 'previousTarget': array([112.,   7.]), 'currentState': array([111.59067357,   6.86959235,   5.91731383]), 'targetState': array([112,   7], dtype=int32), 'currentDistance': 0.4295978116202622}
episode index:603
target Thresh 72.64526485268806
target distance 69.0
model initialize at round 603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.91161212, 21.65255295]), 'dynamicTrap': False, 'previousTarget': array([62.03352192, 20.84252301]), 'currentState': array([80.85374612, 23.17284619,  3.35472834]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5563711760653346
running average episode reward sum: 0.6800684446739487
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'dynamicTrap': False, 'previousTarget': array([13., 18.]), 'currentState': array([13.44808749, 17.61275776,  5.52983727]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.5922321768469183}
episode index:604
target Thresh 72.66199666403831
target distance 62.0
model initialize at round 604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.33455547, 15.21348287]), 'dynamicTrap': False, 'previousTarget': array([36.69246532, 16.50617551]), 'currentState': array([17.55330505, 18.16342045,  5.36788964]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4279293075661157
running average episode reward sum: 0.6796516857696382
{'scaleFactor': 20, 'currentTarget': array([79.,  9.]), 'dynamicTrap': False, 'previousTarget': array([79.,  9.]), 'currentState': array([78.04142761,  8.42086019,  0.97041797]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 1.1199392602754181}
episode index:605
target Thresh 72.67864502513132
target distance 14.0
model initialize at round 605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 17.]), 'dynamicTrap': False, 'previousTarget': array([53., 17.]), 'currentState': array([65.83761526, 11.65040521,  4.83380032]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 13.907642859448588}
done in step count: 17
reward sum = 0.7599381065762542
running average episode reward sum: 0.6797841716125534
{'scaleFactor': 20, 'currentTarget': array([53., 17.]), 'dynamicTrap': False, 'previousTarget': array([53., 17.]), 'currentState': array([52.93544098, 17.91214893,  1.92661003]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 0.9144307203224392}
episode index:606
target Thresh 72.69521035217696
target distance 27.0
model initialize at round 606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.86637219,  15.43421211]), 'dynamicTrap': False, 'previousTarget': array([103.64100589,  15.09400392]), 'currentState': array([88.62690145,  3.76032714,  0.86373347]), 'targetState': array([114,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7654507388601818
running average episode reward sum: 0.6799253026953337
{'scaleFactor': 20, 'currentTarget': array([114.,  22.]), 'dynamicTrap': False, 'previousTarget': array([114.,  22.]), 'currentState': array([114.88398667,  22.16292927,   0.91706682]), 'targetState': array([114,  22], dtype=int32), 'currentDistance': 0.8988761792566686}
episode index:607
target Thresh 72.71169305930928
target distance 64.0
model initialize at round 607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.54313493, 17.32540456]), 'dynamicTrap': False, 'previousTarget': array([56.00975848, 18.37530495]), 'currentState': array([76.54260126, 17.47150827,  4.15963769]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5989946673217128
running average episode reward sum: 0.6797921930976798
{'scaleFactor': 20, 'currentTarget': array([12., 17.]), 'dynamicTrap': False, 'previousTarget': array([12., 17.]), 'currentState': array([11.13893779, 16.02765656,  1.13285372]), 'targetState': array([12, 17], dtype=int32), 'currentDistance': 1.2987994034002883}
episode index:608
target Thresh 72.72809355859684
target distance 25.0
model initialize at round 608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.94745009, 16.1193955 ]), 'dynamicTrap': False, 'previousTarget': array([49.43046618, 15.42781353]), 'currentState': array([66.64240207,  9.0131468 ,  3.49189258]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6481270994223111
running average episode reward sum: 0.6797401978699699
{'scaleFactor': 20, 'currentTarget': array([43., 18.]), 'dynamicTrap': False, 'previousTarget': array([43., 18.]), 'currentState': array([42.93193404, 18.18615116,  5.12234628]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 0.19820501712110142}
episode index:609
target Thresh 72.74441226005293
target distance 31.0
model initialize at round 609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.57246497,  6.57951855]), 'dynamicTrap': True, 'previousTarget': array([97.84855491,  7.3118031 ]), 'currentState': array([79.       , 14.       ,  2.6318567], dtype=float32), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 35
reward sum = 0.5965835328530474
running average episode reward sum: 0.6796038754683028
{'scaleFactor': 20, 'currentTarget': array([110.,   3.]), 'dynamicTrap': False, 'previousTarget': array([110.,   3.]), 'currentState': array([110.13717428,   3.93582676,   3.78948015]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 0.9458268932425289}
episode index:610
target Thresh 72.76064957164596
target distance 63.0
model initialize at round 610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.08693848, 14.10407033]), 'dynamicTrap': False, 'previousTarget': array([48.41266722, 12.95816943]), 'currentState': array([67.55870539, 18.67028249,  2.76187712]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.21956686777872325
running average episode reward sum: 0.6788509507421333
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'dynamicTrap': False, 'previousTarget': array([5., 4.]), 'currentState': array([4.64642339, 4.60817036, 5.90328931]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.7034824834655515}
episode index:611
target Thresh 72.77680589930957
target distance 17.0
model initialize at round 611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 15.]), 'dynamicTrap': False, 'previousTarget': array([98., 15.]), 'currentState': array([114.30569205,   9.46692317,   3.27713823]), 'targetState': array([98, 15], dtype=int32), 'currentDistance': 17.21890044048689}
done in step count: 25
reward sum = 0.6995178038001953
running average episode reward sum: 0.6788847201098752
{'scaleFactor': 20, 'currentTarget': array([98., 15.]), 'dynamicTrap': False, 'previousTarget': array([98., 15.]), 'currentState': array([98.03149204, 15.71755706,  1.59226528]), 'targetState': array([98, 15], dtype=int32), 'currentDistance': 0.7182477844035638}
episode index:612
target Thresh 72.79288164695278
target distance 36.0
model initialize at round 612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.08101568, 10.69885385]), 'dynamicTrap': False, 'previousTarget': array([52.97366596, 10.67544468]), 'currentState': array([34.12733686, 17.08305773,  0.67896779]), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6849482819398965
running average episode reward sum: 0.6788946117278688
{'scaleFactor': 20, 'currentTarget': array([70.,  5.]), 'dynamicTrap': False, 'previousTarget': array([70.,  5.]), 'currentState': array([69.55984695,  4.15906835,  3.27697669]), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 0.949157915865672}
episode index:613
target Thresh 72.80887721647014
target distance 36.0
model initialize at round 613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.04378961, 16.27265585]), 'dynamicTrap': False, 'previousTarget': array([30.93091516, 15.6609096 ]), 'currentState': array([12.06739804, 15.30117236,  1.89377802]), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6775359075233371
running average episode reward sum: 0.6788923988545715
{'scaleFactor': 20, 'currentTarget': array([47., 17.]), 'dynamicTrap': False, 'previousTarget': array([47., 17.]), 'currentState': array([47.4923488 , 17.69842295,  6.03445903]), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 0.8545185550653326}
episode index:614
target Thresh 72.82479300775171
target distance 65.0
model initialize at round 614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.95510209, 23.1625397 ]), 'dynamicTrap': False, 'previousTarget': array([42., 23.]), 'currentState': array([23.95524467, 23.23806017,  6.09903938]), 'targetState': array([87, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5356862557080811
running average episode reward sum: 0.6786595433372601
{'scaleFactor': 20, 'currentTarget': array([87., 23.]), 'dynamicTrap': False, 'previousTarget': array([87., 23.]), 'currentState': array([8.66987447e+01, 2.38096851e+01, 3.52063254e-02]), 'targetState': array([87, 23], dtype=int32), 'currentDistance': 0.8639124215836497}
episode index:615
target Thresh 72.84062941869308
target distance 37.0
model initialize at round 615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.58745945, 12.18821559]), 'dynamicTrap': True, 'previousTarget': array([46.56664794, 12.27296842]), 'currentState': array([66.       , 17.       ,  1.1269927], dtype=float32), 'targetState': array([29,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.4741811986010962
running average episode reward sum: 0.6783275979724287
{'scaleFactor': 20, 'currentTarget': array([29.,  8.]), 'dynamicTrap': False, 'previousTarget': array([29.,  8.]), 'currentState': array([29.77646366,  7.7927922 ,  5.72256729]), 'targetState': array([29,  8], dtype=int32), 'currentDistance': 0.8036360453993515}
episode index:616
target Thresh 72.85638684520538
target distance 52.0
model initialize at round 616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.24678053, 12.46538092]), 'dynamicTrap': False, 'previousTarget': array([32.86817872, 13.29248216]), 'currentState': array([13.44765493,  9.63790409,  5.95911944]), 'targetState': array([65, 17], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.480164257535911
running average episode reward sum: 0.6780064256216403
{'scaleFactor': 20, 'currentTarget': array([65., 17.]), 'dynamicTrap': False, 'previousTarget': array([65., 17.]), 'currentState': array([64.55747947, 16.05920747,  1.75056068]), 'targetState': array([65, 17], dtype=int32), 'currentDistance': 1.0396706273059286}
episode index:617
target Thresh 72.87206568122507
target distance 10.0
model initialize at round 617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.,  8.]), 'dynamicTrap': False, 'previousTarget': array([53.,  8.]), 'currentState': array([51.14085985, 17.0192507 ,  4.86189878]), 'targetState': array([53,  8], dtype=int32), 'currentDistance': 9.208869920591269}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.67841752420139
{'scaleFactor': 20, 'currentTarget': array([53.,  8.]), 'dynamicTrap': False, 'previousTarget': array([53.,  8.]), 'currentState': array([52.36879293,  8.67270856,  1.7702348 ]), 'targetState': array([53,  8], dtype=int32), 'currentDistance': 0.9224744789433518}
episode index:618
target Thresh 72.88766631872389
target distance 3.0
model initialize at round 618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([ 7.50477906, 14.99300312,  4.52333069]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 4.495226386878023}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6788890613189968
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([11.43174714, 15.07807926,  1.9331783 ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.573591917918282}
episode index:619
target Thresh 72.90318914771856
target distance 65.0
model initialize at round 619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.5663138 , 17.27434229]), 'dynamicTrap': True, 'previousTarget': array([62.57970242, 17.21961906]), 'currentState': array([82.     , 22.     ,  5.38502], dtype=float32), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.35542400854825607
running average episode reward sum: 0.6783673434919473
{'scaleFactor': 20, 'currentTarget': array([17.,  6.]), 'dynamicTrap': False, 'previousTarget': array([17.,  6.]), 'currentState': array([17.62836567,  5.65683877,  2.71854497]), 'targetState': array([17,  6], dtype=int32), 'currentDistance': 0.7159630170122725}
episode index:620
target Thresh 72.91863455628065
target distance 27.0
model initialize at round 620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.33087948,   3.89453941]), 'dynamicTrap': False, 'previousTarget': array([105.97366596,   4.67544468]), 'currentState': array([88.0920987 ,  9.35982203,  5.60097706]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6786739110753242
{'scaleFactor': 20, 'currentTarget': array([114.,   2.]), 'dynamicTrap': False, 'previousTarget': array([114.,   2.]), 'currentState': array([113.27794367,   2.91077966,   1.13032263]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 1.1622757601205742}
episode index:621
target Thresh 72.93400293054614
target distance 39.0
model initialize at round 621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.34067078, 11.39674891]), 'dynamicTrap': False, 'previousTarget': array([87.75100648, 10.4292033 ]), 'currentState': array([105.73901214,   6.52803735,   2.50121501]), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6875338071918444
running average episode reward sum: 0.6786881552812993
{'scaleFactor': 20, 'currentTarget': array([68., 16.]), 'dynamicTrap': False, 'previousTarget': array([68., 16.]), 'currentState': array([67.43402482, 15.48331514,  5.16914886]), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 0.7663492396653758}
episode index:622
target Thresh 72.94929465472522
target distance 6.0
model initialize at round 622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  10.]), 'dynamicTrap': False, 'previousTarget': array([115.,  10.]), 'currentState': array([110.0738067 ,  14.31287296,   5.25883839]), 'targetState': array([115,  10], dtype=int32), 'currentDistance': 6.54738524788663}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6791562304734642
{'scaleFactor': 20, 'currentTarget': array([115.,  10.]), 'dynamicTrap': False, 'previousTarget': array([115.,  10.]), 'currentState': array([114.43476412,  10.43183658,   5.87553371]), 'targetState': array([115,  10], dtype=int32), 'currentDistance': 0.7113187978158898}
episode index:623
target Thresh 72.96451011111178
target distance 60.0
model initialize at round 623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.15330625, 14.48059974]), 'dynamicTrap': False, 'previousTarget': array([88.52317581, 15.45540769]), 'currentState': array([108.74170783,  18.5172473 ,   4.16621542]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6307381064766703
running average episode reward sum: 0.6790786373260334
{'scaleFactor': 20, 'currentTarget': array([48.,  6.]), 'dynamicTrap': False, 'previousTarget': array([48.,  6.]), 'currentState': array([48.79284239,  6.87628283,  4.73794095]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 1.1817235975217202}
episode index:624
target Thresh 72.979649680093
target distance 20.0
model initialize at round 624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2.0992562 , 11.00992562]), 'currentState': array([21.64107378, 11.35574939,  3.48746872]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 19.64429527974405}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6793961451431101
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.6310782 , 11.99047171,  3.85796796]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 1.1744334402120071}
episode index:625
target Thresh 72.99471374015893
target distance 18.0
model initialize at round 625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'dynamicTrap': False, 'previousTarget': array([25.,  4.]), 'currentState': array([25.70175961, 20.02454021,  4.48134072]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 16.039898858549634}
done in step count: 17
reward sum = 0.7876709913798344
running average episode reward sum: 0.6795691081562678
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'dynamicTrap': False, 'previousTarget': array([25.,  4.]), 'currentState': array([24.81018606,  4.85813384,  0.409386  ]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.8788760008676979}
episode index:626
target Thresh 73.00970266791184
target distance 57.0
model initialize at round 626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.71298842, 15.80393659]), 'dynamicTrap': False, 'previousTarget': array([31.2557678, 16.5948722]), 'currentState': array([13.40526982, 21.02044907,  5.37674415]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.2100918616546899
running average episode reward sum: 0.6788203406179878
{'scaleFactor': 20, 'currentTarget': array([69.,  6.]), 'dynamicTrap': False, 'previousTarget': array([69.,  6.]), 'currentState': array([68.0336359 ,  6.12609379,  2.62695015]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 0.9745559061839997}
episode index:627
target Thresh 73.02461683807569
target distance 16.0
model initialize at round 627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 16.]), 'currentState': array([22.76684104,  6.21864184,  1.44033611]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 17.71255378862082}
done in step count: 21
reward sum = 0.7714942161282484
running average episode reward sum: 0.67896791048345
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 16.]), 'currentState': array([ 8.64524889, 15.71189007,  2.91584862]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 0.7066494675290574}
episode index:628
target Thresh 73.03945662350553
target distance 8.0
model initialize at round 628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.26360006, 16.45419637]), 'dynamicTrap': True, 'previousTarget': array([36., 18.]), 'currentState': array([44.      , 11.      ,  2.356895], dtype=float32), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 8.667603025910172}
done in step count: 20
reward sum = 0.6714414606413284
running average episode reward sum: 0.6789559447444323
{'scaleFactor': 20, 'currentTarget': array([36., 18.]), 'dynamicTrap': False, 'previousTarget': array([36., 18.]), 'currentState': array([36.52052149, 17.17079197,  3.23015397]), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 0.9790447330231127}
episode index:629
target Thresh 73.05422239519676
target distance 14.0
model initialize at round 629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.90819138, 15.41334497]), 'dynamicTrap': True, 'previousTarget': array([91., 16.]), 'currentState': array([105.       ,  20.       ,   2.9471483], dtype=float32), 'targetState': array([91, 16], dtype=int32), 'currentDistance': 12.932487778723502}
done in step count: 36
reward sum = 0.649652166039737
running average episode reward sum: 0.6789094308099805
{'scaleFactor': 20, 'currentTarget': array([91., 16.]), 'dynamicTrap': False, 'previousTarget': array([91., 16.]), 'currentState': array([91.76668204, 16.81378757,  5.3641875 ]), 'targetState': array([91, 16], dtype=int32), 'currentDistance': 1.1180570505722816}
episode index:630
target Thresh 73.06891452229445
target distance 17.0
model initialize at round 630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.04061359, 14.26947657]), 'dynamicTrap': False, 'previousTarget': array([69.88715666, 14.14900215]), 'currentState': array([54.12848067,  2.15320545,  0.83304248]), 'targetState': array([71, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6791167500451806
{'scaleFactor': 20, 'currentTarget': array([71., 15.]), 'dynamicTrap': False, 'previousTarget': array([71., 15.]), 'currentState': array([71.18743089, 14.25537322,  2.79572271]), 'targetState': array([71, 15], dtype=int32), 'currentDistance': 0.7678537522296485}
episode index:631
target Thresh 73.08353337210254
target distance 27.0
model initialize at round 631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.8379383 , 10.60379194]), 'dynamicTrap': False, 'previousTarget': array([79.0200334 , 10.67631238]), 'currentState': array([60.80626304,  1.95170044,  3.28491693]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 36
reward sum = 0.6017590112620276
running average episode reward sum: 0.6789943485597641
{'scaleFactor': 20, 'currentTarget': array([88., 15.]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([87.2457709 , 14.28944003,  2.52692021]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 1.0362224708379169}
episode index:632
target Thresh 73.09807931009301
target distance 6.0
model initialize at round 632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([32.65979024, 15.24006783,  4.4582715 ]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 5.601092728373289}
done in step count: 59
reward sum = 0.5445044077864127
running average episode reward sum: 0.6787818841983528
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([29.4772541 , 10.26787543,  3.74984563]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.8739438562855245}
episode index:633
target Thresh 73.11255269991511
target distance 45.0
model initialize at round 633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.83344578,  8.99036586]), 'dynamicTrap': False, 'previousTarget': array([83.21428366,  9.44920694]), 'currentState': array([65.62105339, 14.54770144,  0.73790472]), 'targetState': array([109,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.423894748032701
running average episode reward sum: 0.6783798540151262
{'scaleFactor': 20, 'currentTarget': array([109.,   2.]), 'dynamicTrap': False, 'previousTarget': array([109.,   2.]), 'currentState': array([108.47715551,   1.44981175,   0.53828965]), 'targetState': array([109,   2], dtype=int32), 'currentDistance': 0.7589950424998866}
episode index:634
target Thresh 73.12695390340431
target distance 5.0
model initialize at round 634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92., 11.]), 'dynamicTrap': False, 'previousTarget': array([92., 11.]), 'currentState': array([89.48023659,  7.12307963,  2.38317221]), 'targetState': array([92, 11], dtype=int32), 'currentDistance': 4.623820844386869}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6783756795191344
{'scaleFactor': 20, 'currentTarget': array([92., 11.]), 'dynamicTrap': False, 'previousTarget': array([92., 11.]), 'currentState': array([91.22882598, 10.25182532,  2.19051471]), 'targetState': array([92, 11], dtype=int32), 'currentDistance': 1.0744648572563822}
episode index:635
target Thresh 73.14128328059148
target distance 52.0
model initialize at round 635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.28711933, 15.94109803]), 'dynamicTrap': True, 'previousTarget': array([54.45516326, 15.51073901]), 'currentState': array([73.      , 23.      ,  6.234133], dtype=float32), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3612804718331968
running average episode reward sum: 0.677877102148559
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'dynamicTrap': False, 'previousTarget': array([21.,  2.]), 'currentState': array([21.75145818,  2.8830974 ,  5.88570724]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 1.1595475023041433}
episode index:636
target Thresh 73.15554118971176
target distance 5.0
model initialize at round 636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 16.]), 'dynamicTrap': False, 'previousTarget': array([44., 16.]), 'currentState': array([47.59692537, 19.20602175,  3.9713521 ]), 'targetState': array([44, 16], dtype=int32), 'currentDistance': 4.818344896466952}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6782326829536771
{'scaleFactor': 20, 'currentTarget': array([44., 16.]), 'dynamicTrap': False, 'previousTarget': array([44., 16.]), 'currentState': array([44.21685822, 16.265427  ,  4.76612883]), 'targetState': array([44, 16], dtype=int32), 'currentDistance': 0.34275207019172527}
episode index:637
target Thresh 73.16972798721362
target distance 36.0
model initialize at round 637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.3985155, 15.839027 ]), 'dynamicTrap': False, 'previousTarget': array([68.5237412 , 16.66139084]), 'currentState': array([49.7216488 , 19.4196578 ,  6.01491034]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 30
reward sum = 0.6714716082860377
running average episode reward sum: 0.6782220856579597
{'scaleFactor': 20, 'currentTarget': array([85., 13.]), 'dynamicTrap': False, 'previousTarget': array([85., 13.]), 'currentState': array([85.54746634, 12.45973856,  3.17288266]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 0.7691565575402918}
episode index:638
target Thresh 73.18384402776776
target distance 9.0
model initialize at round 638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71., 10.]), 'dynamicTrap': False, 'previousTarget': array([71., 10.]), 'currentState': array([79.05311314,  7.92789231,  2.97893286]), 'targetState': array([71, 10], dtype=int32), 'currentDistance': 8.315423112479705}
done in step count: 23
reward sum = 0.7688283904148263
running average episode reward sum: 0.6783638795621176
{'scaleFactor': 20, 'currentTarget': array([71., 10.]), 'dynamicTrap': False, 'previousTarget': array([71., 10.]), 'currentState': array([71.6813379 , 10.18614786,  3.96060321]), 'targetState': array([71, 10], dtype=int32), 'currentDistance': 0.7063089701547095}
episode index:639
target Thresh 73.19788966427592
target distance 46.0
model initialize at round 639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.58691212, 13.37229216]), 'dynamicTrap': False, 'previousTarget': array([45.45157783, 12.65146426]), 'currentState': array([24.99549286,  9.35031607,  1.20038533]), 'targetState': array([72, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.4251625379338996
running average episode reward sum: 0.6779682524658235
{'scaleFactor': 20, 'currentTarget': array([72., 19.]), 'dynamicTrap': False, 'previousTarget': array([72., 19.]), 'currentState': array([71.24097484, 19.13979609,  1.46885427]), 'targetState': array([72, 19], dtype=int32), 'currentDistance': 0.7717915172571037}
episode index:640
target Thresh 73.21186524787976
target distance 65.0
model initialize at round 640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.82186406, 18.38975831]), 'dynamicTrap': False, 'previousTarget': array([21.85022022, 18.55689597]), 'currentState': array([ 3.97588205, 20.86705485,  5.97040058]), 'targetState': array([67, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.34420124434825305
running average episode reward sum: 0.6774475551052657
{'scaleFactor': 20, 'currentTarget': array([67., 13.]), 'dynamicTrap': False, 'previousTarget': array([67., 13.]), 'currentState': array([66.73317581, 12.6212723 ,  0.93019282]), 'targetState': array([67, 13], dtype=int32), 'currentDistance': 0.4632815729209884}
episode index:641
target Thresh 73.22577112796957
target distance 21.0
model initialize at round 641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.4301637 ,  9.58247163]), 'dynamicTrap': False, 'previousTarget': array([48.23047895,  9.50557744]), 'currentState': array([29.67927953, 16.5397946 ,  0.14532685]), 'targetState': array([50,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6489732702849066
running average episode reward sum: 0.6774032026366982
{'scaleFactor': 20, 'currentTarget': array([50.,  9.]), 'dynamicTrap': False, 'previousTarget': array([50.,  9.]), 'currentState': array([50.99500186,  8.12075009,  1.96940459]), 'targetState': array([50,  9], dtype=int32), 'currentDistance': 1.3278211910190356}
episode index:642
target Thresh 73.23960765219307
target distance 73.0
model initialize at round 642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.03463365,  7.17649755]), 'dynamicTrap': True, 'previousTarget': array([86.02995695,  7.09424893]), 'currentState': array([106.       ,   6.       ,   4.2821445], dtype=float32), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 82
reward sum = 0.25061044045702674
running average episode reward sum: 0.6767394502849413
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([33.96101306,  9.40008383,  3.77438439]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 1.1328925427824632}
episode index:643
target Thresh 73.25337516646411
target distance 53.0
model initialize at round 643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.00208976,  6.71088725]), 'dynamicTrap': True, 'previousTarget': array([63.00355904,  6.62270866]), 'currentState': array([83.       ,  7.       ,  3.3270996], dtype=float32), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 90
reward sum = -0.02947475186801718
running average episode reward sum: 0.6756428443809771
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([30.,  6.]), 'currentState': array([30.40603842,  6.93156429,  0.56088217]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 1.0162082592546502}
episode index:644
target Thresh 73.26707401497127
target distance 5.0
model initialize at round 644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   6.]), 'dynamicTrap': False, 'previousTarget': array([106.,   6.]), 'currentState': array([109.14603674,   4.53335622,   3.09015098]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 3.4711080555376683}
done in step count: 33
reward sum = 0.5815531909659833
running average episode reward sum: 0.6754969689493259
{'scaleFactor': 20, 'currentTarget': array([106.,   6.]), 'dynamicTrap': False, 'previousTarget': array([106.,   6.]), 'currentState': array([105.14728572,   6.41271238,   2.2313474 ]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 0.947340045051808}
episode index:645
target Thresh 73.28070454018646
target distance 27.0
model initialize at round 645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.53693213,  7.27881796]), 'dynamicTrap': True, 'previousTarget': array([80.5237412 ,  7.33860916]), 'currentState': array([61.       ,  3.       ,  0.8180864], dtype=float32), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7050803359578534
running average episode reward sum: 0.6755427636350976
{'scaleFactor': 20, 'currentTarget': array([88.,  9.]), 'dynamicTrap': False, 'previousTarget': array([88.,  9.]), 'currentState': array([88.13592994,  8.29810672,  3.43059821]), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 0.7149343529730802}
episode index:646
target Thresh 73.29426708287353
target distance 13.0
model initialize at round 646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59., 20.]), 'dynamicTrap': False, 'previousTarget': array([59., 20.]), 'currentState': array([70.39386624,  8.32674994,  2.63094723]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 16.312110681459284}
done in step count: 57
reward sum = 0.45078827249152764
running average episode reward sum: 0.6751953842052003
{'scaleFactor': 20, 'currentTarget': array([59., 20.]), 'dynamicTrap': False, 'previousTarget': array([59., 20.]), 'currentState': array([58.0350073 , 20.57739888,  1.83825079]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 1.1245445218670032}
episode index:647
target Thresh 73.30776198209674
target distance 39.0
model initialize at round 647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.15088918,  4.5478989 ]), 'dynamicTrap': True, 'previousTarget': array([25.10437123,  4.95942269]), 'currentState': array([45.       ,  7.       ,  2.5431855], dtype=float32), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.4873633876394131
running average episode reward sum: 0.6749055200129691
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'dynamicTrap': False, 'previousTarget': array([6., 3.]), 'currentState': array([6.81665199, 3.75291849, 5.37385653]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 1.1107685328595454}
episode index:648
target Thresh 73.32118957522928
target distance 36.0
model initialize at round 648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.58761086, 11.56037685]), 'dynamicTrap': False, 'previousTarget': array([52., 12.]), 'currentState': array([33.59690878, 10.95059762,  5.96227813]), 'targetState': array([68, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 44
reward sum = 0.5163489867561424
running average episode reward sum: 0.6746612110249001
{'scaleFactor': 20, 'currentTarget': array([68., 12.]), 'dynamicTrap': False, 'previousTarget': array([68., 12.]), 'currentState': array([68.55757902, 12.85392288,  4.88930711]), 'targetState': array([68, 12], dtype=int32), 'currentDistance': 1.0198424601166645}
episode index:649
target Thresh 73.33455019796169
target distance 39.0
model initialize at round 649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.56318124, 16.64740409]), 'dynamicTrap': False, 'previousTarget': array([50.76743395, 16.04114369]), 'currentState': array([32.74277025, 13.97321114,  0.61867152]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6479709792413044
running average episode reward sum: 0.6746201491298484
{'scaleFactor': 20, 'currentTarget': array([70., 19.]), 'dynamicTrap': False, 'previousTarget': array([70., 19.]), 'currentState': array([69.54436389, 19.77579829,  0.16235629]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 0.8997039816219633}
episode index:650
target Thresh 73.34784418431022
target distance 30.0
model initialize at round 650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.28848284,  5.98670057]), 'dynamicTrap': False, 'previousTarget': array([40.,  6.]), 'currentState': array([58.28845709,  5.95460927,  3.24892896]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5880963456275241
running average episode reward sum: 0.6744872400614884
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([30.,  6.]), 'currentState': array([30.13885843,  5.35843788,  2.94040735]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 0.6564172599739952}
episode index:651
target Thresh 73.36107186662521
target distance 56.0
model initialize at round 651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.42355134,  9.54649247]), 'dynamicTrap': False, 'previousTarget': array([55.23047895,  9.49442256]), 'currentState': array([36.19350208,  4.0505662 ,  0.25611828]), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5808507791384204
running average episode reward sum: 0.6743436258576186
{'scaleFactor': 20, 'currentTarget': array([92., 20.]), 'dynamicTrap': False, 'previousTarget': array([92., 20.]), 'currentState': array([91.62499651, 19.73082275,  2.25843649]), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 0.4616102396749196}
episode index:652
target Thresh 73.37423357559943
target distance 15.0
model initialize at round 652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'dynamicTrap': False, 'previousTarget': array([14.,  6.]), 'currentState': array([24.92528877, 19.67029647,  4.31033166]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 17.499684007299678}
done in step count: 17
reward sum = 0.7773593527050627
running average episode reward sum: 0.6745013834791308
{'scaleFactor': 20, 'currentTarget': array([14.,  6.]), 'dynamicTrap': False, 'previousTarget': array([14.,  6.]), 'currentState': array([13.57179778,  5.93446139,  6.27913254]), 'targetState': array([14,  6], dtype=int32), 'currentDistance': 0.43318869707896196}
episode index:653
target Thresh 73.38732964027629
target distance 38.0
model initialize at round 653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.78310582, 17.04882812]), 'dynamicTrap': False, 'previousTarget': array([90.82908591, 17.39090975]), 'currentState': array([72.94083582, 19.55568302,  6.22379434]), 'targetState': array([109,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7299333939748035
running average episode reward sum: 0.6745861419049652
{'scaleFactor': 20, 'currentTarget': array([109.,  15.]), 'dynamicTrap': False, 'previousTarget': array([109.,  15.]), 'currentState': array([108.2318389 ,  14.07034147,   0.34280033]), 'targetState': array([109,  15], dtype=int32), 'currentDistance': 1.2059587278368493}
episode index:654
target Thresh 73.40036038805808
target distance 48.0
model initialize at round 654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.82723569, 21.82058509]), 'dynamicTrap': False, 'previousTarget': array([33.00433887, 21.41657627]), 'currentState': array([5.48268739e+01, 2.17002845e+01, 3.02126408e-02]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6895966316016163
running average episode reward sum: 0.674609058683128
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.34923214, 21.8251483 ,  3.58819232]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.39055883261501184}
episode index:655
target Thresh 73.41332614471416
target distance 34.0
model initialize at round 655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.49446455,  8.44637061]), 'dynamicTrap': False, 'previousTarget': array([69.81268997,  9.35667352]), 'currentState': array([88.95200725, 13.07281978,  4.8194851 ]), 'targetState': array([55,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7158101704615589
running average episode reward sum: 0.674671865255961
{'scaleFactor': 20, 'currentTarget': array([55.96613156,  6.28963759]), 'dynamicTrap': True, 'previousTarget': array([55.,  5.]), 'currentState': array([55.76485413,  6.69579014,  3.38504941]), 'targetState': array([55,  5], dtype=int32), 'currentDistance': 0.4532907364216276}
episode index:656
target Thresh 73.42622723438915
target distance 21.0
model initialize at round 656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.41930571,   8.67392369]), 'dynamicTrap': False, 'previousTarget': array([106.64100589,   8.90599608]), 'currentState': array([91.39442781, 20.64068132,  5.7038768 ]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7309279438570239
running average episode reward sum: 0.6747574909463736
{'scaleFactor': 20, 'currentTarget': array([111.,   6.]), 'dynamicTrap': False, 'previousTarget': array([111.,   6.]), 'currentState': array([111.30238464,   5.15186097,   0.21235068]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 0.9004311645768525}
episode index:657
target Thresh 73.43906397961094
target distance 1.0
model initialize at round 657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  11.]), 'dynamicTrap': False, 'previousTarget': array([113.,  11.]), 'currentState': array([1.15690653e+02, 1.25206140e+01, 1.90591812e-02]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 3.0906116085414648}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6751918959905281
{'scaleFactor': 20, 'currentTarget': array([113.,  11.]), 'dynamicTrap': False, 'previousTarget': array([113.,  11.]), 'currentState': array([112.80128345,  10.73380442,   2.75983823]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 0.3321872313502587}
episode index:658
target Thresh 73.45183670129884
target distance 9.0
model initialize at round 658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([11.11468674, 15.66931646,  2.79910743]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 8.005164004909137}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6755675451535591
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.01309148, 12.13407653,  0.70827314]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.13471415320382477}
episode index:659
target Thresh 73.46454571877155
target distance 7.0
model initialize at round 659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([72.7375903 ,  1.17990388,  6.17605877]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 6.315879420465375}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6760141079639325
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([78.52483238,  1.60407068,  0.53207786]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 0.618501653907335}
episode index:660
target Thresh 73.47719134975517
target distance 17.0
model initialize at round 660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  16.]), 'dynamicTrap': False, 'previousTarget': array([109.,  16.]), 'currentState': array([90.47395339, 14.70962654,  1.69632697]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 18.570930689943133}
done in step count: 17
reward sum = 0.7874543926050627
running average episode reward sum: 0.6761827014354017
{'scaleFactor': 20, 'currentTarget': array([109.,  16.]), 'dynamicTrap': False, 'previousTarget': array([109.,  16.]), 'currentState': array([109.11470694,  16.42819734,   2.25051609]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 0.44329520628379276}
episode index:661
target Thresh 73.48977391039116
target distance 37.0
model initialize at round 661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.79188946, 20.98744228]), 'dynamicTrap': False, 'previousTarget': array([88.0073006 , 20.45965677]), 'currentState': array([106.75290568,  22.23557358,   2.90493762]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6763967863842866
{'scaleFactor': 20, 'currentTarget': array([71., 20.]), 'dynamicTrap': False, 'previousTarget': array([71., 20.]), 'currentState': array([71.46375835, 20.54087328,  3.91658798]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 0.7124715507989987}
episode index:662
target Thresh 73.50229371524415
target distance 18.0
model initialize at round 662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.14908148, 19.75893168]), 'dynamicTrap': False, 'previousTarget': array([41.90599608, 18.64100589]), 'currentState': array([51.66845529,  2.74885306,  1.7225951 ]), 'targetState': array([41, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6767135104948926
{'scaleFactor': 20, 'currentTarget': array([41., 20.]), 'dynamicTrap': False, 'previousTarget': array([41., 20.]), 'currentState': array([41.87523062, 20.40945607,  2.2213508 ]), 'targetState': array([41, 20], dtype=int32), 'currentDistance': 0.9662726929230583}
episode index:663
target Thresh 73.51475107730994
target distance 53.0
model initialize at round 663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.47279501, 20.03291   ]), 'dynamicTrap': False, 'previousTarget': array([36.1721898 , 20.38123261]), 'currentState': array([54.31059233, 22.57491656,  2.50981617]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6473140829596428
running average episode reward sum: 0.6766692342486046
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.14319551, 16.35951221,  4.99139842]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.9291732650511193}
episode index:664
target Thresh 73.52714630802322
target distance 15.0
model initialize at round 664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.,  7.]), 'dynamicTrap': False, 'previousTarget': array([49.,  7.]), 'currentState': array([63.81022836, 17.98499397,  3.25807122]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 18.43944024871399}
done in step count: 13
reward sum = 0.8491777154556887
running average episode reward sum: 0.67692864549854
{'scaleFactor': 20, 'currentTarget': array([49.,  7.]), 'dynamicTrap': False, 'previousTarget': array([49.,  7.]), 'currentState': array([48.99600915,  6.30141319,  4.74352342]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 0.6985982112008059}
episode index:665
target Thresh 73.53947971726541
target distance 6.0
model initialize at round 665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'dynamicTrap': False, 'previousTarget': array([26., 15.]), 'currentState': array([29.89954272, 19.32003083,  3.642663  ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 5.819716468228798}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.677354572472266
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'dynamicTrap': False, 'previousTarget': array([26., 15.]), 'currentState': array([26.27053016, 15.18800441,  4.58756661]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 0.3294422892710287}
episode index:666
target Thresh 73.55175161337237
target distance 19.0
model initialize at round 666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92., 15.]), 'dynamicTrap': False, 'previousTarget': array([92., 15.]), 'currentState': array([109.33220802,  18.77448283,   2.26599646]), 'targetState': array([92, 15], dtype=int32), 'currentDistance': 17.73843723311403}
done in step count: 18
reward sum = 0.762506768184386
running average episode reward sum: 0.6774822369336035
{'scaleFactor': 20, 'currentTarget': array([92., 15.]), 'dynamicTrap': False, 'previousTarget': array([92., 15.]), 'currentState': array([91.91219989, 15.41144902,  4.6332744 ]), 'targetState': array([92, 15], dtype=int32), 'currentDistance': 0.4207126759882997}
episode index:667
target Thresh 73.56396230314216
target distance 61.0
model initialize at round 667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.76743191,  9.49645843]), 'dynamicTrap': False, 'previousTarget': array([28.86960143, 10.2801182 ]), 'currentState': array([9.95196984, 6.78583804, 5.96287262]), 'targetState': array([70, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5110043198795489
running average episode reward sum: 0.6772330184948999
{'scaleFactor': 20, 'currentTarget': array([70., 15.]), 'dynamicTrap': False, 'previousTarget': array([70., 15.]), 'currentState': array([70.58431279, 14.59171591,  1.67797912]), 'targetState': array([70, 15], dtype=int32), 'currentDistance': 0.712823490124273}
episode index:668
target Thresh 73.57611209184266
target distance 10.0
model initialize at round 668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.93676825,  19.82823503]), 'dynamicTrap': True, 'previousTarget': array([108.,  20.]), 'currentState': array([98.      , 14.      ,  1.133339], dtype=float32), 'targetState': array([108,  20], dtype=int32), 'currentDistance': 11.519882236072446}
done in step count: 13
reward sum = 0.8298570619766188
running average episode reward sum: 0.6774611560785796
{'scaleFactor': 20, 'currentTarget': array([108.,  20.]), 'dynamicTrap': False, 'previousTarget': array([108.,  20.]), 'currentState': array([107.56818461,  20.42545623,   1.68137287]), 'targetState': array([108,  20], dtype=int32), 'currentDistance': 0.606199257595786}
episode index:669
target Thresh 73.58820128321919
target distance 11.0
model initialize at round 669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.16882424, 21.06877869]), 'dynamicTrap': True, 'previousTarget': array([71., 21.]), 'currentState': array([60.       , 21.       ,  3.6274605], dtype=float32), 'targetState': array([71, 21], dtype=int32), 'currentDistance': 11.169036008274443}
done in step count: 12
reward sum = 0.8469808817161293
running average episode reward sum: 0.6777141705944565
{'scaleFactor': 20, 'currentTarget': array([71., 21.]), 'dynamicTrap': False, 'previousTarget': array([71., 21.]), 'currentState': array([70.73705235, 20.919142  ,  0.550308  ]), 'targetState': array([71, 21], dtype=int32), 'currentDistance': 0.2750990463505806}
episode index:670
target Thresh 73.6002301795022
target distance 12.0
model initialize at round 670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,   6.]), 'dynamicTrap': False, 'previousTarget': array([103.,   6.]), 'currentState': array([106.60262046,  16.09741327,   4.20177811]), 'targetState': array([103,   6], dtype=int32), 'currentDistance': 10.720850199146922}
done in step count: 10
reward sum = 0.8573033124924453
running average episode reward sum: 0.67798181462113
{'scaleFactor': 20, 'currentTarget': array([103.,   6.]), 'dynamicTrap': False, 'previousTarget': array([103.,   6.]), 'currentState': array([103.40100873,   6.3530208 ,   4.44199336]), 'targetState': array([103,   6], dtype=int32), 'currentDistance': 0.5342580714643933}
episode index:671
target Thresh 73.6121990814147
target distance 56.0
model initialize at round 671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.85864146, 13.62748445]), 'dynamicTrap': False, 'previousTarget': array([21.04057123, 14.87981639]), 'currentState': array([ 2.67099705, 19.26967392,  5.75387084]), 'targetState': array([58,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6074042102963085
running average episode reward sum: 0.6778767884242181
{'scaleFactor': 20, 'currentTarget': array([58.,  3.]), 'dynamicTrap': False, 'previousTarget': array([58.,  3.]), 'currentState': array([57.89852393,  2.54740609,  0.60839398]), 'targetState': array([58,  3], dtype=int32), 'currentDistance': 0.4638303952203145}
episode index:672
target Thresh 73.62410828817988
target distance 11.0
model initialize at round 672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([9.95007121, 4.78671633, 2.98904419]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 12.367985920486621}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6781347096466114
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.96060868, 17.05400352,  2.03535792]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.06684351847270774}
episode index:673
target Thresh 73.63595809752853
target distance 50.0
model initialize at round 673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.2575011 , 16.91653326]), 'dynamicTrap': False, 'previousTarget': array([49., 16.]), 'currentState': array([68.24769486, 17.5427548 ,  2.09531477]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6082080607135185
running average episode reward sum: 0.6780309609093219
{'scaleFactor': 20, 'currentTarget': array([17.87125136, 16.63165007]), 'dynamicTrap': True, 'previousTarget': array([19., 16.]), 'currentState': array([17.05813802, 16.7987278 ,  5.53797329]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.8301013576606211}
episode index:674
target Thresh 73.64774880570647
target distance 11.0
model initialize at round 674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84., 10.]), 'dynamicTrap': False, 'previousTarget': array([84., 10.]), 'currentState': array([87.24729605, 21.00567782,  4.02526855]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 11.474749492590343}
done in step count: 9
reward sum = 0.8849865853906308
running average episode reward sum: 0.6783375618344794
{'scaleFactor': 20, 'currentTarget': array([84., 10.]), 'dynamicTrap': False, 'previousTarget': array([84., 10.]), 'currentState': array([83.81998886, 10.1222874 ,  4.56975394]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 0.2176194349806962}
episode index:675
target Thresh 73.65948070748205
target distance 42.0
model initialize at round 675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.82080144, 16.93844946]), 'dynamicTrap': False, 'previousTarget': array([84.97736275, 16.04869701]), 'currentState': array([64.8967532 , 18.67980004,  0.62218285]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.591791203545563
running average episode reward sum: 0.6782095346772472
{'scaleFactor': 20, 'currentTarget': array([107.,  15.]), 'dynamicTrap': False, 'previousTarget': array([107.,  15.]), 'currentState': array([107.42486793,  14.35037561,   0.67357419]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 0.7762245832038666}
episode index:676
target Thresh 73.6711540961534
target distance 39.0
model initialize at round 676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.91699618, 16.49827755]), 'dynamicTrap': False, 'previousTarget': array([87.24899352, 16.4292033 ]), 'currentState': array([69.78298799, 10.67678885,  5.33431071]), 'targetState': array([107,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5573544613628757
running average episode reward sum: 0.678031019059353
{'scaleFactor': 20, 'currentTarget': array([107.,  22.]), 'dynamicTrap': False, 'previousTarget': array([107.,  22.]), 'currentState': array([107.31884587,  21.93048222,   3.27583553]), 'targetState': array([107,  22], dtype=int32), 'currentDistance': 0.32633634142996426}
episode index:677
target Thresh 73.68276926355587
target distance 57.0
model initialize at round 677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.74773175, 15.77533268]), 'dynamicTrap': False, 'previousTarget': array([49.19412046, 15.22022743]), 'currentState': array([67.49172263, 18.96513271,  3.38594067]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6319265187657215
running average episode reward sum: 0.6779630183214569
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([12.04532694,  9.69450881,  0.58318597]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.3088355524210982}
episode index:678
target Thresh 73.69432650006925
target distance 45.0
model initialize at round 678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.10389484, 10.78892561]), 'dynamicTrap': False, 'previousTarget': array([43.21428366, 10.55079306]), 'currentState': array([25.95671022,  5.0109224 ,  0.37063986]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5594528570418017
running average episode reward sum: 0.6777884820014574
{'scaleFactor': 20, 'currentTarget': array([69., 18.]), 'dynamicTrap': False, 'previousTarget': array([69., 18.]), 'currentState': array([68.3646406 , 17.30878295,  2.24195578]), 'targetState': array([69, 18], dtype=int32), 'currentDistance': 0.9388623843050129}
episode index:679
target Thresh 73.70582609462504
target distance 13.0
model initialize at round 679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([19.68385682, 20.04888284,  1.45872235]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 14.448488285534667}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.678148711725614
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([33.65247472, 21.87541234,  0.58834058]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 0.369182754012924}
episode index:680
target Thresh 73.7172683347137
target distance 14.0
model initialize at round 680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.29298339, 21.37381926]), 'dynamicTrap': False, 'previousTarget': array([81., 22.]), 'currentState': array([65.32086276,  8.11351746,  2.06409073]), 'targetState': array([81, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7641788450800352
running average episode reward sum: 0.6782750408494824
{'scaleFactor': 20, 'currentTarget': array([81., 22.]), 'dynamicTrap': False, 'previousTarget': array([81., 22.]), 'currentState': array([80.83501336, 22.08816317,  1.41790867]), 'targetState': array([81, 22], dtype=int32), 'currentDistance': 0.18706505484203306}
episode index:681
target Thresh 73.72865350639184
target distance 24.0
model initialize at round 681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.07043419,  8.18050792]), 'dynamicTrap': False, 'previousTarget': array([12.,  8.]), 'currentState': array([31.035962  ,  9.3542624 ,  3.10695481]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7118295938357577
running average episode reward sum: 0.6783242410738025
{'scaleFactor': 20, 'currentTarget': array([8., 8.]), 'dynamicTrap': False, 'previousTarget': array([8., 8.]), 'currentState': array([8.67871736, 8.86107631, 4.87909169]), 'targetState': array([8, 8], dtype=int32), 'currentDistance': 1.0964076252336183}
episode index:682
target Thresh 73.73998189428933
target distance 12.0
model initialize at round 682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.,  5.]), 'dynamicTrap': False, 'previousTarget': array([74.,  5.]), 'currentState': array([71.81571968, 15.5279302 ,  4.20839834]), 'targetState': array([74,  5], dtype=int32), 'currentDistance': 10.752134434182373}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6787095352294792
{'scaleFactor': 20, 'currentTarget': array([74.,  5.]), 'dynamicTrap': False, 'previousTarget': array([74.,  5.]), 'currentState': array([73.88993294,  5.95614446,  0.37189185]), 'targetState': array([74,  5], dtype=int32), 'currentDistance': 0.9624588269618644}
episode index:683
target Thresh 73.75125378161648
target distance 58.0
model initialize at round 683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.98362642,  2.65206148]), 'dynamicTrap': False, 'previousTarget': array([69.98811999,  2.68924552]), 'currentState': array([51.99761857,  1.90407085,  6.21961003]), 'targetState': array([108,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6678203537910768
running average episode reward sum: 0.6786936153735751
{'scaleFactor': 20, 'currentTarget': array([108.,   4.]), 'dynamicTrap': False, 'previousTarget': array([108.,   4.]), 'currentState': array([108.44749021,   4.27439567,   0.80526857]), 'targetState': array([108,   4], dtype=int32), 'currentDistance': 0.5249194951979054}
episode index:684
target Thresh 73.76246945017104
target distance 70.0
model initialize at round 684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.7370738 ,  5.62957608]), 'dynamicTrap': False, 'previousTarget': array([46.92693298,  6.29197717]), 'currentState': array([28.79339179,  7.12942528,  5.97206625]), 'targetState': array([97,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6254062412764726
running average episode reward sum: 0.6786158235865721
{'scaleFactor': 20, 'currentTarget': array([97.,  2.]), 'dynamicTrap': False, 'previousTarget': array([97.,  2.]), 'currentState': array([96.54852958,  2.72710825,  0.23925028]), 'targetState': array([97,  2], dtype=int32), 'currentDistance': 0.855869120555923}
episode index:685
target Thresh 73.77362918034532
target distance 11.0
model initialize at round 685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54., 14.]), 'dynamicTrap': False, 'previousTarget': array([54., 14.]), 'currentState': array([44.16947118, 21.41725523,  5.66279912]), 'targetState': array([54, 14], dtype=int32), 'currentDistance': 12.314827321882866}
done in step count: 7
reward sum = 0.9221653479069899
running average episode reward sum: 0.6789708520476806
{'scaleFactor': 20, 'currentTarget': array([54., 14.]), 'dynamicTrap': False, 'previousTarget': array([54., 14.]), 'currentState': array([53.06538491, 14.79591798,  0.25985142]), 'targetState': array([54, 14], dtype=int32), 'currentDistance': 1.227595531249753}
episode index:686
target Thresh 73.78473325113316
target distance 27.0
model initialize at round 686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.29669734,   1.9838612 ]), 'dynamicTrap': False, 'previousTarget': array([109.98629668,   2.25976679]), 'currentState': array([91.29677742,  1.92726685,  0.31887453]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6792470892539707
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([117.29924617,   2.85355766,   6.05034697]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 0.9044937502281583}
episode index:687
target Thresh 73.79578194013689
target distance 13.0
model initialize at round 687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67., 10.]), 'dynamicTrap': False, 'previousTarget': array([67., 10.]), 'currentState': array([79.56281804,  5.62519538,  2.8435784 ]), 'targetState': array([67, 10], dtype=int32), 'currentDistance': 13.302755829165834}
done in step count: 10
reward sum = 0.8751731249088044
running average episode reward sum: 0.6795318654685853
{'scaleFactor': 20, 'currentTarget': array([67., 10.]), 'dynamicTrap': False, 'previousTarget': array([67., 10.]), 'currentState': array([67.40363118,  9.8387669 ,  3.83426337]), 'targetState': array([67, 10], dtype=int32), 'currentDistance': 0.4346426601351012}
episode index:688
target Thresh 73.80677552357432
target distance 62.0
model initialize at round 688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.62354447,  9.31201982]), 'dynamicTrap': False, 'previousTarget': array([69.83555733, 10.44057325]), 'currentState': array([50.73127403, 11.38507957,  5.98638761]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 47
reward sum = 0.5903561238851933
running average episode reward sum: 0.6794024376868969
{'scaleFactor': 20, 'currentTarget': array([112.,   5.]), 'dynamicTrap': False, 'previousTarget': array([112.,   5.]), 'currentState': array([111.82842183,   5.87850413,   2.97180994]), 'targetState': array([112,   5], dtype=int32), 'currentDistance': 0.8951025482450801}
episode index:689
target Thresh 73.81771427628561
target distance 28.0
model initialize at round 689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.58138753,  4.44470598]), 'dynamicTrap': False, 'previousTarget': array([88.949174,  4.424941]), 'currentState': array([70.65581516,  2.72088309,  5.16594428]), 'targetState': array([97,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6796642578563961
{'scaleFactor': 20, 'currentTarget': array([97.,  5.]), 'dynamicTrap': False, 'previousTarget': array([97.,  5.]), 'currentState': array([96.45541279,  4.33908386,  2.59108597]), 'targetState': array([97,  5], dtype=int32), 'currentDistance': 0.8563792250097501}
episode index:690
target Thresh 73.82859847174015
target distance 6.0
model initialize at round 690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76., 23.]), 'dynamicTrap': False, 'previousTarget': array([76., 23.]), 'currentState': array([71.17245217, 18.39585598,  0.66919884]), 'targetState': array([76, 23], dtype=int32), 'currentDistance': 6.671083886221705}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6800708161084128
{'scaleFactor': 20, 'currentTarget': array([76., 23.]), 'dynamicTrap': False, 'previousTarget': array([76., 23.]), 'currentState': array([75.45538078, 23.84878927,  0.90898277]), 'targetState': array([76, 23], dtype=int32), 'currentDistance': 1.0084906140079164}
episode index:691
target Thresh 73.8394283820434
target distance 44.0
model initialize at round 691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.01825742, 12.53873044]), 'dynamicTrap': False, 'previousTarget': array([69.95367385, 11.63952224]), 'currentState': array([49.12073756, 14.56078471,  1.27179885]), 'targetState': array([94, 10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 37
reward sum = 0.5945229908633133
running average episode reward sum: 0.6799471920834921
{'scaleFactor': 20, 'currentTarget': array([94., 10.]), 'dynamicTrap': False, 'previousTarget': array([94., 10.]), 'currentState': array([93.39761232,  9.83372989,  1.03112441]), 'targetState': array([94, 10], dtype=int32), 'currentDistance': 0.624913326722214}
episode index:692
target Thresh 73.85020427794365
target distance 45.0
model initialize at round 692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.99439856,  7.05324392]), 'dynamicTrap': False, 'previousTarget': array([32.23767062,  6.07414013]), 'currentState': array([51.85683475,  4.71152906,  2.60493165]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6457027610918087
running average episode reward sum: 0.6798977773201563
{'scaleFactor': 20, 'currentTarget': array([ 7., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 10.]), 'currentState': array([7.09856785, 9.89304638, 4.482512  ]), 'targetState': array([ 7, 10], dtype=int32), 'currentDistance': 0.14544654485905514}
episode index:693
target Thresh 73.8609264288389
target distance 40.0
model initialize at round 693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.18654339, 10.16876526]), 'dynamicTrap': False, 'previousTarget': array([67.77872706, 11.03319094]), 'currentState': array([48.30529057, 12.34495143,  5.90479875]), 'targetState': array([88,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6383422094418879
running average episode reward sum: 0.679837898980274
{'scaleFactor': 20, 'currentTarget': array([88.,  8.]), 'dynamicTrap': False, 'previousTarget': array([88.,  8.]), 'currentState': array([87.4033453 ,  8.18004697,  1.93746222]), 'targetState': array([88,  8], dtype=int32), 'currentDistance': 0.6232284817496115}
episode index:694
target Thresh 73.87159510278343
target distance 26.0
model initialize at round 694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.18844962, 17.20277647]), 'dynamicTrap': False, 'previousTarget': array([47.86817872, 16.70751784]), 'currentState': array([27.49314383, 20.68055304,  0.98168683]), 'targetState': array([54, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7726832661682786
running average episode reward sum: 0.6799714894366596
{'scaleFactor': 20, 'currentTarget': array([52.51816054, 15.0144753 ]), 'dynamicTrap': True, 'previousTarget': array([54., 16.]), 'currentState': array([52.30683542, 15.49607517,  1.29271872]), 'targetState': array([54, 16], dtype=int32), 'currentDistance': 0.5259246516915352}
episode index:695
target Thresh 73.8822105664947
target distance 29.0
model initialize at round 695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.94821794, 15.61006182]), 'dynamicTrap': True, 'previousTarget': array([23.98896727, 15.69498132]), 'currentState': array([42.       ,  7.       ,  0.5956926], dtype=float32), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7512044699573434
running average episode reward sum: 0.6800738356730399
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'dynamicTrap': False, 'previousTarget': array([13., 21.]), 'currentState': array([12.89673869, 21.93675662,  4.57950491]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.9424308245968213}
episode index:696
target Thresh 73.89277308535983
target distance 5.0
model initialize at round 696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.,  9.]), 'dynamicTrap': False, 'previousTarget': array([70.,  9.]), 'currentState': array([71.61249026,  5.5735644 ,  1.86413151]), 'targetState': array([70,  9], dtype=int32), 'currentDistance': 3.7868965849799436}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6805042892803956
{'scaleFactor': 20, 'currentTarget': array([70.,  9.]), 'dynamicTrap': False, 'previousTarget': array([70.,  9.]), 'currentState': array([70.51788984,  8.37511305,  3.26388648]), 'targetState': array([70,  9], dtype=int32), 'currentDistance': 0.8115993970736456}
episode index:697
target Thresh 73.90328292344233
target distance 61.0
model initialize at round 697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.11760226, 10.44980859]), 'dynamicTrap': False, 'previousTarget': array([48.00268691, 11.3278248 ]), 'currentState': array([67.10268755,  9.67756135,  5.09650886]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6061236675205094
running average episode reward sum: 0.6803977267850376
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 12.]), 'currentState': array([ 6.1041591 , 11.36209614,  4.7522427 ]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 1.099751000266304}
episode index:698
target Thresh 73.91374034348873
target distance 2.0
model initialize at round 698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 19.]), 'dynamicTrap': False, 'previousTarget': array([35., 19.]), 'currentState': array([38.81741159, 20.90366836,  5.47753793]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 4.265745474730695}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6808124639427128
{'scaleFactor': 20, 'currentTarget': array([35., 19.]), 'dynamicTrap': False, 'previousTarget': array([35., 19.]), 'currentState': array([34.71893504, 19.1185374 ,  3.58111167]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 0.3050387252296309}
episode index:699
target Thresh 73.92414560693504
target distance 23.0
model initialize at round 699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.96436853,   4.9639712 ]), 'dynamicTrap': False, 'previousTarget': array([109.7042351,   4.5731765]), 'currentState': array([89.51162346,  9.6105516 ,  0.85521889]), 'targetState': array([113,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7966624458780076
running average episode reward sum: 0.680977963916906
{'scaleFactor': 20, 'currentTarget': array([113.,   4.]), 'dynamicTrap': False, 'previousTarget': array([113.,   4.]), 'currentState': array([112.40555072,   3.78590475,   0.54568884]), 'targetState': array([113,   4], dtype=int32), 'currentDistance': 0.6318280850097355}
episode index:700
target Thresh 73.9344989739134
target distance 61.0
model initialize at round 700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.72979256,  9.67273961]), 'dynamicTrap': False, 'previousTarget': array([43.37611087, 10.86043721]), 'currentState': array([63.24817295,  5.31007675,  3.89098346]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.37121496106456064
running average episode reward sum: 0.6805360766089853
{'scaleFactor': 20, 'currentTarget': array([ 2., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 19.]), 'currentState': array([ 2.15488107, 19.16500411,  5.36709733]), 'targetState': array([ 2, 19], dtype=int32), 'currentDistance': 0.22630621063564288}
episode index:701
target Thresh 73.94480070325855
target distance 10.0
model initialize at round 701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.95284949, 20.81491147]), 'dynamicTrap': True, 'previousTarget': array([32., 21.]), 'currentState': array([22.       , 22.       ,  1.4162562], dtype=float32), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 10.023155584620854}
done in step count: 6
reward sum = 0.931480149401
running average episode reward sum: 0.6808935467981477
{'scaleFactor': 20, 'currentTarget': array([32., 21.]), 'dynamicTrap': False, 'previousTarget': array([32., 21.]), 'currentState': array([31.07391079, 21.1123509 ,  0.61116716]), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 0.9328793890910017}
episode index:702
target Thresh 73.95505105251424
target distance 48.0
model initialize at round 702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.97975678,  8.61324524]), 'dynamicTrap': False, 'previousTarget': array([29.49464637,  9.53247687]), 'currentState': array([11.39784034, 12.68123597,  5.91697043]), 'targetState': array([58,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5919806883918821
running average episode reward sum: 0.6807670704704005
{'scaleFactor': 20, 'currentTarget': array([56.14565216,  2.42103714]), 'dynamicTrap': True, 'previousTarget': array([58.,  3.]), 'currentState': array([55.76281757,  1.42925343,  0.71515593]), 'targetState': array([58,  3], dtype=int32), 'currentDistance': 1.0631073553203183}
episode index:703
target Thresh 73.96525027793973
target distance 37.0
model initialize at round 703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.56694482, 19.169439  ]), 'dynamicTrap': False, 'previousTarget': array([39.02915453, 19.92049484]), 'currentState': array([58.56589887, 19.373979  ,  3.44150209]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7394639868565595
running average episode reward sum: 0.6808504467720854
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'dynamicTrap': False, 'previousTarget': array([22., 19.]), 'currentState': array([22.5943777 , 19.55473078,  4.06332956]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.8130258824373469}
episode index:704
target Thresh 73.97539863451618
target distance 70.0
model initialize at round 704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.30584141,  5.10212564]), 'dynamicTrap': False, 'previousTarget': array([48.98165792,  4.14364323]), 'currentState': array([28.34318236,  6.32369985,  1.1273309 ]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.47794816958715547
running average episode reward sum: 0.6805626421235962
{'scaleFactor': 20, 'currentTarget': array([99.,  2.]), 'dynamicTrap': False, 'previousTarget': array([99.,  2.]), 'currentState': array([98.21780629,  1.20635986,  6.2424452 ]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 1.1143121988034461}
episode index:705
target Thresh 73.98549637595306
target distance 71.0
model initialize at round 705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.27372016,  7.89537851]), 'dynamicTrap': False, 'previousTarget': array([62.61331824,  6.9149334 ]), 'currentState': array([80.71804426,  3.21368433,  2.86222929]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.45644694417387954
running average episode reward sum: 0.6802451977922227
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([10.93451653, 19.51074034,  4.99981649]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.4936224266717713}
episode index:706
target Thresh 73.99554375469441
target distance 54.0
model initialize at round 706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.5863446 ,  6.78743132]), 'dynamicTrap': False, 'previousTarget': array([39.16594987,  7.57108057]), 'currentState': array([60.37518241,  3.88882858,  4.74557613]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5597791588488269
running average episode reward sum: 0.6800748073552447
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 12.]), 'currentState': array([ 5.74557987, 12.24772623,  3.82221215]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.7856574520031615}
episode index:707
target Thresh 74.00554102192523
target distance 31.0
model initialize at round 707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.2366827 ,  9.47266285]), 'dynamicTrap': True, 'previousTarget': array([58.20692454,  9.5762039 ]), 'currentState': array([39.       ,  4.       ,  1.3716476], dtype=float32), 'targetState': array([70, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4099279428363889
running average episode reward sum: 0.6796932439872801
{'scaleFactor': 20, 'currentTarget': array([70., 13.]), 'dynamicTrap': False, 'previousTarget': array([70., 13.]), 'currentState': array([69.55500364, 12.12528846,  1.10688196]), 'targetState': array([70, 13], dtype=int32), 'currentDistance': 0.9813979997746295}
episode index:708
target Thresh 74.01548842757772
target distance 11.0
model initialize at round 708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  13.]), 'dynamicTrap': False, 'previousTarget': array([117.,  13.]), 'currentState': array([106.53537737,   2.93693089,   1.65968502]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 14.518046933995825}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6800101534809635
{'scaleFactor': 20, 'currentTarget': array([117.,  13.]), 'dynamicTrap': False, 'previousTarget': array([117.,  13.]), 'currentState': array([117.78148578,  12.33514583,   1.75953729]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 1.026036592394978}
episode index:709
target Thresh 74.02538622033754
target distance 7.0
model initialize at round 709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.53636717, 10.7482978 ]), 'dynamicTrap': True, 'previousTarget': array([32., 12.]), 'currentState': array([39.      , 16.      ,  2.225724], dtype=float32), 'targetState': array([32, 12], dtype=int32), 'currentDistance': 7.5783678766232345}
done in step count: 7
reward sum = 0.9023643479069899
running average episode reward sum: 0.6803233284026903
{'scaleFactor': 20, 'currentTarget': array([32., 12.]), 'dynamicTrap': False, 'previousTarget': array([32., 12.]), 'currentState': array([31.463856  , 12.34817998,  3.57300568]), 'targetState': array([32, 12], dtype=int32), 'currentDistance': 0.6392805956062515}
episode index:710
target Thresh 74.03523464765003
target distance 70.0
model initialize at round 710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.55002883,  3.95270724]), 'dynamicTrap': False, 'previousTarget': array([81.050826,  4.424941]), 'currentState': array([99.48089448,  2.29120363,  2.59969199]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6011573913225842
running average episode reward sum: 0.6802119839060938
{'scaleFactor': 20, 'currentTarget': array([31.,  8.]), 'dynamicTrap': False, 'previousTarget': array([31.,  8.]), 'currentState': array([31.16369627,  7.90042188,  3.32842007]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 0.1916044605907316}
episode index:711
target Thresh 74.04503395572637
target distance 13.0
model initialize at round 711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([41.76588768, 20.29218405,  3.63658667]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 13.468137668770266}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6805657105409266
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([29.47044939, 16.43441369,  4.93219465]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 0.640342004169521}
episode index:712
target Thresh 74.05478438954978
target distance 66.0
model initialize at round 712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.68832397, 21.85528066]), 'dynamicTrap': False, 'previousTarget': array([47.96336993, 20.79009879]), 'currentState': array([28.76032547, 23.55082657,  0.18144441]), 'targetState': array([94, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.39849735344426246
running average episode reward sum: 0.6801701027469622
{'scaleFactor': 20, 'currentTarget': array([94., 18.]), 'dynamicTrap': False, 'previousTarget': array([94., 18.]), 'currentState': array([93.76914289, 18.50151014,  1.7111238 ]), 'targetState': array([94, 18], dtype=int32), 'currentDistance': 0.552093670687926}
episode index:713
target Thresh 74.06448619288162
target distance 36.0
model initialize at round 713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.76732564, 12.42954344]), 'dynamicTrap': False, 'previousTarget': array([91.91502707, 12.54012611]), 'currentState': array([108.57580833,   3.32690952,   2.57309103]), 'targetState': array([74, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2193947039798398
running average episode reward sum: 0.6795247590512099
{'scaleFactor': 20, 'currentTarget': array([74., 21.]), 'dynamicTrap': False, 'previousTarget': array([74., 21.]), 'currentState': array([74.69191938, 20.37990082,  2.59051071]), 'targetState': array([74, 21], dtype=int32), 'currentDistance': 0.9291261595071005}
episode index:714
target Thresh 74.07413960826747
target distance 1.0
model initialize at round 714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,  11.]), 'dynamicTrap': False, 'previousTarget': array([105.,  11.]), 'currentState': array([105.63966435,  13.51386252,   0.26137078]), 'targetState': array([105,  11], dtype=int32), 'currentDistance': 2.593969017429939}
done in step count: 4
reward sum = 0.9509900498999999
running average episode reward sum: 0.6799044307866627
{'scaleFactor': 20, 'currentTarget': array([105.91698851,   9.93728925]), 'dynamicTrap': True, 'previousTarget': array([105.,  11.]), 'currentState': array([105.19148802,   9.4468893 ,   3.24437531]), 'targetState': array([105,  11], dtype=int32), 'currentDistance': 0.8756957647549851}
episode index:715
target Thresh 74.08374487704323
target distance 27.0
model initialize at round 715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.5051837 ,  9.33284551]), 'dynamicTrap': False, 'previousTarget': array([70.3343599,  9.3582148]), 'currentState': array([91.26403671, 12.42925112,  4.85973001]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5255967426478569
running average episode reward sum: 0.6796889172557427
{'scaleFactor': 20, 'currentTarget': array([65.3441064 ,  8.01394998]), 'dynamicTrap': True, 'previousTarget': array([65.15409541,  8.07622502]), 'currentState': array([66.34393179,  8.15095517,  3.52636505]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 1.0091685871124014}
episode index:716
target Thresh 74.0933022393411
target distance 38.0
model initialize at round 716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.56589365, 12.48879519]), 'dynamicTrap': False, 'previousTarget': array([90.9232797 , 11.52624642]), 'currentState': array([71.90353441, 19.68020311,  0.61814642]), 'targetState': array([110,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6097715937138459
running average episode reward sum: 0.6795914035548474
{'scaleFactor': 20, 'currentTarget': array([110.,   5.]), 'dynamicTrap': False, 'previousTarget': array([110.,   5.]), 'currentState': array([110.62379761,   5.48874337,   0.39794974]), 'targetState': array([110,   5], dtype=int32), 'currentDistance': 0.7924604345099476}
episode index:717
target Thresh 74.10281193409566
target distance 73.0
model initialize at round 717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.4266197 , 17.11204207]), 'dynamicTrap': False, 'previousTarget': array([53.41839643, 18.21190225]), 'currentState': array([3.49371502e+01, 2.16020953e+01, 2.97839006e-02]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.44575882814466455
running average episode reward sum: 0.6792657314442483
{'scaleFactor': 20, 'currentTarget': array([107.,   5.]), 'dynamicTrap': False, 'previousTarget': array([107.,   5.]), 'currentState': array([106.72263354,   4.69914467,   0.58070781]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 0.40920176214016435}
episode index:718
target Thresh 74.11227419904975
target distance 45.0
model initialize at round 718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.00668884, 13.48278753]), 'dynamicTrap': True, 'previousTarget': array([57.00493644, 13.55566525]), 'currentState': array([77.     , 14.     ,  2.22541], dtype=float32), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.4914720928305846
running average episode reward sum: 0.6790045441860929
{'scaleFactor': 20, 'currentTarget': array([32., 13.]), 'dynamicTrap': False, 'previousTarget': array([32., 13.]), 'currentState': array([31.86072225, 12.92243019,  2.51349691]), 'targetState': array([32, 13], dtype=int32), 'currentDistance': 0.15942197488799328}
episode index:719
target Thresh 74.1216892707605
target distance 38.0
model initialize at round 719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.99797523, 17.6048084 ]), 'dynamicTrap': False, 'previousTarget': array([88.4290043 , 16.87979038]), 'currentState': array([107.37385478,  22.56215564,   2.9620105 ]), 'targetState': array([70, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6510788443142849
running average episode reward sum: 0.6789657584918265
{'scaleFactor': 20, 'currentTarget': array([70., 13.]), 'dynamicTrap': False, 'previousTarget': array([70., 13.]), 'currentState': array([69.97352748, 12.69151396,  3.14278111]), 'targetState': array([70, 13], dtype=int32), 'currentDistance': 0.3096198177258622}
episode index:720
target Thresh 74.13105738460519
target distance 31.0
model initialize at round 720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.71654264, 20.09463943]), 'dynamicTrap': False, 'previousTarget': array([16.04149382, 20.28764556]), 'currentState': array([34.63028394, 18.23913209,  2.6484704 ]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.679169923353607
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.4818687 , 21.62891962,  4.12674859]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.7922987628957092}
episode index:721
target Thresh 74.14037877478715
target distance 69.0
model initialize at round 721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.87734721, 12.97065742]), 'dynamicTrap': False, 'previousTarget': array([97.2957649, 11.5731765]), 'currentState': array([117.48932451,  16.89116077,   0.93878174]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6121357586498314
running average episode reward sum: 0.6790770782501393
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([48.11897859,  3.11861793,  3.9403501 ]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 0.1680063044858217}
episode index:722
target Thresh 74.14965367434164
target distance 15.0
model initialize at round 722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.17415672,  9.90172234]), 'dynamicTrap': True, 'previousTarget': array([68., 10.]), 'currentState': array([83.       ,  8.       ,  2.5994725], dtype=float32), 'targetState': array([68, 10], dtype=int32), 'currentDistance': 14.947313367949544}
done in step count: 17
reward sum = 0.7564604408675677
running average episode reward sum: 0.6791841091804538
{'scaleFactor': 20, 'currentTarget': array([68., 10.]), 'dynamicTrap': False, 'previousTarget': array([68., 10.]), 'currentState': array([68.6602481 , 10.23609479,  4.38007948]), 'targetState': array([68, 10], dtype=int32), 'currentDistance': 0.7011906369632211}
episode index:723
target Thresh 74.1588823151416
target distance 46.0
model initialize at round 723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.87132008, 14.20690868]), 'dynamicTrap': False, 'previousTarget': array([53.51477147, 14.36479691]), 'currentState': array([70.22346675, 22.15667047,  2.70891565]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5732006595173498
running average episode reward sum: 0.6790377232002561
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'dynamicTrap': False, 'previousTarget': array([26.,  3.]), 'currentState': array([25.03250586,  3.20531296,  5.83638455]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 0.9890390933033975}
episode index:724
target Thresh 74.16806492790354
target distance 10.0
model initialize at round 724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([8.88352993e+01, 1.74610486e+01, 4.14364338e-02]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 11.889952328125112}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6793867268205412
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([91.384701  ,  5.4658346 ,  4.22017001]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 0.8148162580935973}
episode index:725
target Thresh 74.17720174219329
target distance 11.0
model initialize at round 725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.,  7.]), 'dynamicTrap': False, 'previousTarget': array([84.,  7.]), 'currentState': array([93.9889948 ,  9.48328873,  3.37784588]), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 10.293043279022637}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6797608360809813
{'scaleFactor': 20, 'currentTarget': array([84.,  7.]), 'dynamicTrap': False, 'previousTarget': array([84.,  7.]), 'currentState': array([84.81840946,  7.42705876,  3.40041466]), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 0.9231322938299268}
episode index:726
target Thresh 74.18629298643164
target distance 62.0
model initialize at round 726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.17234893, 21.27793365]), 'dynamicTrap': False, 'previousTarget': array([67.95850618, 20.28764556]), 'currentState': array([47.18849716, 20.4743986 ,  1.06394339]), 'targetState': array([110,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5396862808890557
running average episode reward sum: 0.679568161314555
{'scaleFactor': 20, 'currentTarget': array([110.,  23.]), 'dynamicTrap': False, 'previousTarget': array([110.,  23.]), 'currentState': array([110.32570652,  23.15646795,   5.94976187]), 'targetState': array([110,  23], dtype=int32), 'currentDistance': 0.36134050153604785}
episode index:727
target Thresh 74.19533888790019
target distance 51.0
model initialize at round 727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.93109182,  4.57130463]), 'dynamicTrap': False, 'previousTarget': array([28.96548746,  4.82555956]), 'currentState': array([10.96024681,  5.65081821,  6.27201873]), 'targetState': array([60,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7395486671881252
running average episode reward sum: 0.6796505521193263
{'scaleFactor': 20, 'currentTarget': array([58.48027388,  2.15161744]), 'dynamicTrap': True, 'previousTarget': array([58.66497129,  2.14972783]), 'currentState': array([57.51241127,  1.5936898 ,  1.11381884]), 'targetState': array([60,  3], dtype=int32), 'currentDistance': 1.1171576859434278}
episode index:728
target Thresh 74.20433967274694
target distance 22.0
model initialize at round 728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.91708332, 20.88696378]), 'dynamicTrap': True, 'previousTarget': array([77.88854382, 20.94427191]), 'currentState': array([60.        , 12.        ,  0.79840606], dtype=float32), 'targetState': array([82, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6108091548445036
running average episode reward sum: 0.6795561194756022
{'scaleFactor': 20, 'currentTarget': array([82., 23.]), 'dynamicTrap': False, 'previousTarget': array([82., 23.]), 'currentState': array([82.29338749, 23.74045998,  0.54780466]), 'targetState': array([82, 23], dtype=int32), 'currentDistance': 0.796465441087718}
episode index:729
target Thresh 74.21329556599198
target distance 16.0
model initialize at round 729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56., 19.]), 'dynamicTrap': False, 'previousTarget': array([56., 19.]), 'currentState': array([41.62303693, 19.2609619 ,  0.17003005]), 'targetState': array([56, 19], dtype=int32), 'currentDistance': 14.379331281375315}
done in step count: 8
reward sum = 0.9134240409488502
running average episode reward sum: 0.6798764864913189
{'scaleFactor': 20, 'currentTarget': array([56., 19.]), 'dynamicTrap': False, 'previousTarget': array([54.10823096, 18.50247127]), 'currentState': array([55.06025554, 18.86680154,  6.1093591 ]), 'targetState': array([56, 19], dtype=int32), 'currentDistance': 0.9491372329277011}
episode index:730
target Thresh 74.22220679153314
target distance 67.0
model initialize at round 730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.44790761,  6.98213051]), 'dynamicTrap': False, 'previousTarget': array([50.05546015,  6.4883985 ]), 'currentState': array([68.40395954,  5.65699228,  1.81541717]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.506346803027047
running average episode reward sum: 0.6796390997834335
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 10.]), 'currentState': array([2.57143923, 9.32626009, 3.4965127 ]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.7984922022242984}
episode index:731
target Thresh 74.23107357215147
target distance 45.0
model initialize at round 731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.19294981, 21.53001277]), 'dynamicTrap': False, 'previousTarget': array([46.04429684, 21.66961979]), 'currentState': array([64.14957242, 22.84652868,  2.55495894]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6797839618613348
{'scaleFactor': 20, 'currentTarget': array([21., 20.]), 'dynamicTrap': False, 'previousTarget': array([21., 20.]), 'currentState': array([20.46656124, 19.71952296,  3.10411049]), 'targetState': array([21, 20], dtype=int32), 'currentDistance': 0.602680907056921}
episode index:732
target Thresh 74.23989612951698
target distance 16.0
model initialize at round 732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.06120292,  5.25246487]), 'dynamicTrap': False, 'previousTarget': array([24.14213562,  4.85786438]), 'currentState': array([11.01393018, 20.41058725,  0.03206873]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7801200584331746
running average episode reward sum: 0.6799208460312828
{'scaleFactor': 20, 'currentTarget': array([26.,  3.]), 'dynamicTrap': False, 'previousTarget': array([26.,  3.]), 'currentState': array([25.32847401,  3.96123387,  0.17689196]), 'targetState': array([26,  3], dtype=int32), 'currentDistance': 1.1725688474940472}
episode index:733
target Thresh 74.24867468419406
target distance 56.0
model initialize at round 733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.58824549,  4.28828401]), 'dynamicTrap': False, 'previousTarget': array([68.00318801,  4.64291407]), 'currentState': array([89.5876573 ,  4.44167001,  4.93502433]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 43
reward sum = 0.5539173091688777
running average episode reward sum: 0.6797491790873286
{'scaleFactor': 20, 'currentTarget': array([32.,  4.]), 'dynamicTrap': False, 'previousTarget': array([32.,  4.]), 'currentState': array([31.83016809,  4.06931522,  5.01057912]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 0.18343248519402955}
episode index:734
target Thresh 74.25740945564702
target distance 71.0
model initialize at round 734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.63851415,  5.26516362]), 'dynamicTrap': False, 'previousTarget': array([65.92896584,  5.68413796]), 'currentState': array([47.72989373,  3.35549541,  0.215761  ]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5806419170115569
running average episode reward sum: 0.6796143392749806
{'scaleFactor': 20, 'currentTarget': array([117.,  10.]), 'dynamicTrap': False, 'previousTarget': array([117.,  10.]), 'currentState': array([116.09623051,   9.0183981 ,   1.00371507]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 1.3342944119372626}
episode index:735
target Thresh 74.2661006622456
target distance 54.0
model initialize at round 735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.58772087,  6.32714395]), 'dynamicTrap': False, 'previousTarget': array([82.27212152,  5.28797975]), 'currentState': array([101.39693136,   3.57120855,   2.94728118]), 'targetState': array([48, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5430512134091727
running average episode reward sum: 0.6794287915496194
{'scaleFactor': 20, 'currentTarget': array([48., 11.]), 'dynamicTrap': False, 'previousTarget': array([48., 11.]), 'currentState': array([48.53985967, 11.38410466,  3.60615581]), 'targetState': array([48, 11], dtype=int32), 'currentDistance': 0.6625593188153831}
episode index:736
target Thresh 74.27474852127047
target distance 25.0
model initialize at round 736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.10366976, 16.57261992]), 'dynamicTrap': False, 'previousTarget': array([32.95151706, 16.90448546]), 'currentState': array([52.47114998, 21.56268105,  4.0191493 ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6796622094323131
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([27.57362912, 15.02617435,  4.9196012 ]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.5742259654130747}
episode index:737
target Thresh 74.28335324891847
target distance 11.0
model initialize at round 737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51., 12.]), 'dynamicTrap': False, 'previousTarget': array([51., 12.]), 'currentState': array([62.17185455,  3.07807224,  0.52741788]), 'targetState': array([51, 12], dtype=int32), 'currentDistance': 14.2972420080283}
done in step count: 14
reward sum = 0.8128032015354578
running average episode reward sum: 0.6798426172806914
{'scaleFactor': 20, 'currentTarget': array([51., 12.]), 'dynamicTrap': False, 'previousTarget': array([51., 12.]), 'currentState': array([51.62563683, 12.47594398,  2.26945946]), 'targetState': array([51, 12], dtype=int32), 'currentDistance': 0.7860942172006823}
episode index:738
target Thresh 74.29191506030831
target distance 26.0
model initialize at round 738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.60687589, 17.81485384]), 'dynamicTrap': False, 'previousTarget': array([30.68768483, 18.19946947]), 'currentState': array([49.59527331, 11.53466567,  3.43290663]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.750688983631635
running average episode reward sum: 0.6799384851647928
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'dynamicTrap': False, 'previousTarget': array([24., 20.]), 'currentState': array([23.1771013 , 20.34370587,  3.66816838]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.8917936925812316}
episode index:739
target Thresh 74.30043416948568
target distance 6.0
model initialize at round 739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([109.90695367,  10.30344129,   3.24786413]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 5.915312224833811}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6803308642388944
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([105.48651282,   7.64980801,   4.88800389]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 0.8117543801077988}
episode index:740
target Thresh 74.3089107894288
target distance 26.0
model initialize at round 740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.26214059,   6.31542386]), 'dynamicTrap': False, 'previousTarget': array([111.76743395,   6.04114369]), 'currentState': array([93.46770273,  3.45531372,  5.61518687]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8144125312909167
running average episode reward sum: 0.6805118111579931
{'scaleFactor': 20, 'currentTarget': array([118.,   7.]), 'dynamicTrap': False, 'previousTarget': array([118.,   7.]), 'currentState': array([118.26984862,   7.26780448,   1.27115064]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 0.38018090176710345}
episode index:741
target Thresh 74.31734513205355
target distance 16.0
model initialize at round 741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 23.]), 'dynamicTrap': False, 'previousTarget': array([32., 23.]), 'currentState': array([46.43519798, 18.72826454,  2.01283145]), 'targetState': array([32, 23], dtype=int32), 'currentDistance': 15.053991652125264}
done in step count: 11
reward sum = 0.8757352642587164
running average episode reward sum: 0.6807749155422259
{'scaleFactor': 20, 'currentTarget': array([32., 23.]), 'dynamicTrap': False, 'previousTarget': array([32., 23.]), 'currentState': array([31.99267004, 22.80289068,  3.48548149]), 'targetState': array([32, 23], dtype=int32), 'currentDistance': 0.19724555914499942}
episode index:742
target Thresh 74.32573740821896
target distance 1.0
model initialize at round 742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([ 7.74405575, 15.6867935 ,  0.52237129]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 2.698956607580717}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6811777756828151
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([ 8.17935011, 13.51131087,  3.44287211]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.5418535483025937}
episode index:743
target Thresh 74.33408782773238
target distance 72.0
model initialize at round 743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.50108904, 16.22398925]), 'dynamicTrap': False, 'previousTarget': array([56.84555753, 15.51930531]), 'currentState': array([35.68096553, 18.90031213,  1.53267169]), 'targetState': array([109,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4502452211178948
running average episode reward sum: 0.6808673824643139
{'scaleFactor': 20, 'currentTarget': array([109.,   9.]), 'dynamicTrap': False, 'previousTarget': array([109.,   9.]), 'currentState': array([108.60700123,   8.59411709,   1.09778161]), 'targetState': array([109,   9], dtype=int32), 'currentDistance': 0.5649681144448314}
episode index:744
target Thresh 74.34239659935473
target distance 14.0
model initialize at round 744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  5.]), 'dynamicTrap': False, 'previousTarget': array([92.,  5.]), 'currentState': array([104.12459922,   5.25346751,   2.72477931]), 'targetState': array([92,  5], dtype=int32), 'currentDistance': 12.127248325748548}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6812171982588597
{'scaleFactor': 20, 'currentTarget': array([92.,  5.]), 'dynamicTrap': False, 'previousTarget': array([92.,  5.]), 'currentState': array([92.53396783,  5.30051691,  2.82482937]), 'targetState': array([92,  5], dtype=int32), 'currentDistance': 0.6127251093000944}
episode index:745
target Thresh 74.35066393080572
target distance 55.0
model initialize at round 745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.2238353, 19.107673 ]), 'dynamicTrap': False, 'previousTarget': array([50.79172879, 18.12120309]), 'currentState': array([31.52533061, 22.56728467,  0.23738003]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.309973673172491
running average episode reward sum: 0.6807195527828726
{'scaleFactor': 20, 'currentTarget': array([86., 13.]), 'dynamicTrap': False, 'previousTarget': array([86., 13.]), 'currentState': array([8.66676986e+01, 1.32898940e+01, 2.21084718e-02]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 0.7279147967799035}
episode index:746
target Thresh 74.3588900287691
target distance 33.0
model initialize at round 746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.31287642,  3.4058169 ]), 'dynamicTrap': False, 'previousTarget': array([34.22568992,  4.00389241]), 'currentState': array([54.20229045,  5.50610489,  3.8637327 ]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 22
reward sum = 0.7757031122071192
running average episode reward sum: 0.6808467061422089
{'scaleFactor': 20, 'currentTarget': array([21.,  2.]), 'dynamicTrap': False, 'previousTarget': array([21.,  2.]), 'currentState': array([21.64093491,  2.34795071,  4.38915329]), 'targetState': array([21,  2], dtype=int32), 'currentDistance': 0.729292296790998}
episode index:747
target Thresh 74.3670750988977
target distance 34.0
model initialize at round 747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.3972959, 14.4106449]), 'dynamicTrap': False, 'previousTarget': array([69.69567118, 14.47570668]), 'currentState': array([51.80652348, 10.38551932,  6.13811535]), 'targetState': array([84, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6651615733199573
running average episode reward sum: 0.6808257367133022
{'scaleFactor': 20, 'currentTarget': array([84., 17.]), 'dynamicTrap': False, 'previousTarget': array([84., 17.]), 'currentState': array([84.47444711, 17.67435849,  0.87132162]), 'targetState': array([84, 17], dtype=int32), 'currentDistance': 0.8245358932188935}
episode index:748
target Thresh 74.37521934581873
target distance 3.0
model initialize at round 748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97., 15.]), 'dynamicTrap': False, 'previousTarget': array([97., 15.]), 'currentState': array([93.55628131, 13.54202411,  1.24679589]), 'targetState': array([97, 15], dtype=int32), 'currentDistance': 3.7396379660696124}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6812253018178238
{'scaleFactor': 20, 'currentTarget': array([97., 15.]), 'dynamicTrap': False, 'previousTarget': array([97., 15.]), 'currentState': array([96.537     , 14.55753964,  6.16966948]), 'targetState': array([97, 15], dtype=int32), 'currentDistance': 0.6404218709946343}
episode index:749
target Thresh 74.3833229731388
target distance 12.0
model initialize at round 749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76., 19.]), 'dynamicTrap': False, 'previousTarget': array([76., 19.]), 'currentState': array([86.08153223,  8.8392757 ,  1.64629881]), 'targetState': array([76, 19], dtype=int32), 'currentDistance': 14.313546397294722}
done in step count: 99
reward sum = -0.6239676587267707
running average episode reward sum: 0.6794850445370977
{'scaleFactor': 20, 'currentTarget': array([81.25376201,  6.78612821]), 'dynamicTrap': True, 'previousTarget': array([81.07439262,  6.73915905]), 'currentState': array([86.08153223,  8.8392757 ,  1.64629881]), 'targetState': array([76, 19], dtype=int32), 'currentDistance': 5.24621576672672}
episode index:750
target Thresh 74.39138618344897
target distance 37.0
model initialize at round 750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.70581327, 18.80297613]), 'dynamicTrap': False, 'previousTarget': array([47.54828331, 18.22665585]), 'currentState': array([29.08001267, 14.95226828,  6.14694947]), 'targetState': array([65, 22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 24
reward sum = 0.7481092787898597
running average episode reward sum: 0.679576421679911
{'scaleFactor': 20, 'currentTarget': array([65., 22.]), 'dynamicTrap': False, 'previousTarget': array([65., 22.]), 'currentState': array([65.21882175, 21.97951247,  6.2801712 ]), 'targetState': array([65, 22], dtype=int32), 'currentDistance': 0.21977874795809446}
episode index:751
target Thresh 74.39940917832996
target distance 31.0
model initialize at round 751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.99946406, 20.14641496]), 'dynamicTrap': True, 'previousTarget': array([44.98960229, 20.64482588]), 'currentState': array([25.       , 20.       ,  3.6597583], dtype=float32), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7980069375972307
running average episode reward sum: 0.6797339090680989
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([55.47699267, 20.40913898,  0.76041478]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 0.7890839042391852}
episode index:752
target Thresh 74.40739215835704
target distance 36.0
model initialize at round 752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.30227123, 12.40698131]), 'dynamicTrap': False, 'previousTarget': array([50.63230779, 13.18260682]), 'currentState': array([30.50688083, 15.26049123,  5.21407986]), 'targetState': array([67, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6447434198522586
running average episode reward sum: 0.6796874409549305
{'scaleFactor': 20, 'currentTarget': array([67., 10.]), 'dynamicTrap': False, 'previousTarget': array([67., 10.]), 'currentState': array([66.9063723 ,  9.22169486,  1.16001254]), 'targetState': array([67, 10], dtype=int32), 'currentDistance': 0.7839164740562543}
episode index:753
target Thresh 74.41533532310514
target distance 19.0
model initialize at round 753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.17653686, 13.4644685 ]), 'dynamicTrap': False, 'previousTarget': array([93.92524322, 13.43827311]), 'currentState': array([112.7793926 ,  20.80842919,   4.2816329 ]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.49962973943116556
running average episode reward sum: 0.6794486376372598
{'scaleFactor': 20, 'currentTarget': array([93., 13.]), 'dynamicTrap': False, 'previousTarget': array([93., 13.]), 'currentState': array([92.99305982, 12.82334216,  3.728113  ]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 0.17679411588246338}
episode index:754
target Thresh 74.42323887115377
target distance 38.0
model initialize at round 754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.05892836, 17.06277574]), 'dynamicTrap': True, 'previousTarget': array([99.0716533 , 17.02262736]), 'currentState': array([80.       , 11.       ,  6.1489263], dtype=float32), 'targetState': array([118,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6391447413021818
running average episode reward sum: 0.6793952549931074
{'scaleFactor': 20, 'currentTarget': array([118.,  23.]), 'dynamicTrap': False, 'previousTarget': array([118.,  23.]), 'currentState': array([118.30611746,  23.79945794,   6.24689536]), 'targetState': array([118,  23], dtype=int32), 'currentDistance': 0.8560612688132977}
episode index:755
target Thresh 74.43110300009207
target distance 65.0
model initialize at round 755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.58903453,  5.11920969]), 'dynamicTrap': False, 'previousTarget': array([62.11497719,  4.14146399]), 'currentState': array([81.51371191,  3.38507268,  2.91844374]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4768976942285972
running average episode reward sum: 0.6791274010767522
{'scaleFactor': 20, 'currentTarget': array([17.,  9.]), 'dynamicTrap': False, 'previousTarget': array([17.,  9.]), 'currentState': array([17.21575548,  9.33917783,  4.53752169]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 0.401985108930699}
episode index:756
target Thresh 74.43892790652365
target distance 56.0
model initialize at round 756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.58397255, 12.32402059]), 'dynamicTrap': False, 'previousTarget': array([33.84555753, 11.51930531]), 'currentState': array([12.78342051, 15.14149091,  1.37802696]), 'targetState': array([70,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5760708553427042
running average episode reward sum: 0.6789912629714231
{'scaleFactor': 20, 'currentTarget': array([70.,  7.]), 'dynamicTrap': False, 'previousTarget': array([70.,  7.]), 'currentState': array([69.96814404,  7.55224595,  2.07870831]), 'targetState': array([70,  7], dtype=int32), 'currentDistance': 0.5531639787140543}
episode index:757
target Thresh 74.44671378607157
target distance 30.0
model initialize at round 757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.42077868,   9.27116203]), 'dynamicTrap': False, 'previousTarget': array([105.47682419,   9.54459231]), 'currentState': array([87.18601862,  3.7917455 ,  0.19824665]), 'targetState': array([116,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7800556220251509
running average episode reward sum: 0.6791245932604123
{'scaleFactor': 20, 'currentTarget': array([116.,  12.]), 'dynamicTrap': False, 'previousTarget': array([116.,  12.]), 'currentState': array([116.32159461,  12.13677748,   1.25627538]), 'targetState': array([116,  12], dtype=int32), 'currentDistance': 0.3494727028243105}
episode index:758
target Thresh 74.45446083338327
target distance 67.0
model initialize at round 758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.58032784, 21.69046737]), 'dynamicTrap': False, 'previousTarget': array([54., 21.]), 'currentState': array([72.57803351, 21.99339988,  1.61161721]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5641511954088059
running average episode reward sum: 0.678973113157841
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 21.]), 'currentState': array([ 7.479927  , 20.92866666,  3.6867313 ]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.4851993050207929}
episode index:759
target Thresh 74.46216924213529
target distance 6.0
model initialize at round 759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.30123905,  3.51133729]), 'dynamicTrap': True, 'previousTarget': array([31.,  2.]), 'currentState': array([37.      ,  7.      ,  4.130699], dtype=float32), 'targetState': array([31,  2], dtype=int32), 'currentDistance': 5.852274941784309}
done in step count: 5
reward sum = 0.9310900498999999
running average episode reward sum: 0.6793048459693439
{'scaleFactor': 20, 'currentTarget': array([31.,  2.]), 'dynamicTrap': False, 'previousTarget': array([31.,  2.]), 'currentState': array([30.58179813,  1.69943052,  3.48163107]), 'targetState': array([31,  2], dtype=int32), 'currentDistance': 0.5150095343859312}
episode index:760
target Thresh 74.46983920503827
target distance 20.0
model initialize at round 760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.15240858, 14.07153713]), 'dynamicTrap': False, 'previousTarget': array([60.7615699 , 14.79270645]), 'currentState': array([77.25722005, 22.56952522,  2.57928872]), 'targetState': array([59, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6795769616404961
{'scaleFactor': 20, 'currentTarget': array([59., 14.]), 'dynamicTrap': False, 'previousTarget': array([59., 14.]), 'currentState': array([58.73109946, 13.71309899,  5.66767886]), 'targetState': array([59, 14], dtype=int32), 'currentDistance': 0.39321710601760396}
episode index:761
target Thresh 74.47747091384169
target distance 41.0
model initialize at round 761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.710396  , 14.31677535]), 'dynamicTrap': False, 'previousTarget': array([30.07300288, 13.46287707]), 'currentState': array([47.7439099 ,  8.17467892,  1.44299698]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6612531230314119
running average episode reward sum: 0.6795529146082007
{'scaleFactor': 20, 'currentTarget': array([ 8., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 21.]), 'currentState': array([ 7.81897102, 21.15933366,  3.43176223]), 'targetState': array([ 8, 21], dtype=int32), 'currentDistance': 0.2411611624143844}
episode index:762
target Thresh 74.48506455933865
target distance 10.0
model initialize at round 762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 15.]), 'dynamicTrap': False, 'previousTarget': array([93., 15.]), 'currentState': array([91.59903907,  5.9151488 ,  1.58732843]), 'targetState': array([93, 15], dtype=int32), 'currentDistance': 9.192236553232268}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6799086644578624
{'scaleFactor': 20, 'currentTarget': array([93., 15.]), 'dynamicTrap': False, 'previousTarget': array([93., 15.]), 'currentState': array([92.11922345, 15.15337457,  2.14749818]), 'targetState': array([93, 15], dtype=int32), 'currentDistance': 0.8940308130470482}
episode index:763
target Thresh 74.49262033137069
target distance 1.0
model initialize at round 763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([11.12511867,  7.29791704,  1.6133184 ]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 1.163892854663663}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6803015850541217
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([9.36127764, 6.52045406, 5.26581903]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.7987055535160469}
episode index:764
target Thresh 74.5001384188325
target distance 52.0
model initialize at round 764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.00517834,  3.45508988]), 'dynamicTrap': True, 'previousTarget': array([41.0036972 ,  3.38454428]), 'currentState': array([61.       ,  3.       ,  4.7900276], dtype=float32), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5658680249060579
running average episode reward sum: 0.680151998700987
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'dynamicTrap': False, 'previousTarget': array([9., 4.]), 'currentState': array([9.24789322, 3.81831898, 2.93473932]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 0.30734189970784076}
episode index:765
target Thresh 74.50761900967667
target distance 27.0
model initialize at round 765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.9139019 , 11.90657216]), 'dynamicTrap': False, 'previousTarget': array([83.98629668, 11.74023321]), 'currentState': array([65.91727535, 11.539249  ,  0.43398233]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.680375635479569
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91., 12.]), 'currentState': array([90.55701067, 11.01506092,  0.63823409]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 1.0799743276403426}
episode index:766
target Thresh 74.51506229091835
target distance 25.0
model initialize at round 766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.03601074,  9.36809421]), 'dynamicTrap': False, 'previousTarget': array([91.69369935,  9.94522771]), 'currentState': array([108.13486689,  17.87875845,   2.56709981]), 'targetState': array([85,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 28
reward sum = 0.6790013622124371
running average episode reward sum: 0.6803738437282429
{'scaleFactor': 20, 'currentTarget': array([85.,  7.]), 'dynamicTrap': False, 'previousTarget': array([85.,  7.]), 'currentState': array([84.70828383,  7.12612489,  5.06189749]), 'targetState': array([85,  7], dtype=int32), 'currentDistance': 0.3178141191017963}
episode index:767
target Thresh 74.52246844863997
target distance 19.0
model initialize at round 767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([47.23313766, 11.91410718]), 'currentState': array([64.72063564,  4.24679347,  2.7348659 ]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 19.342521522555383}
done in step count: 16
reward sum = 0.7946760185785164
running average episode reward sum: 0.6805226746850793
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([47., 12.]), 'currentState': array([47.41654566, 11.87084642,  3.47240862]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 0.4361088570340282}
episode index:768
target Thresh 74.52983766799585
target distance 8.0
model initialize at round 768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  3.]), 'dynamicTrap': False, 'previousTarget': array([92.,  3.]), 'currentState': array([87.41484339, 10.57427406,  4.41397142]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 8.853998458160772}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6808743877867893
{'scaleFactor': 20, 'currentTarget': array([92.,  3.]), 'dynamicTrap': False, 'previousTarget': array([92.,  3.]), 'currentState': array([92.85486398,  3.14755759,  5.42650291]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 0.8675054301997497}
episode index:769
target Thresh 74.53717013321686
target distance 45.0
model initialize at round 769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.78389443, 14.48902946]), 'dynamicTrap': False, 'previousTarget': array([75.67530121, 15.15325301]), 'currentState': array([93.92423258,  8.68838677,  3.03810048]), 'targetState': array([50, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 29
reward sum = 0.7109922436527321
running average episode reward sum: 0.6809135018853165
{'scaleFactor': 20, 'currentTarget': array([50., 22.]), 'dynamicTrap': False, 'previousTarget': array([50., 22.]), 'currentState': array([50.18056784, 21.20290787,  4.72687534]), 'targetState': array([50, 22], dtype=int32), 'currentDistance': 0.8172885678878992}
episode index:770
target Thresh 74.544466027615
target distance 20.0
model initialize at round 770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.,  9.]), 'dynamicTrap': False, 'previousTarget': array([91.02495322,  9.00124766]), 'currentState': array([109.53753359,  10.83281455,   1.61393261]), 'targetState': array([91,  9], dtype=int32), 'currentDistance': 18.62791884993765}
done in step count: 16
reward sum = 0.8145359214553651
running average episode reward sum: 0.6810868124165358
{'scaleFactor': 20, 'currentTarget': array([91.,  9.]), 'dynamicTrap': False, 'previousTarget': array([91.,  9.]), 'currentState': array([91.18889876,  9.0168089 ,  4.58724121]), 'targetState': array([91,  9], dtype=int32), 'currentDistance': 0.18964514465249777}
episode index:771
target Thresh 74.55172553358803
target distance 33.0
model initialize at round 771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.69468158,  6.07239747]), 'dynamicTrap': False, 'previousTarget': array([18.22568992,  6.00389241]), 'currentState': array([36.38786058,  9.562197  ,  3.77447486]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6694457520686103
running average episode reward sum: 0.6810717333228208
{'scaleFactor': 20, 'currentTarget': array([5., 4.]), 'dynamicTrap': False, 'previousTarget': array([5., 4.]), 'currentState': array([5.55344805, 3.8654481 , 4.98540203]), 'targetState': array([5, 4], dtype=int32), 'currentDistance': 0.5695690959532348}
episode index:772
target Thresh 74.55894883262397
target distance 20.0
model initialize at round 772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.93680984, 15.13036202]), 'dynamicTrap': False, 'previousTarget': array([19.23878636, 14.9529684 ]), 'currentState': array([5.45393206, 1.33738369, 0.53658884]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7735509788119324
running average episode reward sum: 0.6811913701216424
{'scaleFactor': 20, 'currentTarget': array([24.35375181, 17.28747938]), 'dynamicTrap': True, 'previousTarget': array([24., 19.]), 'currentState': array([24.41780953, 16.81498129,  1.12257952]), 'targetState': array([24, 19], dtype=int32), 'currentDistance': 0.4768205567668556}
episode index:773
target Thresh 74.56613610530566
target distance 17.0
model initialize at round 773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  15.]), 'dynamicTrap': False, 'previousTarget': array([106.,  15.]), 'currentState': array([90.82260967, 13.41130182,  5.45840671]), 'targetState': array([106,  15], dtype=int32), 'currentDistance': 15.260312552649125}
done in step count: 14
reward sum = 0.8216670502526191
running average episode reward sum: 0.6813728632484266
{'scaleFactor': 20, 'currentTarget': array([106.,  15.]), 'dynamicTrap': False, 'previousTarget': array([106.,  15.]), 'currentState': array([106.57547151,  15.18767838,   6.13740387]), 'targetState': array([106,  15], dtype=int32), 'currentDistance': 0.6053021034247786}
episode index:774
target Thresh 74.57328753131532
target distance 66.0
model initialize at round 774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.64177141, 14.83008277]), 'dynamicTrap': False, 'previousTarget': array([68.43700211, 14.28799949]), 'currentState': array([47.21464248, 19.5826254 ,  2.20941675]), 'targetState': array([115,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.570055702817904
running average episode reward sum: 0.6812292282027097
{'scaleFactor': 20, 'currentTarget': array([115.,   3.]), 'dynamicTrap': False, 'previousTarget': array([115.,   3.]), 'currentState': array([114.63532511,   2.89384918,   1.01541931]), 'targetState': array([115,   3], dtype=int32), 'currentDistance': 0.3798101818560262}
episode index:775
target Thresh 74.58040328943893
target distance 8.0
model initialize at round 775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.,  6.]), 'dynamicTrap': False, 'previousTarget': array([58.,  6.]), 'currentState': array([65.58429147, 14.82729569,  1.43368042]), 'targetState': array([58,  6], dtype=int32), 'currentDistance': 11.637982052392521}
done in step count: 12
reward sum = 0.8485335561440494
running average episode reward sum: 0.6814448265634588
{'scaleFactor': 20, 'currentTarget': array([58.,  6.]), 'dynamicTrap': False, 'previousTarget': array([58.,  6.]), 'currentState': array([58.24226584,  6.28263788,  4.16807744]), 'targetState': array([58,  6], dtype=int32), 'currentDistance': 0.3722591932853794}
episode index:776
target Thresh 74.58748355757086
target distance 45.0
model initialize at round 776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.94718501,  2.76510241]), 'dynamicTrap': False, 'previousTarget': array([61.98027613,  2.88801227]), 'currentState': array([43.97581891,  1.69527247,  6.08005151]), 'targetState': array([87,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.70994785248897
running average episode reward sum: 0.6814815099945084
{'scaleFactor': 20, 'currentTarget': array([87.,  4.]), 'dynamicTrap': False, 'previousTarget': array([87.,  4.]), 'currentState': array([87.33360056,  3.53534591,  1.37571177]), 'targetState': array([87,  4], dtype=int32), 'currentDistance': 0.5720076598417018}
episode index:777
target Thresh 74.59452851271816
target distance 48.0
model initialize at round 777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.43345962, 12.45956157]), 'dynamicTrap': False, 'previousTarget': array([27.8525445 , 12.67694284]), 'currentState': array([10.75248862,  5.31664041,  0.23718577]), 'targetState': array([57, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6709105406365722
running average episode reward sum: 0.6814679226302951
{'scaleFactor': 20, 'currentTarget': array([57., 23.]), 'dynamicTrap': False, 'previousTarget': array([57., 23.]), 'currentState': array([56.5408663 , 22.28563523,  0.91812237]), 'targetState': array([57, 23], dtype=int32), 'currentDistance': 0.8491883053333271}
episode index:778
target Thresh 74.60153833100507
target distance 16.0
model initialize at round 778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.81292143,   9.07065819]), 'dynamicTrap': True, 'previousTarget': array([118.,   9.]), 'currentState': array([102.       ,   5.       ,   5.8962936], dtype=float32), 'targetState': array([118,   9], dtype=int32), 'currentDistance': 16.328464170350816}
done in step count: 11
reward sum = 0.8754382542587164
running average episode reward sum: 0.6817169217723085
{'scaleFactor': 20, 'currentTarget': array([118.,   9.]), 'dynamicTrap': False, 'previousTarget': array([118.,   9.]), 'currentState': array([117.67273108,   9.31303164,   1.7160788 ]), 'targetState': array([118,   9], dtype=int32), 'currentDistance': 0.4528727786432907}
episode index:779
target Thresh 74.60851318767742
target distance 8.0
model initialize at round 779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,  13.]), 'dynamicTrap': False, 'previousTarget': array([103.,  13.]), 'currentState': array([110.67389574,  19.58543404,   3.47581434]), 'targetState': array([103,  13], dtype=int32), 'currentDistance': 10.112201407387976}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.682037881292994
{'scaleFactor': 20, 'currentTarget': array([103.,  13.]), 'dynamicTrap': False, 'previousTarget': array([103.,  13.]), 'currentState': array([103.09740732,  12.01218612,   2.74397181]), 'targetState': array([103,  13], dtype=int32), 'currentDistance': 0.9926048819678959}
episode index:780
target Thresh 74.61545325710698
target distance 50.0
model initialize at round 780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.58624638, 11.74200822]), 'dynamicTrap': False, 'previousTarget': array([97.46711067, 11.70276435]), 'currentState': array([115.05218337,  16.33300997,   3.24333441]), 'targetState': array([67,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 67
reward sum = 0.2263886413898331
running average episode reward sum: 0.6814544635722474
{'scaleFactor': 20, 'currentTarget': array([67.,  5.]), 'dynamicTrap': False, 'previousTarget': array([67.,  5.]), 'currentState': array([67.987082  ,  5.08476011,  3.77509298]), 'targetState': array([67,  5], dtype=int32), 'currentDistance': 0.990714463531699}
episode index:781
target Thresh 74.62235871279586
target distance 23.0
model initialize at round 781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.23144422, 12.40626282]), 'dynamicTrap': False, 'previousTarget': array([90.86652239, 11.82323232]), 'currentState': array([108.55897489,   7.26364106,   2.77984977]), 'targetState': array([87, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8323750817431029
running average episode reward sum: 0.6816474566901128
{'scaleFactor': 20, 'currentTarget': array([87., 13.]), 'dynamicTrap': False, 'previousTarget': array([87., 13.]), 'currentState': array([86.63632801, 12.24610311,  3.45462141]), 'targetState': array([87, 13], dtype=int32), 'currentDistance': 0.8370291759334232}
episode index:782
target Thresh 74.62922972738082
target distance 58.0
model initialize at round 782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.65134471, 17.13872259]), 'dynamicTrap': False, 'previousTarget': array([46.92609538, 16.28223316]), 'currentState': array([27.77302082, 19.34150252,  0.07592249]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3590851197508964
running average episode reward sum: 0.6812354996825277
{'scaleFactor': 20, 'currentTarget': array([85., 13.]), 'dynamicTrap': False, 'previousTarget': array([85., 13.]), 'currentState': array([84.34727915, 13.04060056,  0.6706502 ]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 0.6539823477309873}
episode index:783
target Thresh 74.63606647263755
target distance 18.0
model initialize at round 783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 23.]), 'dynamicTrap': False, 'previousTarget': array([69.94427191, 22.88854382]), 'currentState': array([62.06552823,  6.30881379,  0.29631305]), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 18.48111304737929}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6815201254163622
{'scaleFactor': 20, 'currentTarget': array([70., 23.]), 'dynamicTrap': False, 'previousTarget': array([70., 23.]), 'currentState': array([70.08260739, 22.79095858,  1.49892545]), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 0.22477165808870458}
episode index:784
target Thresh 74.64286911948507
target distance 61.0
model initialize at round 784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.11535261, 16.39358102]), 'dynamicTrap': False, 'previousTarget': array([67.02414326, 15.98241918]), 'currentState': array([85.09850748, 15.57289766,  3.05674982]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5869233820736959
running average episode reward sum: 0.6813996200108302
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([25.77951932, 18.39706743,  4.42607604]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.4541742805291812}
episode index:785
target Thresh 74.6496378379899
target distance 3.0
model initialize at round 785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.98347683,  9.10293437]), 'dynamicTrap': True, 'previousTarget': array([60.,  9.]), 'currentState': array([58.       , 12.       ,  4.9637995], dtype=float32), 'targetState': array([60,  9], dtype=int32), 'currentDistance': 3.5110069216665383}
done in step count: 14
reward sum = 0.7381248208893366
running average episode reward sum: 0.6814717894775967
{'scaleFactor': 20, 'currentTarget': array([58.48579799, 10.26255278]), 'dynamicTrap': True, 'previousTarget': array([58.6024381 , 10.38527617]), 'currentState': array([57.54591619, 10.63595146,  4.224703  ]), 'targetState': array([60,  9], dtype=int32), 'currentDistance': 1.011337911716224}
episode index:786
target Thresh 74.65637279737034
target distance 9.0
model initialize at round 786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,   5.]), 'dynamicTrap': False, 'previousTarget': array([110.,   5.]), 'currentState': array([100.01482695,   4.15202916,   1.30820012]), 'targetState': array([110,   5], dtype=int32), 'currentDistance': 10.021114477057552}
done in step count: 9
reward sum = 0.8841132574836408
running average episode reward sum: 0.6817292754598152
{'scaleFactor': 20, 'currentTarget': array([110.,   5.]), 'dynamicTrap': False, 'previousTarget': array([110.,   5.]), 'currentState': array([109.59991283,   4.98433629,   0.45967163]), 'targetState': array([110,   5], dtype=int32), 'currentDistance': 0.4003936735834452}
episode index:787
target Thresh 74.66307416600073
target distance 67.0
model initialize at round 787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.44020795, 17.30124328]), 'dynamicTrap': True, 'previousTarget': array([39.45300462, 17.35450636]), 'currentState': array([20.       , 22.       ,  1.5648298], dtype=float32), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3753805890310452
running average episode reward sum: 0.6813405080912508
{'scaleFactor': 20, 'currentTarget': array([87.,  6.]), 'dynamicTrap': False, 'previousTarget': array([87.,  6.]), 'currentState': array([86.6791076 ,  5.29609647,  0.78916049]), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 0.7735968700267533}
episode index:788
target Thresh 74.66974211141564
target distance 18.0
model initialize at round 788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.62959163,  4.19081019]), 'dynamicTrap': False, 'previousTarget': array([50.,  4.]), 'currentState': array([69.76987003,  9.99165001,  5.42409832]), 'targetState': array([50,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8053949583830847
running average episode reward sum: 0.6814977380662721
{'scaleFactor': 20, 'currentTarget': array([50.,  4.]), 'dynamicTrap': False, 'previousTarget': array([50.,  4.]), 'currentState': array([49.45272735,  3.82021705,  3.41009733]), 'targetState': array([50,  4], dtype=int32), 'currentDistance': 0.5760462321953796}
episode index:789
target Thresh 74.67637680031405
target distance 24.0
model initialize at round 789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.84768657, 14.97138101]), 'dynamicTrap': False, 'previousTarget': array([20.18129646, 14.33309421]), 'currentState': array([3.87760777, 6.19207193, 0.16590215]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7898120243686386
running average episode reward sum: 0.6816348447577941
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([25.87374186, 16.65041802,  1.41156414]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 0.3716835717214686}
episode index:790
target Thresh 74.68297839856353
target distance 21.0
model initialize at round 790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([48., 12.]), 'currentState': array([66.12999435, 12.11439475,  2.45222163]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 19.13033637622147}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6819164468187942
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([47., 12.]), 'currentState': array([47.95022064, 12.73834311,  3.45719607]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 1.2033577205063357}
episode index:791
target Thresh 74.68954707120439
target distance 39.0
model initialize at round 791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.97768539, 20.94450326]), 'dynamicTrap': True, 'previousTarget': array([34.97375327, 21.02429504]), 'currentState': array([15.      , 20.      ,  2.295935], dtype=float32), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6346121635633828
running average episode reward sum: 0.6818567191884211
{'scaleFactor': 20, 'currentTarget': array([54., 22.]), 'dynamicTrap': False, 'previousTarget': array([54., 22.]), 'currentState': array([53.75865331, 21.95506811,  0.27458608]), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 0.24549357966736374}
episode index:792
target Thresh 74.69608298245376
target distance 8.0
model initialize at round 792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77., 23.]), 'dynamicTrap': False, 'previousTarget': array([77., 23.]), 'currentState': array([71.33783572, 16.48358357,  0.75318272]), 'targetState': array([77, 23], dtype=int32), 'currentDistance': 8.632716107784328}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6821961054818784
{'scaleFactor': 20, 'currentTarget': array([77., 23.]), 'dynamicTrap': False, 'previousTarget': array([77., 23.]), 'currentState': array([76.50812912, 23.4407192 ,  2.58083621]), 'targetState': array([77, 23], dtype=int32), 'currentDistance': 0.6604319568755415}
episode index:793
target Thresh 74.70258629570979
target distance 62.0
model initialize at round 793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.97835229, 13.06970993]), 'dynamicTrap': True, 'previousTarget': array([72.97662792, 13.03338897]), 'currentState': array([53.       , 14.       ,  5.5553823], dtype=float32), 'targetState': array([115,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.49720919201024405
running average episode reward sum: 0.6819631244825438
{'scaleFactor': 20, 'currentTarget': array([115.,  11.]), 'dynamicTrap': False, 'previousTarget': array([115.,  11.]), 'currentState': array([114.98430087,  10.49106572,   0.99688292]), 'targetState': array([115,  11], dtype=int32), 'currentDistance': 0.50917635919894}
episode index:794
target Thresh 74.70905717355564
target distance 56.0
model initialize at round 794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.77906102,  4.34666661]), 'dynamicTrap': False, 'previousTarget': array([41.02863735,  3.93010557]), 'currentState': array([59.73368909,  5.69307614,  3.57053363]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5257504794665757
running average episode reward sum: 0.6817666305894419
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.87043001, 2.28395743, 4.4155969 ]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.31212210186282674}
episode index:795
target Thresh 74.71549577776359
target distance 59.0
model initialize at round 795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.3590808 , 12.97687399]), 'dynamicTrap': False, 'previousTarget': array([71.30280656, 12.2346594 ]), 'currentState': array([52.99976491,  7.95523199,  6.03580839]), 'targetState': array([111,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5574781748524505
running average episode reward sum: 0.6816104893133904
{'scaleFactor': 20, 'currentTarget': array([111.,  23.]), 'dynamicTrap': False, 'previousTarget': array([111.,  23.]), 'currentState': array([110.68931534,  22.99594861,   2.43241228]), 'targetState': array([111,  23], dtype=int32), 'currentDistance': 0.31071107461763436}
episode index:796
target Thresh 74.72190226929908
target distance 8.0
model initialize at round 796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  3.]), 'dynamicTrap': False, 'previousTarget': array([89.,  3.]), 'currentState': array([81.54720715,  3.59152519,  0.22963262]), 'targetState': array([89,  3], dtype=int32), 'currentDistance': 7.476230558353555}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6819605338813787
{'scaleFactor': 20, 'currentTarget': array([89.,  3.]), 'dynamicTrap': False, 'previousTarget': array([89.,  3.]), 'currentState': array([88.96338342,  2.98485685,  0.93057358]), 'targetState': array([89,  3], dtype=int32), 'currentDistance': 0.03962434757544405}
episode index:797
target Thresh 74.72827680832474
target distance 9.0
model initialize at round 797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.,  9.]), 'dynamicTrap': False, 'previousTarget': array([66.,  9.]), 'currentState': array([70.95808582, 16.5839143 ,  4.34294927]), 'targetState': array([66,  9], dtype=int32), 'currentDistance': 9.060815140544456}
done in step count: 5
reward sum = 0.9414801494009999
running average episode reward sum: 0.682285746432155
{'scaleFactor': 20, 'currentTarget': array([66.63913181, 10.85878572]), 'dynamicTrap': True, 'previousTarget': array([66.,  9.]), 'currentState': array([66.68352392, 10.36994434,  4.0623209 ]), 'targetState': array([66,  9], dtype=int32), 'currentDistance': 0.4908528851079788}
episode index:798
target Thresh 74.73461955420436
target distance 25.0
model initialize at round 798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.86320772, 15.03393392]), 'dynamicTrap': False, 'previousTarget': array([97.43046618, 14.42781353]), 'currentState': array([114.4052918 ,   7.53786042,   1.80629587]), 'targetState': array([91, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7644317365526281
running average episode reward sum: 0.6823885574335574
{'scaleFactor': 20, 'currentTarget': array([91., 17.]), 'dynamicTrap': False, 'previousTarget': array([91., 17.]), 'currentState': array([90.95808388, 17.99460717,  4.14061645]), 'targetState': array([91, 17], dtype=int32), 'currentDistance': 0.9954900177879425}
episode index:799
target Thresh 74.74093066550694
target distance 9.0
model initialize at round 799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([ 4.19764323, 16.16353549,  5.40889377]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 9.94313536574046}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6827124219235167
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([12.2413681 ,  9.78224264,  6.0533202 ]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.32507972643311706}
episode index:800
target Thresh 74.74721030001058
target distance 64.0
model initialize at round 800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.55910003, 21.78605386]), 'dynamicTrap': False, 'previousTarget': array([96.00975848, 21.37530495]), 'currentState': array([114.54151144,  22.6246445 ,   1.79396582]), 'targetState': array([52, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.571871491448843
running average episode reward sum: 0.6825740437331612
{'scaleFactor': 20, 'currentTarget': array([52., 20.]), 'dynamicTrap': False, 'previousTarget': array([52., 20.]), 'currentState': array([51.68859066, 19.57452139,  4.31461766]), 'targetState': array([52, 20], dtype=int32), 'currentDistance': 0.5272644765174845}
episode index:801
target Thresh 74.75345861470647
target distance 9.0
model initialize at round 801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([88.6519296 , 13.40887914,  1.0934363 ]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 8.748498674362839}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6829087270326212
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([86.4492293 , 22.02448053,  2.20036137]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 0.5513144853140344}
episode index:802
target Thresh 74.7596757658028
target distance 17.0
model initialize at round 802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.1944528 ,  8.14449164]), 'dynamicTrap': False, 'previousTarget': array([54.43600015,  9.29270602]), 'currentState': array([67.63537824, 21.98138697,  2.80663741]), 'targetState': array([52,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7885399314712473
running average episode reward sum: 0.6830402727417603
{'scaleFactor': 20, 'currentTarget': array([52.,  7.]), 'dynamicTrap': False, 'previousTarget': array([52.,  7.]), 'currentState': array([52.27249741,  6.88416829,  4.71969361]), 'targetState': array([52,  7], dtype=int32), 'currentDistance': 0.2960942827525595}
episode index:803
target Thresh 74.76586190872868
target distance 6.0
model initialize at round 803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'dynamicTrap': False, 'previousTarget': array([13., 17.]), 'currentState': array([ 7.98966238, 18.55921832,  6.21005106]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 5.247346453594572}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.683397559715962
{'scaleFactor': 20, 'currentTarget': array([13., 17.]), 'dynamicTrap': False, 'previousTarget': array([13., 17.]), 'currentState': array([12.84025043, 17.80543439,  5.63445759]), 'targetState': array([13, 17], dtype=int32), 'currentDistance': 0.821123909733373}
episode index:804
target Thresh 74.772017198138
target distance 41.0
model initialize at round 804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.40693257, 13.02719517]), 'dynamicTrap': False, 'previousTarget': array([66.92699712, 12.46287707]), 'currentState': array([49.46340767,  6.61292482,  5.77953118]), 'targetState': array([89, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6773402294657985
running average episode reward sum: 0.6833900350821109
{'scaleFactor': 20, 'currentTarget': array([89., 20.]), 'dynamicTrap': False, 'previousTarget': array([89., 20.]), 'currentState': array([88.8163955 , 19.29672778,  2.06814094]), 'targetState': array([89, 20], dtype=int32), 'currentDistance': 0.7268441546695906}
episode index:805
target Thresh 74.77814178791333
target distance 27.0
model initialize at round 805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.65778354,   8.29485419]), 'dynamicTrap': False, 'previousTarget': array([106.17596225,   7.68176659]), 'currentState': array([88.34365466,  3.10212606,  6.05584604]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7892900900211354
running average episode reward sum: 0.6835214247284372
{'scaleFactor': 20, 'currentTarget': array([114.,  10.]), 'dynamicTrap': False, 'previousTarget': array([114.,  10.]), 'currentState': array([113.6085323 ,  10.20826492,   0.52663244]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 0.4434199277258435}
episode index:806
target Thresh 74.7842358311697
target distance 42.0
model initialize at round 806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.7520687 ,  7.03061594]), 'dynamicTrap': False, 'previousTarget': array([49.14023452,  6.6357422 ]), 'currentState': array([67.54214609,  9.92074342,  3.51699054]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6640971073305642
running average episode reward sum: 0.6834973549423184
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'dynamicTrap': False, 'previousTarget': array([27.,  4.]), 'currentState': array([27.9622149 ,  4.52209206,  4.13065775]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 1.0947317641094774}
episode index:807
target Thresh 74.79029948025853
target distance 60.0
model initialize at round 807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.3464888 , 19.69420386]), 'dynamicTrap': False, 'previousTarget': array([61.01110186, 19.3337034 ]), 'currentState': array([79.32699732, 20.5769719 ,  1.95475709]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6441771538162071
running average episode reward sum: 0.6834486913270634
{'scaleFactor': 20, 'currentTarget': array([21., 18.]), 'dynamicTrap': False, 'previousTarget': array([21., 18.]), 'currentState': array([20.32819773, 17.81394666,  1.61320258]), 'targetState': array([21, 18], dtype=int32), 'currentDistance': 0.6970897564554381}
episode index:808
target Thresh 74.79633288677137
target distance 11.0
model initialize at round 808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  10.]), 'dynamicTrap': False, 'previousTarget': array([101.,  10.]), 'currentState': array([90.50733798, 16.48520135,  0.23162842]), 'targetState': array([101,  10], dtype=int32), 'currentDistance': 12.335063555564798}
done in step count: 11
reward sum = 0.8859234527647064
running average episode reward sum: 0.6836989691533152
{'scaleFactor': 20, 'currentTarget': array([101.,  10.]), 'dynamicTrap': False, 'previousTarget': array([101.,  10.]), 'currentState': array([100.91951124,  10.69446221,   0.84554967]), 'targetState': array([101,  10], dtype=int32), 'currentDistance': 0.6991110135298081}
episode index:809
target Thresh 74.80233620154367
target distance 30.0
model initialize at round 809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.36455779,  6.75613027]), 'dynamicTrap': False, 'previousTarget': array([71.9007438 ,  6.99007438]), 'currentState': array([53.56886605,  3.90471154,  6.01204497]), 'targetState': array([82,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8084505095459952
running average episode reward sum: 0.6838529834007135
{'scaleFactor': 20, 'currentTarget': array([82.,  8.]), 'dynamicTrap': False, 'previousTarget': array([82.,  8.]), 'currentState': array([82.00869068,  7.0385477 ,  1.89217736]), 'targetState': array([82,  8], dtype=int32), 'currentDistance': 0.9614915765776136}
episode index:810
target Thresh 74.80830957465864
target distance 44.0
model initialize at round 810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.99501504, 13.19461861]), 'dynamicTrap': False, 'previousTarget': array([43.59429865, 13.99207528]), 'currentState': array([25.3194097 , 16.78217411,  6.02911192]), 'targetState': array([68,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5989225709955918
running average episode reward sum: 0.6837482603274643
{'scaleFactor': 20, 'currentTarget': array([68.,  9.]), 'dynamicTrap': False, 'previousTarget': array([68.,  9.]), 'currentState': array([68.65118931,  9.11260415,  1.14727362]), 'targetState': array([68,  9], dtype=int32), 'currentDistance': 0.6608534012072266}
episode index:811
target Thresh 74.81425315545091
target distance 46.0
model initialize at round 811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.22773877, 13.29452793]), 'dynamicTrap': False, 'previousTarget': array([27.7042351, 12.4268235]), 'currentState': array([ 7.41659504, 10.55252421,  0.95161676]), 'targetState': array([54, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5204135035711286
running average episode reward sum: 0.6835471091491929
{'scaleFactor': 20, 'currentTarget': array([54., 17.]), 'dynamicTrap': False, 'previousTarget': array([54., 17.]), 'currentState': array([54.68536705, 16.47123042,  3.03175873]), 'targetState': array([54, 17], dtype=int32), 'currentDistance': 0.8656357579648197}
episode index:812
target Thresh 74.82016709251032
target distance 13.0
model initialize at round 812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51., 11.]), 'dynamicTrap': False, 'previousTarget': array([51., 11.]), 'currentState': array([65.29615217, 19.18065578,  4.74725512]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 16.471280939518934}
done in step count: 32
reward sum = 0.545771640176703
running average episode reward sum: 0.6833776436277015
{'scaleFactor': 20, 'currentTarget': array([51., 11.]), 'dynamicTrap': False, 'previousTarget': array([51., 11.]), 'currentState': array([50.66477148, 10.47098527,  5.17191755]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 0.6262864721568617}
episode index:813
target Thresh 74.82605153368559
target distance 25.0
model initialize at round 813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.41353052, 16.19221124]), 'dynamicTrap': True, 'previousTarget': array([75.44774604, 16.33254095]), 'currentState': array([56.       , 21.       ,  1.8035952], dtype=float32), 'targetState': array([81, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7457948169023677
running average episode reward sum: 0.683454323201749
{'scaleFactor': 20, 'currentTarget': array([81., 15.]), 'dynamicTrap': False, 'previousTarget': array([81., 15.]), 'currentState': array([80.61517561, 15.19189725,  0.52245548]), 'targetState': array([81, 15], dtype=int32), 'currentDistance': 0.43001670140796305}
episode index:814
target Thresh 74.83190662608804
target distance 33.0
model initialize at round 814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.11876643,  9.06159993]), 'dynamicTrap': False, 'previousTarget': array([35.20413153,  9.16513874]), 'currentState': array([53.9454887 , 15.81100943,  4.33057671]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7416342000286466
running average episode reward sum: 0.683525709553684
{'scaleFactor': 20, 'currentTarget': array([21.,  4.]), 'dynamicTrap': False, 'previousTarget': array([21.,  4.]), 'currentState': array([20.94088091,  3.59024567,  4.29333517]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 0.413997192529995}
episode index:815
target Thresh 74.83773251609531
target distance 23.0
model initialize at round 815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.17289607, 20.063139  ]), 'dynamicTrap': False, 'previousTarget': array([ 9.37514441, 20.28798697]), 'currentState': array([27.31399694, 11.64289797,  3.3427211 ]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7377193775263664
running average episode reward sum: 0.683592123362474
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.28031155, 22.03646244,  2.59208359]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.7206115252536371}
episode index:816
target Thresh 74.84352934935497
target distance 50.0
model initialize at round 816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.11013344,  8.58483252]), 'dynamicTrap': False, 'previousTarget': array([73.55225396,  9.33254095]), 'currentState': array([91.61728534, 12.99743093,  2.8725462 ]), 'targetState': array([43,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6443652906264131
running average episode reward sum: 0.6835441101033111
{'scaleFactor': 20, 'currentTarget': array([43.,  2.]), 'dynamicTrap': False, 'previousTarget': array([43.,  2.]), 'currentState': array([42.32147494,  1.16561838,  5.33548812]), 'targetState': array([43,  2], dtype=int32), 'currentDistance': 1.075448249400518}
episode index:817
target Thresh 74.84929727078811
target distance 38.0
model initialize at round 817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.06225876, 14.34894048]), 'dynamicTrap': False, 'previousTarget': array([98.24474069, 15.11925147]), 'currentState': array([118.70520274,  10.58666033,   4.27629089]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6104588477257284
running average episode reward sum: 0.6834547638167858
{'scaleFactor': 20, 'currentTarget': array([80., 18.]), 'dynamicTrap': False, 'previousTarget': array([80., 18.]), 'currentState': array([79.33041081, 18.31719604,  1.17745653]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 0.740920385256898}
episode index:818
target Thresh 74.85503642459308
target distance 28.0
model initialize at round 818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.17302821, 11.60431873]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([30.1426686 , 10.50274593,  3.34057999]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6352423650964352
running average episode reward sum: 0.6833958964190809
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.94602938, 11.68197758,  3.33951594]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.3225695033888617}
episode index:819
target Thresh 74.86074695424905
target distance 67.0
model initialize at round 819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.71688076, 10.08285468]), 'dynamicTrap': False, 'previousTarget': array([71.1780353 ,  9.66265197]), 'currentState': array([92.57096826,  7.67138007,  6.03016228]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3685931762405953
running average episode reward sum: 0.6830119906627656
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'dynamicTrap': False, 'previousTarget': array([24., 16.]), 'currentState': array([23.60419131, 16.01153374,  2.8839401 ]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.3959766965536095}
episode index:820
target Thresh 74.86642900251952
target distance 38.0
model initialize at round 820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.86573485,  9.2805382 ]), 'dynamicTrap': False, 'previousTarget': array([32.17091409,  8.60909025]), 'currentState': array([50.76260008,  7.25205263,  3.22290111]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5677082350521516
running average episode reward sum: 0.6828715475986845
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'dynamicTrap': False, 'previousTarget': array([14., 11.]), 'currentState': array([14.40935892, 11.99757551,  4.52094673]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 1.0783003433102114}
episode index:821
target Thresh 74.87208271145603
target distance 11.0
model initialize at round 821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.82578724, 16.94682483]), 'dynamicTrap': True, 'previousTarget': array([87., 17.]), 'currentState': array([98.       , 22.       ,  0.3952491], dtype=float32), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 12.263670338505166}
done in step count: 15
reward sum = 0.7921237025482785
running average episode reward sum: 0.6830044577628567
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87., 17.]), 'currentState': array([86.97097082, 17.64567908,  4.62971232]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 0.6463313094844719}
episode index:822
target Thresh 74.87770822240158
target distance 8.0
model initialize at round 822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  6.]), 'dynamicTrap': False, 'previousTarget': array([90.,  6.]), 'currentState': array([89.52478667, 13.28209717,  4.83712602]), 'targetState': array([90,  6], dtype=int32), 'currentDistance': 7.2975863774380985}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6833417500499006
{'scaleFactor': 20, 'currentTarget': array([90.,  6.]), 'dynamicTrap': False, 'previousTarget': array([90.,  6.]), 'currentState': array([90.60558679,  5.84892311,  4.33917016]), 'targetState': array([90,  6], dtype=int32), 'currentDistance': 0.624147088772445}
episode index:823
target Thresh 74.88330567599424
target distance 46.0
model initialize at round 823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.47539085, 10.99444089]), 'dynamicTrap': False, 'previousTarget': array([91.64765455, 12.04843794]), 'currentState': array([111.67691111,   5.39965586,   4.19783485]), 'targetState': array([65, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.2781288269752241
running average episode reward sum: 0.6828499867937421
{'scaleFactor': 20, 'currentTarget': array([65., 19.]), 'dynamicTrap': False, 'previousTarget': array([65., 19.]), 'currentState': array([65.8210627 , 19.11047757,  3.07403587]), 'targetState': array([65, 19], dtype=int32), 'currentDistance': 0.8284619749523604}
episode index:824
target Thresh 74.88887521217063
target distance 4.0
model initialize at round 824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 15.]), 'dynamicTrap': False, 'previousTarget': array([87., 15.]), 'currentState': array([89.35973391, 12.64407409,  2.36538482]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 3.334476125348302}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6832102898400527
{'scaleFactor': 20, 'currentTarget': array([87., 15.]), 'dynamicTrap': False, 'previousTarget': array([87., 15.]), 'currentState': array([86.84921986, 15.69472093,  2.13656263]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 0.7108950811430931}
episode index:825
target Thresh 74.89441697016946
target distance 32.0
model initialize at round 825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.50771683, 16.55526347]), 'dynamicTrap': False, 'previousTarget': array([91.33768561, 15.38310452]), 'currentState': array([108.55844302,   7.94298827,   1.94554621]), 'targetState': array([77, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5238091122232873
running average episode reward sum: 0.6830173102061341
{'scaleFactor': 20, 'currentTarget': array([77., 23.]), 'dynamicTrap': False, 'previousTarget': array([77., 23.]), 'currentState': array([77.21668558, 23.50602241,  3.97169609]), 'targetState': array([77, 23], dtype=int32), 'currentDistance': 0.5504646405986786}
episode index:826
target Thresh 74.89993108853496
target distance 67.0
model initialize at round 826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.11854543, 17.3302001 ]), 'dynamicTrap': False, 'previousTarget': array([67.422826  , 16.90924722]), 'currentState': array([85.63494326, 21.70172337,  2.69645547]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5461493199300265
running average episode reward sum: 0.682851810822487
{'scaleFactor': 20, 'currentTarget': array([20.,  7.]), 'dynamicTrap': False, 'previousTarget': array([20.,  7.]), 'currentState': array([20.09507505,  7.00366259,  5.02406452]), 'targetState': array([20,  7], dtype=int32), 'currentDistance': 0.09514557290391379}
episode index:827
target Thresh 74.90541770512036
target distance 8.0
model initialize at round 827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  13.]), 'dynamicTrap': False, 'previousTarget': array([117.,  13.]), 'currentState': array([110.92737907,   7.51313895,   0.38978624]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 8.184275726180275}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6831756492754792
{'scaleFactor': 20, 'currentTarget': array([117.,  13.]), 'dynamicTrap': False, 'previousTarget': array([117.,  13.]), 'currentState': array([117.00754892,  12.05900483,   1.57743351]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 0.9410254444678398}
episode index:828
target Thresh 74.9108769570914
target distance 21.0
model initialize at round 828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.5509306 ,  4.07383339]), 'dynamicTrap': False, 'previousTarget': array([21.90990945,  4.10381815]), 'currentState': array([2.81589107e+00, 7.31855441e+00, 6.68454170e-03]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6345382584320763
running average episode reward sum: 0.6831169793227126
{'scaleFactor': 20, 'currentTarget': array([23.,  4.]), 'dynamicTrap': False, 'previousTarget': array([23.,  4.]), 'currentState': array([23.61255465,  3.4316819 ,  2.03945414]), 'targetState': array([23,  4], dtype=int32), 'currentDistance': 0.8355888072884735}
episode index:829
target Thresh 74.91630898092963
target distance 12.0
model initialize at round 829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81., 18.]), 'dynamicTrap': False, 'previousTarget': array([81., 18.]), 'currentState': array([70.98396827, 13.97749999,  0.20955064]), 'targetState': array([81, 18], dtype=int32), 'currentDistance': 10.793581330004917}
done in step count: 7
reward sum = 0.9127563978069899
running average episode reward sum: 0.6833936533208865
{'scaleFactor': 20, 'currentTarget': array([81., 18.]), 'dynamicTrap': False, 'previousTarget': array([81., 18.]), 'currentState': array([80.3408037 , 17.32145324,  1.05434228]), 'targetState': array([81, 18], dtype=int32), 'currentDistance': 0.9460261461753365}
episode index:830
target Thresh 74.92171391243595
target distance 19.0
model initialize at round 830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.85397212,  21.60207255]), 'dynamicTrap': False, 'previousTarget': array([108.91410718,  21.76686234]), 'currentState': array([101.96384043,   2.82639453,   4.40603905]), 'targetState': array([109,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8054703909675677
running average episode reward sum: 0.6835405567356239
{'scaleFactor': 20, 'currentTarget': array([109.,  22.]), 'dynamicTrap': False, 'previousTarget': array([109.,  22.]), 'currentState': array([109.39538204,  21.6410865 ,   2.99094536]), 'targetState': array([109,  22], dtype=int32), 'currentDistance': 0.5339905082223112}
episode index:831
target Thresh 74.92709188673392
target distance 64.0
model initialize at round 831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.12937689, 17.23541713]), 'dynamicTrap': False, 'previousTarget': array([27.96105157, 16.24756572]), 'currentState': array([ 7.14482433, 16.44950428,  1.09381413]), 'targetState': array([72, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5958867484399792
running average episode reward sum: 0.6834352036006531
{'scaleFactor': 20, 'currentTarget': array([72., 19.]), 'dynamicTrap': False, 'previousTarget': array([72., 19.]), 'currentState': array([71.86789268, 19.05441629,  6.03147316]), 'targetState': array([72, 19], dtype=int32), 'currentDistance': 0.14287573829150874}
episode index:832
target Thresh 74.93244303827318
target distance 67.0
model initialize at round 832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.39119844,  4.15477313]), 'dynamicTrap': False, 'previousTarget': array([65.92028312,  4.78390595]), 'currentState': array([47.5031097 ,  2.04197256,  0.12966114]), 'targetState': array([113,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6278552982029275
running average episode reward sum: 0.6833684810251456
{'scaleFactor': 20, 'currentTarget': array([113.,   9.]), 'dynamicTrap': False, 'previousTarget': array([113.,   9.]), 'currentState': array([112.31728739,   8.27296094,   6.05432434]), 'targetState': array([113,   9], dtype=int32), 'currentDistance': 0.9973376045099177}
episode index:833
target Thresh 74.93776750083279
target distance 72.0
model initialize at round 833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.04840849,  4.39068191]), 'dynamicTrap': True, 'previousTarget': array([55.04805158,  4.38555197]), 'currentState': array([75.      ,  3.      ,  6.203685], dtype=float32), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.46510824844492105
running average episode reward sum: 0.6831067781083828
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'dynamicTrap': False, 'previousTarget': array([3., 8.]), 'currentState': array([3.78616799, 8.19591901, 2.6592304 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.8102125403083215}
episode index:834
target Thresh 74.94306540752459
target distance 6.0
model initialize at round 834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'dynamicTrap': False, 'previousTarget': array([23., 15.]), 'currentState': array([26.0051539 , 10.7846018 ,  1.49053934]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 5.176923019359409}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6834507208890912
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'dynamicTrap': False, 'previousTarget': array([23., 15.]), 'currentState': array([23.31527677, 14.41011227,  3.18596426]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.6688549775669639}
episode index:835
target Thresh 74.94833689079654
target distance 50.0
model initialize at round 835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.83335517,  9.73087907]), 'dynamicTrap': True, 'previousTarget': array([59.81774824,  9.77438937]), 'currentState': array([41.      ,  3.      ,  2.857721], dtype=float32), 'targetState': array([91, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6342521716245689
running average episode reward sum: 0.6833918709497796
{'scaleFactor': 20, 'currentTarget': array([91., 21.]), 'dynamicTrap': False, 'previousTarget': array([91., 21.]), 'currentState': array([90.91453305, 20.39156397,  2.33706092]), 'targetState': array([91, 21], dtype=int32), 'currentDistance': 0.6144094732637795}
episode index:836
target Thresh 74.95358208243597
target distance 43.0
model initialize at round 836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.24241142, 15.82305854]), 'dynamicTrap': False, 'previousTarget': array([92.13385239, 16.68998284]), 'currentState': array([112.18117067,  17.38698943,   3.81423855]), 'targetState': array([69, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6562446198961198
running average episode reward sum: 0.6833594369580787
{'scaleFactor': 20, 'currentTarget': array([69., 14.]), 'dynamicTrap': False, 'previousTarget': array([69., 14.]), 'currentState': array([69.17017278, 14.45315345,  4.47833287]), 'targetState': array([69, 14], dtype=int32), 'currentDistance': 0.48405250425265517}
episode index:837
target Thresh 74.95880111357297
target distance 44.0
model initialize at round 837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.86005583,  3.72470982]), 'dynamicTrap': False, 'previousTarget': array([90.9793708 ,  4.09184678]), 'currentState': array([72.87076181,  4.37902211,  5.66480279]), 'targetState': array([115,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7539975767326027
running average episode reward sum: 0.68344373068096
{'scaleFactor': 20, 'currentTarget': array([115.,   3.]), 'dynamicTrap': False, 'previousTarget': array([115.,   3.]), 'currentState': array([114.99962959,   2.56975133,   1.33210888]), 'targetState': array([115,   3], dtype=int32), 'currentDistance': 0.43024883157028243}
episode index:838
target Thresh 74.96399411468359
target distance 69.0
model initialize at round 838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.65283033,  4.15931582]), 'dynamicTrap': False, 'previousTarget': array([57.01887683,  5.13125551]), 'currentState': array([77.64738107,  4.62615726,  4.1721499 ]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3808053129745034
running average episode reward sum: 0.68308301742982
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'dynamicTrap': False, 'previousTarget': array([8., 3.]), 'currentState': array([8.88702682, 2.63299997, 5.73590191]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.9599508343547116}
episode index:839
target Thresh 74.9691612155931
target distance 3.0
model initialize at round 839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.,  7.]), 'dynamicTrap': False, 'previousTarget': array([68.,  7.]), 'currentState': array([67.67454278,  4.53579784,  1.1863535 ]), 'targetState': array([68,  7], dtype=int32), 'currentDistance': 2.4856014780315863}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6834483947900226
{'scaleFactor': 20, 'currentTarget': array([68.,  7.]), 'dynamicTrap': False, 'previousTarget': array([68.,  7.]), 'currentState': array([68.09337365,  6.48113165,  1.53459531]), 'targetState': array([68,  7], dtype=int32), 'currentDistance': 0.5272030012163573}
episode index:840
target Thresh 74.97430254547933
target distance 63.0
model initialize at round 840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.72559787, 10.87805436]), 'dynamicTrap': False, 'previousTarget': array([69.09009055,  9.89618185]), 'currentState': array([87.66985019,  9.385808  ,  2.83077621]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.551087017705167
running average episode reward sum: 0.6832910090859978
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'dynamicTrap': False, 'previousTarget': array([26., 14.]), 'currentState': array([26.05434829, 14.55278578,  1.88482253]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.555451037032981}
episode index:841
target Thresh 74.97941823287576
target distance 32.0
model initialize at round 841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.82163574, 18.03048339]), 'dynamicTrap': False, 'previousTarget': array([76.53800035, 18.72606242]), 'currentState': array([57.09395589, 21.31965469,  5.77824545]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7147057752369312
running average episode reward sum: 0.6833283187845143
{'scaleFactor': 20, 'currentTarget': array([89., 16.]), 'dynamicTrap': False, 'previousTarget': array([89., 16.]), 'currentState': array([89.19600599, 15.77207984,  0.39818172]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 0.3006092896691096}
episode index:842
target Thresh 74.98450840567486
target distance 39.0
model initialize at round 842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.04201777, 12.97760721]), 'dynamicTrap': False, 'previousTarget': array([47.66691212, 13.17958159]), 'currentState': array([27.25652181,  6.11428863,  2.17902184]), 'targetState': array([68, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5897383612060948
running average episode reward sum: 0.6832172986687629
{'scaleFactor': 20, 'currentTarget': array([68., 21.]), 'dynamicTrap': False, 'previousTarget': array([68., 21.]), 'currentState': array([67.48328416, 21.20018413,  1.21747954]), 'targetState': array([68, 21], dtype=int32), 'currentDistance': 0.5541380228328573}
episode index:843
target Thresh 74.98957319113121
target distance 46.0
model initialize at round 843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.63706615, 18.11046206]), 'dynamicTrap': False, 'previousTarget': array([30.00472422, 18.56532009]), 'currentState': array([48.63686513, 18.2001326 ,  2.79667687]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6957128765678167
running average episode reward sum: 0.683232103855847
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([ 3.33396904, 18.9746784 ,  3.95725542]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 1.1805063417605213}
episode index:844
target Thresh 74.99461271586472
target distance 13.0
model initialize at round 844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.05004145,   5.19338768]), 'dynamicTrap': True, 'previousTarget': array([106.,   5.]), 'currentState': array([102.       ,  18.       ,   4.3746696], dtype=float32), 'targetState': array([106,   5], dtype=int32), 'currentDistance': 13.431759183923631}
done in step count: 15
reward sum = 0.7760158326133363
running average episode reward sum: 0.6833419070851459
{'scaleFactor': 20, 'currentTarget': array([106.,   5.]), 'dynamicTrap': False, 'previousTarget': array([106.,   5.]), 'currentState': array([105.90374579,   5.73455633,   5.81580783]), 'targetState': array([106,   5], dtype=int32), 'currentDistance': 0.7408359309792973}
episode index:845
target Thresh 74.99962710586375
target distance 52.0
model initialize at round 845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.99697782, 21.34767511]), 'dynamicTrap': True, 'previousTarget': array([61.9963028 , 21.38454428]), 'currentState': array([42.       , 21.       ,  0.6479764], dtype=float32), 'targetState': array([94, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.33337694931158046
running average episode reward sum: 0.682928236922293
{'scaleFactor': 20, 'currentTarget': array([94., 22.]), 'dynamicTrap': False, 'previousTarget': array([94., 22.]), 'currentState': array([93.04283493, 21.36904021,  0.2947678 ]), 'targetState': array([94, 22], dtype=int32), 'currentDistance': 1.1464184311985204}
episode index:846
target Thresh 75.00461648648835
target distance 40.0
model initialize at round 846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.13184107, 20.22380751]), 'dynamicTrap': False, 'previousTarget': array([69.0992562 , 20.99007438]), 'currentState': array([89.96145004, 17.61869558,  4.31045565]), 'targetState': array([49, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7669313300571308
running average episode reward sum: 0.683027414127883
{'scaleFactor': 20, 'currentTarget': array([49., 23.]), 'dynamicTrap': False, 'previousTarget': array([49., 23.]), 'currentState': array([49.18880869, 23.13688425,  3.60664147]), 'targetState': array([49, 23], dtype=int32), 'currentDistance': 0.23320810230821218}
episode index:847
target Thresh 75.00958098247324
target distance 53.0
model initialize at round 847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.57302098, 22.13078091]), 'dynamicTrap': False, 'previousTarget': array([27.98577525, 21.7541802 ]), 'currentState': array([ 9.58066645, 21.57782493,  5.62365228]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.43559363231371706
running average episode reward sum: 0.6827356290078191
{'scaleFactor': 20, 'currentTarget': array([61., 23.]), 'dynamicTrap': False, 'previousTarget': array([61., 23.]), 'currentState': array([60.16051274, 23.83804645,  0.76421871]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 1.1861959012709054}
episode index:848
target Thresh 75.0145207179311
target distance 42.0
model initialize at round 848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.82467192, 12.51842844]), 'dynamicTrap': False, 'previousTarget': array([31.97366596, 13.32455532]), 'currentState': array([14.14441363,  5.37364362,  5.51213509]), 'targetState': array([55, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.669742005595422
running average episode reward sum: 0.6827203243866031
{'scaleFactor': 20, 'currentTarget': array([54.88686012, 19.26745176]), 'dynamicTrap': True, 'previousTarget': array([55., 21.]), 'currentState': array([54.29906168, 18.4308824 ,  2.21940957]), 'targetState': array([55, 21], dtype=int32), 'currentDistance': 1.0224261930735465}
episode index:849
target Thresh 75.01943581635561
target distance 6.0
model initialize at round 849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83., 18.]), 'dynamicTrap': False, 'previousTarget': array([83., 18.]), 'currentState': array([89.14903845, 16.50483438,  2.41308314]), 'targetState': array([83, 18], dtype=int32), 'currentDistance': 6.3282062331691415}
done in step count: 3
reward sum = 0.9605960100000001
running average episode reward sum: 0.683047236957913
{'scaleFactor': 20, 'currentTarget': array([84.98594202, 18.11393934]), 'dynamicTrap': True, 'previousTarget': array([83., 18.]), 'currentState': array([85.79162095, 18.22004085,  3.02061416]), 'targetState': array([83, 18], dtype=int32), 'currentDistance': 0.8126352612011919}
episode index:850
target Thresh 75.02432640062446
target distance 13.0
model initialize at round 850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99., 16.]), 'dynamicTrap': False, 'previousTarget': array([99., 16.]), 'currentState': array([86.06979581, 12.68152212,  0.51931262]), 'targetState': array([99, 16], dtype=int32), 'currentDistance': 13.349250009119757}
done in step count: 11
reward sum = 0.8585123517980137
running average episode reward sum: 0.6832534239318732
{'scaleFactor': 20, 'currentTarget': array([97.20402391, 15.15053211]), 'dynamicTrap': True, 'previousTarget': array([99., 16.]), 'currentState': array([97.88239412, 15.46218994,  0.6394824 ]), 'targetState': array([99, 16], dtype=int32), 'currentDistance': 0.7465365038362993}
episode index:851
target Thresh 75.02919259300248
target distance 6.0
model initialize at round 851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([52.15876991,  4.52600728,  3.9298377 ]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 4.4299057996495526}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6836018354061315
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([48.66052161,  3.44003705,  4.34966695]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 0.7936758816173968}
episode index:852
target Thresh 75.03403451514478
target distance 5.0
model initialize at round 852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 18.]), 'currentState': array([ 4.4215518 , 13.11469462,  2.05905724]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 5.0879286694914}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6839379399367221
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.43549339, 18.24951482,  1.08242404]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.6171915035398549}
episode index:853
target Thresh 75.03885228809965
target distance 10.0
model initialize at round 853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([72.6062067 ,  7.30020441,  1.17225259]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 9.718720129691865}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6842395116105678
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([71.73914898, 16.18696123,  6.07399853]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 0.8538590631900632}
episode index:854
target Thresh 75.04364603231166
target distance 27.0
model initialize at round 854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.50884637, 19.63679206]), 'dynamicTrap': False, 'previousTarget': array([13.64006204, 19.01924318]), 'currentState': array([32.08412123, 15.53696105,  2.9247489 ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6095173254318601
running average episode reward sum: 0.6841521172407682
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.68052366, 21.98331304,  3.65670746]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 1.0339098895679955}
episode index:855
target Thresh 75.04841586762467
target distance 75.0
model initialize at round 855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.93418209, 13.37877072]), 'dynamicTrap': True, 'previousTarget': array([61.95570316, 13.66961979]), 'currentState': array([42.       , 15.       ,  1.6181035], dtype=float32), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5334546718831993
running average episode reward sum: 0.6839760688232944
{'scaleFactor': 20, 'currentTarget': array([117.,  10.]), 'dynamicTrap': False, 'previousTarget': array([117.,  10.]), 'currentState': array([116.92502145,   9.9897539 ,   1.4568927 ]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 0.07567539218146485}
episode index:856
target Thresh 75.0531619132848
target distance 5.0
model initialize at round 856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'dynamicTrap': False, 'previousTarget': array([14., 18.]), 'currentState': array([13.78904179, 21.18904646,  4.84829855]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 3.196016373577133}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6843216043322521
{'scaleFactor': 20, 'currentTarget': array([14., 18.]), 'dynamicTrap': False, 'previousTarget': array([14., 18.]), 'currentState': array([13.93272938, 17.27637809,  5.2516593 ]), 'targetState': array([14, 18], dtype=int32), 'currentDistance': 0.7267420531802652}
episode index:857
target Thresh 75.05788428794345
target distance 1.0
model initialize at round 857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 19.]), 'dynamicTrap': False, 'previousTarget': array([70., 19.]), 'currentState': array([68.42266109, 21.01378287,  3.19473469]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 2.557991299350409}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6846549113202097
{'scaleFactor': 20, 'currentTarget': array([70., 19.]), 'dynamicTrap': False, 'previousTarget': array([70., 19.]), 'currentState': array([70.67680314, 18.68607373,  6.0738557 ]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 0.7460644690885763}
episode index:858
target Thresh 75.06258310966024
target distance 63.0
model initialize at round 858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.24191222,  5.50965413]), 'dynamicTrap': False, 'previousTarget': array([82.00251905,  6.31742033]), 'currentState': array([103.23057421,   4.83631027,   4.53377147]), 'targetState': array([39,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.47581660547757565
running average episode reward sum: 0.684411793385585
{'scaleFactor': 20, 'currentTarget': array([40.94855096,  7.33725946]), 'dynamicTrap': True, 'previousTarget': array([39.,  7.]), 'currentState': array([41.00163269,  7.44507605,  3.57050652]), 'targetState': array([39,  7], dtype=int32), 'currentDistance': 0.12017523175278802}
episode index:859
target Thresh 75.06725849590595
target distance 36.0
model initialize at round 859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.22404164,  3.80630196]), 'dynamicTrap': False, 'previousTarget': array([66.12232531,  3.79136948]), 'currentState': array([84.06470325,  6.32585489,  3.07269125]), 'targetState': array([50,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 74
reward sum = 0.06325368860694625
running average episode reward sum: 0.6836895165195633
{'scaleFactor': 20, 'currentTarget': array([50.,  2.]), 'dynamicTrap': False, 'previousTarget': array([50.,  2.]), 'currentState': array([50.54612719,  2.62630001,  3.75375388]), 'targetState': array([50,  2], dtype=int32), 'currentDistance': 0.8309672745331758}
episode index:860
target Thresh 75.07191056356545
target distance 21.0
model initialize at round 860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.38408404, 12.90074877]), 'dynamicTrap': True, 'previousTarget': array([75.35322867, 12.74224216]), 'currentState': array([95.      ,  9.      ,  4.088362], dtype=float32), 'targetState': array([74, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = -0.08703710251225655
running average episode reward sum: 0.6827943636519306
{'scaleFactor': 20, 'currentTarget': array([74., 13.]), 'dynamicTrap': False, 'previousTarget': array([74., 13.]), 'currentState': array([73.1301935 , 12.81568283,  4.62701123]), 'targetState': array([74, 13], dtype=int32), 'currentDistance': 0.8891210099898512}
episode index:861
target Thresh 75.07653942894073
target distance 17.0
model initialize at round 861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.,  5.]), 'dynamicTrap': False, 'previousTarget': array([83.,  5.]), 'currentState': array([86.79753811, 20.56610188,  4.20998192]), 'targetState': array([83,  5], dtype=int32), 'currentDistance': 16.022634719794997}
done in step count: 99
reward sum = -0.5960875893507983
running average episode reward sum: 0.681310741896707
{'scaleFactor': 20, 'currentTarget': array([87.14498961, 13.15333026]), 'dynamicTrap': True, 'previousTarget': array([87.13171372, 13.15431089]), 'currentState': array([84.68558097, 13.31134297,  5.29476591]), 'targetState': array([83,  5], dtype=int32), 'currentDistance': 2.464479426327606}
episode index:862
target Thresh 75.08114520775362
target distance 52.0
model initialize at round 862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.47111247,  8.85055686]), 'dynamicTrap': False, 'previousTarget': array([45.17878677,  9.33175976]), 'currentState': array([66.34006701, 11.13630518,  4.70067484]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.43987585742001706
running average episode reward sum: 0.6810309795740226
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([12.95426654,  5.90539096,  4.43260825]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.9065452725925045}
episode index:863
target Thresh 75.08572801514885
target distance 39.0
model initialize at round 863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.64002926,  9.57374186]), 'dynamicTrap': False, 'previousTarget': array([69.75100648, 10.4292033 ]), 'currentState': array([88.64836886,  3.35417388,  3.49190331]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6378850371397639
running average episode reward sum: 0.6809810421406496
{'scaleFactor': 20, 'currentTarget': array([50., 16.]), 'dynamicTrap': False, 'previousTarget': array([50., 16.]), 'currentState': array([50.05475471, 16.61997592,  2.98763709]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 0.6223891207678038}
episode index:864
target Thresh 75.09028796569685
target distance 18.0
model initialize at round 864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.71283305, 10.05830017]), 'dynamicTrap': False, 'previousTarget': array([60.36442559,  9.19631201]), 'currentState': array([45.0701434 , 23.68150243,  0.51910591]), 'targetState': array([63,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6812082559913528
{'scaleFactor': 20, 'currentTarget': array([63.,  7.]), 'dynamicTrap': False, 'previousTarget': array([63.,  7.]), 'currentState': array([6.31167666e+01, 7.39378445e+00, 3.37190963e-02]), 'targetState': array([63,  7], dtype=int32), 'currentDistance': 0.4107318125618643}
episode index:865
target Thresh 75.0948251733966
target distance 16.0
model initialize at round 865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65., 11.]), 'dynamicTrap': False, 'previousTarget': array([65., 11.]), 'currentState': array([49.34184939, 12.64788564,  0.35625029]), 'targetState': array([65, 11], dtype=int32), 'currentDistance': 15.744624721807842}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6814765111778336
{'scaleFactor': 20, 'currentTarget': array([65., 11.]), 'dynamicTrap': False, 'previousTarget': array([65., 11.]), 'currentState': array([65.73644735, 11.03081118,  0.38960244]), 'targetState': array([65, 11], dtype=int32), 'currentDistance': 0.7370916057311124}
episode index:866
target Thresh 75.09933975167854
target distance 54.0
model initialize at round 866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.13680566,  5.86827718]), 'dynamicTrap': False, 'previousTarget': array([75.96920706,  4.89059961]), 'currentState': array([55.20415138,  7.50818363,  1.08433855]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5750700879120314
running average episode reward sum: 0.6813537817392341
{'scaleFactor': 20, 'currentTarget': array([110.,   3.]), 'dynamicTrap': False, 'previousTarget': array([110.,   3.]), 'currentState': array([110.32247239,   3.42264517,   2.35462284]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 0.531617698599467}
episode index:867
target Thresh 75.10383181340737
target distance 52.0
model initialize at round 867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.85316387, 13.41134296]), 'dynamicTrap': False, 'previousTarget': array([92.01477651, 12.23133756]), 'currentState': array([111.79610133,  14.92105992,   1.69970988]), 'targetState': array([60, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6274578627719745
running average episode reward sum: 0.6812916896666912
{'scaleFactor': 20, 'currentTarget': array([60., 11.]), 'dynamicTrap': False, 'previousTarget': array([60., 11.]), 'currentState': array([59.38135095, 10.13313424,  5.47815596]), 'targetState': array([60, 11], dtype=int32), 'currentDistance': 1.0649802283187877}
episode index:868
target Thresh 75.10830147088485
target distance 11.0
model initialize at round 868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([27.27050463, 14.11017047,  5.99845845]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 15.534455538363897}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6815589227596912
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([37.39970355,  1.14914714,  4.5264203 ]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 0.9400603822200463}
episode index:869
target Thresh 75.11274883585267
target distance 61.0
model initialize at round 869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.78789185, 15.52573871]), 'dynamicTrap': False, 'previousTarget': array([71.21419273, 15.08078253]), 'currentState': array([89.52419696, 18.76275289,  3.64573467]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6404814658690822
running average episode reward sum: 0.6815117072920008
{'scaleFactor': 20, 'currentTarget': array([30.,  9.]), 'dynamicTrap': False, 'previousTarget': array([30.,  9.]), 'currentState': array([29.11903844,  9.07340726,  3.8830961 ]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 0.8840146492629674}
episode index:870
target Thresh 75.11717401949518
target distance 3.0
model initialize at round 870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'dynamicTrap': False, 'previousTarget': array([13., 20.]), 'currentState': array([10.49629597, 22.64428592,  4.53212082]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 3.6415356529967466}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6818432656073946
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'dynamicTrap': False, 'previousTarget': array([13., 20.]), 'currentState': array([13.35935546, 19.39217625,  5.44370407]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.7061062653640927}
episode index:871
target Thresh 75.12157713244218
target distance 20.0
model initialize at round 871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.78181652,  8.4516242 ]), 'dynamicTrap': False, 'previousTarget': array([78.1565257 ,  8.74695771]), 'currentState': array([57.36632583,  3.6517574 ,  1.88665247]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5233383267014096
running average episode reward sum: 0.6816614938884658
{'scaleFactor': 20, 'currentTarget': array([79.,  9.]), 'dynamicTrap': False, 'previousTarget': array([79.,  9.]), 'currentState': array([79.85560952,  9.50331741,  1.50119961]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 0.9926711794629086}
episode index:872
target Thresh 75.12595828477176
target distance 18.0
model initialize at round 872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.,  8.]), 'dynamicTrap': False, 'previousTarget': array([54.,  8.]), 'currentState': array([36.67293204, 14.29665363,  0.08208346]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 18.435702619267836}
done in step count: 16
reward sum = 0.7962492126572671
running average episode reward sum: 0.681792751298281
{'scaleFactor': 20, 'currentTarget': array([54.,  8.]), 'dynamicTrap': False, 'previousTarget': array([52.07294261,  7.65336035]), 'currentState': array([53.08110091,  8.61329919,  0.48878009]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 1.1047675901085239}
episode index:873
target Thresh 75.13031758601292
target distance 18.0
model initialize at round 873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80., 21.]), 'dynamicTrap': False, 'previousTarget': array([80., 21.]), 'currentState': array([61.19507569, 23.50491317,  1.09230733]), 'targetState': array([80, 21], dtype=int32), 'currentDistance': 18.97102443690564}
done in step count: 13
reward sum = 0.8687458127689781
running average episode reward sum: 0.6820066564029387
{'scaleFactor': 20, 'currentTarget': array([78.88276759, 19.6118638 ]), 'dynamicTrap': True, 'previousTarget': array([80., 21.]), 'currentState': array([79.63921499, 19.07454635,  0.55405562]), 'targetState': array([80, 21], dtype=int32), 'currentDistance': 0.9278592152199749}
episode index:874
target Thresh 75.13465514514843
target distance 74.0
model initialize at round 874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.00238821, 22.69093279]), 'dynamicTrap': True, 'previousTarget': array([60.0018259, 22.7297544]), 'currentState': array([80.        , 23.        ,  0.92305833], dtype=float32), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5617235986365465
running average episode reward sum: 0.6818691900512055
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([ 5.33252148, 22.18832032,  4.01568959]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.6935359515753307}
episode index:875
target Thresh 75.13897107061752
target distance 58.0
model initialize at round 875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.18949968,  9.79932037]), 'dynamicTrap': False, 'previousTarget': array([72.51579154,  9.37422914]), 'currentState': array([54.66807115,  5.45031724,  5.67166227]), 'targetState': array([111,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.40991564496756816
running average episode reward sum: 0.681558740798827
{'scaleFactor': 20, 'currentTarget': array([111.,  18.]), 'dynamicTrap': False, 'previousTarget': array([111.,  18.]), 'currentState': array([111.37862809,  18.10154237,   0.89540697]), 'targetState': array([111,  18], dtype=int32), 'currentDistance': 0.3920077570143333}
episode index:876
target Thresh 75.14326547031851
target distance 58.0
model initialize at round 876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.9959533 , 15.59769202]), 'dynamicTrap': True, 'previousTarget': array([78.99702801, 15.65522365]), 'currentState': array([59.      , 16.      ,  2.076152], dtype=float32), 'targetState': array([117,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5038889642837726
running average episode reward sum: 0.6813561526842146
{'scaleFactor': 20, 'currentTarget': array([117.,  15.]), 'dynamicTrap': False, 'previousTarget': array([117.,  15.]), 'currentState': array([116.20889796,  14.40283895,   1.75632467]), 'targetState': array([117,  15], dtype=int32), 'currentDistance': 0.9911830117599287}
episode index:877
target Thresh 75.14753845161165
target distance 39.0
model initialize at round 877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.25041822, 23.18601921]), 'dynamicTrap': False, 'previousTarget': array([54., 23.]), 'currentState': array([72.24925549, 23.40167589,  2.28636754]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7841573368551186
running average episode reward sum: 0.6814732383153888
{'scaleFactor': 20, 'currentTarget': array([35., 23.]), 'dynamicTrap': False, 'previousTarget': array([35., 23.]), 'currentState': array([35.8670096 , 23.83350381,  2.75364282]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 1.2026779474499534}
episode index:878
target Thresh 75.15179012132168
target distance 48.0
model initialize at round 878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.73473571,  9.52427868]), 'dynamicTrap': False, 'previousTarget': array([86.57960839, 10.07908508]), 'currentState': array([68.27604016,  4.90268073,  0.2997629 ]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5181516648195702
running average episode reward sum: 0.6812874344775096
{'scaleFactor': 20, 'currentTarget': array([115.,  16.]), 'dynamicTrap': False, 'previousTarget': array([115.,  16.]), 'currentState': array([114.93270259,  15.07105988,   1.88784842]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 0.9313746204520653}
episode index:879
target Thresh 75.15602058574055
target distance 8.0
model initialize at round 879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74., 19.]), 'dynamicTrap': False, 'previousTarget': array([74., 19.]), 'currentState': array([71.3068318 , 12.48966981,  1.08801731]), 'targetState': array([74, 19], dtype=int32), 'currentDistance': 7.045392403076169}
done in step count: 5
reward sum = 0.9412870599
running average episode reward sum: 0.6815828885973079
{'scaleFactor': 20, 'currentTarget': array([74., 19.]), 'dynamicTrap': False, 'previousTarget': array([74., 19.]), 'currentState': array([74.37781114, 19.23213962,  2.00293828]), 'targetState': array([74, 19], dtype=int32), 'currentDistance': 0.44342988047721427}
episode index:880
target Thresh 75.16022995063011
target distance 40.0
model initialize at round 880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.26969263, 18.18554115]), 'dynamicTrap': False, 'previousTarget': array([97.02495322, 17.99875234]), 'currentState': array([115.24984864,  17.29483023,   2.12985182]), 'targetState': array([77, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7119835166350562
running average episode reward sum: 0.6816173955530829
{'scaleFactor': 20, 'currentTarget': array([77., 19.]), 'dynamicTrap': False, 'previousTarget': array([77., 19.]), 'currentState': array([76.90450853, 18.82060398,  5.80476931]), 'targetState': array([77, 19], dtype=int32), 'currentDistance': 0.20322783260355073}
episode index:881
target Thresh 75.16441832122472
target distance 59.0
model initialize at round 881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.1919719 , 12.56460223]), 'dynamicTrap': False, 'previousTarget': array([69.46850103, 13.30355062]), 'currentState': array([90.68707458,  8.09907073,  5.27193176]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5827436098934419
running average episode reward sum: 0.6815052937552828
{'scaleFactor': 20, 'currentTarget': array([30., 22.]), 'dynamicTrap': False, 'previousTarget': array([30., 22.]), 'currentState': array([29.69009254, 21.29330479,  0.03245177]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 0.7716610364794392}
episode index:882
target Thresh 75.16858580223382
target distance 17.0
model initialize at round 882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.40379628, 19.33949433]), 'dynamicTrap': False, 'previousTarget': array([64.71414506, 19.43860471]), 'currentState': array([50.5435908 ,  4.92094018,  1.63704014]), 'targetState': array([66, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.681717344173192
{'scaleFactor': 20, 'currentTarget': array([66., 21.]), 'dynamicTrap': False, 'previousTarget': array([66., 21.]), 'currentState': array([66.22959273, 20.84656984,  2.73638332]), 'targetState': array([66, 21], dtype=int32), 'currentDistance': 0.276140607454389}
episode index:883
target Thresh 75.17273249784468
target distance 67.0
model initialize at round 883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.38904736, 19.72198781]), 'dynamicTrap': False, 'previousTarget': array([67.99109528, 20.59674911]), 'currentState': array([49.41394523, 18.72434308,  6.09417784]), 'targetState': array([115,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6026144981727223
running average episode reward sum: 0.6816278613157253
{'scaleFactor': 20, 'currentTarget': array([115.,  22.]), 'dynamicTrap': False, 'previousTarget': array([115.,  22.]), 'currentState': array([115.16128288,  22.15592633,   1.49423524]), 'targetState': array([115,  22], dtype=int32), 'currentDistance': 0.22433275760248164}
episode index:884
target Thresh 75.17685851172489
target distance 29.0
model initialize at round 884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.27432245,  5.01920881]), 'dynamicTrap': False, 'previousTarget': array([22.69995053,  5.90691532]), 'currentState': array([ 5.37378557, 11.55906399,  5.64116591]), 'targetState': array([33,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6818101385271017
{'scaleFactor': 20, 'currentTarget': array([33.,  2.]), 'dynamicTrap': False, 'previousTarget': array([33.,  2.]), 'currentState': array([33.42588992,  2.27356956,  5.96818793]), 'targetState': array([33,  2], dtype=int32), 'currentDistance': 0.5061842874876802}
episode index:885
target Thresh 75.18096394702503
target distance 1.0
model initialize at round 885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([116.41047635,   1.47948168,   4.49705604]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 0.7864333790488527}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6821692692962585
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([116.41047635,   1.47948168,   4.49705604]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 0.7864333790488527}
episode index:886
target Thresh 75.18504890638118
target distance 36.0
model initialize at round 886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.63579619,  5.72724179]), 'dynamicTrap': False, 'previousTarget': array([34.80984546,  5.75136743]), 'currentState': array([14.82594283,  2.97593122,  3.18203353]), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7417977903330752
running average episode reward sum: 0.6822364942354207
{'scaleFactor': 20, 'currentTarget': array([51.,  8.]), 'dynamicTrap': False, 'previousTarget': array([51.,  8.]), 'currentState': array([51.03757718,  8.58692609,  2.00637491]), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 0.5881277739041966}
episode index:887
target Thresh 75.18911349191757
target distance 21.0
model initialize at round 887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.4134033 , 18.64836572]), 'dynamicTrap': False, 'previousTarget': array([91.79898987, 18.82842712]), 'currentState': array([71.8872082 , 14.32081386,  5.65531921]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7957559304857412
running average episode reward sum: 0.6823643314384052
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([93.37807934, 18.04335682,  0.69245985]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 1.028644822419775}
episode index:888
target Thresh 75.193157805249
target distance 66.0
model initialize at round 888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.90792973, 16.34567698]), 'dynamicTrap': False, 'previousTarget': array([50.4353175 , 16.84991583]), 'currentState': array([68.48842058, 20.42052398,  2.49767065]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5852114494544036
running average episode reward sum: 0.6822550481065897
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.28104905, 7.69453408, 3.74299621]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9996339575176713}
episode index:889
target Thresh 75.19718194748353
target distance 20.0
model initialize at round 889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.58117618,  2.78092045]), 'dynamicTrap': True, 'previousTarget': array([43.12283287,  2.39299151]), 'currentState': array([62.       ,  9.       ,  2.7509117], dtype=float32), 'targetState': array([42,  2], dtype=int32), 'currentDistance': 19.440422352830424}
done in step count: 15
reward sum = 0.8214935059241272
running average episode reward sum: 0.6824114958120026
{'scaleFactor': 20, 'currentTarget': array([42.,  2.]), 'dynamicTrap': False, 'previousTarget': array([42.,  2.]), 'currentState': array([41.87458415,  2.74888012,  4.39872366]), 'targetState': array([42,  2], dtype=int32), 'currentDistance': 0.7593092667431781}
episode index:890
target Thresh 75.20118601922495
target distance 64.0
model initialize at round 890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.60451567, 14.37859137]), 'dynamicTrap': False, 'previousTarget': array([51.03894843, 14.75243428]), 'currentState': array([69.57341894, 15.49344645,  2.47278786]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5949065372761941
running average episode reward sum: 0.682313285981996
{'scaleFactor': 20, 'currentTarget': array([ 7., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 12.]), 'currentState': array([ 7.29400815, 11.27458489,  3.88110913]), 'targetState': array([ 7, 12], dtype=int32), 'currentDistance': 0.7827310321923235}
episode index:891
target Thresh 75.20517012057525
target distance 28.0
model initialize at round 891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.44239057, 11.90728622]), 'dynamicTrap': False, 'previousTarget': array([20.63513716, 12.07722123]), 'currentState': array([36.39768055, 22.51474311,  3.84075737]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7491087733438901
running average episode reward sum: 0.6823881688153615
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'dynamicTrap': False, 'previousTarget': array([10.,  6.]), 'currentState': array([9.42502395, 6.44977891, 5.30139398]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.7299989962704203}
episode index:892
target Thresh 75.20913435113715
target distance 13.0
model initialize at round 892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 17.]), 'dynamicTrap': False, 'previousTarget': array([30., 17.]), 'currentState': array([41.52342988, 18.84620229,  2.40199244]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 11.670385560521293}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6826677625209512
{'scaleFactor': 20, 'currentTarget': array([30., 17.]), 'dynamicTrap': False, 'previousTarget': array([30., 17.]), 'currentState': array([30.40941713, 17.81952811,  4.13245619]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 0.9161051819654142}
episode index:893
target Thresh 75.21307881001664
target distance 50.0
model initialize at round 893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.9371933 ,  5.41623123]), 'dynamicTrap': True, 'previousTarget': array([43.93630557,  5.40509555]), 'currentState': array([24.       ,  7.       ,  3.1309774], dtype=float32), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5396387256662815
running average episode reward sum: 0.6825077747839773
{'scaleFactor': 20, 'currentTarget': array([74.,  3.]), 'dynamicTrap': False, 'previousTarget': array([74.,  3.]), 'currentState': array([73.37871532,  3.52380544,  5.16539906]), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 0.8126295576278809}
episode index:894
target Thresh 75.2170035958254
target distance 57.0
model initialize at round 894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.03713848, 10.71746995]), 'dynamicTrap': False, 'previousTarget': array([48.04906478, 11.40006563]), 'currentState': array([66.95468205,  8.90323225,  2.94100475]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = -0.03264839692816446
running average episode reward sum: 0.6817087176088799
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'dynamicTrap': False, 'previousTarget': array([11., 14.]), 'currentState': array([11.31516853, 14.94996438,  4.88322301]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 1.000881378633761}
episode index:895
target Thresh 75.22090880668328
target distance 34.0
model initialize at round 895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.16288539, 10.07289851]), 'dynamicTrap': False, 'previousTarget': array([93.53165663,  9.41921333]), 'currentState': array([112.38388619,  15.60038696,   2.88501763]), 'targetState': array([79,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7249980433525612
running average episode reward sum: 0.6817570315885045
{'scaleFactor': 20, 'currentTarget': array([79.885813  ,  7.50038463]), 'dynamicTrap': True, 'previousTarget': array([79.,  6.]), 'currentState': array([80.34223649,  8.09732823,  3.86871068]), 'targetState': array([79,  6], dtype=int32), 'currentDistance': 0.751441327118488}
episode index:896
target Thresh 75.22479454022074
target distance 15.0
model initialize at round 896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.,  3.]), 'dynamicTrap': False, 'previousTarget': array([51.,  3.]), 'currentState': array([37.86608433,  1.49504202,  5.7505002 ]), 'targetState': array([51,  3], dtype=int32), 'currentDistance': 13.219857767116231}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6820052200427078
{'scaleFactor': 20, 'currentTarget': array([51.,  3.]), 'dynamicTrap': False, 'previousTarget': array([51.,  3.]), 'currentState': array([50.17020284,  3.81641519,  3.99709211]), 'targetState': array([51,  3], dtype=int32), 'currentDistance': 1.1640863693058796}
episode index:897
target Thresh 75.22866089358135
target distance 43.0
model initialize at round 897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.0015158 , 12.24623097]), 'dynamicTrap': True, 'previousTarget': array([35.00540614, 11.53500945]), 'currentState': array([55.       , 12.       ,  5.2594485], dtype=float32), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6251051224464064
running average episode reward sum: 0.6819418569050727
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([11.89656442, 10.38099149,  5.7420532 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.6275909905040905}
episode index:898
target Thresh 75.23250796342411
target distance 9.0
model initialize at round 898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.,  3.]), 'dynamicTrap': False, 'previousTarget': array([41.,  3.]), 'currentState': array([49.87209599, 10.32171447,  3.62663066]), 'targetState': array([41,  3], dtype=int32), 'currentDistance': 11.503112189830343}
done in step count: 8
reward sum = 0.8944993389558402
running average episode reward sum: 0.682178294593672
{'scaleFactor': 20, 'currentTarget': array([41.,  3.]), 'dynamicTrap': False, 'previousTarget': array([43.06233086,  4.21409704]), 'currentState': array([41.46180627,  3.94189579,  4.02285891]), 'targetState': array([41,  3], dtype=int32), 'currentDistance': 1.0490151103789678}
episode index:899
target Thresh 75.23633584592598
target distance 22.0
model initialize at round 899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.29701083, 17.76806617]), 'dynamicTrap': False, 'previousTarget': array([92.79880147, 16.45345588]), 'currentState': array([79.55932751,  4.24768561,  0.57983974]), 'targetState': array([100,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.79158042052239
running average episode reward sum: 0.6822998525113705
{'scaleFactor': 20, 'currentTarget': array([100.,  23.]), 'dynamicTrap': False, 'previousTarget': array([100.,  23.]), 'currentState': array([100.30639585,  23.80387372,   3.08171625]), 'targetState': array([100,  23], dtype=int32), 'currentDistance': 0.8602856314305826}
episode index:900
target Thresh 75.24014463678422
target distance 62.0
model initialize at round 900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.34935593, 18.89488748]), 'dynamicTrap': False, 'previousTarget': array([98.01039771, 18.35517412]), 'currentState': array([116.32733803,  19.83309398,   2.03903794]), 'targetState': array([56, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6036343476303082
running average episode reward sum: 0.6822125434049542
{'scaleFactor': 20, 'currentTarget': array([56., 17.]), 'dynamicTrap': False, 'previousTarget': array([56., 17.]), 'currentState': array([55.13011047, 17.30126876,  4.71530151]), 'targetState': array([56, 17], dtype=int32), 'currentDistance': 0.9205816969261844}
episode index:901
target Thresh 75.24393443121882
target distance 10.0
model initialize at round 901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.05368298, 16.81919105]), 'dynamicTrap': True, 'previousTarget': array([92., 17.]), 'currentState': array([82.       , 21.       ,  1.9584107], dtype=float32), 'targetState': array([92, 17], dtype=int32), 'currentDistance': 10.888328841640565}
done in step count: 14
reward sum = 0.7914905071968983
running average episode reward sum: 0.6823336941408655
{'scaleFactor': 20, 'currentTarget': array([92., 17.]), 'dynamicTrap': False, 'previousTarget': array([92., 17.]), 'currentState': array([91.46545168, 17.6891796 ,  0.83660628]), 'targetState': array([92, 17], dtype=int32), 'currentDistance': 0.8721871496846489}
episode index:902
target Thresh 75.2477053239748
target distance 59.0
model initialize at round 902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.51336615,  9.25035982]), 'dynamicTrap': False, 'previousTarget': array([67.05868241,  9.42078562]), 'currentState': array([84.34899947,  2.52585853,  2.47097123]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5037831535555162
running average episode reward sum: 0.6821359637526204
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'dynamicTrap': False, 'previousTarget': array([27., 23.]), 'currentState': array([27.3218446 , 23.34976359,  4.0501625 ]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.4753088646455569}
episode index:903
target Thresh 75.25145740932473
target distance 73.0
model initialize at round 903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.50562367, 12.69748249]), 'dynamicTrap': False, 'previousTarget': array([95.09132042, 13.90905147]), 'currentState': array([115.36830221,  10.35782414,   3.92301512]), 'targetState': array([42, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.42331319341287965
running average episode reward sum: 0.6818496553783508
{'scaleFactor': 20, 'currentTarget': array([42., 19.]), 'dynamicTrap': False, 'previousTarget': array([42., 19.]), 'currentState': array([41.80469974, 19.68385487,  4.55741661]), 'targetState': array([42, 19], dtype=int32), 'currentDistance': 0.7111959521335329}
episode index:904
target Thresh 75.25519078107087
target distance 1.0
model initialize at round 904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77., 18.]), 'dynamicTrap': False, 'previousTarget': array([77., 18.]), 'currentState': array([78.83648504, 19.30168335,  2.00962814]), 'targetState': array([77, 18], dtype=int32), 'currentDistance': 2.25101240620321}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.682179213770198
{'scaleFactor': 20, 'currentTarget': array([77., 18.]), 'dynamicTrap': False, 'previousTarget': array([77., 18.]), 'currentState': array([76.6851923 , 17.66829795,  4.86112668]), 'targetState': array([77, 18], dtype=int32), 'currentDistance': 0.4573074837647367}
episode index:905
target Thresh 75.25890553254777
target distance 45.0
model initialize at round 905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.31000229, 18.11416559]), 'dynamicTrap': False, 'previousTarget': array([36.92145281, 17.22920419]), 'currentState': array([17.46721832, 20.61694893,  0.27972078]), 'targetState': array([62, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.33206929612018604
running average episode reward sum: 0.6817927789825048
{'scaleFactor': 20, 'currentTarget': array([62., 15.]), 'dynamicTrap': False, 'previousTarget': array([62., 15.]), 'currentState': array([62.45317705, 14.87407933,  0.70786041]), 'targetState': array([62, 15], dtype=int32), 'currentDistance': 0.4703461009086198}
episode index:906
target Thresh 75.26260175662438
target distance 35.0
model initialize at round 906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.57250398,  5.84191482]), 'dynamicTrap': False, 'previousTarget': array([30.7124451,  5.3792763]), 'currentState': array([12.82591839,  2.66821708,  6.21956164]), 'targetState': array([46,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 29
reward sum = 0.637504441926361
running average episode reward sum: 0.6817439495039423
{'scaleFactor': 20, 'currentTarget': array([46.,  8.]), 'dynamicTrap': False, 'previousTarget': array([46.,  8.]), 'currentState': array([46.83646848,  8.36582852,  0.6189579 ]), 'targetState': array([46,  8], dtype=int32), 'currentDistance': 0.9129676970693419}
episode index:907
target Thresh 75.26627954570648
target distance 72.0
model initialize at round 907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.42960493, 16.70319481]), 'dynamicTrap': False, 'previousTarget': array([35.57960839, 17.92091492]), 'currentState': array([15.76182047, 20.33338141,  5.57946217]), 'targetState': array([88,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.45226382814900334
running average episode reward sum: 0.680495042259831
{'scaleFactor': 20, 'currentTarget': array([81.20711961,  6.54244936]), 'dynamicTrap': False, 'previousTarget': array([79.4450381 ,  6.70068906]), 'currentState': array([61.25233598,  5.19834737,  6.04801842]), 'targetState': array([88,  7], dtype=int32), 'currentDistance': 20.0}
episode index:908
target Thresh 75.26993899173904
target distance 12.0
model initialize at round 908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  23.]), 'dynamicTrap': False, 'previousTarget': array([106.,  23.]), 'currentState': array([93.66569663, 17.64943304,  0.76076531]), 'targetState': array([106,  23], dtype=int32), 'currentDistance': 13.44483568394768}
done in step count: 11
reward sum = 0.8563283041587164
running average episode reward sum: 0.680688478191513
{'scaleFactor': 20, 'currentTarget': array([106.,  23.]), 'dynamicTrap': False, 'previousTarget': array([106.,  23.]), 'currentState': array([105.53658338,  23.15023017,   1.58866872]), 'targetState': array([106,  23], dtype=int32), 'currentDistance': 0.48715917681935034}
episode index:909
target Thresh 75.27358018620835
target distance 30.0
model initialize at round 909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.82031482,  5.99951479]), 'dynamicTrap': False, 'previousTarget': array([99.43046618,  6.57218647]), 'currentState': array([116.34305438,  13.54326183,   2.46206176]), 'targetState': array([88,  2], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6808667800763398
{'scaleFactor': 20, 'currentTarget': array([88.,  2.]), 'dynamicTrap': False, 'previousTarget': array([88.,  2.]), 'currentState': array([88.4389472 ,  2.76450121,  2.99658199]), 'targetState': array([88,  2], dtype=int32), 'currentDistance': 0.8815535949311356}
episode index:910
target Thresh 75.27720322014449
target distance 51.0
model initialize at round 910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.14354712,  9.39191954]), 'dynamicTrap': True, 'previousTarget': array([40.13698791,  9.33682495]), 'currentState': array([60.       ,  7.       ,  5.4577856], dtype=float32), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5416385081585042
running average episode reward sum: 0.6795248423285519
{'scaleFactor': 20, 'currentTarget': array([24.86762097, 11.73476468]), 'dynamicTrap': True, 'previousTarget': array([24.84701896, 11.63122378]), 'currentState': array([44.47272361,  7.78002693,  4.98011917]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 20.0}
episode index:911
target Thresh 75.28080818412347
target distance 38.0
model initialize at round 911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.95615519, 11.17484976]), 'dynamicTrap': False, 'previousTarget': array([74.5672925, 11.23886  ]), 'currentState': array([91.15853195, 19.46179614,  3.45760274]), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.4528318656720349
running average episode reward sum: 0.6792762754681829
{'scaleFactor': 20, 'currentTarget': array([55.,  3.]), 'dynamicTrap': False, 'previousTarget': array([55.,  3.]), 'currentState': array([55.81146401,  3.7218582 ,  3.80663645]), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 1.0860723268072572}
episode index:912
target Thresh 75.28439516826961
target distance 23.0
model initialize at round 912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.37713248,  7.94372685]), 'dynamicTrap': False, 'previousTarget': array([35.24778806,  8.07464439]), 'currentState': array([50.50452242, 19.7719682 ,  3.67506778]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7627783773221922
running average episode reward sum: 0.6793677345063581
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([29.63061391,  3.73901255,  4.16213697]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 0.682486886524327}
episode index:913
target Thresh 75.28796426225767
target distance 25.0
model initialize at round 913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.33639681,  11.49948461]), 'dynamicTrap': False, 'previousTarget': array([112.61161351,  11.9223227 ]), 'currentState': array([93.00341267,  6.37740258,  5.66434586]), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8062684059780076
running average episode reward sum: 0.679506575503592
{'scaleFactor': 20, 'currentTarget': array([118.,  13.]), 'dynamicTrap': False, 'previousTarget': array([118.,  13.]), 'currentState': array([117.97724446,  12.58438927,   0.47581606]), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 0.4162332171380083}
episode index:914
target Thresh 75.29151555531521
target distance 27.0
model initialize at round 914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.91858589,  6.46347103]), 'dynamicTrap': False, 'previousTarget': array([93.21593075,  6.06902678]), 'currentState': array([112.48562347,  10.60243716,   2.89140284]), 'targetState': array([86,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7465353118541751
running average episode reward sum: 0.6795798309531553
{'scaleFactor': 20, 'currentTarget': array([86.,  5.]), 'dynamicTrap': False, 'previousTarget': array([86.,  5.]), 'currentState': array([86.06517986,  5.04688526,  3.46384778]), 'targetState': array([86,  5], dtype=int32), 'currentDistance': 0.08029098449988573}
episode index:915
target Thresh 75.29504913622475
target distance 52.0
model initialize at round 915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.5357184 ,  8.10982272]), 'dynamicTrap': False, 'previousTarget': array([96.51217609,  7.49719013]), 'currentState': array([117.12650013,   4.08474242,   1.77651614]), 'targetState': array([64, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6796214801907592
{'scaleFactor': 20, 'currentTarget': array([64., 15.]), 'dynamicTrap': False, 'previousTarget': array([64., 15.]), 'currentState': array([64.83370133, 15.41983185,  5.02250699]), 'targetState': array([64, 15], dtype=int32), 'currentDistance': 0.9334434627671846}
episode index:916
target Thresh 75.29856509332596
target distance 59.0
model initialize at round 916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.89040488, 18.34363042]), 'dynamicTrap': False, 'previousTarget': array([76.33879402, 17.33435143]), 'currentState': array([96.46676081, 22.43829628,  2.26699555]), 'targetState': array([37, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6274578627719745
running average episode reward sum: 0.6795645951117856
{'scaleFactor': 20, 'currentTarget': array([37., 10.]), 'dynamicTrap': False, 'previousTarget': array([37., 10.]), 'currentState': array([36.08596274,  9.70923283,  5.67261849]), 'targetState': array([37, 10], dtype=int32), 'currentDistance': 0.9591713361592685}
episode index:917
target Thresh 75.30206351451798
target distance 16.0
model initialize at round 917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'dynamicTrap': False, 'previousTarget': array([20.,  8.]), 'currentState': array([ 5.58135392, 11.73506658,  5.10719109]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 14.89456535777294}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6797996426707691
{'scaleFactor': 20, 'currentTarget': array([20.,  8.]), 'dynamicTrap': False, 'previousTarget': array([20.,  8.]), 'currentState': array([20.60744306,  7.98783911,  0.53395045]), 'targetState': array([20,  8], dtype=int32), 'currentDistance': 0.6075647801181562}
episode index:918
target Thresh 75.30554448726151
target distance 18.0
model initialize at round 918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.18410505,   8.40331551]), 'dynamicTrap': False, 'previousTarget': array([102.36442559,   9.19631201]), 'currentState': array([87.35892771, 20.63294192,  5.28418767]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7485117617556983
running average episode reward sum: 0.6798744110266831
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([1.05635437e+02, 6.97921455e+00, 1.77922828e-02]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 0.6357768403158446}
episode index:919
target Thresh 75.30900809858105
target distance 7.0
model initialize at round 919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,   7.]), 'dynamicTrap': False, 'previousTarget': array([108.,   7.]), 'currentState': array([112.45927876,  13.16220691,   4.52457452]), 'targetState': array([108,   7], dtype=int32), 'currentDistance': 7.606442075412371}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6801795431994803
{'scaleFactor': 20, 'currentTarget': array([108.,   7.]), 'dynamicTrap': False, 'previousTarget': array([108.,   7.]), 'currentState': array([108.36304261,   7.43660165,   3.22708642]), 'targetState': array([108,   7], dtype=int32), 'currentDistance': 0.5678212139099399}
episode index:920
target Thresh 75.31245443506707
target distance 64.0
model initialize at round 920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.3142643 , 15.87113824]), 'dynamicTrap': False, 'previousTarget': array([68.23975932, 16.91246239]), 'currentState': array([87.13302681, 18.55752103,  3.16121244]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5402067625549174
running average episode reward sum: 0.6800275640674014
{'scaleFactor': 20, 'currentTarget': array([24., 10.]), 'dynamicTrap': False, 'previousTarget': array([24., 10.]), 'currentState': array([24.80405729,  9.51762666,  5.01300872]), 'targetState': array([24, 10], dtype=int32), 'currentDistance': 0.9376524745891645}
episode index:921
target Thresh 75.31588358287814
target distance 5.0
model initialize at round 921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  18.]), 'dynamicTrap': False, 'previousTarget': array([112.,  18.]), 'currentState': array([108.72193607,  22.02987066,   5.35813433]), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 5.19476280890647}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6803423920890203
{'scaleFactor': 20, 'currentTarget': array([112.,  18.]), 'dynamicTrap': False, 'previousTarget': array([112.,  18.]), 'currentState': array([111.69211208,  17.54414449,   6.10675842]), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 0.5500901967281023}
episode index:922
target Thresh 75.31929562774317
target distance 29.0
model initialize at round 922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.96475625,  7.38007878]), 'dynamicTrap': False, 'previousTarget': array([73.89871726,  8.0720157 ]), 'currentState': array([92.29509241, 12.51214503,  3.2599411 ]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7736994577085721
running average episode reward sum: 0.6804435373388789
{'scaleFactor': 20, 'currentTarget': array([64.,  5.]), 'dynamicTrap': False, 'previousTarget': array([64.,  5.]), 'currentState': array([64.17651988,  5.51605853,  4.97087634]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 0.5454133096701156}
episode index:923
target Thresh 75.32269065496342
target distance 37.0
model initialize at round 923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.90345786, 10.06224532]), 'dynamicTrap': False, 'previousTarget': array([57.02445638, 10.17009396]), 'currentState': array([37.9002547 ,  3.82700116,  4.28616469]), 'targetState': array([75, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7541636449711312
running average episode reward sum: 0.6805233210051476
{'scaleFactor': 20, 'currentTarget': array([75.02586369, 17.79505294]), 'dynamicTrap': True, 'previousTarget': array([75., 16.]), 'currentState': array([74.96341467, 18.24024731,  4.9899139 ]), 'targetState': array([75, 16], dtype=int32), 'currentDistance': 0.44955300830518496}
episode index:924
target Thresh 75.32606874941479
target distance 35.0
model initialize at round 924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.22508577,  8.88828012]), 'dynamicTrap': False, 'previousTarget': array([26.76952105,  9.50557744]), 'currentState': array([44.51736003, 14.16162382,  3.13998413]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.657145192353336
running average episode reward sum: 0.680498047352551
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'dynamicTrap': False, 'previousTarget': array([11.,  5.]), 'currentState': array([11.90722668,  5.97233674,  3.47108907]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 1.3298492332243366}
episode index:925
target Thresh 75.32942999554979
target distance 42.0
model initialize at round 925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.48844034, 12.31843871]), 'dynamicTrap': False, 'previousTarget': array([87.050826, 11.575059]), 'currentState': array([106.37303925,  14.46383516,   2.88587952]), 'targetState': array([65, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5821365818047524
running average episode reward sum: 0.6803918254675103
{'scaleFactor': 20, 'currentTarget': array([65., 10.]), 'dynamicTrap': False, 'previousTarget': array([65., 10.]), 'currentState': array([65.88382824,  9.42200813,  4.8881164 ]), 'targetState': array([65, 10], dtype=int32), 'currentDistance': 1.0560430609787608}
episode index:926
target Thresh 75.33277447739974
target distance 73.0
model initialize at round 926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.28837737,  3.14754974]), 'dynamicTrap': False, 'previousTarget': array([24.93278547,  3.63831113]), 'currentState': array([6.37585344, 1.27902335, 0.39628666]), 'targetState': array([78,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 48
reward sum = 0.5630395854707363
running average episode reward sum: 0.6802652318968557
{'scaleFactor': 20, 'currentTarget': array([78.,  8.]), 'dynamicTrap': False, 'previousTarget': array([78.,  8.]), 'currentState': array([78.93639619,  8.07726658,  0.43815942]), 'targetState': array([78,  8], dtype=int32), 'currentDistance': 0.9395786020998129}
episode index:927
target Thresh 75.33610227857689
target distance 11.0
model initialize at round 927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'dynamicTrap': False, 'previousTarget': array([22., 17.]), 'currentState': array([11.81879232,  7.3749205 ,  0.69183612]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 14.010679684182994}
done in step count: 15
reward sum = 0.7954419898843544
running average episode reward sum: 0.6803893447826181
{'scaleFactor': 20, 'currentTarget': array([22., 17.]), 'dynamicTrap': False, 'previousTarget': array([22., 17.]), 'currentState': array([22.17367839, 16.2526452 ,  2.32424872]), 'targetState': array([22, 17], dtype=int32), 'currentDistance': 0.7672700856275538}
episode index:928
target Thresh 75.33941348227641
target distance 33.0
model initialize at round 928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.34516445, 16.00260272]), 'dynamicTrap': False, 'previousTarget': array([92.77431008, 16.00389241]), 'currentState': array([74.63402511, 19.38948715,  6.00619466]), 'targetState': array([106,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.14957535235467834
running average episode reward sum: 0.6798179626594448
{'scaleFactor': 20, 'currentTarget': array([106.,  14.]), 'dynamicTrap': False, 'previousTarget': array([106.,  14.]), 'currentState': array([106.174608  ,  14.05415789,   2.7367787 ]), 'targetState': array([106,  14], dtype=int32), 'currentDistance': 0.18281419542882577}
episode index:929
target Thresh 75.3427081712786
target distance 63.0
model initialize at round 929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.47770507, 10.22335033]), 'dynamicTrap': False, 'previousTarget': array([25.97736275, 11.04869701]), 'currentState': array([ 7.48637981, 10.81234462,  5.05895925]), 'targetState': array([69,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5311882027474717
running average episode reward sum: 0.679658145713303
{'scaleFactor': 20, 'currentTarget': array([69.,  9.]), 'dynamicTrap': False, 'previousTarget': array([69.,  9.]), 'currentState': array([68.9722643 ,  9.33978891,  2.65260181]), 'targetState': array([69,  9], dtype=int32), 'currentDistance': 0.34091901263609925}
episode index:930
target Thresh 75.34598642795083
target distance 10.0
model initialize at round 930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  22.]), 'dynamicTrap': False, 'previousTarget': array([118.,  22.]), 'currentState': array([112.73695591,  13.53375378,   0.75360489]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 9.968799229603897}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6799393723552875
{'scaleFactor': 20, 'currentTarget': array([118.,  22.]), 'dynamicTrap': False, 'previousTarget': array([118.,  22.]), 'currentState': array([118.02551781,  22.2152436 ,   2.27879536]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 0.21675092546368452}
episode index:931
target Thresh 75.34924833424968
target distance 64.0
model initialize at round 931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.88517728,  6.05382108]), 'dynamicTrap': False, 'previousTarget': array([54.46199965,  6.27393758]), 'currentState': array([72.36805239,  1.53523975,  2.43273139]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.49872911657752833
running average episode reward sum: 0.6797449407503758
{'scaleFactor': 20, 'currentTarget': array([10., 16.]), 'dynamicTrap': False, 'previousTarget': array([10., 16.]), 'currentState': array([10.5497502 , 16.59812308,  2.31920741]), 'targetState': array([10, 16], dtype=int32), 'currentDistance': 0.812389376295462}
episode index:932
target Thresh 75.352493971723
target distance 9.0
model initialize at round 932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  14.]), 'dynamicTrap': False, 'previousTarget': array([107.,  14.]), 'currentState': array([109.1992872 ,  21.58502636,   3.75563252]), 'targetState': array([107,  14], dtype=int32), 'currentDistance': 7.897435603422016}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6800459601172029
{'scaleFactor': 20, 'currentTarget': array([107.,  14.]), 'dynamicTrap': False, 'previousTarget': array([107.,  14.]), 'currentState': array([107.94184157,  14.85496756,   3.89196938]), 'targetState': array([107,  14], dtype=int32), 'currentDistance': 1.2720200786690918}
episode index:933
target Thresh 75.35572342151191
target distance 54.0
model initialize at round 933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.1558581, 10.8096139]), 'dynamicTrap': False, 'previousTarget': array([76.17596225, 10.68176659]), 'currentState': array([56.95993718,  5.19564739,  1.67807462]), 'targetState': array([111,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5187971697904217
running average episode reward sum: 0.6798733168727417
{'scaleFactor': 20, 'currentTarget': array([111.,  21.]), 'dynamicTrap': False, 'previousTarget': array([111.,  21.]), 'currentState': array([111.23247112,  21.06567684,   1.9764878 ]), 'targetState': array([111,  21], dtype=int32), 'currentDistance': 0.2415704241412563}
episode index:934
target Thresh 75.35893676435278
target distance 4.0
model initialize at round 934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([13.36439298, 18.55832981,  2.9680748 ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 3.3988391343182167}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6801944149295622
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([11.3288928 , 21.35952088,  3.25193453]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.4872635159823114}
episode index:935
target Thresh 75.36213408057938
target distance 31.0
model initialize at round 935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.52053558,  8.34442352]), 'dynamicTrap': False, 'previousTarget': array([19.36440299,  7.80043813]), 'currentState': array([37.22482966,  4.91793919,  2.59569621]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6673050259505081
running average episode reward sum: 0.680180644214841
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 10.]), 'currentState': array([ 7.83619576, 10.42393953,  2.9824341 ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.454484929148443}
episode index:936
target Thresh 75.36531545012477
target distance 54.0
model initialize at round 936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.07394843,  7.20718356]), 'dynamicTrap': False, 'previousTarget': array([66.35993796,  7.01924318]), 'currentState': array([48.75108243,  2.04706245,  0.91564577]), 'targetState': array([101,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6421543502957715
running average episode reward sum: 0.680140061190381
{'scaleFactor': 20, 'currentTarget': array([100.00809504,  14.61875153]), 'dynamicTrap': True, 'previousTarget': array([101.,  16.]), 'currentState': array([99.21751533, 14.54985232,  6.2703655 ]), 'targetState': array([101,  16], dtype=int32), 'currentDistance': 0.7935763222368174}
episode index:937
target Thresh 75.36848095252336
target distance 15.0
model initialize at round 937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67., 15.]), 'dynamicTrap': False, 'previousTarget': array([67., 15.]), 'currentState': array([81.85578402, 21.86164881,  3.83806847]), 'targetState': array([67, 15], dtype=int32), 'currentDistance': 16.363879220720538}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6803791251709976
{'scaleFactor': 20, 'currentTarget': array([67., 15.]), 'dynamicTrap': False, 'previousTarget': array([67., 15.]), 'currentState': array([67.04215776, 15.15193859,  5.6376084 ]), 'targetState': array([67, 15], dtype=int32), 'currentDistance': 0.15767882429021246}
episode index:938
target Thresh 75.37163066691286
target distance 17.0
model initialize at round 938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([46.96210604, 15.37357309,  1.4168061 ]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 17.690427257892818}
done in step count: 11
reward sum = 0.8856352642587164
running average episode reward sum: 0.6805977153084712
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([30.57778435, 23.5497021 ,  3.86202253]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 0.693136675948388}
episode index:939
target Thresh 75.37476467203632
target distance 73.0
model initialize at round 939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.5873599 , 15.17787339]), 'dynamicTrap': False, 'previousTarget': array([88.15028739, 16.44722484]), 'currentState': array([108.3776342 ,  12.28909455,   4.04154301]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.34679786544063484
running average episode reward sum: 0.6795047412863977
{'scaleFactor': 20, 'currentTarget': array([41.22754436, 28.09649362]), 'dynamicTrap': True, 'previousTarget': array([41.03431066, 28.05977759]), 'currentState': array([43.28374698, 23.84500325,  2.36246409]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 4.722619994726889}
episode index:940
target Thresh 75.37788304624401
target distance 31.0
model initialize at round 940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.91292949,  6.16992354]), 'dynamicTrap': False, 'previousTarget': array([96.74482241,  6.18464878]), 'currentState': array([77.17994664,  2.912718  ,  5.84057826]), 'targetState': array([108,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.809219479469541
running average episode reward sum: 0.6796425890421716
{'scaleFactor': 20, 'currentTarget': array([108.,   8.]), 'dynamicTrap': False, 'previousTarget': array([108.,   8.]), 'currentState': array([107.15092914,   7.9651235 ,   0.99251244]), 'targetState': array([108,   8], dtype=int32), 'currentDistance': 0.849786850693939}
episode index:941
target Thresh 75.38098586749544
target distance 61.0
model initialize at round 941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.28201141, 10.57046345]), 'dynamicTrap': False, 'previousTarget': array([69.00268691, 10.3278248 ]), 'currentState': array([87.28081584, 10.35178276,  2.68252683]), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.48708417903907053
running average episode reward sum: 0.6794381745941852
{'scaleFactor': 20, 'currentTarget': array([28., 11.]), 'dynamicTrap': False, 'previousTarget': array([28., 11.]), 'currentState': array([28.69181378, 10.8127544 ,  5.15918034]), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 0.7167058112867942}
episode index:942
target Thresh 75.38407321336133
target distance 10.0
model initialize at round 942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([107.66462413,  23.81564391,   2.43876576]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 11.932326445145325}
done in step count: 18
reward sum = 0.7869594558780076
running average episode reward sum: 0.6795521950409337
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([106.85612427,  11.88332931,   5.42229333]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 0.8640375074729516}
episode index:943
target Thresh 75.38714516102549
target distance 52.0
model initialize at round 943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.18130961,  9.23966969]), 'dynamicTrap': False, 'previousTarget': array([25.56699406,  8.13917182]), 'currentState': array([6.50120801, 5.67686166, 0.38869286]), 'targetState': array([58, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5609557729582328
running average episode reward sum: 0.6794265632378799
{'scaleFactor': 20, 'currentTarget': array([58., 15.]), 'dynamicTrap': False, 'previousTarget': array([58., 15.]), 'currentState': array([58.99116088, 14.15937312,  1.56605968]), 'targetState': array([58, 15], dtype=int32), 'currentDistance': 1.299635883616493}
episode index:944
target Thresh 75.39020178728673
target distance 23.0
model initialize at round 944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.95048477,  9.59353019]), 'dynamicTrap': True, 'previousTarget': array([44.83200822,  8.41321632]), 'currentState': array([25.       , 11.       ,  3.0634246], dtype=float32), 'targetState': array([48,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8230431933839267
running average episode reward sum: 0.6795785385078756
{'scaleFactor': 20, 'currentTarget': array([48.,  8.]), 'dynamicTrap': False, 'previousTarget': array([48.,  8.]), 'currentState': array([48.27851841,  8.51216035,  0.27656167]), 'targetState': array([48,  8], dtype=int32), 'currentDistance': 0.5829929070732868}
episode index:945
target Thresh 75.3932431685609
target distance 25.0
model initialize at round 945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.56601122, 10.031868  ]), 'dynamicTrap': False, 'previousTarget': array([25.03046115, 10.34537865]), 'currentState': array([ 6.04695582, 17.5846547 ,  5.08999527]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.679724763031226
{'scaleFactor': 20, 'currentTarget': array([32.,  7.]), 'dynamicTrap': False, 'previousTarget': array([32.,  7.]), 'currentState': array([32.44063008,  7.00562901,  5.0367451 ]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 0.4406660293178792}
episode index:946
target Thresh 75.39626938088269
target distance 47.0
model initialize at round 946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.4074337 , 18.97342264]), 'dynamicTrap': False, 'previousTarget': array([74.0045254 , 18.57456437]), 'currentState': array([92.39277135, 19.73911115,  3.38177884]), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6798039547146183
{'scaleFactor': 20, 'currentTarget': array([47., 18.]), 'dynamicTrap': False, 'previousTarget': array([47., 18.]), 'currentState': array([46.14527258, 17.59774408,  5.00093675]), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 0.9446527349048613}
episode index:947
target Thresh 75.39928049990755
target distance 45.0
model initialize at round 947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.98495115, 11.90752328]), 'dynamicTrap': False, 'previousTarget': array([30.30874984, 11.49933331]), 'currentState': array([48.57900748, 15.91663244,  3.43675661]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6798829793269485
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'dynamicTrap': False, 'previousTarget': array([5., 7.]), 'currentState': array([4.07359865, 6.89684144, 4.55241059]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.9321272196123633}
episode index:948
target Thresh 75.40227660091362
target distance 13.0
model initialize at round 948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94., 23.]), 'dynamicTrap': False, 'previousTarget': array([94., 23.]), 'currentState': array([89.98022887, 11.65461424,  0.57274485]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 12.036458698866143}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6801487141726599
{'scaleFactor': 20, 'currentTarget': array([94., 23.]), 'dynamicTrap': False, 'previousTarget': array([94., 23.]), 'currentState': array([93.60683992, 22.16153882,  0.62209296]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 0.9260626325543623}
episode index:949
target Thresh 75.40525775880359
target distance 3.0
model initialize at round 949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 18.]), 'dynamicTrap': False, 'previousTarget': array([88., 18.]), 'currentState': array([89.49603745, 14.50842221,  2.51331246]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 3.798584405351929}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6804541355261623
{'scaleFactor': 20, 'currentTarget': array([88., 18.]), 'dynamicTrap': False, 'previousTarget': array([88., 18.]), 'currentState': array([87.90202243, 17.3987778 ,  4.15036106]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 0.609153300963993}
episode index:950
target Thresh 75.40822404810655
target distance 18.0
model initialize at round 950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.14444109,  16.13982978]), 'dynamicTrap': True, 'previousTarget': array([114.14213562,  16.14213562]), 'currentState': array([100.       ,   2.       ,   0.8508267], dtype=float32), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7737142836436554
running average episode reward sum: 0.6805522008764436
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([118.36311149,  19.73791499,   5.98981374]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 0.4478152589222527}
episode index:951
target Thresh 75.41117554297989
target distance 40.0
model initialize at round 951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.69743077,  6.68259049]), 'dynamicTrap': False, 'previousTarget': array([27.9007438 ,  6.00992562]), 'currentState': array([8.88782005, 9.43564749, 0.07414138]), 'targetState': array([48,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7495760039270668
running average episode reward sum: 0.6806247048712446
{'scaleFactor': 20, 'currentTarget': array([48.,  4.]), 'dynamicTrap': False, 'previousTarget': array([48.,  4.]), 'currentState': array([47.47699601,  3.55889287,  1.73208852]), 'targetState': array([48,  4], dtype=int32), 'currentDistance': 0.6841846808436932}
episode index:952
target Thresh 75.41411231721113
target distance 62.0
model initialize at round 952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.60818484, 13.18277044]), 'dynamicTrap': False, 'previousTarget': array([69.12626552, 12.24380873]), 'currentState': array([87.52040337, 11.31099239,  2.68322843]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5534993874521881
running average episode reward sum: 0.6804913099946245
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([26.52047823, 17.19457966,  3.4108012 ]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.5174962591038507}
episode index:953
target Thresh 75.4170344442198
target distance 19.0
model initialize at round 953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.95788587, 16.50656625]), 'dynamicTrap': False, 'previousTarget': array([55.85786438, 17.14213562]), 'currentState': array([71.66152106,  2.94916561,  0.97063833]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 26
reward sum = 0.727628463590349
running average episode reward sum: 0.6805407200088758
{'scaleFactor': 20, 'currentTarget': array([51., 22.]), 'dynamicTrap': False, 'previousTarget': array([51., 22.]), 'currentState': array([51.89599434, 22.3791073 ,  2.2439993 ]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 0.972896813004328}
episode index:954
target Thresh 75.4199419970592
target distance 6.0
model initialize at round 954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 21.]), 'dynamicTrap': False, 'previousTarget': array([48., 21.]), 'currentState': array([54.57505981, 14.66562091,  0.71418017]), 'targetState': array([48, 21], dtype=int32), 'currentDistance': 9.129938114023474}
done in step count: 7
reward sum = 0.9221653479069899
running average episode reward sum: 0.6807937300904445
{'scaleFactor': 20, 'currentTarget': array([48., 21.]), 'dynamicTrap': False, 'previousTarget': array([48., 21.]), 'currentState': array([47.75373616, 21.68182238,  2.95220563]), 'targetState': array([48, 21], dtype=int32), 'currentDistance': 0.7249328523253443}
episode index:955
target Thresh 75.4228350484183
target distance 9.0
model initialize at round 955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.,  8.]), 'dynamicTrap': False, 'previousTarget': array([81.,  8.]), 'currentState': array([89.20469576,  9.42837167,  2.94299114]), 'targetState': array([81,  8], dtype=int32), 'currentDistance': 8.328101713979017}
done in step count: 7
reward sum = 0.9224593878069899
running average episode reward sum: 0.6810465184353363
{'scaleFactor': 20, 'currentTarget': array([81.,  8.]), 'dynamicTrap': False, 'previousTarget': array([81.,  8.]), 'currentState': array([80.47698624,  8.46739985,  4.70965203]), 'targetState': array([81,  8], dtype=int32), 'currentDistance': 0.7014314082807525}
episode index:956
target Thresh 75.42571367062358
target distance 54.0
model initialize at round 956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.59655293,  8.76765612]), 'dynamicTrap': False, 'previousTarget': array([63.21593075,  7.93097322]), 'currentState': array([82.43971881,  6.26790259,  2.8819108 ]), 'targetState': array([29, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5162159974931224
running average episode reward sum: 0.680874281736337
{'scaleFactor': 20, 'currentTarget': array([29., 13.]), 'dynamicTrap': False, 'previousTarget': array([29., 13.]), 'currentState': array([29.26135393, 13.53111803,  0.5182044 ]), 'targetState': array([29, 13], dtype=int32), 'currentDistance': 0.591939390737144}
episode index:957
target Thresh 75.4285779356407
target distance 55.0
model initialize at round 957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.03944478, 14.2894016 ]), 'dynamicTrap': False, 'previousTarget': array([84.38838649, 15.0776773 ]), 'currentState': array([102.70655217,  17.92325454,   2.82581615]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5990015354483653
running average episode reward sum: 0.6807888195794601
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([49.55806372,  8.91141672,  5.83978094]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 1.0686980679809857}
episode index:958
target Thresh 75.43142791507645
target distance 17.0
model initialize at round 958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,   3.]), 'dynamicTrap': False, 'previousTarget': array([107.,   3.]), 'currentState': array([91.45357761,  2.72859745,  5.73782784]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 15.54879122632057}
done in step count: 11
reward sum = 0.8855372542587164
running average episode reward sum: 0.6810023215968525
{'scaleFactor': 20, 'currentTarget': array([107.,   3.]), 'dynamicTrap': False, 'previousTarget': array([107.,   3.]), 'currentState': array([106.87833113,   2.69357195,   1.50194651]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 0.32969905217319406}
episode index:959
target Thresh 75.43426368018048
target distance 6.0
model initialize at round 959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62., 11.]), 'dynamicTrap': False, 'previousTarget': array([62., 11.]), 'currentState': array([63.3094457 ,  6.03493376,  1.63639587]), 'targetState': array([62, 11], dtype=int32), 'currentDistance': 5.1348350357099894}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6813036723035224
{'scaleFactor': 20, 'currentTarget': array([62., 11.]), 'dynamicTrap': False, 'previousTarget': array([62., 11.]), 'currentState': array([61.87910726, 10.544784  ,  2.24357605]), 'targetState': array([62, 11], dtype=int32), 'currentDistance': 0.4709953944264981}
episode index:960
target Thresh 75.43708530184703
target distance 49.0
model initialize at round 960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.22194614, 13.83092502]), 'dynamicTrap': False, 'previousTarget': array([32.01663894, 14.81564739]), 'currentState': array([52.16707541, 12.35044591,  3.80332994]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5984952851037821
running average episode reward sum: 0.6812175033262073
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.67639558, 16.23443184,  2.96693489]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.7158695896000142}
episode index:961
target Thresh 75.43989285061681
target distance 3.0
model initialize at round 961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([ 8.43149624, 17.33289305,  4.02299535]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 2.8937545608498634}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6815281919921884
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([10.7736994 , 16.21876164,  0.79362255]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.31475167651103697}
episode index:962
target Thresh 75.44268639667868
target distance 2.0
model initialize at round 962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([3.03709407, 8.93303793, 4.38033426]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 2.1733751356241786}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6818080070055922
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([5.6341957 , 8.32938613, 4.01037591]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.7146323626392956}
episode index:963
target Thresh 75.44546600987145
target distance 69.0
model initialize at round 963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.36446288,  8.57052658]), 'dynamicTrap': False, 'previousTarget': array([52.99789993,  9.71017536]), 'currentState': array([33.3652426 ,  8.39392457,  5.75915647]), 'targetState': array([102,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.3494537516935885
running average episode reward sum: 0.6814632411805798
{'scaleFactor': 20, 'currentTarget': array([102.,   9.]), 'dynamicTrap': False, 'previousTarget': array([102.,   9.]), 'currentState': array([101.57543016,   9.67164542,   5.25274053]), 'targetState': array([102,   9], dtype=int32), 'currentDistance': 0.7945861241208267}
episode index:964
target Thresh 75.44823175968558
target distance 60.0
model initialize at round 964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.99893754, 15.20614863]), 'dynamicTrap': True, 'previousTarget': array([38.98889814, 15.6662966 ]), 'currentState': array([19.       , 15.       ,  2.2352688], dtype=float32), 'targetState': array([79, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.32490211249724
running average episode reward sum: 0.6810937477829805
{'scaleFactor': 20, 'currentTarget': array([79., 17.]), 'dynamicTrap': False, 'previousTarget': array([79., 17.]), 'currentState': array([79.46030601, 16.64462426,  4.57837706]), 'targetState': array([79, 17], dtype=int32), 'currentDistance': 0.5815268999731225}
episode index:965
target Thresh 75.45098371526498
target distance 10.0
model initialize at round 965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82., 14.]), 'dynamicTrap': False, 'previousTarget': array([82., 14.]), 'currentState': array([72.54549997, 13.40788886,  6.05248022]), 'targetState': array([82, 14], dtype=int32), 'currentDistance': 9.473023088609066}
done in step count: 18
reward sum = 0.725874603271963
running average episode reward sum: 0.6811401047762403
{'scaleFactor': 20, 'currentTarget': array([82., 14.]), 'dynamicTrap': False, 'previousTarget': array([81.96697489, 13.76011313]), 'currentState': array([81.26779885, 13.14559772,  1.69475215]), 'targetState': array([82, 14], dtype=int32), 'currentDistance': 1.1252207655019428}
episode index:966
target Thresh 75.45372194540865
target distance 49.0
model initialize at round 966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.02895029, 14.15322183]), 'dynamicTrap': False, 'previousTarget': array([68.6170994 , 15.12161403]), 'currentState': array([87.65147122, 21.44717234,  4.1012696 ]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5362315570132615
running average episode reward sum: 0.6809902510556995
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([38.55278532,  2.01708281,  3.26472357]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 0.5530492162638346}
episode index:967
target Thresh 75.45644651857249
target distance 14.0
model initialize at round 967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   6.]), 'dynamicTrap': False, 'previousTarget': array([118.,   6.]), 'currentState': array([113.2473106 ,  18.14826983,   4.38535326]), 'targetState': array([118,   6], dtype=int32), 'currentDistance': 13.044865520354609}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6812399973814973
{'scaleFactor': 20, 'currentTarget': array([118.,   6.]), 'dynamicTrap': False, 'previousTarget': array([118.,   6.]), 'currentState': array([117.80549851,   6.74450159,   1.65130851]), 'targetState': array([118,   6], dtype=int32), 'currentDistance': 0.7694890790003374}
episode index:968
target Thresh 75.45915750287101
target distance 58.0
model initialize at round 968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.02332945,  3.9657296 ]), 'dynamicTrap': True, 'previousTarget': array([84.0267003 ,  4.03310171]), 'currentState': array([104.       ,   3.       ,   1.3472395], dtype=float32), 'targetState': array([46,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5014558897538796
running average episode reward sum: 0.6810544616667112
{'scaleFactor': 20, 'currentTarget': array([46.,  6.]), 'dynamicTrap': False, 'previousTarget': array([46.,  6.]), 'currentState': array([46.69366353,  6.26307747,  4.96857436]), 'targetState': array([46,  6], dtype=int32), 'currentDistance': 0.741875220964324}
episode index:969
target Thresh 75.46185496607892
target distance 52.0
model initialize at round 969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.48133199, 15.75421022]), 'dynamicTrap': False, 'previousTarget': array([22.78529848, 16.13614094]), 'currentState': array([ 5.74201724, 22.74263514,  5.26502473]), 'targetState': array([56,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5596865383102779
running average episode reward sum: 0.6809293400962407
{'scaleFactor': 20, 'currentTarget': array([56.,  4.]), 'dynamicTrap': False, 'previousTarget': array([56.,  4.]), 'currentState': array([56.55396691,  3.92810627,  1.14957666]), 'targetState': array([56,  4], dtype=int32), 'currentDistance': 0.5586126096502605}
episode index:970
target Thresh 75.46453897563296
target distance 55.0
model initialize at round 970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.7296686, 12.5291668]), 'dynamicTrap': False, 'previousTarget': array([51.94731634, 12.45071392]), 'currentState': array([33.78459494, 11.04793869,  5.64771545]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 49
reward sum = 0.49261614570028767
running average episode reward sum: 0.680735402717872
{'scaleFactor': 20, 'currentTarget': array([87., 15.]), 'dynamicTrap': False, 'previousTarget': array([87., 15.]), 'currentState': array([86.20634536, 14.33779348,  1.50626108]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 1.0336368660403454}
episode index:971
target Thresh 75.46720959863349
target distance 42.0
model initialize at round 971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.49683806, 13.11463992]), 'dynamicTrap': False, 'previousTarget': array([70.10571781, 14.08632544]), 'currentState': array([52.25949249, 18.58497222,  5.1015715 ]), 'targetState': array([93,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5333489227717814
running average episode reward sum: 0.6794863447698374
{'scaleFactor': 20, 'currentTarget': array([76.45136989, 14.08449523]), 'dynamicTrap': True, 'previousTarget': array([76.44548858, 14.06606929]), 'currentState': array([57.39545474, 20.15673517,  4.77529419]), 'targetState': array([93,  7], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:972
target Thresh 75.46986690184625
target distance 43.0
model initialize at round 972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.2157978 ,  9.06992432]), 'dynamicTrap': True, 'previousTarget': array([74.2598546 ,  8.78648796]), 'currentState': array([94.       , 12.       ,  6.0194054], dtype=float32), 'targetState': array([51,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6717672806941826
running average episode reward sum: 0.6794784115076836
{'scaleFactor': 20, 'currentTarget': array([51.,  5.]), 'dynamicTrap': False, 'previousTarget': array([51.,  5.]), 'currentState': array([51.41494016,  5.61723269,  2.70089796]), 'targetState': array([51,  5], dtype=int32), 'currentDistance': 0.7437415710269238}
episode index:973
target Thresh 75.47251095170394
target distance 37.0
model initialize at round 973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.42542308, 10.97421344]), 'dynamicTrap': False, 'previousTarget': array([33.64285928,  9.93822301]), 'currentState': array([50.97447171,  3.49539035,  2.49032414]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.620221255144178
running average episode reward sum: 0.6794175725381113
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'dynamicTrap': False, 'previousTarget': array([15., 18.]), 'currentState': array([15.9998649 , 18.68350517,  4.71294242]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 1.2111602433349207}
episode index:974
target Thresh 75.47514181430795
target distance 65.0
model initialize at round 974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.19691382, 11.94807065]), 'dynamicTrap': False, 'previousTarget': array([67.88502281, 11.14146399]), 'currentState': array([49.28193714, 10.1058699 ,  6.00194461]), 'targetState': array([113,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.40233283953143845
running average episode reward sum: 0.6783080849359886
{'scaleFactor': 20, 'currentTarget': array([107.89974976,   9.30785695]), 'dynamicTrap': True, 'previousTarget': array([107.79871457,   9.48043888]), 'currentState': array([93.60772627,  4.85798909,  6.20983186]), 'targetState': array([113,  16], dtype=int32), 'currentDistance': 14.968742751315832}
episode index:975
target Thresh 75.47775955542998
target distance 25.0
model initialize at round 975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.71727671,  9.363186  ]), 'dynamicTrap': False, 'previousTarget': array([54.15457199, 10.21892607]), 'currentState': array([69.48962104, 20.25760962,  2.58843803]), 'targetState': array([46,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6784681317356958
{'scaleFactor': 20, 'currentTarget': array([46.,  5.]), 'dynamicTrap': False, 'previousTarget': array([46.,  5.]), 'currentState': array([46.55169703,  4.79191347,  2.50979821]), 'targetState': array([46,  5], dtype=int32), 'currentDistance': 0.5896351556636203}
episode index:976
target Thresh 75.4803642405137
target distance 36.0
model initialize at round 976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.02127884, 14.32627479]), 'dynamicTrap': True, 'previousTarget': array([77.68769506, 13.66482761]), 'currentState': array([60.     , 23.     ,  3.52996], dtype=float32), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.600795293058378
running average episode reward sum: 0.678388630365504
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([96.08070563,  3.42788846,  6.18743263]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 0.57777592325694}
episode index:977
target Thresh 75.48295593467635
target distance 44.0
model initialize at round 977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.89857211, 20.47362374]), 'dynamicTrap': False, 'previousTarget': array([67.95367385, 21.36047776]), 'currentState': array([49.01710643, 18.29938311,  5.48925382]), 'targetState': array([92, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.562437247618349
running average episode reward sum: 0.6782700706694436
{'scaleFactor': 20, 'currentTarget': array([92., 23.]), 'dynamicTrap': False, 'previousTarget': array([92., 23.]), 'currentState': array([91.92302462, 22.93458264,  1.59082055]), 'targetState': array([92, 23], dtype=int32), 'currentDistance': 0.10101802102767667}
episode index:978
target Thresh 75.48553470271047
target distance 27.0
model initialize at round 978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.3693659 , 14.35093442]), 'dynamicTrap': False, 'previousTarget': array([19.9246568 , 13.65626539]), 'currentState': array([4.63526738, 3.39785508, 5.51184506]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.700412498196338
running average episode reward sum: 0.6782926880622188
{'scaleFactor': 20, 'currentTarget': array([30., 20.]), 'dynamicTrap': False, 'previousTarget': array([30., 20.]), 'currentState': array([30.37944055, 20.980316  ,  0.93721576]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 1.051187232551646}
episode index:979
target Thresh 75.48810060908534
target distance 62.0
model initialize at round 979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.3843903, 16.478992 ]), 'dynamicTrap': False, 'previousTarget': array([73.79255473, 15.87311278]), 'currentState': array([55.56664661, 13.78510355,  1.30698686]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.44483003658865455
running average episode reward sum: 0.6771466444656362
{'scaleFactor': 20, 'currentTarget': array([98.24776148, 23.12436009]), 'dynamicTrap': True, 'previousTarget': array([98.25073863, 23.21168594]), 'currentState': array([78.26091055, 23.84947397,  0.97303046]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 19.999999999999996}
episode index:980
target Thresh 75.49065371794877
target distance 2.0
model initialize at round 980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([72.96239169,  1.14738488,  4.6360904 ]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 3.5056041776516986}
done in step count: 4
reward sum = 0.95079501
running average episode reward sum: 0.6774255928504826
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([75.57616512,  4.49210337,  0.95349346]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 0.7577149698918313}
episode index:981
target Thresh 75.49319409312864
target distance 66.0
model initialize at round 981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.52644934,  8.3040527 ]), 'dynamicTrap': False, 'previousTarget': array([44.72787848,  7.28797975]), 'currentState': array([25.73980942,  5.39048435,  0.07183743]), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6023979805871281
running average episode reward sum: 0.6773491899866707
{'scaleFactor': 20, 'currentTarget': array([91., 15.]), 'dynamicTrap': False, 'previousTarget': array([91., 15.]), 'currentState': array([91.17587392, 14.84902535,  0.6114182 ]), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 0.23178649471551838}
episode index:982
target Thresh 75.49572179813443
target distance 32.0
model initialize at round 982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.92321217,  5.1777815 ]), 'dynamicTrap': False, 'previousTarget': array([30.2530188 ,  5.58508846]), 'currentState': array([12.69869749, 10.69303629,  0.01670035]), 'targetState': array([43,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7143080622540936
running average episode reward sum: 0.6773867880255999
{'scaleFactor': 20, 'currentTarget': array([43.,  2.]), 'dynamicTrap': False, 'previousTarget': array([43.,  2.]), 'currentState': array([42.86839052,  1.71983836,  1.04667515]), 'targetState': array([43,  2], dtype=int32), 'currentDistance': 0.3095344900824376}
episode index:983
target Thresh 75.49823689615891
target distance 28.0
model initialize at round 983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.7380231 ,   4.15787588]), 'dynamicTrap': False, 'previousTarget': array([98.88618308,  4.13066247]), 'currentState': array([80.91646086,  1.49222956,  5.79302925]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6775724298615915
{'scaleFactor': 20, 'currentTarget': array([107.,   5.]), 'dynamicTrap': False, 'previousTarget': array([107.,   5.]), 'currentState': array([107.48731479,   5.18296225,   1.12521332]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 0.5205294273979006}
episode index:984
target Thresh 75.50073945007966
target distance 31.0
model initialize at round 984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.06184459, 21.0959624 ]), 'dynamicTrap': False, 'previousTarget': array([48.01039771, 21.64482588]), 'currentState': array([67.99538633, 19.46687866,  3.75337029]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7896615821251509
running average episode reward sum: 0.6776862259552601
{'scaleFactor': 20, 'currentTarget': array([37., 22.]), 'dynamicTrap': False, 'previousTarget': array([37., 22.]), 'currentState': array([37.03186515, 22.99886832,  3.41683513]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 0.9993764574645322}
episode index:985
target Thresh 75.50322952246066
target distance 64.0
model initialize at round 985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.382813  , 19.82938398]), 'dynamicTrap': False, 'previousTarget': array([73.03894843, 19.24756572]), 'currentState': array([91.3566352 , 18.80643398,  1.95582509]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6707136308959978
running average episode reward sum: 0.6776791543578369
{'scaleFactor': 20, 'currentTarget': array([29., 22.]), 'dynamicTrap': False, 'previousTarget': array([29., 22.]), 'currentState': array([29.20044459, 22.75974504,  2.61795263]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 0.7857420459626614}
episode index:986
target Thresh 75.50570717555385
target distance 43.0
model initialize at round 986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.72042137, 18.72309796]), 'dynamicTrap': False, 'previousTarget': array([23.48016054, 18.46973011]), 'currentState': array([ 2.21405897, 23.13918513,  3.7532692 ]), 'targetState': array([47, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3628161937940035
running average episode reward sum: 0.6773601442660802
{'scaleFactor': 20, 'currentTarget': array([47., 13.]), 'dynamicTrap': False, 'previousTarget': array([47., 13.]), 'currentState': array([47.7331494 , 12.9468522 ,  0.22142713]), 'targetState': array([47, 13], dtype=int32), 'currentDistance': 0.7350732877539002}
episode index:987
target Thresh 75.50817247130068
target distance 38.0
model initialize at round 987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.84154383, 20.53138827]), 'dynamicTrap': False, 'previousTarget': array([38.99307839, 20.47386636]), 'currentState': array([18.85010212, 21.11641696,  2.39302706]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6409748288557705
running average episode reward sum: 0.677323317023762
{'scaleFactor': 20, 'currentTarget': array([57., 20.]), 'dynamicTrap': False, 'previousTarget': array([57., 20.]), 'currentState': array([56.88606726, 19.64321443,  4.66000846]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 0.37453519227957854}
episode index:988
target Thresh 75.51062547133368
target distance 8.0
model initialize at round 988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([84.39022256,  5.40935119,  1.04221761]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 7.514462382192852}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6776097403735863
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([87.21392625, 12.14413277,  2.63990358]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 0.7991784546088109}
episode index:989
target Thresh 75.51306623697798
target distance 60.0
model initialize at round 989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.07820989, 17.32661796]), 'dynamicTrap': False, 'previousTarget': array([67.8434743 , 17.25304229]), 'currentState': array([85.15180608, 23.34308938,  3.57025945]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5370776891107506
running average episode reward sum: 0.6774677888066541
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([27.36305705,  4.03785473,  3.96282337]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.0283646888025746}
episode index:990
target Thresh 75.51549482925283
target distance 41.0
model initialize at round 990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.86547043,  7.53273582]), 'dynamicTrap': False, 'previousTarget': array([49.56959322,  6.7391236 ]), 'currentState': array([68.42226885,  3.34565624,  2.92686188]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.43139475373157027
running average episode reward sum: 0.6772194810013311
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([28.39314379, 11.46744916,  5.2725838 ]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 0.66194594738562}
episode index:991
target Thresh 75.5179113088732
target distance 28.0
model initialize at round 991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.56027467, 16.79060675]), 'dynamicTrap': True, 'previousTarget': array([88.63513716, 16.92277877]), 'currentState': array([106.      ,   7.      ,   3.327673], dtype=float32), 'targetState': array([78, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7800268682212584
running average episode reward sum: 0.6773231174803835
{'scaleFactor': 20, 'currentTarget': array([78., 23.]), 'dynamicTrap': False, 'previousTarget': array([78., 23.]), 'currentState': array([78.98826961, 23.36295066,  3.63913938]), 'targetState': array([78, 23], dtype=int32), 'currentDistance': 1.0528105252452482}
episode index:992
target Thresh 75.52031573625116
target distance 74.0
model initialize at round 992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.60098533,  8.86714459]), 'dynamicTrap': False, 'previousTarget': array([51.97084547, 10.07950516]), 'currentState': array([31.65845561,  7.35205112,  5.51802802]), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.576503537798897
running average episode reward sum: 0.67722158718866
{'scaleFactor': 20, 'currentTarget': array([106.,  13.]), 'dynamicTrap': False, 'previousTarget': array([106.,  13.]), 'currentState': array([106.23875423,  13.75811538,   5.73988788]), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 0.7948223161179694}
episode index:993
target Thresh 75.52270817149754
target distance 39.0
model initialize at round 993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.87433807,  3.4262468 ]), 'dynamicTrap': False, 'previousTarget': array([55.99342862,  3.48734798]), 'currentState': array([35.87930316,  3.871869  ,  4.05785866]), 'targetState': array([75,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7845704628935674
running average episode reward sum: 0.677329584045506
{'scaleFactor': 20, 'currentTarget': array([75.,  3.]), 'dynamicTrap': False, 'previousTarget': array([75.,  3.]), 'currentState': array([74.50837139,  2.83755429,  1.29495087]), 'targetState': array([75,  3], dtype=int32), 'currentDistance': 0.5177714720679928}
episode index:994
target Thresh 75.52508867442337
target distance 62.0
model initialize at round 994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.59925636, 12.04094808]), 'dynamicTrap': False, 'previousTarget': array([29.98960229, 11.35517412]), 'currentState': array([ 8.62133387, 12.98042283,  3.50235927]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5855987697257732
running average episode reward sum: 0.6772373922723203
{'scaleFactor': 20, 'currentTarget': array([72., 10.]), 'dynamicTrap': False, 'previousTarget': array([72., 10.]), 'currentState': array([71.89185078,  9.40326685,  2.22743784]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 0.6064542117611482}
episode index:995
target Thresh 75.52745730454131
target distance 16.0
model initialize at round 995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 14.]), 'currentState': array([25.07985273, 14.43889399,  3.9400388 ]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 16.08584134562555}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6774037635585769
{'scaleFactor': 20, 'currentTarget': array([ 9., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 14.]), 'currentState': array([ 9.73516646, 13.68199832,  2.03141728]), 'targetState': array([ 9, 14], dtype=int32), 'currentDistance': 0.800996129773762}
episode index:996
target Thresh 75.52981412106726
target distance 49.0
model initialize at round 996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.1324247, 12.4686127]), 'dynamicTrap': False, 'previousTarget': array([34.40391882, 12.99920024]), 'currentState': array([52.61427018,  7.94559416,  2.88398457]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5539938115886553
running average episode reward sum: 0.6772799822627193
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.29485248, 19.23214419,  5.25908714]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.7423772306559047}
episode index:997
target Thresh 75.53215918292175
target distance 38.0
model initialize at round 997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.81498679,  9.85016699]), 'dynamicTrap': False, 'previousTarget': array([51.5709957 ,  9.87979038]), 'currentState': array([30.16918263, 13.59748318,  3.98363626]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6205491647439126
running average episode reward sum: 0.6772231377561875
{'scaleFactor': 20, 'currentTarget': array([70.,  6.]), 'dynamicTrap': False, 'previousTarget': array([70.,  6.]), 'currentState': array([70.21432996,  6.09514315,  0.27997755]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 0.2344985131574332}
episode index:998
target Thresh 75.53449254873142
target distance 21.0
model initialize at round 998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.24656069, 19.70215199]), 'dynamicTrap': False, 'previousTarget': array([23.97366596, 19.32455532]), 'currentState': array([ 6.64714928, 12.34947253,  5.81864026]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8241417691331727
running average episode reward sum: 0.6773702034532615
{'scaleFactor': 20, 'currentTarget': array([26., 20.]), 'dynamicTrap': False, 'previousTarget': array([26., 20.]), 'currentState': array([26.17580869, 20.56257795,  0.32751786]), 'targetState': array([26, 20], dtype=int32), 'currentDistance': 0.589408721707729}
episode index:999
target Thresh 75.5368142768306
target distance 17.0
model initialize at round 999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.45105185, 16.00254637]), 'dynamicTrap': False, 'previousTarget': array([62.43600015, 16.70729398]), 'currentState': array([78.55065809,  2.88759721,  0.84045773]), 'targetState': array([60, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.3634781224751025
running average episode reward sum: 0.676329355127333
{'scaleFactor': 20, 'currentTarget': array([58.3825472, 18.1816785]), 'dynamicTrap': True, 'previousTarget': array([58.52208287, 18.08920849]), 'currentState': array([52.17213409, 19.8277711 ,  1.56286644]), 'targetState': array([60, 19], dtype=int32), 'currentDistance': 6.424862003046611}
episode index:1000
target Thresh 75.53912442526256
target distance 45.0
model initialize at round 1000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.46713558, 13.43078293]), 'dynamicTrap': False, 'previousTarget': array([92.69125016, 13.49933331]), 'currentState': array([74.8124766 , 17.13137483,  6.24665262]), 'targetState': array([118,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.645262389037053
running average episode reward sum: 0.676298319197173
{'scaleFactor': 20, 'currentTarget': array([118.,   9.]), 'dynamicTrap': False, 'previousTarget': array([118.,   9.]), 'currentState': array([118.32564003,   8.95842546,   6.16662907]), 'targetState': array([118,   9], dtype=int32), 'currentDistance': 0.3282832157451784}
episode index:1001
target Thresh 75.54142305178114
target distance 54.0
model initialize at round 1001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.30933455, 15.95007676]), 'dynamicTrap': False, 'previousTarget': array([27.96920706, 16.89059961]), 'currentState': array([ 7.31683086, 16.49761348,  5.28374839]), 'targetState': array([62, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.36352036078826727
running average episode reward sum: 0.6759861655460663
{'scaleFactor': 20, 'currentTarget': array([62., 15.]), 'dynamicTrap': False, 'previousTarget': array([62., 15.]), 'currentState': array([61.40026379, 15.52568064,  6.24062845]), 'targetState': array([62, 15], dtype=int32), 'currentDistance': 0.79751091112411}
episode index:1002
target Thresh 75.54371021385214
target distance 68.0
model initialize at round 1002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.28081494, 20.39422497]), 'dynamicTrap': False, 'previousTarget': array([67.077403  , 20.24212379]), 'currentState': array([85.19127057, 22.28466319,  2.25685704]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.641519211351425
running average episode reward sum: 0.6759518016834595
{'scaleFactor': 20, 'currentTarget': array([19.10568183, 17.33547989]), 'dynamicTrap': True, 'previousTarget': array([19., 16.]), 'currentState': array([19.4044637 , 17.69303631,  3.64341957]), 'targetState': array([19, 16], dtype=int32), 'currentDistance': 0.4659583708054391}
episode index:1003
target Thresh 75.54598596865472
target distance 44.0
model initialize at round 1003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.40473383,  8.1622016 ]), 'dynamicTrap': False, 'previousTarget': array([43.91786413,  8.81071492]), 'currentState': array([24.54783157,  5.77401554,  6.14262056]), 'targetState': array([68, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7067587764631965
running average episode reward sum: 0.6759824859212878
{'scaleFactor': 20, 'currentTarget': array([68., 11.]), 'dynamicTrap': False, 'previousTarget': array([68., 11.]), 'currentState': array([68.57151426, 11.27151517,  2.72717889]), 'targetState': array([68, 11], dtype=int32), 'currentDistance': 0.6327314099794337}
episode index:1004
target Thresh 75.54825037308287
target distance 16.0
model initialize at round 1004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,  22.]), 'dynamicTrap': False, 'previousTarget': array([102.,  22.]), 'currentState': array([90.57972753,  7.56629514,  0.24276161]), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 18.405283465045258}
done in step count: 15
reward sum = 0.8132572902490929
running average episode reward sum: 0.6761190777663901
{'scaleFactor': 20, 'currentTarget': array([102.,  22.]), 'dynamicTrap': False, 'previousTarget': array([102.,  22.]), 'currentState': array([101.56816154,  21.97461265,   1.46746828]), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 0.43258406261380045}
episode index:1005
target Thresh 75.55050348374682
target distance 25.0
model initialize at round 1005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.34969788,  8.43236791]), 'dynamicTrap': False, 'previousTarget': array([11.95151706,  8.90448546]), 'currentState': array([29.34620824, 14.68797294,  2.31807423]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.676284906907163
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'dynamicTrap': False, 'previousTarget': array([6., 7.]), 'currentState': array([5.67965538, 6.72949884, 4.70877664]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.41927503396108123}
episode index:1006
target Thresh 75.55274535697444
target distance 45.0
model initialize at round 1006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.23646039, 15.59425575]), 'dynamicTrap': False, 'previousTarget': array([33.17544199, 15.6432744 ]), 'currentState': array([51.02503614, 12.69386443,  3.22874725]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.742080243887796
running average episode reward sum: 0.6763502448783454
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.99395859, 19.77194511,  2.32571827]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.2585121899543623}
episode index:1007
target Thresh 75.5549760488127
target distance 6.0
model initialize at round 1007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89., 13.]), 'dynamicTrap': False, 'previousTarget': array([89., 13.]), 'currentState': array([89.84574408,  8.47448236,  2.03144839]), 'targetState': array([89, 13], dtype=int32), 'currentDistance': 4.603867176880329}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6766515839211249
{'scaleFactor': 20, 'currentTarget': array([89., 13.]), 'dynamicTrap': False, 'previousTarget': array([89., 13.]), 'currentState': array([89.48527547, 12.25663602,  2.36486182]), 'targetState': array([89, 13], dtype=int32), 'currentDistance': 0.8877399908601378}
episode index:1008
target Thresh 75.557195615029
target distance 26.0
model initialize at round 1008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.13429728, 15.75216552]), 'dynamicTrap': False, 'previousTarget': array([50.31231517, 15.19946947]), 'currentState': array([32.76122918, 10.78384261,  0.89145964]), 'targetState': array([57, 17], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 32
reward sum = 0.6037586105470721
running average episode reward sum: 0.6765793411328453
{'scaleFactor': 20, 'currentTarget': array([57., 17.]), 'dynamicTrap': False, 'previousTarget': array([57., 17.]), 'currentState': array([56.96091811, 16.94179115,  2.79495389]), 'targetState': array([57, 17], dtype=int32), 'currentDistance': 0.07011179784056606}
episode index:1009
target Thresh 75.55940411111261
target distance 57.0
model initialize at round 1009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.89594774, 11.0624734 ]), 'dynamicTrap': False, 'previousTarget': array([21.5709957 , 12.12020962]), 'currentState': array([2.45207968, 6.37888556, 5.99434924]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 86
reward sum = 0.049773054703048936
running average episode reward sum: 0.6759587408492513
{'scaleFactor': 20, 'currentTarget': array([59., 20.]), 'dynamicTrap': False, 'previousTarget': array([59., 20.]), 'currentState': array([58.38820165, 19.8044504 ,  0.17996039]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 0.6422903277271207}
episode index:1010
target Thresh 75.56160159227606
target distance 63.0
model initialize at round 1010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.2421614 , 19.89059533]), 'dynamicTrap': False, 'previousTarget': array([68.97736275, 20.95130299]), 'currentState': array([48.29246532, 18.4729824 ,  5.28849363]), 'targetState': array([112,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.321007341182019
running average episode reward sum: 0.6756076514331611
{'scaleFactor': 20, 'currentTarget': array([112.,  23.]), 'dynamicTrap': False, 'previousTarget': array([112.,  23.]), 'currentState': array([111.76212487,  23.79883595,   6.22752385]), 'targetState': array([112,  23], dtype=int32), 'currentDistance': 0.833500716589406}
episode index:1011
target Thresh 75.56378811345648
target distance 13.0
model initialize at round 1011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.8184981 ,   6.91667259]), 'dynamicTrap': True, 'previousTarget': array([111.,   7.]), 'currentState': array([98.       ,  9.       ,  0.5218343], dtype=float32), 'targetState': array([111,   7], dtype=int32), 'currentDistance': 12.986691138012736}
done in step count: 18
reward sum = 0.7759939108510875
running average episode reward sum: 0.6757068473416769
{'scaleFactor': 20, 'currentTarget': array([111.,   7.]), 'dynamicTrap': False, 'previousTarget': array([111.,   7.]), 'currentState': array([111.45273777,   6.35837122,   0.77574393]), 'targetState': array([111,   7], dtype=int32), 'currentDistance': 0.7852763712336204}
episode index:1012
target Thresh 75.56596372931702
target distance 31.0
model initialize at round 1012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.31749233, 11.41901547]), 'dynamicTrap': False, 'previousTarget': array([51.25517759, 11.18464878]), 'currentState': array([69.03565349,  8.07325391,  2.77953172]), 'targetState': array([40, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6313083207976313
running average episode reward sum: 0.6756630185889188
{'scaleFactor': 20, 'currentTarget': array([40., 13.]), 'dynamicTrap': False, 'previousTarget': array([40., 13.]), 'currentState': array([40.18736273, 12.9996241 ,  1.78752114]), 'targetState': array([40, 13], dtype=int32), 'currentDistance': 0.1873631034697753}
episode index:1013
target Thresh 75.56812849424817
target distance 13.0
model initialize at round 1013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.34838528, 10.14411254]), 'dynamicTrap': True, 'previousTarget': array([45., 10.]), 'currentState': array([32.       , 11.       ,  0.9029649], dtype=float32), 'targetState': array([45, 10], dtype=int32), 'currentDistance': 11.38061473877159}
done in step count: 12
reward sum = 0.8293061091997701
running average episode reward sum: 0.675814540374531
{'scaleFactor': 20, 'currentTarget': array([45., 10.]), 'dynamicTrap': False, 'previousTarget': array([45., 10.]), 'currentState': array([44.38669428,  9.64955076,  1.48492652]), 'targetState': array([45, 10], dtype=int32), 'currentDistance': 0.7063699972517936}
episode index:1014
target Thresh 75.5702824623692
target distance 32.0
model initialize at round 1014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.81624123,  9.64257605]), 'dynamicTrap': False, 'previousTarget': array([46.72658355,  9.97753117]), 'currentState': array([26.80854374, 15.86410608,  4.9131217 ]), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6621453089244498
running average episode reward sum: 0.6758010731514276
{'scaleFactor': 20, 'currentTarget': array([61.67221863,  4.43342694]), 'dynamicTrap': True, 'previousTarget': array([61.51831905,  4.32321065]), 'currentState': array([61.25241168,  5.36430362,  1.79912805]), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 1.0211607444320587}
episode index:1015
target Thresh 75.5724256875294
target distance 60.0
model initialize at round 1015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.212416  ,  3.99746489]), 'dynamicTrap': False, 'previousTarget': array([87.0027772 ,  2.66671295]), 'currentState': array([107.18778765,   4.98969832,   1.40984952]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5772690747461511
running average episode reward sum: 0.6757040928380365
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([47.7317258 ,  2.81571969,  3.8401174 ]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 1.09581989606501}
episode index:1016
target Thresh 75.57455822330952
target distance 39.0
model initialize at round 1016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.42694934, 11.93735739]), 'dynamicTrap': False, 'previousTarget': array([22.31457586, 11.53328126]), 'currentState': array([43.20587519,  8.9718739 ,  6.125359  ]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6068914851459634
running average episode reward sum: 0.6756364304902567
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 15.]), 'currentState': array([ 2.05672669, 15.92102935,  4.31068117]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 1.3183548825654063}
episode index:1017
target Thresh 75.57668012302307
target distance 8.0
model initialize at round 1017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  8.]), 'dynamicTrap': False, 'previousTarget': array([35.,  8.]), 'currentState': array([28.86087373,  5.72407466,  0.47035736]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 6.54741991429453}
done in step count: 4
reward sum = 0.9509900498999999
running average episode reward sum: 0.675906915381622
{'scaleFactor': 20, 'currentTarget': array([34.03452244,  6.46422284]), 'dynamicTrap': True, 'previousTarget': array([35.,  8.]), 'currentState': array([33.79911845,  7.28910717,  1.7597394 ]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 0.8578165331133841}
episode index:1018
target Thresh 75.57879143971765
target distance 7.0
model initialize at round 1018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([4.94673007, 9.46485965, 0.09428895]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 6.535871013756277}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6761958183105898
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([10.03362885,  6.9865888 ,  6.07927311]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.9664642079077018}
episode index:1019
target Thresh 75.58089222617627
target distance 35.0
model initialize at round 1019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.06993494, 23.09697617]), 'dynamicTrap': False, 'previousTarget': array([81.00815827, 22.57119548]), 'currentState': array([100.0694599 ,  23.23482168,   3.22658634]), 'targetState': array([66, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7996356699028635
running average episode reward sum: 0.6763168377729352
{'scaleFactor': 20, 'currentTarget': array([66., 23.]), 'dynamicTrap': False, 'previousTarget': array([66., 23.]), 'currentState': array([65.55151051, 23.42011854,  3.33423138]), 'targetState': array([66, 23], dtype=int32), 'currentDistance': 0.6145261638899294}
episode index:1020
target Thresh 75.58298253491873
target distance 15.0
model initialize at round 1020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,   3.]), 'dynamicTrap': False, 'previousTarget': array([114.,   3.]), 'currentState': array([113.44997791,  16.92192546,   4.00065911]), 'targetState': array([114,   3], dtype=int32), 'currentDistance': 13.93278625738505}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6765581970840566
{'scaleFactor': 20, 'currentTarget': array([114.,   3.]), 'dynamicTrap': False, 'previousTarget': array([114.,   3.]), 'currentState': array([113.95425964,   2.90302241,   4.42035669]), 'targetState': array([114,   3], dtype=int32), 'currentDistance': 0.10722328738705428}
episode index:1021
target Thresh 75.58506241820284
target distance 16.0
model initialize at round 1021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  20.]), 'dynamicTrap': False, 'previousTarget': array([106.,  20.]), 'currentState': array([91.87048892, 10.58153641,  6.20903057]), 'targetState': array([106,  20], dtype=int32), 'currentDistance': 16.980887480924995}
done in step count: 17
reward sum = 0.7686873020280618
running average episode reward sum: 0.6766483429793052
{'scaleFactor': 20, 'currentTarget': array([106.,  20.]), 'dynamicTrap': False, 'previousTarget': array([106.,  20.]), 'currentState': array([1.05432241e+02, 1.98266714e+01, 3.16512724e-02]), 'targetState': array([106,  20], dtype=int32), 'currentDistance': 0.5936267065527335}
episode index:1022
target Thresh 75.58713192802578
target distance 56.0
model initialize at round 1022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.37230208,  6.84102517]), 'dynamicTrap': True, 'previousTarget': array([67.37502359,  6.85490608]), 'currentState': array([87.      ,  3.      ,  2.990056], dtype=float32), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.47064192662518
running average episode reward sum: 0.6764469681832601
{'scaleFactor': 20, 'currentTarget': array([31., 14.]), 'dynamicTrap': False, 'previousTarget': array([31., 14.]), 'currentState': array([31.7003986 , 14.28265247,  6.14620635]), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 0.7552818127632194}
episode index:1023
target Thresh 75.58919111612543
target distance 74.0
model initialize at round 1023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.85893816, 11.05139439]), 'dynamicTrap': False, 'previousTarget': array([46.9111147 , 12.11651618]), 'currentState': array([27.91680909, 12.57175223,  5.43376482]), 'targetState': array([101,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.431771507338339
running average episode reward sum: 0.6762080273035287
{'scaleFactor': 20, 'currentTarget': array([101.,   7.]), 'dynamicTrap': False, 'previousTarget': array([101.,   7.]), 'currentState': array([101.44609076,   6.39168971,   1.94117883]), 'targetState': array([101,   7], dtype=int32), 'currentDistance': 0.7543463235513739}
episode index:1024
target Thresh 75.59124003398159
target distance 58.0
model initialize at round 1024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.42943621, 14.84623115]), 'dynamicTrap': False, 'previousTarget': array([48.35026951, 14.72667302]), 'currentState': array([66.05461772, 10.99236936,  3.55600679]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.45091159717448887
running average episode reward sum: 0.6759882259082808
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'dynamicTrap': False, 'previousTarget': array([10., 22.]), 'currentState': array([ 9.07412268, 22.80684127,  3.63464044]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 1.228104903805144}
episode index:1025
target Thresh 75.59327873281731
target distance 10.0
model initialize at round 1025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.,  9.]), 'dynamicTrap': False, 'previousTarget': array([84.,  9.]), 'currentState': array([85.07772666, 17.47658033,  3.90018165]), 'targetState': array([84,  9], dtype=int32), 'currentDistance': 8.54481766398752}
done in step count: 8
reward sum = 0.89392584382892
running average episode reward sum: 0.6762006407405622
{'scaleFactor': 20, 'currentTarget': array([84.,  9.]), 'dynamicTrap': False, 'previousTarget': array([84.,  9.]), 'currentState': array([84.12755875,  8.98985135,  5.76797535]), 'targetState': array([84,  9], dtype=int32), 'currentDistance': 0.12796183251581467}
episode index:1026
target Thresh 75.59530726360016
target distance 7.0
model initialize at round 1026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  18.]), 'dynamicTrap': False, 'previousTarget': array([111.,  18.]), 'currentState': array([116.67393503,  10.69427138,   0.79766935]), 'targetState': array([111,  18], dtype=int32), 'currentDistance': 9.250254564279626}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6764682058906688
{'scaleFactor': 20, 'currentTarget': array([111.,  18.]), 'dynamicTrap': False, 'previousTarget': array([111.,  18.]), 'currentState': array([111.94765964,  18.04495534,   3.59290897]), 'targetState': array([111,  18], dtype=int32), 'currentDistance': 0.9487253438233655}
episode index:1027
target Thresh 75.59732567704353
target distance 74.0
model initialize at round 1027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.00651279, 22.489638  ]), 'dynamicTrap': True, 'previousTarget': array([85.0018259, 22.7297544]), 'currentState': array([105.      ,  23.      ,   2.802546], dtype=float32), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 57
reward sum = 0.4734743137879174
running average episode reward sum: 0.6762707410150824
{'scaleFactor': 20, 'currentTarget': array([31., 22.]), 'dynamicTrap': False, 'previousTarget': array([31., 22.]), 'currentState': array([31.24906508, 22.91320128,  2.97148879]), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 0.9465569173075585}
episode index:1028
target Thresh 75.59933402360784
target distance 22.0
model initialize at round 1028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.89392301,  6.79604391]), 'dynamicTrap': False, 'previousTarget': array([73.17472169,  6.23656605]), 'currentState': array([55.31634583, 17.9846066 ,  2.14200115]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6671341697708465
running average episode reward sum: 0.6762618619370997
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([78.47498017,  1.30455395,  0.41149061]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 0.8713730702443154}
episode index:1029
target Thresh 75.60133235350189
target distance 26.0
model initialize at round 1029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.71623802, 14.91158851]), 'dynamicTrap': False, 'previousTarget': array([59.88441983, 14.88171698]), 'currentState': array([77.50176312,  8.04834972,  4.55210936]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7743803775064761
running average episode reward sum: 0.6763571226318273
{'scaleFactor': 20, 'currentTarget': array([53., 17.]), 'dynamicTrap': False, 'previousTarget': array([53., 17.]), 'currentState': array([52.34842765, 17.01695585,  4.23730768]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 0.6517929315363288}
episode index:1030
target Thresh 75.603320716684
target distance 50.0
model initialize at round 1030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.1172545 , 15.49457766]), 'dynamicTrap': False, 'previousTarget': array([42.56035789, 15.25535031]), 'currentState': array([59.3688569 , 23.67253644,  3.58881938]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.616114963658546
running average episode reward sum: 0.6762986918277796
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'dynamicTrap': False, 'previousTarget': array([11.,  2.]), 'currentState': array([10.38323231,  1.88168115,  2.90973748]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.6280141139969506}
episode index:1031
target Thresh 75.60529916286336
target distance 38.0
model initialize at round 1031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.37543577, 14.25264087]), 'dynamicTrap': False, 'previousTarget': array([51.5672925, 15.23886  ]), 'currentState': array([68.97882392, 21.59525276,  3.16742468]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6775660432093887
running average episode reward sum: 0.6762999198814439
{'scaleFactor': 20, 'currentTarget': array([32.,  7.]), 'dynamicTrap': False, 'previousTarget': array([32.,  7.]), 'currentState': array([31.30288193,  6.60693357,  3.10436524]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 0.8002967113767514}
episode index:1032
target Thresh 75.60726774150125
target distance 54.0
model initialize at round 1032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.35893794,  6.35247036]), 'dynamicTrap': False, 'previousTarget': array([55.83405013,  7.42891943]), 'currentState': array([35.4519437 ,  8.2790166 ,  3.62508202]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.49585713713387436
running average episode reward sum: 0.676125241485754
{'scaleFactor': 20, 'currentTarget': array([90.,  3.]), 'dynamicTrap': False, 'previousTarget': array([90.,  3.]), 'currentState': array([90.37859484,  2.81878328,  5.49427073]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 0.41973033185363273}
episode index:1033
target Thresh 75.60922650181222
target distance 67.0
model initialize at round 1033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.14005282, 10.11291293]), 'dynamicTrap': False, 'previousTarget': array([89.21911266, 11.04762875]), 'currentState': array([109.98073305,  12.6323192 ,   4.32448461]), 'targetState': array([42,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5245570800628452
running average episode reward sum: 0.6759786571903741
{'scaleFactor': 20, 'currentTarget': array([42.,  4.]), 'dynamicTrap': False, 'previousTarget': array([42.,  4.]), 'currentState': array([41.41840853,  3.77051602,  3.27357262]), 'targetState': array([42,  4], dtype=int32), 'currentDistance': 0.6252291813462699}
episode index:1034
target Thresh 75.61117549276538
target distance 31.0
model initialize at round 1034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.77808409,  18.95458099]), 'dynamicTrap': False, 'previousTarget': array([105.65136197,  18.21988205]), 'currentState': array([86.7911529 , 12.66996137,  0.68521142]), 'targetState': array([118,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.19577840375415723
running average episode reward sum: 0.6755146955928512
{'scaleFactor': 20, 'currentTarget': array([118.,  23.]), 'dynamicTrap': False, 'previousTarget': array([118.,  23.]), 'currentState': array([117.39489726,  23.08851072,   1.29799057]), 'targetState': array([118,  23], dtype=int32), 'currentDistance': 0.6115418773606704}
episode index:1035
target Thresh 75.6131147630856
target distance 23.0
model initialize at round 1035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.83340559, 14.16296709]), 'dynamicTrap': False, 'previousTarget': array([83.41810404, 13.42128976]), 'currentState': array([68.77702822,  2.23850711,  0.12468489]), 'targetState': array([90, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7800556220251509
running average episode reward sum: 0.6756156038229982
{'scaleFactor': 20, 'currentTarget': array([90., 18.]), 'dynamicTrap': False, 'previousTarget': array([90., 18.]), 'currentState': array([89.17508046, 18.13306035,  1.08629257]), 'targetState': array([90, 18], dtype=int32), 'currentDistance': 0.8355820139334162}
episode index:1036
target Thresh 75.61504436125476
target distance 16.0
model initialize at round 1036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'dynamicTrap': False, 'previousTarget': array([13., 22.]), 'currentState': array([11.52569639,  7.62328855,  0.85888278]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 14.452107226394912}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6758539153857802
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'dynamicTrap': False, 'previousTarget': array([13., 22.]), 'currentState': array([12.74289187, 22.28556216,  0.65318555]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.38425295684830707}
episode index:1037
target Thresh 75.6169643355129
target distance 27.0
model initialize at round 1037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.45464313, 15.29144955]), 'dynamicTrap': False, 'previousTarget': array([72.35899411, 14.09400392]), 'currentState': array([88.57586258,  4.95395255,  1.74372993]), 'targetState': array([62, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6006455040460371
running average episode reward sum: 0.6757814602688826
{'scaleFactor': 20, 'currentTarget': array([62., 21.]), 'dynamicTrap': False, 'previousTarget': array([62., 21.]), 'currentState': array([62.09806155, 20.56755695,  0.72020819]), 'targetState': array([62, 21], dtype=int32), 'currentDistance': 0.443421982665801}
episode index:1038
target Thresh 75.61887473385947
target distance 11.0
model initialize at round 1038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.8095255 , 11.06096149]), 'dynamicTrap': True, 'previousTarget': array([48., 11.]), 'currentState': array([37.      , 21.      ,  5.988678], dtype=float32), 'targetState': array([48, 11], dtype=int32), 'currentDistance': 14.684356571277092}
done in step count: 11
reward sum = 0.8662108073144372
running average episode reward sum: 0.6759647416423625
{'scaleFactor': 20, 'currentTarget': array([48., 11.]), 'dynamicTrap': False, 'previousTarget': array([48., 11.]), 'currentState': array([47.79208057, 10.71120075,  0.83271167]), 'targetState': array([48, 11], dtype=int32), 'currentDistance': 0.35585881939861314}
episode index:1039
target Thresh 75.62077560405454
target distance 10.0
model initialize at round 1039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'dynamicTrap': False, 'previousTarget': array([18., 10.]), 'currentState': array([26.82180671, 11.20176897,  3.35628891]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 8.903287161061964}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6762200449190534
{'scaleFactor': 20, 'currentTarget': array([18., 10.]), 'dynamicTrap': False, 'previousTarget': array([18., 10.]), 'currentState': array([17.79267355, 10.13691344,  3.2793839 ]), 'targetState': array([18, 10], dtype=int32), 'currentDistance': 0.24845431599835394}
episode index:1040
target Thresh 75.62266699361997
target distance 69.0
model initialize at round 1040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.14953618,  6.56147651]), 'dynamicTrap': False, 'previousTarget': array([72.05230408,  5.55451479]), 'currentState': array([91.06038742,  8.44774349,  2.88499403]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.44446307215461406
running average episode reward sum: 0.6759974157425266
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'dynamicTrap': False, 'previousTarget': array([23.,  2.]), 'currentState': array([23.99210284,  2.04043595,  4.75745587]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.9929265356486974}
episode index:1041
target Thresh 75.62454894984057
target distance 15.0
model initialize at round 1041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41., 23.]), 'dynamicTrap': False, 'previousTarget': array([41., 23.]), 'currentState': array([56.65542088, 22.37169198,  2.12978248]), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 15.668023927431834}
done in step count: 10
reward sum = 0.8945810750088045
running average episode reward sum: 0.6762071889280028
{'scaleFactor': 20, 'currentTarget': array([41., 23.]), 'dynamicTrap': False, 'previousTarget': array([41., 23.]), 'currentState': array([41.55224187, 23.18146247,  4.34985076]), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 0.5812914191879427}
episode index:1042
target Thresh 75.62642151976537
target distance 45.0
model initialize at round 1042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.27918487, 10.55900633]), 'dynamicTrap': False, 'previousTarget': array([22.61161351, 11.0776773 ]), 'currentState': array([ 4.63864317, 14.33380722,  5.16755125]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7310129152605904
running average episode reward sum: 0.6762597351660974
{'scaleFactor': 20, 'currentTarget': array([48.,  6.]), 'dynamicTrap': False, 'previousTarget': array([48.,  6.]), 'currentState': array([48.94185578,  5.68298867,  2.35304142]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 0.9937748650031002}
episode index:1043
target Thresh 75.6282847502087
target distance 15.0
model initialize at round 1043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.8228268 , 18.04955545]), 'dynamicTrap': True, 'previousTarget': array([72., 18.]), 'currentState': array([57.      ,  6.      ,  6.025371], dtype=float32), 'targetState': array([72, 18], dtype=int32), 'currentDistance': 19.10256477501791}
done in step count: 31
reward sum = 0.525917653298053
running average episode reward sum: 0.6761157293405533
{'scaleFactor': 20, 'currentTarget': array([72., 18.]), 'dynamicTrap': False, 'previousTarget': array([72., 18.]), 'currentState': array([71.32487244, 18.83948775,  2.00953701]), 'targetState': array([72, 18], dtype=int32), 'currentDistance': 1.07728218850001}
episode index:1044
target Thresh 75.63013868775144
target distance 37.0
model initialize at round 1044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.29937741,  9.52162631]), 'dynamicTrap': False, 'previousTarget': array([70.65140491,  9.71783336]), 'currentState': array([52.77283449,  5.19564416,  6.2624886 ]), 'targetState': array([88, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7028481769375909
running average episode reward sum: 0.6761413106301197
{'scaleFactor': 20, 'currentTarget': array([88., 13.]), 'dynamicTrap': False, 'previousTarget': array([88., 13.]), 'currentState': array([88.30780754, 12.45459153,  2.32853818]), 'targetState': array([88, 13], dtype=int32), 'currentDistance': 0.6262714138806333}
episode index:1045
target Thresh 75.6319833787421
target distance 65.0
model initialize at round 1045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.66312659, 15.54005546]), 'dynamicTrap': False, 'previousTarget': array([66.94108971, 15.53392998]), 'currentState': array([48.7265651 , 13.94835325,  5.93388564]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5194171208443128
running average episode reward sum: 0.6759914787087185
{'scaleFactor': 20, 'currentTarget': array([112.,  19.]), 'dynamicTrap': False, 'previousTarget': array([112.,  19.]), 'currentState': array([112.53540235,  18.5059893 ,   0.33160279]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 0.7284931357474651}
episode index:1046
target Thresh 75.63381886929807
target distance 71.0
model initialize at round 1046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.2058516, 16.638261 ]), 'dynamicTrap': False, 'previousTarget': array([37.56806656, 17.86590143]), 'currentState': array([17.54339674, 20.29720467,  4.27985889]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.2700846023765765
running average episode reward sum: 0.6756037930579714
{'scaleFactor': 20, 'currentTarget': array([89.,  7.]), 'dynamicTrap': False, 'previousTarget': array([89.,  7.]), 'currentState': array([89.25712634,  7.46506832,  1.71216956]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 0.5314155553801356}
episode index:1047
target Thresh 75.63564520530669
target distance 66.0
model initialize at round 1047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.31554617,  8.14428133]), 'dynamicTrap': False, 'previousTarget': array([90.22568992,  9.00389241]), 'currentState': array([109.13419955,  10.83146911,   5.08471751]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.2852703048702032
running average episode reward sum: 0.6752313374394716
{'scaleFactor': 20, 'currentTarget': array([44.,  2.]), 'dynamicTrap': False, 'previousTarget': array([44.,  2.]), 'currentState': array([43.93317563,  1.77418639,  2.03364873]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 0.2354936999714062}
episode index:1048
target Thresh 75.63746243242646
target distance 18.0
model initialize at round 1048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.97920178, 18.98590344]), 'dynamicTrap': False, 'previousTarget': array([26.64100589, 18.09400392]), 'currentState': array([11.42357711,  7.76488335,  5.76622326]), 'targetState': array([28, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6754326277486009
{'scaleFactor': 20, 'currentTarget': array([28., 19.]), 'dynamicTrap': False, 'previousTarget': array([28., 19.]), 'currentState': array([27.97880214, 19.37710489,  1.07271706]), 'targetState': array([28, 19], dtype=int32), 'currentDistance': 0.37770021121706143}
episode index:1049
target Thresh 75.63927059608817
target distance 16.0
model initialize at round 1049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  10.]), 'dynamicTrap': False, 'previousTarget': array([109.,  10.]), 'currentState': array([9.39238121e+01, 1.60748529e+00, 2.49502023e-02]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 17.254731121406437}
done in step count: 12
reward sum = 0.8581395162440494
running average episode reward sum: 0.6756066343090728
{'scaleFactor': 20, 'currentTarget': array([109.,  10.]), 'dynamicTrap': False, 'previousTarget': array([109.,  10.]), 'currentState': array([109.46794946,   9.33313595,   0.70204278]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 0.8146682551515806}
episode index:1050
target Thresh 75.64106974149598
target distance 33.0
model initialize at round 1050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.70036512, 13.3080558 ]), 'dynamicTrap': False, 'previousTarget': array([48.44208854, 13.42295739]), 'currentState': array([67.60694021, 22.21617391,  5.29817074]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5550883281745812
running average episode reward sum: 0.6754919641795443
{'scaleFactor': 20, 'currentTarget': array([33.,  5.]), 'dynamicTrap': False, 'previousTarget': array([33.,  5.]), 'currentState': array([33.58191879,  4.95724342,  0.16660199]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 0.5834874478440977}
episode index:1051
target Thresh 75.64285991362866
target distance 58.0
model initialize at round 1051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.30226828, 17.39177161]), 'dynamicTrap': False, 'previousTarget': array([54.81242258, 18.26725206]), 'currentState': array([36.44396717, 19.76829786,  5.74375331]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3510717757265054
running average episode reward sum: 0.6751835799699881
{'scaleFactor': 20, 'currentTarget': array([93., 13.]), 'dynamicTrap': False, 'previousTarget': array([93., 13.]), 'currentState': array([93.25118258, 13.93496559,  3.12042524]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 0.9681184558518711}
episode index:1052
target Thresh 75.64464115724056
target distance 66.0
model initialize at round 1052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.45825489,  9.20654826]), 'dynamicTrap': False, 'previousTarget': array([69.91786413,  9.81071492]), 'currentState': array([51.57307285,  7.0665634 ,  5.06539849]), 'targetState': array([116,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.626384486063871
running average episode reward sum: 0.6751372370507991
{'scaleFactor': 20, 'currentTarget': array([116.,  14.]), 'dynamicTrap': False, 'previousTarget': array([116.,  14.]), 'currentState': array([115.29942552,  13.56079099,   5.64817275]), 'targetState': array([116,  14], dtype=int32), 'currentDistance': 0.826867076384169}
episode index:1053
target Thresh 75.64641351686289
target distance 9.0
model initialize at round 1053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.,  9.]), 'dynamicTrap': False, 'previousTarget': array([98.,  9.]), 'currentState': array([90.03150017, 16.39911034,  4.73997414]), 'targetState': array([98,  9], dtype=int32), 'currentDistance': 10.8739975776929}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6753721587371151
{'scaleFactor': 20, 'currentTarget': array([98.,  9.]), 'dynamicTrap': False, 'previousTarget': array([98.,  9.]), 'currentState': array([98.19060607,  8.30772073,  1.86292669]), 'targetState': array([98,  9], dtype=int32), 'currentDistance': 0.7180398778709022}
episode index:1054
target Thresh 75.64817703680473
target distance 64.0
model initialize at round 1054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.38008186, 10.20345914]), 'dynamicTrap': False, 'previousTarget': array([81.67029746, 11.13445224]), 'currentState': array([102.6597173 ,   4.88409288,   5.68297114]), 'targetState': array([37, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4786598213555305
running average episode reward sum: 0.6751857015452842
{'scaleFactor': 20, 'currentTarget': array([37., 23.]), 'dynamicTrap': False, 'previousTarget': array([37., 23.]), 'currentState': array([37.11221561, 23.23987115,  4.19193773]), 'targetState': array([37, 23], dtype=int32), 'currentDistance': 0.2648216607940182}
episode index:1055
target Thresh 75.64993176115415
target distance 28.0
model initialize at round 1055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.85896861,  6.83928043]), 'dynamicTrap': False, 'previousTarget': array([30.14017935,  6.57777387]), 'currentState': array([13.18313902, 16.19690023,  6.06436533]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6657439525060287
running average episode reward sum: 0.6751767604950576
{'scaleFactor': 20, 'currentTarget': array([40.,  2.]), 'dynamicTrap': False, 'previousTarget': array([40.,  2.]), 'currentState': array([39.32266093,  1.14875112,  5.14880618]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 1.087847814204341}
episode index:1056
target Thresh 75.65167773377938
target distance 49.0
model initialize at round 1056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.62851295, 13.3340426 ]), 'dynamicTrap': False, 'previousTarget': array([31.51695287, 14.44206005]), 'currentState': array([12.89074365, 20.32661018,  5.5804069 ]), 'targetState': array([62,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6458696863741662
running average episode reward sum: 0.67514903384026
{'scaleFactor': 20, 'currentTarget': array([62.,  2.]), 'dynamicTrap': False, 'previousTarget': array([62.,  2.]), 'currentState': array([61.11146993,  2.38435796,  1.24453621]), 'targetState': array([62,  2], dtype=int32), 'currentDistance': 0.9680995396689647}
episode index:1057
target Thresh 75.65341499832981
target distance 51.0
model initialize at round 1057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.27428233,  9.37578742]), 'dynamicTrap': False, 'previousTarget': array([32.55042088, 10.21675745]), 'currentState': array([13.86295688,  4.55910346,  6.20542455]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6221621311805771
running average episode reward sum: 0.6750989517016404
{'scaleFactor': 20, 'currentTarget': array([65.18071964, 15.72092784]), 'dynamicTrap': True, 'previousTarget': array([64., 17.]), 'currentState': array([66.10609938, 16.49059953,  1.42518746]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 1.2036287548284583}
episode index:1058
target Thresh 75.65514359823715
target distance 14.0
model initialize at round 1058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.13131088, 14.1504138 ]), 'dynamicTrap': True, 'previousTarget': array([10., 14.]), 'currentState': array([24.      ,  5.      ,  4.093694], dtype=float32), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 16.615372715429398}
done in step count: 15
reward sum = 0.8034312609930149
running average episode reward sum: 0.6752201342411034
{'scaleFactor': 20, 'currentTarget': array([10., 14.]), 'dynamicTrap': False, 'previousTarget': array([10., 14.]), 'currentState': array([10.54932976, 14.13352457,  2.17202165]), 'targetState': array([10, 14], dtype=int32), 'currentDistance': 0.565324681849505}
episode index:1059
target Thresh 75.65686357671646
target distance 24.0
model initialize at round 1059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.53145544,  5.6962518 ]), 'dynamicTrap': True, 'previousTarget': array([63.2,  4.4]), 'currentState': array([44.      , 10.      ,  4.944555], dtype=float32), 'targetState': array([68,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7544053335436554
running average episode reward sum: 0.6752948372593134
{'scaleFactor': 20, 'currentTarget': array([68.,  3.]), 'dynamicTrap': False, 'previousTarget': array([68.,  3.]), 'currentState': array([67.76732574,  3.65833614,  4.19599924]), 'targetState': array([68,  3], dtype=int32), 'currentDistance': 0.6982433530108392}
episode index:1060
target Thresh 75.65857497676734
target distance 6.0
model initialize at round 1060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.8769756,  6.1486299]), 'dynamicTrap': True, 'previousTarget': array([76.,  6.]), 'currentState': array([82.      ,  9.      ,  5.502807], dtype=float32), 'targetState': array([76,  6], dtype=int32), 'currentDistance': 6.754386665014608}
done in step count: 10
reward sum = 0.8473033124924453
running average episode reward sum: 0.6754569564631147
{'scaleFactor': 20, 'currentTarget': array([76.,  6.]), 'dynamicTrap': False, 'previousTarget': array([76.,  6.]), 'currentState': array([75.26033549,  6.15585427,  4.67267384]), 'targetState': array([76,  6], dtype=int32), 'currentDistance': 0.7559061717569903}
episode index:1061
target Thresh 75.66027784117485
target distance 61.0
model initialize at round 1061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.0883984, 17.4343765]), 'dynamicTrap': False, 'previousTarget': array([94.57856614, 16.22423757]), 'currentState': array([114.38881974,  22.67782346,   2.28530899]), 'targetState': array([53,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5255227922592175
running average episode reward sum: 0.6753157755175366
{'scaleFactor': 20, 'currentTarget': array([53.,  6.]), 'dynamicTrap': False, 'previousTarget': array([53.,  6.]), 'currentState': array([53.6663291 ,  6.92967092,  3.82738258]), 'targetState': array([53,  6], dtype=int32), 'currentDistance': 1.1438017695479723}
episode index:1062
target Thresh 75.6619722125107
target distance 24.0
model initialize at round 1062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.454989  , 19.17310041]), 'dynamicTrap': False, 'previousTarget': array([47.42039161, 19.07908508]), 'currentState': array([68.22909174, 16.17562463,  1.74288553]), 'targetState': array([43, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7812321501913116
running average episode reward sum: 0.6754154146282363
{'scaleFactor': 20, 'currentTarget': array([43., 20.]), 'dynamicTrap': False, 'previousTarget': array([43., 20.]), 'currentState': array([42.46484335, 20.95350272,  4.73704789]), 'targetState': array([43, 20], dtype=int32), 'currentDistance': 1.093416694304688}
episode index:1063
target Thresh 75.66365813313425
target distance 9.0
model initialize at round 1063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69., 14.]), 'dynamicTrap': False, 'previousTarget': array([69., 14.]), 'currentState': array([68.724697  , 23.20190862,  5.50783819]), 'targetState': array([69, 14], dtype=int32), 'currentDistance': 9.206025961793658}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.675674413345597
{'scaleFactor': 20, 'currentTarget': array([69., 14.]), 'dynamicTrap': False, 'previousTarget': array([69., 14.]), 'currentState': array([68.60976774, 14.25807893,  6.03589049]), 'targetState': array([69, 14], dtype=int32), 'currentDistance': 0.4678524871836704}
episode index:1064
target Thresh 75.66533564519362
target distance 42.0
model initialize at round 1064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.53926123, 20.04279396]), 'dynamicTrap': False, 'previousTarget': array([52.97736275, 20.95130299]), 'currentState': array([32.61476356, 18.3065934 ,  5.40512836]), 'targetState': array([75, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4623692581854639
running average episode reward sum: 0.6754741268149302
{'scaleFactor': 20, 'currentTarget': array([75., 22.]), 'dynamicTrap': False, 'previousTarget': array([75., 22.]), 'currentState': array([75.29561248, 21.72957251,  4.09737637]), 'targetState': array([75, 22], dtype=int32), 'currentDistance': 0.40064668483925614}
episode index:1065
target Thresh 75.66700479062669
target distance 15.0
model initialize at round 1065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76., 22.]), 'dynamicTrap': False, 'previousTarget': array([76., 22.]), 'currentState': array([89.49746872, 16.70731666,  2.32397699]), 'targetState': array([76, 22], dtype=int32), 'currentDistance': 14.498074311137904}
done in step count: 15
reward sum = 0.8413228996682085
running average episode reward sum: 0.6756297072772691
{'scaleFactor': 20, 'currentTarget': array([76., 22.]), 'dynamicTrap': False, 'previousTarget': array([76., 22.]), 'currentState': array([75.29676058, 21.57346651,  1.93052728]), 'targetState': array([76, 22], dtype=int32), 'currentDistance': 0.8224819155109594}
episode index:1066
target Thresh 75.66866561116217
target distance 53.0
model initialize at round 1066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.52604487,  8.7568446 ]), 'dynamicTrap': False, 'previousTarget': array([83.96803691,  9.86973376]), 'currentState': array([63.53115501,  9.20892815,  3.77428162]), 'targetState': array([117,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6877257599218837
running average episode reward sum: 0.6756410437839652
{'scaleFactor': 20, 'currentTarget': array([117.,   8.]), 'dynamicTrap': False, 'previousTarget': array([117.,   8.]), 'currentState': array([117.46694209,   7.51764831,   2.51006929]), 'targetState': array([117,   8], dtype=int32), 'currentDistance': 0.6713404986644891}
episode index:1067
target Thresh 75.67031814832069
target distance 44.0
model initialize at round 1067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.39872893, 11.97368499]), 'dynamicTrap': True, 'previousTarget': array([76.40570135, 12.00792472]), 'currentState': array([96.       ,  8.       ,  2.4505374], dtype=float32), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.3736749449882335
running average episode reward sum: 0.6753583039910853
{'scaleFactor': 20, 'currentTarget': array([52., 17.]), 'dynamicTrap': False, 'previousTarget': array([52., 17.]), 'currentState': array([51.32452105, 16.74741168,  3.50451697]), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 0.7211606437376588}
episode index:1068
target Thresh 75.67196244341572
target distance 33.0
model initialize at round 1068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.72181431, 11.23283291]), 'dynamicTrap': False, 'previousTarget': array([83.41163564, 11.18900306]), 'currentState': array([66.61718827, 19.73121603,  5.86299104]), 'targetState': array([98,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6895648292310405
running average episode reward sum: 0.6753715935376147
{'scaleFactor': 20, 'currentTarget': array([98.,  5.]), 'dynamicTrap': False, 'previousTarget': array([98.,  5.]), 'currentState': array([97.13298398,  4.0379012 ,  4.41409789]), 'targetState': array([98,  5], dtype=int32), 'currentDistance': 1.2951258184130736}
episode index:1069
target Thresh 75.67359853755477
target distance 47.0
model initialize at round 1069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.1714471 ,  8.38686203]), 'dynamicTrap': True, 'previousTarget': array([77.16100441,  8.46736226]), 'currentState': array([97.       , 11.       ,  1.3304952], dtype=float32), 'targetState': array([50,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6031732751706209
running average episode reward sum: 0.6753041184737203
{'scaleFactor': 20, 'currentTarget': array([50.,  5.]), 'dynamicTrap': False, 'previousTarget': array([50.,  5.]), 'currentState': array([49.34173402,  5.3014786 ,  3.61562908]), 'targetState': array([50,  5], dtype=int32), 'currentDistance': 0.7240189561729021}
episode index:1070
target Thresh 75.67522647164026
target distance 41.0
model initialize at round 1070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.48429323, 20.44673257]), 'dynamicTrap': False, 'previousTarget': array([64.85291755, 20.57891249]), 'currentState': array([46.63964679, 22.93470513,  5.67557246]), 'targetState': array([86, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.47447535791254863
running average episode reward sum: 0.6742305615396529
{'scaleFactor': 20, 'currentTarget': array([78.84833415, 14.87649747]), 'dynamicTrap': True, 'previousTarget': array([78.69261139, 14.75101667]), 'currentState': array([72.73816028, 12.52271819,  0.69826678]), 'targetState': array([86, 18], dtype=int32), 'currentDistance': 6.547862371503917}
episode index:1071
target Thresh 75.67684628637062
target distance 40.0
model initialize at round 1071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.17701049, 11.41017491]), 'dynamicTrap': False, 'previousTarget': array([88.56953382, 11.57218647]), 'currentState': array([68.41060299, 18.32551654,  4.47750139]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.508977003313216
running average episode reward sum: 0.6740764071010088
{'scaleFactor': 20, 'currentTarget': array([110.,   3.]), 'dynamicTrap': False, 'previousTarget': array([110.,   3.]), 'currentState': array([110.83552887,   3.50000369,   2.3486608 ]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 0.9737105268166413}
episode index:1072
target Thresh 75.6784580222413
target distance 5.0
model initialize at round 1072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'dynamicTrap': False, 'previousTarget': array([13.,  7.]), 'currentState': array([8.87998193, 8.44862429, 6.05602562]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 4.367271595228076}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.674352476619088
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'dynamicTrap': False, 'previousTarget': array([13.,  7.]), 'currentState': array([13.20149619,  7.40360803,  0.6015436 ]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.45110991068738865}
episode index:1073
target Thresh 75.6800617195458
target distance 37.0
model initialize at round 1073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.83049872, 18.08019896]), 'dynamicTrap': False, 'previousTarget': array([69.56664794, 17.72703158]), 'currentState': array([87.24421372, 13.27315519,  2.86031899]), 'targetState': array([52, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.456202904606221
running average episode reward sum: 0.6741493578369532
{'scaleFactor': 20, 'currentTarget': array([52., 22.]), 'dynamicTrap': False, 'previousTarget': array([52., 22.]), 'currentState': array([51.51005023, 22.5770627 ,  2.62167247]), 'targetState': array([52, 22], dtype=int32), 'currentDistance': 0.7570020725345149}
episode index:1074
target Thresh 75.68165741837663
target distance 43.0
model initialize at round 1074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.69311871, 12.99061267]), 'dynamicTrap': False, 'previousTarget': array([88.33739901, 13.34184168]), 'currentState': array([106.3630685 ,  16.60904816,   2.36603308]), 'targetState': array([65,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6290328390632827
running average episode reward sum: 0.67410738898228
{'scaleFactor': 20, 'currentTarget': array([65.,  9.]), 'dynamicTrap': False, 'previousTarget': array([65.,  9.]), 'currentState': array([65.57128784,  9.71415907,  3.68535433]), 'targetState': array([65,  9], dtype=int32), 'currentDistance': 0.914545226149173}
episode index:1075
target Thresh 75.68324515862632
target distance 56.0
model initialize at round 1075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.40771109, 10.19684404]), 'dynamicTrap': False, 'previousTarget': array([67.01274291, 10.71383061]), 'currentState': array([85.38030408,  9.15016877,  3.10416806]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5813399720459868
running average episode reward sum: 0.6740211739107778
{'scaleFactor': 20, 'currentTarget': array([31., 12.]), 'dynamicTrap': False, 'previousTarget': array([31., 12.]), 'currentState': array([30.03657527, 11.88257002,  1.08790214]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 0.97055500111922}
episode index:1076
target Thresh 75.68482497998849
target distance 25.0
model initialize at round 1076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.95898346,  9.71958414]), 'dynamicTrap': False, 'previousTarget': array([44.74881264, 10.15981002]), 'currentState': array([24.39363597,  5.57263616,  5.32011199]), 'targetState': array([50, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7588966574061367
running average episode reward sum: 0.6740999812306434
{'scaleFactor': 20, 'currentTarget': array([50., 11.]), 'dynamicTrap': False, 'previousTarget': array([50., 11.]), 'currentState': array([50.40856706, 11.15584084,  0.15896143]), 'targetState': array([50, 11], dtype=int32), 'currentDistance': 0.43727955272823427}
episode index:1077
target Thresh 75.68639692195875
target distance 19.0
model initialize at round 1077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 16.]), 'dynamicTrap': False, 'previousTarget': array([47.76686234, 16.08589282]), 'currentState': array([29.12830578, 21.14090258,  4.89524412]), 'targetState': array([48, 16], dtype=int32), 'currentDistance': 19.55938961777182}
done in step count: 41
reward sum = 0.43366438807656293
running average episode reward sum: 0.6738769426470126
{'scaleFactor': 20, 'currentTarget': array([48., 16.]), 'dynamicTrap': False, 'previousTarget': array([48., 16.]), 'currentState': array([47.6142159 , 15.8487084 ,  4.57033852]), 'targetState': array([48, 16], dtype=int32), 'currentDistance': 0.41438933182348914}
episode index:1078
target Thresh 75.68796102383573
target distance 50.0
model initialize at round 1078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.00005368, 16.95366357]), 'dynamicTrap': True, 'previousTarget': array([57., 17.]), 'currentState': array([77.       , 17.       ,  0.5820916], dtype=float32), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3232118913255312
running average episode reward sum: 0.6735519518672892
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([26.93513402, 16.88994353,  5.61304749]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.1277498411712562}
episode index:1079
target Thresh 75.68951732472203
target distance 39.0
model initialize at round 1079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.2125172 ,  9.19418362]), 'dynamicTrap': False, 'previousTarget': array([71.68542414,  8.53328126]), 'currentState': array([53.45679041,  6.07789434,  6.15098995]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5091812681689875
running average episode reward sum: 0.6733997567897907
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91., 12.]), 'currentState': array([90.165167  , 11.37761005,  1.32407827]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 1.0413046578809193}
episode index:1080
target Thresh 75.6910658635253
target distance 7.0
model initialize at round 1080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66., 16.]), 'dynamicTrap': False, 'previousTarget': array([66., 16.]), 'currentState': array([69.53730442, 22.16690719,  4.668764  ]), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 7.109378793390507}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6736654332497447
{'scaleFactor': 20, 'currentTarget': array([66., 16.]), 'dynamicTrap': False, 'previousTarget': array([66., 16.]), 'currentState': array([65.58406239, 15.63840964,  3.81684624]), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 0.5511367199054709}
episode index:1081
target Thresh 75.69260667895905
target distance 39.0
model initialize at round 1081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.99557171,  4.57915334]), 'dynamicTrap': True, 'previousTarget': array([76.99342862,  4.48734798]), 'currentState': array([57.       ,  5.       ,  4.1381307], dtype=float32), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.5874427501837873
running average episode reward sum: 0.6735857450029185
{'scaleFactor': 20, 'currentTarget': array([96.,  4.]), 'dynamicTrap': False, 'previousTarget': array([96.,  4.]), 'currentState': array([96.39115938,  3.6514323 ,  0.77900882]), 'targetState': array([96,  4], dtype=int32), 'currentDistance': 0.5239323421303865}
episode index:1082
target Thresh 75.69413980954377
target distance 47.0
model initialize at round 1082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.88063238, 19.12480108]), 'dynamicTrap': False, 'previousTarget': array([48.64310384, 18.23855458]), 'currentState': array([29.37197818, 23.53075287,  0.32241344]), 'targetState': array([76, 13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.347264378240932
running average episode reward sum: 0.6732844325682351
{'scaleFactor': 20, 'currentTarget': array([76., 13.]), 'dynamicTrap': False, 'previousTarget': array([76., 13.]), 'currentState': array([75.12824563, 12.65381961,  0.8000038 ]), 'targetState': array([76, 13], dtype=int32), 'currentDistance': 0.9379747033449066}
episode index:1083
target Thresh 75.6956652936078
target distance 73.0
model initialize at round 1083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.02845927,  4.933435  ]), 'dynamicTrap': True, 'previousTarget': array([58.02995695,  4.90575107]), 'currentState': array([78.       ,  6.       ,  5.8108377], dtype=float32), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.32702394083506775
running average episode reward sum: 0.6729650040703262
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.50889956, 1.14883039, 5.17485767]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.9826847659569805}
episode index:1084
target Thresh 75.69718316928832
target distance 62.0
model initialize at round 1084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.58957927, 11.7616444 ]), 'dynamicTrap': False, 'previousTarget': array([72.25517759, 11.81535122]), 'currentState': array([90.31771664, 15.04807043,  2.10302591]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.602023865852043
running average episode reward sum: 0.6728996205327978
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([29.62550137,  5.67140529,  5.90190409]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 0.768787546593933}
episode index:1085
target Thresh 75.6986934745323
target distance 61.0
model initialize at round 1085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.77020867, 14.26818805]), 'dynamicTrap': False, 'previousTarget': array([24.83019061, 13.39931926]), 'currentState': array([ 6.00864242, 17.34722996,  6.20132667]), 'targetState': array([66,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 46
reward sum = 0.5656679435626575
running average episode reward sum: 0.6728008804987553
{'scaleFactor': 20, 'currentTarget': array([66.,  8.]), 'dynamicTrap': False, 'previousTarget': array([66.,  8.]), 'currentState': array([66.38345643,  7.67038792,  1.30945524]), 'targetState': array([66,  8], dtype=int32), 'currentDistance': 0.5056510284831046}
episode index:1086
target Thresh 75.70019624709745
target distance 10.0
model initialize at round 1086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'dynamicTrap': False, 'previousTarget': array([22.,  3.]), 'currentState': array([12.22341855, 10.5017682 ,  0.88652503]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 12.323070681269497}
done in step count: 9
reward sum = 0.9043820750088044
running average episode reward sum: 0.6730139266758575
{'scaleFactor': 20, 'currentTarget': array([20.4258308 ,  2.83454575]), 'dynamicTrap': True, 'previousTarget': array([22.,  3.]), 'currentState': array([20.86981573,  2.72958281,  5.60333532]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.4562234505151698}
episode index:1087
target Thresh 75.70169152455317
target distance 22.0
model initialize at round 1087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.80242582, 19.19572971]), 'dynamicTrap': True, 'previousTarget': array([97.81660336, 19.29773591]), 'currentState': array([78.      , 22.      ,  2.425161], dtype=float32), 'targetState': array([100,  19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 16
reward sum = 0.8325939223777142
running average episode reward sum: 0.6731605994660246
{'scaleFactor': 20, 'currentTarget': array([100.,  19.]), 'dynamicTrap': False, 'previousTarget': array([100.,  19.]), 'currentState': array([99.48893944, 18.70764218,  1.76364524]), 'targetState': array([100,  19], dtype=int32), 'currentDistance': 0.5887749960703071}
episode index:1088
target Thresh 75.70317934428147
target distance 67.0
model initialize at round 1088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.99008902,  9.24913097]), 'dynamicTrap': False, 'previousTarget': array([68.03554768,  8.1919076 ]), 'currentState': array([88.97679149,  8.51993669,  0.68560505]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.4308011649736615
running average episode reward sum: 0.6729380471845807
{'scaleFactor': 20, 'currentTarget': array([21.93474939, 13.17935498]), 'dynamicTrap': True, 'previousTarget': array([21.87891021, 12.99609517]), 'currentState': array([22.1571336 , 14.12106791,  4.44104641]), 'targetState': array([21, 11], dtype=int32), 'currentDistance': 0.9676145781249599}
episode index:1089
target Thresh 75.7046597434779
target distance 71.0
model initialize at round 1089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.0689454,  9.6543389]), 'dynamicTrap': False, 'previousTarget': array([51.96833562, 10.87502335]), 'currentState': array([33.07991397, 10.31662478,  5.27770078]), 'targetState': array([103,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 73
reward sum = 0.24351704622142373
running average episode reward sum: 0.6725440829635136
{'scaleFactor': 20, 'currentTarget': array([103.,   8.]), 'dynamicTrap': False, 'previousTarget': array([103.,   8.]), 'currentState': array([102.63039288,   7.70187649,   0.28572071]), 'targetState': array([103,   8], dtype=int32), 'currentDistance': 0.47485476612613037}
episode index:1090
target Thresh 75.70613275915254
target distance 69.0
model initialize at round 1090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.83533954,  7.4388859 ]), 'dynamicTrap': True, 'previousTarget': array([51.86691472,  7.6965896 ]), 'currentState': array([32.       , 10.       ,  1.3063865], dtype=float32), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6130363877777993
running average episode reward sum: 0.6724895387882747
{'scaleFactor': 20, 'currentTarget': array([101.,   2.]), 'dynamicTrap': False, 'previousTarget': array([101.,   2.]), 'currentState': array([100.3674965 ,   1.3299756 ,   5.69764997]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 0.9214083662387333}
episode index:1091
target Thresh 75.70759842813086
target distance 11.0
model initialize at round 1091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.85002246,  7.89088096]), 'dynamicTrap': True, 'previousTarget': array([29.,  8.]), 'currentState': array([39.       , 19.       ,  0.5299902], dtype=float32), 'targetState': array([29,  8], dtype=int32), 'currentDistance': 15.047743015880124}
done in step count: 25
reward sum = 0.6913386068827876
running average episode reward sum: 0.6725067998396433
{'scaleFactor': 20, 'currentTarget': array([29.,  8.]), 'dynamicTrap': False, 'previousTarget': array([29.,  8.]), 'currentState': array([29.00853094,  7.55995367,  3.60025858]), 'targetState': array([29,  8], dtype=int32), 'currentDistance': 0.4401290126277893}
episode index:1092
target Thresh 75.70905678705465
target distance 58.0
model initialize at round 1092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.1058274 , 18.57704371]), 'dynamicTrap': False, 'previousTarget': array([32.9732997 , 17.96689829]), 'currentState': array([14.15443928, 19.97064047,  5.97230786]), 'targetState': array([71, 16], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 62
reward sum = 0.31219666501841836
running average episode reward sum: 0.6721771473832652
{'scaleFactor': 20, 'currentTarget': array([71., 16.]), 'dynamicTrap': False, 'previousTarget': array([71., 16.]), 'currentState': array([70.92582945, 16.72973508,  1.33019784]), 'targetState': array([71, 16], dtype=int32), 'currentDistance': 0.733494753620103}
episode index:1093
target Thresh 75.71050787238296
target distance 14.0
model initialize at round 1093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.37255251,  5.37212427]), 'dynamicTrap': True, 'previousTarget': array([87.,  5.]), 'currentState': array([73.        , 17.        ,  0.76520926], dtype=float32), 'targetState': array([87,  5], dtype=int32), 'currentDistance': 16.9790326442256}
done in step count: 19
reward sum = 0.7423483214081452
running average episode reward sum: 0.6722412892242386
{'scaleFactor': 20, 'currentTarget': array([87.,  5.]), 'dynamicTrap': False, 'previousTarget': array([87.,  5.]), 'currentState': array([86.17934727,  4.27295654,  0.20600593]), 'targetState': array([87,  5], dtype=int32), 'currentDistance': 1.0963863847833437}
episode index:1094
target Thresh 75.711951720393
target distance 8.0
model initialize at round 1094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([ 7.32574641, 15.54372026,  0.35283327]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 9.416217754686853}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6724958543024814
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([14.04557906, 20.09177003,  0.12489708]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 1.3174980159196492}
episode index:1095
target Thresh 75.71338836718105
target distance 36.0
model initialize at round 1095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.53997432, 10.73509632]), 'dynamicTrap': True, 'previousTarget': array([96.5237412 , 10.66139084]), 'currentState': array([77.      , 15.      ,  5.278985], dtype=float32), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.653037395338986
running average episode reward sum: 0.6724781002340839
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([112.90232034,   6.30203533,   6.01561254]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 0.70476662663893}
episode index:1096
target Thresh 75.71481784866336
target distance 7.0
model initialize at round 1096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'dynamicTrap': False, 'previousTarget': array([14., 11.]), 'currentState': array([ 9.85268472, 16.20631492,  4.94927087]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 6.656270655446249}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6726978259836278
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'dynamicTrap': False, 'previousTarget': array([14., 11.]), 'currentState': array([14.11209654, 10.88213683,  3.83096809]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.16265718837727364}
episode index:1097
target Thresh 75.71624020057703
target distance 25.0
model initialize at round 1097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.80320327, 11.95358911]), 'dynamicTrap': False, 'previousTarget': array([72.43046618, 12.57218647]), 'currentState': array([90.75798345, 18.33452227,  3.55808544]), 'targetState': array([66, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7175439350510425
running average episode reward sum: 0.6727386694345089
{'scaleFactor': 20, 'currentTarget': array([66., 10.]), 'dynamicTrap': False, 'previousTarget': array([66., 10.]), 'currentState': array([65.77511721, 10.15339504,  1.7514658 ]), 'targetState': array([66, 10], dtype=int32), 'currentDistance': 0.2722173927322737}
episode index:1098
target Thresh 75.71765545848093
target distance 67.0
model initialize at round 1098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.77010498, 13.63427777]), 'dynamicTrap': False, 'previousTarget': array([57.1780353 , 12.33734803]), 'currentState': array([77.51949383, 16.79048437,  0.63727927]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6157505500748817
running average episode reward sum: 0.6726868149127986
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'dynamicTrap': False, 'previousTarget': array([10.,  6.]), 'currentState': array([9.4479109 , 5.77736287, 4.09909137]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 0.5952895654451936}
episode index:1099
target Thresh 75.71906365775659
target distance 50.0
model initialize at round 1099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.60381016,  6.93317578]), 'dynamicTrap': False, 'previousTarget': array([32.9960012 ,  7.60007998]), 'currentState': array([14.60386554,  6.88611028,  5.29441565]), 'targetState': array([63,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6161123192417189
running average episode reward sum: 0.6726353835530975
{'scaleFactor': 20, 'currentTarget': array([63.,  7.]), 'dynamicTrap': False, 'previousTarget': array([63.,  7.]), 'currentState': array([63.57369413,  6.23237994,  1.15802025]), 'targetState': array([63,  7], dtype=int32), 'currentDistance': 0.9583138843687758}
episode index:1100
target Thresh 75.72046483360906
target distance 27.0
model initialize at round 1100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.02880401,  8.12700059]), 'dynamicTrap': False, 'previousTarget': array([49.94535509,  8.47743371]), 'currentState': array([30.18380754,  5.64182154,  5.76148117]), 'targetState': array([57,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8345137614500875
running average episode reward sum: 0.6727824120525498
{'scaleFactor': 20, 'currentTarget': array([56.72122699,  7.28027441]), 'dynamicTrap': True, 'previousTarget': array([57.,  9.]), 'currentState': array([56.18451001,  7.85230694,  2.19478532]), 'targetState': array([57,  9], dtype=int32), 'currentDistance': 0.7844018925697629}
episode index:1101
target Thresh 75.72185902106781
target distance 23.0
model initialize at round 1101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.47073382, 22.43732286]), 'dynamicTrap': False, 'previousTarget': array([65.92481176, 22.73259233]), 'currentState': array([45.72017337, 19.28845612,  5.47110987]), 'targetState': array([69, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 21
reward sum = 0.7460317315135858
running average episode reward sum: 0.6728488814894473
{'scaleFactor': 20, 'currentTarget': array([69., 23.]), 'dynamicTrap': False, 'previousTarget': array([69., 23.]), 'currentState': array([69.45746019, 22.83517046,  1.876769  ]), 'targetState': array([69, 23], dtype=int32), 'currentDistance': 0.4862495235351495}
episode index:1102
target Thresh 75.7232462549876
target distance 32.0
model initialize at round 1102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.38171506, 19.36714602]), 'dynamicTrap': False, 'previousTarget': array([89.76024068, 19.91246239]), 'currentState': array([71.54545778, 21.9211423 ,  5.45292959]), 'targetState': array([102,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5151421512247993
running average episode reward sum: 0.6727059016795972
{'scaleFactor': 20, 'currentTarget': array([102.,  18.]), 'dynamicTrap': False, 'previousTarget': array([102.,  18.]), 'currentState': array([102.81114803,  18.51091919,   2.31034031]), 'targetState': array([102,  18], dtype=int32), 'currentDistance': 0.9586446432891638}
episode index:1103
target Thresh 75.72462657004935
target distance 2.0
model initialize at round 1103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([31.40203829, 18.37857575,  0.53486615]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 1.7281709435820534}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6729933057541628
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([32.01395832, 20.11274962,  1.94236117]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 0.11361035107790428}
episode index:1104
target Thresh 75.72600000076102
target distance 70.0
model initialize at round 1104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.82943294,  2.85730535]), 'dynamicTrap': False, 'previousTarget': array([54.01834208,  2.85635677]), 'currentState': array([73.81096805,  1.9980877 ,  3.12741526]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = -0.025695761854486132
running average episode reward sum: 0.672361007955422
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'dynamicTrap': False, 'previousTarget': array([4., 5.]), 'currentState': array([3.27105534, 4.53077117, 4.11288855]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8669117704632663}
episode index:1105
target Thresh 75.72736658145843
target distance 34.0
model initialize at round 1105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.94008187, 13.94033656]), 'dynamicTrap': True, 'previousTarget': array([22.97110963, 13.84359429]), 'currentState': array([42.       , 20.       ,  4.9610944], dtype=float32), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6746030393230901
running average episode reward sum: 0.6723630351085572
{'scaleFactor': 20, 'currentTarget': array([8., 9.]), 'dynamicTrap': False, 'previousTarget': array([8., 9.]), 'currentState': array([8.81720917, 9.64358479, 3.15995684]), 'targetState': array([8, 9], dtype=int32), 'currentDistance': 1.0402077682444646}
episode index:1106
target Thresh 75.72872634630617
target distance 30.0
model initialize at round 1106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.69534208, 10.55453177]), 'dynamicTrap': False, 'previousTarget': array([63.8434743 ,  9.74695771]), 'currentState': array([82.08796823,  5.66310547,  2.45795649]), 'targetState': array([53, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7210281364009612
running average episode reward sum: 0.6724069963563372
{'scaleFactor': 20, 'currentTarget': array([53., 13.]), 'dynamicTrap': False, 'previousTarget': array([53., 13.]), 'currentState': array([52.59821228, 12.90738158,  4.2570861 ]), 'targetState': array([53, 13], dtype=int32), 'currentDistance': 0.4123245587976692}
episode index:1107
target Thresh 75.73007932929845
target distance 8.0
model initialize at round 1107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 22.]), 'currentState': array([17.59983081, 15.52242651,  1.32563418]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 11.580833728751097}
done in step count: 10
reward sum = 0.8661484229157944
running average episode reward sum: 0.6725818532395136
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.81255765, 21.8052386 ,  2.55653074]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.8355728242769963}
episode index:1108
target Thresh 75.7314255642599
target distance 16.0
model initialize at round 1108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([27.26726045, 11.3585305 ,  3.23641968]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 15.620039398702248}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6728074283893679
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([13.74198865,  4.99944498,  4.76883473]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.7419888530733171}
episode index:1109
target Thresh 75.73276508484646
target distance 4.0
model initialize at round 1109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.57149697,  6.98340259]), 'dynamicTrap': True, 'previousTarget': array([73.,  6.]), 'currentState': array([74.        , 10.        ,  0.29963097], dtype=float32), 'targetState': array([73,  6], dtype=int32), 'currentDistance': 3.872658888342261}
done in step count: 5
reward sum = 0.9212890498999999
running average episode reward sum: 0.6730312857060441
{'scaleFactor': 20, 'currentTarget': array([73.,  6.]), 'dynamicTrap': False, 'previousTarget': array([73.,  6.]), 'currentState': array([72.62645455,  6.20316908,  4.99972445]), 'targetState': array([73,  6], dtype=int32), 'currentDistance': 0.42522215573729233}
episode index:1110
target Thresh 75.73409792454623
target distance 37.0
model initialize at round 1110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.96521249, 19.17910562]), 'dynamicTrap': True, 'previousTarget': array([81.97084547, 19.07950516]), 'currentState': array([62.      , 18.      ,  5.042414], dtype=float32), 'targetState': array([99, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.43651325367353055
running average episode reward sum: 0.6728183981884631
{'scaleFactor': 20, 'currentTarget': array([99., 20.]), 'dynamicTrap': False, 'previousTarget': array([99., 20.]), 'currentState': array([99.15622555, 19.45911672,  1.3464918 ]), 'targetState': array([99, 20], dtype=int32), 'currentDistance': 0.5629930220302025}
episode index:1111
target Thresh 75.73542411668026
target distance 3.0
model initialize at round 1111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.15917637,  5.1068566 ]), 'dynamicTrap': True, 'previousTarget': array([40.,  4.]), 'currentState': array([38.        ,  7.        ,  0.04096813], dtype=float32), 'targetState': array([40,  4], dtype=int32), 'currentDistance': 2.2198382328336774}
done in step count: 3
reward sum = 0.950399
running average episode reward sum: 0.6730680210318187
{'scaleFactor': 20, 'currentTarget': array([40.,  4.]), 'dynamicTrap': False, 'previousTarget': array([40.,  4.]), 'currentState': array([39.20178335,  4.34137645,  6.09276005]), 'targetState': array([40,  4], dtype=int32), 'currentDistance': 0.8681518868802313}
episode index:1112
target Thresh 75.73674369440344
target distance 61.0
model initialize at round 1112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.51511934,  5.07639869]), 'dynamicTrap': False, 'previousTarget': array([71.02414326,  6.01758082]), 'currentState': array([90.50806458,  5.6075678 ,  3.4404428 ]), 'targetState': array([30,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5242751558071485
running average episode reward sum: 0.6729343347198469
{'scaleFactor': 20, 'currentTarget': array([30.,  4.]), 'dynamicTrap': False, 'previousTarget': array([30.,  4.]), 'currentState': array([29.81134643,  4.80339692,  4.54522662]), 'targetState': array([30,  4], dtype=int32), 'currentDistance': 0.8252495310430704}
episode index:1113
target Thresh 75.73805669070526
target distance 3.0
model initialize at round 1113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82., 20.]), 'dynamicTrap': False, 'previousTarget': array([82., 20.]), 'currentState': array([7.98871258e+01, 2.04035928e+01, 7.46293068e-02]), 'targetState': array([82, 20], dtype=int32), 'currentDistance': 2.151075241355592}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6732189538089673
{'scaleFactor': 20, 'currentTarget': array([82., 20.]), 'dynamicTrap': False, 'previousTarget': array([82., 20.]), 'currentState': array([81.66469757, 19.96783381,  5.72138769]), 'targetState': array([82, 20], dtype=int32), 'currentDistance': 0.3368417767228322}
episode index:1114
target Thresh 75.73936313841071
target distance 25.0
model initialize at round 1114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.56152844,  9.99785892]), 'dynamicTrap': False, 'previousTarget': array([98.55225396, 10.33254095]), 'currentState': array([118.24717666,  13.52989151,   3.88059199]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7826162387488592
running average episode reward sum: 0.6733170679658641
{'scaleFactor': 20, 'currentTarget': array([93.,  9.]), 'dynamicTrap': False, 'previousTarget': array([93.,  9.]), 'currentState': array([92.51661683,  8.48592178,  3.80501811]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 0.7056455929451318}
episode index:1115
target Thresh 75.74066307018103
target distance 15.0
model initialize at round 1115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42., 21.]), 'dynamicTrap': False, 'previousTarget': array([42., 21.]), 'currentState': array([31.84238454,  4.47779084,  6.1286602 ]), 'targetState': array([42, 21], dtype=int32), 'currentDistance': 19.394858790437944}
done in step count: 18
reward sum = 0.7705168281943031
running average episode reward sum: 0.6734041645252086
{'scaleFactor': 20, 'currentTarget': array([43.05652721, 19.76227652]), 'dynamicTrap': True, 'previousTarget': array([42., 21.]), 'currentState': array([43.72238703, 18.93766579,  3.12057815]), 'targetState': array([42, 21], dtype=int32), 'currentDistance': 1.0598830874279368}
episode index:1116
target Thresh 75.74195651851461
target distance 17.0
model initialize at round 1116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.10793247,   8.13525099]), 'dynamicTrap': True, 'previousTarget': array([112.,   8.]), 'currentState': array([95.      ,  8.      ,  3.939859], dtype=float32), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 17.108467090326645}
done in step count: 52
reward sum = 0.25524848738548317
running average episode reward sum: 0.6730298085027022
{'scaleFactor': 20, 'currentTarget': array([112.,   8.]), 'dynamicTrap': False, 'previousTarget': array([112.,   8.]), 'currentState': array([111.41886892,   8.26521132,   0.41636838]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 0.6387882111196125}
episode index:1117
target Thresh 75.74324351574771
target distance 15.0
model initialize at round 1117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'dynamicTrap': False, 'previousTarget': array([5., 7.]), 'currentState': array([ 8.60183241, 21.33758505,  4.59403992]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 14.78308296014303}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.673253167076875
{'scaleFactor': 20, 'currentTarget': array([5., 7.]), 'dynamicTrap': False, 'previousTarget': array([5., 7.]), 'currentState': array([4.21431647, 6.96174735, 4.8317748 ]), 'targetState': array([5, 7], dtype=int32), 'currentDistance': 0.7866141878658958}
episode index:1118
target Thresh 75.74452409405534
target distance 13.0
model initialize at round 1118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.,  2.]), 'dynamicTrap': False, 'previousTarget': array([98.,  2.]), 'currentState': array([107.77773064,  13.95358894,   3.05493057]), 'targetState': array([98,  2], dtype=int32), 'currentDistance': 15.443196074082909}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6734678802854602
{'scaleFactor': 20, 'currentTarget': array([98.,  2.]), 'dynamicTrap': False, 'previousTarget': array([98.,  2.]), 'currentState': array([97.84539911,  1.91957318,  5.23451436]), 'targetState': array([98,  2], dtype=int32), 'currentDistance': 0.17426964646723983}
episode index:1119
target Thresh 75.745798285452
target distance 35.0
model initialize at round 1119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.82105028,  6.96223164]), 'dynamicTrap': False, 'previousTarget': array([55.,  7.]), 'currentState': array([74.82098535,  6.91126597,  3.52349615]), 'targetState': array([40,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6346859847822394
running average episode reward sum: 0.6734332535930466
{'scaleFactor': 20, 'currentTarget': array([40.59566834,  8.73421178]), 'dynamicTrap': True, 'previousTarget': array([40.,  7.]), 'currentState': array([40.83327926,  9.72066011,  5.07501022]), 'targetState': array([40,  7], dtype=int32), 'currentDistance': 1.0146621441548709}
episode index:1120
target Thresh 75.74706612179257
target distance 66.0
model initialize at round 1120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.15853835, 14.87320461]), 'dynamicTrap': False, 'previousTarget': array([65.88845169, 15.89061876]), 'currentState': array([47.23273076, 16.59430662,  6.03760171]), 'targetState': array([112,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.580676438283043
running average episode reward sum: 0.6733505088871501
{'scaleFactor': 20, 'currentTarget': array([112.,  11.]), 'dynamicTrap': False, 'previousTarget': array([112.,  11.]), 'currentState': array([112.8985038 ,  11.53375876,   2.17668688]), 'targetState': array([112,  11], dtype=int32), 'currentDistance': 1.0450873132540666}
episode index:1121
target Thresh 75.748327634773
target distance 54.0
model initialize at round 1121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.74386529, 16.62793383]), 'dynamicTrap': False, 'previousTarget': array([67.83405013, 16.57108057]), 'currentState': array([49.92508882, 13.94165406,  5.77421424]), 'targetState': array([102,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4914762004175416
running average episode reward sum: 0.673188410573006
{'scaleFactor': 20, 'currentTarget': array([102.,  21.]), 'dynamicTrap': False, 'previousTarget': array([102.,  21.]), 'currentState': array([102.97814181,  21.15993629,   2.41449692]), 'targetState': array([102,  21], dtype=int32), 'currentDistance': 0.9911311774261827}
episode index:1122
target Thresh 75.7495828559312
target distance 19.0
model initialize at round 1122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.23660259,  4.83107618]), 'dynamicTrap': False, 'previousTarget': array([16.69147429,  4.97927459]), 'currentState': array([32.83614151, 15.98703009,  3.4459635 ]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.542756090515098
running average episode reward sum: 0.6730722642506036
{'scaleFactor': 20, 'currentTarget': array([15.,  4.]), 'dynamicTrap': False, 'previousTarget': array([15.,  4.]), 'currentState': array([14.34030778,  3.72778553,  4.59134839]), 'targetState': array([15,  4], dtype=int32), 'currentDistance': 0.7136487520255792}
episode index:1123
target Thresh 75.75083181664776
target distance 49.0
model initialize at round 1123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.00005805,  8.04818664]), 'dynamicTrap': True, 'previousTarget': array([80.,  8.]), 'currentState': array([100.      ,   8.      ,   3.812688], dtype=float32), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6880295325982749
running average episode reward sum: 0.673085571428849
{'scaleFactor': 20, 'currentTarget': array([51.,  8.]), 'dynamicTrap': False, 'previousTarget': array([51.,  8.]), 'currentState': array([50.64796336,  7.8642487 ,  3.31693028]), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 0.3773038738701669}
episode index:1124
target Thresh 75.75207454814674
target distance 55.0
model initialize at round 1124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.90193542, 11.69797543]), 'dynamicTrap': False, 'previousTarget': array([43.45968477, 10.7366585 ]), 'currentState': array([62.30837901, 16.53429087,  3.00429511]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6048151725671168
running average episode reward sum: 0.6730248866298607
{'scaleFactor': 20, 'currentTarget': array([8., 3.]), 'dynamicTrap': False, 'previousTarget': array([8., 3.]), 'currentState': array([7.6792155 , 2.73605709, 5.07846422]), 'targetState': array([8, 3], dtype=int32), 'currentDistance': 0.4154137153297155}
episode index:1125
target Thresh 75.75331108149652
target distance 32.0
model initialize at round 1125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.04748404,  3.0047765 ]), 'dynamicTrap': False, 'previousTarget': array([15.03894843,  3.24756572]), 'currentState': array([33.96681983,  1.21032349,  3.00097859]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6731683048135376
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'dynamicTrap': False, 'previousTarget': array([3., 4.]), 'currentState': array([2.56990397, 3.77628394, 3.99546486]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.4848004467363636}
episode index:1126
target Thresh 75.75454144761049
target distance 63.0
model initialize at round 1126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.09386574,  7.5633442 ]), 'dynamicTrap': False, 'previousTarget': array([45.99748095,  8.68257967]), 'currentState': array([27.09495139,  7.35495843,  5.57044733]), 'targetState': array([89,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6507568765366443
running average episode reward sum: 0.6731484188966993
{'scaleFactor': 20, 'currentTarget': array([89.,  8.]), 'dynamicTrap': False, 'previousTarget': array([89.,  8.]), 'currentState': array([88.82119619,  7.20269619,  2.53115492]), 'targetState': array([89,  8], dtype=int32), 'currentDistance': 0.8171071927584969}
episode index:1127
target Thresh 75.75576567724787
target distance 9.0
model initialize at round 1127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([47.72437422, 10.59760696,  1.99279812]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 10.756460562347574}
done in step count: 7
reward sum = 0.92226434790699
running average episode reward sum: 0.6733692663514956
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([38.09081661,  6.66587261,  2.75775821]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 0.6720371976212967}
episode index:1128
target Thresh 75.75698380101446
target distance 13.0
model initialize at round 1128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58., 17.]), 'dynamicTrap': False, 'previousTarget': array([58., 17.]), 'currentState': array([66.51299634,  3.04300087,  0.21725195]), 'targetState': array([58, 17], dtype=int32), 'currentDistance': 16.34836173467187}
done in step count: 34
reward sum = 0.5694216114905201
running average episode reward sum: 0.6732771957980315
{'scaleFactor': 20, 'currentTarget': array([58., 17.]), 'dynamicTrap': False, 'previousTarget': array([58., 17.]), 'currentState': array([57.68538048, 16.25959964,  0.66244201]), 'targetState': array([58, 17], dtype=int32), 'currentDistance': 0.8044738275393128}
episode index:1129
target Thresh 75.7581958493634
target distance 20.0
model initialize at round 1129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.,  2.]), 'dynamicTrap': False, 'previousTarget': array([67.,  2.]), 'currentState': array([85.70912763,  1.25071476,  2.82820463]), 'targetState': array([67,  2], dtype=int32), 'currentDistance': 18.724125752766668}
done in step count: 20
reward sum = 0.7356627004662091
running average episode reward sum: 0.6733324042092423
{'scaleFactor': 20, 'currentTarget': array([67.,  2.]), 'dynamicTrap': False, 'previousTarget': array([67.,  2.]), 'currentState': array([67.59075928,  2.16497468,  2.93105938]), 'targetState': array([67,  2], dtype=int32), 'currentDistance': 0.6133621881713313}
episode index:1130
target Thresh 75.75940185259601
target distance 5.0
model initialize at round 1130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([107.09836196,  21.40454103,   5.78396225]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 3.405961643524809}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6736036399261218
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([107.36979526,  17.85655441,   4.82788289]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 0.39664237353031034}
episode index:1131
target Thresh 75.76060184086239
target distance 24.0
model initialize at round 1131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.83820835, 16.53879689]), 'dynamicTrap': True, 'previousTarget': array([39.84555753, 16.48069469]), 'currentState': array([20.     , 14.     ,  5.91482], dtype=float32), 'targetState': array([44, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7210565354393319
running average episode reward sum: 0.6736455594451264
{'scaleFactor': 20, 'currentTarget': array([44., 17.]), 'dynamicTrap': False, 'previousTarget': array([44., 17.]), 'currentState': array([44.30352983, 17.11084535,  5.87452051]), 'targetState': array([44, 17], dtype=int32), 'currentDistance': 0.32313627157552627}
episode index:1132
target Thresh 75.76179584416232
target distance 67.0
model initialize at round 1132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.94129354, 17.53127792]), 'dynamicTrap': True, 'previousTarget': array([58.94453985, 17.4883985 ]), 'currentState': array([39.     , 16.     ,  5.50142], dtype=float32), 'targetState': array([106,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 85
reward sum = 0.0999307813660264
running average episode reward sum: 0.6731391915915702
{'scaleFactor': 20, 'currentTarget': array([106.,  21.]), 'dynamicTrap': False, 'previousTarget': array([106.,  21.]), 'currentState': array([1.06464005e+02, 2.05031026e+01, 8.15060106e-02]), 'targetState': array([106,  21], dtype=int32), 'currentDistance': 0.6798588102503288}
episode index:1133
target Thresh 75.76298389234596
target distance 53.0
model initialize at round 1133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.35056829, 10.25839121]), 'dynamicTrap': False, 'previousTarget': array([56.04430633, 11.10855109]), 'currentState': array([37.46716422,  3.66922962,  6.05999708]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.602591342452729
running average episode reward sum: 0.6730769800843931
{'scaleFactor': 20, 'currentTarget': array([90., 22.]), 'dynamicTrap': False, 'previousTarget': array([90., 22.]), 'currentState': array([90.64408913, 21.53598521,  2.25255249]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 0.7938265165499168}
episode index:1134
target Thresh 75.76416601511454
target distance 39.0
model initialize at round 1134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.00712464, 17.46620699]), 'dynamicTrap': True, 'previousTarget': array([45.00657138, 17.48734798]), 'currentState': array([65.      , 18.      ,  3.825721], dtype=float32), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5638345978236774
running average episode reward sum: 0.6729807312894497
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([26.81621413, 16.41496467,  2.56531077]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 1.0042269905857022}
episode index:1135
target Thresh 75.76534224202123
target distance 42.0
model initialize at round 1135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.05463291, 11.10200642]), 'dynamicTrap': False, 'previousTarget': array([92.050826, 10.424941]), 'currentState': array([113.0394786 ,  10.32358314,   1.91505116]), 'targetState': array([70, 12], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 36
reward sum = 0.6067250683923997
running average episode reward sum: 0.6729224076425333
{'scaleFactor': 20, 'currentTarget': array([70., 12.]), 'dynamicTrap': False, 'previousTarget': array([70., 12.]), 'currentState': array([69.78978968, 11.89883321,  3.65512265]), 'targetState': array([70, 12], dtype=int32), 'currentDistance': 0.23328758571971184}
episode index:1136
target Thresh 75.76651260247175
target distance 25.0
model initialize at round 1136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.12885461, 14.7925052 ]), 'dynamicTrap': False, 'previousTarget': array([79.76931317, 13.68609452]), 'currentState': array([94.27889216,  2.9952052 ,  2.42652667]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6730794308293867
{'scaleFactor': 20, 'currentTarget': array([71., 20.]), 'dynamicTrap': False, 'previousTarget': array([71., 20.]), 'currentState': array([71.95739337, 20.12787148,  3.59701765]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 0.965895017485833}
episode index:1137
target Thresh 75.76767712572517
target distance 16.0
model initialize at round 1137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([36.4790055 , 21.43936004,  3.87038374]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 17.670033124264332}
done in step count: 12
reward sum = 0.8569808817161292
running average episode reward sum: 0.6732410314013434
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([30.35126553,  5.85443848,  6.09324373]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 0.9238249795046601}
episode index:1138
target Thresh 75.76883584089464
target distance 32.0
model initialize at round 1138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.0386221 , 15.83811237]), 'dynamicTrap': False, 'previousTarget': array([88.43151074, 14.80522479]), 'currentState': array([71.24341501,  6.70955253,  0.49515235]), 'targetState': array([103,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.35772319424019805
running average episode reward sum: 0.672964018374863
{'scaleFactor': 20, 'currentTarget': array([103.,  23.]), 'dynamicTrap': False, 'previousTarget': array([103.,  23.]), 'currentState': array([102.25945901,  23.63880979,   3.21924741]), 'targetState': array([103,  23], dtype=int32), 'currentDistance': 0.9779973933041668}
episode index:1139
target Thresh 75.76998877694808
target distance 57.0
model initialize at round 1139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.55967292, 11.73028905]), 'dynamicTrap': False, 'previousTarget': array([66.50070893, 11.44720674]), 'currentState': array([84.0398311 ,  7.20000899,  3.34184134]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5840595956454409
running average episode reward sum: 0.6728860320391354
{'scaleFactor': 20, 'currentTarget': array([29., 20.]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([28.92192588, 19.79728891,  1.58744418]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 0.2172265055263559}
episode index:1140
target Thresh 75.77113596270897
target distance 42.0
model initialize at round 1140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.32255873, 15.5774719 ]), 'dynamicTrap': True, 'previousTarget': array([64.20101013, 14.82842712]), 'currentState': array([84.       , 12.       ,  3.7008443], dtype=float32), 'targetState': array([42, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6224789756238268
running average episode reward sum: 0.6728418540755813
{'scaleFactor': 20, 'currentTarget': array([42., 18.]), 'dynamicTrap': False, 'previousTarget': array([42., 18.]), 'currentState': array([41.0382748 , 18.29404587,  5.12579246]), 'targetState': array([42, 18], dtype=int32), 'currentDistance': 1.0056730768491153}
episode index:1141
target Thresh 75.772277426857
target distance 47.0
model initialize at round 1141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.71399276, 19.80783226]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([10.7145703 , 19.65584134,  5.61494533]), 'targetState': array([56, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6193636771226231
running average episode reward sum: 0.6727950255493528
{'scaleFactor': 20, 'currentTarget': array([56., 20.]), 'dynamicTrap': False, 'previousTarget': array([56., 20.]), 'currentState': array([56.40635877, 20.01846883,  0.59684791]), 'targetState': array([56, 20], dtype=int32), 'currentDistance': 0.4067782531388623}
episode index:1142
target Thresh 75.77341319792885
target distance 58.0
model initialize at round 1142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.73090624, 14.93750182]), 'dynamicTrap': False, 'previousTarget': array([32.9732997 , 15.03310171]), 'currentState': array([14.76316607, 13.80200535,  0.47733837]), 'targetState': array([71, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5353737350694635
running average episode reward sum: 0.672674796948758
{'scaleFactor': 20, 'currentTarget': array([71., 17.]), 'dynamicTrap': False, 'previousTarget': array([71., 17.]), 'currentState': array([70.40691517, 16.71279895,  1.88398486]), 'targetState': array([71, 17], dtype=int32), 'currentDistance': 0.6589643822173119}
episode index:1143
target Thresh 75.77454330431884
target distance 36.0
model initialize at round 1143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.82762294,  6.60337234]), 'dynamicTrap': False, 'previousTarget': array([86.4762588 ,  6.33860916]), 'currentState': array([104.32266121,   2.13755967,   2.04976511]), 'targetState': array([70, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.4920584678757017
running average episode reward sum: 0.6725169155422256
{'scaleFactor': 20, 'currentTarget': array([70., 10.]), 'dynamicTrap': False, 'previousTarget': array([70., 10.]), 'currentState': array([70.28666925,  9.58008405,  5.70624443]), 'targetState': array([70, 10], dtype=int32), 'currentDistance': 0.5084374723505104}
episode index:1144
target Thresh 75.7756677742797
target distance 17.0
model initialize at round 1144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.95775179,  4.32140332]), 'dynamicTrap': True, 'previousTarget': array([69.,  3.]), 'currentState': array([63.        , 20.        ,  0.08704308], dtype=float32), 'targetState': array([69,  3], dtype=int32), 'currentDistance': 16.44377379112991}
done in step count: 16
reward sum = 0.8130096706715264
running average episode reward sum: 0.6726396166384083
{'scaleFactor': 20, 'currentTarget': array([69.,  3.]), 'dynamicTrap': False, 'previousTarget': array([69.,  3.]), 'currentState': array([69.87705963,  3.18359782,  1.90512052]), 'targetState': array([69,  3], dtype=int32), 'currentDistance': 0.8960701687293756}
episode index:1145
target Thresh 75.77678663592323
target distance 43.0
model initialize at round 1145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.56545464, 13.24322637]), 'dynamicTrap': True, 'previousTarget': array([61.55118023, 13.27723824]), 'currentState': array([80.       , 21.       ,  3.0713835], dtype=float32), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.5967469973542097
running average episode reward sum: 0.6725733927123314
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([36.43878269,  3.04831999,  3.161743  ]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 0.5632936088749967}
episode index:1146
target Thresh 75.77789991722103
target distance 20.0
model initialize at round 1146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.5913968 ,  3.77690284]), 'dynamicTrap': False, 'previousTarget': array([11.20729355,  4.7615699 ]), 'currentState': array([ 2.28169678, 21.47801821,  5.28143001]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6727368495230802
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([11.51166145,  2.20626938,  1.58179968]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.9319242640645157}
episode index:1147
target Thresh 75.77900764600521
target distance 17.0
model initialize at round 1147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 20.]), 'dynamicTrap': False, 'previousTarget': array([87., 20.]), 'currentState': array([71.71607453, 15.5564501 ,  5.97745169]), 'targetState': array([87, 20], dtype=int32), 'currentDistance': 15.916768316322308}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.672946588545694
{'scaleFactor': 20, 'currentTarget': array([87., 20.]), 'dynamicTrap': False, 'previousTarget': array([87., 20.]), 'currentState': array([86.37409888, 20.25485252,  5.46788715]), 'targetState': array([87, 20], dtype=int32), 'currentDistance': 0.6757973196892736}
episode index:1148
target Thresh 75.78010984996901
target distance 18.0
model initialize at round 1148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.36514476,  11.35853204]), 'dynamicTrap': False, 'previousTarget': array([113.48314552,  11.28714138]), 'currentState': array([95.95037356, 21.19345674,  1.72940373]), 'targetState': array([114,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8320003537189393
running average episode reward sum: 0.6730850165397526
{'scaleFactor': 20, 'currentTarget': array([114.,  11.]), 'dynamicTrap': False, 'previousTarget': array([114.,  11.]), 'currentState': array([113.77371102,  10.05003496,   4.89037383]), 'targetState': array([114,  11], dtype=int32), 'currentDistance': 0.9765450732507288}
episode index:1149
target Thresh 75.78120655666761
target distance 19.0
model initialize at round 1149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.50294381, 17.69208803]), 'dynamicTrap': True, 'previousTarget': array([58., 16.]), 'currentState': array([77.       , 17.       ,  5.1129675], dtype=float32), 'targetState': array([58, 16], dtype=int32), 'currentDistance': 18.50999928286275}
done in step count: 18
reward sum = 0.7611548686077351
running average episode reward sum: 0.6731615990198117
{'scaleFactor': 20, 'currentTarget': array([58., 16.]), 'dynamicTrap': False, 'previousTarget': array([58., 16.]), 'currentState': array([57.08485504, 15.60375905,  5.37485408]), 'targetState': array([58, 16], dtype=int32), 'currentDistance': 0.9972448000359404}
episode index:1150
target Thresh 75.78229779351874
target distance 60.0
model initialize at round 1150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.01383244, 17.76373296]), 'dynamicTrap': False, 'previousTarget': array([73.0992562 , 16.99007438]), 'currentState': array([93.95185898, 16.19048865,  1.9063279 ]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.554577986432863
running average episode reward sum: 0.6730585724232983
{'scaleFactor': 20, 'currentTarget': array([33., 21.]), 'dynamicTrap': False, 'previousTarget': array([33., 21.]), 'currentState': array([33.56116071, 20.92964662,  3.48221573]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 0.5655536560098693}
episode index:1151
target Thresh 75.78338358780337
target distance 4.0
model initialize at round 1151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.,  2.]), 'dynamicTrap': False, 'previousTarget': array([46.,  2.]), 'currentState': array([48.20712035,  4.35942919,  4.00216603]), 'targetState': array([46,  2], dtype=int32), 'currentDistance': 3.23083369345785}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6733251014402919
{'scaleFactor': 20, 'currentTarget': array([46.,  2.]), 'dynamicTrap': False, 'previousTarget': array([46.,  2.]), 'currentState': array([45.51766933,  1.9931239 ,  3.35634194]), 'targetState': array([46,  2], dtype=int32), 'currentDistance': 0.48237967854582736}
episode index:1152
target Thresh 75.7844639666664
target distance 42.0
model initialize at round 1152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.11722591, 10.24277857]), 'dynamicTrap': False, 'previousTarget': array([69.00566653,  9.52394444]), 'currentState': array([88.08268073, 11.41777378,  3.14549935]), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6448508404339643
running average episode reward sum: 0.6733004056371641
{'scaleFactor': 20, 'currentTarget': array([47.,  9.]), 'dynamicTrap': False, 'previousTarget': array([47.,  9.]), 'currentState': array([47.29452646,  9.7985522 ,  3.88335908]), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 0.8511353909421171}
episode index:1153
target Thresh 75.78553895711738
target distance 44.0
model initialize at round 1153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.49118779, 12.17603555]), 'dynamicTrap': False, 'previousTarget': array([52.06979624, 11.45347856]), 'currentState': array([69.4664117 ,  5.856156  ,  1.75238299]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6224508592542839
running average episode reward sum: 0.6732563419054631
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'dynamicTrap': False, 'previousTarget': array([27., 20.]), 'currentState': array([27.12812208, 20.05380121,  5.17805023]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.13895984014470583}
episode index:1154
target Thresh 75.78660858603111
target distance 8.0
model initialize at round 1154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 18.]), 'dynamicTrap': False, 'previousTarget': array([53., 18.]), 'currentState': array([60.15592793, 22.50656406,  3.01550174]), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 8.456738391125748}
done in step count: 7
reward sum = 0.9026613579069899
running average episode reward sum: 0.6734549609669364
{'scaleFactor': 20, 'currentTarget': array([53., 18.]), 'dynamicTrap': False, 'previousTarget': array([53., 18.]), 'currentState': array([53.08902703, 18.23093484,  4.04506381]), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 0.24750093620783817}
episode index:1155
target Thresh 75.78767288014839
target distance 53.0
model initialize at round 1155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.01152356,  8.67882966]), 'dynamicTrap': True, 'previousTarget': array([83.01422475,  8.7541802 ]), 'currentState': array([103.       ,   8.       ,   1.5806283], dtype=float32), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.42341156326666035
running average episode reward sum: 0.6732386604498947
{'scaleFactor': 20, 'currentTarget': array([50., 10.]), 'dynamicTrap': False, 'previousTarget': array([50., 10.]), 'currentState': array([49.90675093, 10.38847185,  4.39323282]), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 0.39950691047603476}
episode index:1156
target Thresh 75.7887318660766
target distance 55.0
model initialize at round 1156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.24581144, 16.16176761]), 'dynamicTrap': False, 'previousTarget': array([70.88204353, 15.8310498 ]), 'currentState': array([52.3961197 , 18.60916177,  5.684609  ]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 50
reward sum = 0.4755839510416555
running average episode reward sum: 0.6730678266474674
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([106.36091137,  12.13304567,   1.83758224]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 0.3846533111277216}
episode index:1157
target Thresh 75.78978557029048
target distance 17.0
model initialize at round 1157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.90235184, 10.60616478]), 'dynamicTrap': True, 'previousTarget': array([68., 10.]), 'currentState': array([85.      ,  8.      ,  3.349923], dtype=float32), 'targetState': array([68, 10], dtype=int32), 'currentDistance': 15.320935837013845}
done in step count: 11
reward sum = 0.8754382542587164
running average episode reward sum: 0.6732425852205342
{'scaleFactor': 20, 'currentTarget': array([68., 10.]), 'dynamicTrap': False, 'previousTarget': array([68., 10.]), 'currentState': array([67.4506411 , 10.69547267,  3.91722414]), 'targetState': array([68, 10], dtype=int32), 'currentDistance': 0.8862716485931911}
episode index:1158
target Thresh 75.79083401913265
target distance 33.0
model initialize at round 1158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.56317858, 15.04934821]), 'dynamicTrap': False, 'previousTarget': array([96.85467564, 14.40662735]), 'currentState': array([76.61304672, 13.63788133,  2.81279474]), 'targetState': array([110,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6969711775152752
running average episode reward sum: 0.6732630585529713
{'scaleFactor': 20, 'currentTarget': array([110.,  16.]), 'dynamicTrap': False, 'previousTarget': array([110.,  16.]), 'currentState': array([109.11038168,  16.48447852,   0.64713824]), 'targetState': array([110,  16], dtype=int32), 'currentDistance': 1.0129857861655776}
episode index:1159
target Thresh 75.79187723881441
target distance 21.0
model initialize at round 1159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  12.]), 'dynamicTrap': False, 'previousTarget': array([108.97736275,  12.04869701]), 'currentState': array([90.25602139, 11.91299683,  0.29661387]), 'targetState': array([110,  12], dtype=int32), 'currentDistance': 19.744170302805976}
done in step count: 17
reward sum = 0.7784105718549746
running average episode reward sum: 0.6733537029609903
{'scaleFactor': 20, 'currentTarget': array([110.,  12.]), 'dynamicTrap': False, 'previousTarget': array([110.,  12.]), 'currentState': array([109.1531897 ,  11.51874229,   1.08088255]), 'targetState': array([110,  12], dtype=int32), 'currentDistance': 0.9740106114750093}
episode index:1160
target Thresh 75.79291525541629
target distance 6.0
model initialize at round 1160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.80156762,  6.97509237]), 'dynamicTrap': True, 'previousTarget': array([64.,  7.]), 'currentState': array([58.        ,  3.        ,  0.10729603], dtype=float32), 'targetState': array([64,  7], dtype=int32), 'currentDistance': 7.032748124319889}
done in step count: 16
reward sum = 0.8024478209948755
running average episode reward sum: 0.6734648951384528
{'scaleFactor': 20, 'currentTarget': array([64.,  7.]), 'dynamicTrap': False, 'previousTarget': array([64.,  7.]), 'currentState': array([63.48073171,  7.65972963,  5.33126021]), 'targetState': array([64,  7], dtype=int32), 'currentDistance': 0.8395729517592858}
episode index:1161
target Thresh 75.79394809488879
target distance 58.0
model initialize at round 1161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.12762448, 15.19234589]), 'dynamicTrap': False, 'previousTarget': array([89.04739343, 14.37604183]), 'currentState': array([110.10631518,  14.26935189,   1.81502872]), 'targetState': array([51, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5981706593850756
running average episode reward sum: 0.6734000980336736
{'scaleFactor': 20, 'currentTarget': array([51., 17.]), 'dynamicTrap': False, 'previousTarget': array([51., 17.]), 'currentState': array([50.42990427, 17.54257394,  3.81009653]), 'targetState': array([51, 17], dtype=int32), 'currentDistance': 0.787016908838751}
episode index:1162
target Thresh 75.79497578305292
target distance 44.0
model initialize at round 1162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.96301031,  9.07222621]), 'dynamicTrap': False, 'previousTarget': array([89.79586847,  9.83486126]), 'currentState': array([71.37127727,  1.70015304,  5.947631  ]), 'targetState': array([115,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5455341176047679
running average episode reward sum: 0.6732901530805964
{'scaleFactor': 20, 'currentTarget': array([115.,  19.]), 'dynamicTrap': False, 'previousTarget': array([115.,  19.]), 'currentState': array([115.24268601,  19.16928678,   1.60109743]), 'targetState': array([115,  19], dtype=int32), 'currentDistance': 0.2958961198143558}
episode index:1163
target Thresh 75.79599834560094
target distance 8.0
model initialize at round 1163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([112.40305601,  19.69115826,   3.64332998]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 10.007649157511036}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6735124685400693
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([106.50890101,  12.00003832,   2.10776517]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 0.5089010149983105}
episode index:1164
target Thresh 75.79701580809699
target distance 25.0
model initialize at round 1164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.74059846, 22.51598584]), 'dynamicTrap': False, 'previousTarget': array([28.74881264, 22.15981002]), 'currentState': array([ 9.86848879, 20.25783234,  6.23968393]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.4677727634073868
running average episode reward sum: 0.6733358679348052
{'scaleFactor': 20, 'currentTarget': array([34., 23.]), 'dynamicTrap': False, 'previousTarget': array([34., 23.]), 'currentState': array([34.31725273, 23.39631055,  6.01485532]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 0.5076527823715872}
episode index:1165
target Thresh 75.79802819597765
target distance 11.0
model initialize at round 1165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.07238314, 16.81727755]), 'dynamicTrap': True, 'previousTarget': array([69., 17.]), 'currentState': array([74.       ,  6.       ,  1.9211481], dtype=float32), 'targetState': array([69, 17], dtype=int32), 'currentDistance': 11.88675320057365}
done in step count: 10
reward sum = 0.8649780850088045
running average episode reward sum: 0.6735002266115411
{'scaleFactor': 20, 'currentTarget': array([69., 17.]), 'dynamicTrap': False, 'previousTarget': array([69., 17.]), 'currentState': array([69.33961166, 16.50659264,  2.36212728]), 'targetState': array([69, 17], dtype=int32), 'currentDistance': 0.598988239105852}
episode index:1166
target Thresh 75.79903553455271
target distance 42.0
model initialize at round 1166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.93438519,  8.85752021]), 'dynamicTrap': False, 'previousTarget': array([28.6897547 ,  9.11990655]), 'currentState': array([1.14575652e+01, 1.20199124e+00, 1.88642343e-03]), 'targetState': array([52, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6735896191846238
{'scaleFactor': 20, 'currentTarget': array([52., 18.]), 'dynamicTrap': False, 'previousTarget': array([52., 18.]), 'currentState': array([52.82648632, 18.71500988,  1.81810414]), 'targetState': array([52, 18], dtype=int32), 'currentDistance': 1.0928489243879178}
episode index:1167
target Thresh 75.80003784900566
target distance 1.0
model initialize at round 1167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.10039291,  3.82704832]), 'dynamicTrap': True, 'previousTarget': array([38.,  4.]), 'currentState': array([39.       ,  3.       ,  2.0710378], dtype=float32), 'targetState': array([38,  4], dtype=int32), 'currentDistance': 1.2220072950091687}
done in step count: 0
reward sum = 0.99
running average episode reward sum: 0.6738605184832671
{'scaleFactor': 20, 'currentTarget': array([38.10039291,  3.82704832]), 'dynamicTrap': True, 'previousTarget': array([38.,  4.]), 'currentState': array([39.       ,  3.       ,  2.0710378], dtype=float32), 'targetState': array([38,  4], dtype=int32), 'currentDistance': 1.2220072950091687}
episode index:1168
target Thresh 75.80103516439442
target distance 29.0
model initialize at round 1168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.17243503,  5.4454051 ]), 'dynamicTrap': False, 'previousTarget': array([90.04739343,  5.37604183]), 'currentState': array([108.11291319,   3.90354556,   2.6063261 ]), 'targetState': array([81,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7792870869982308
running average episode reward sum: 0.6739507037429036
{'scaleFactor': 20, 'currentTarget': array([81.,  6.]), 'dynamicTrap': False, 'previousTarget': array([81.,  6.]), 'currentState': array([80.23808812,  6.58672194,  4.40829708]), 'targetState': array([81,  6], dtype=int32), 'currentDistance': 0.9616404426949863}
episode index:1169
target Thresh 75.80202750565194
target distance 3.0
model initialize at round 1169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'dynamicTrap': False, 'previousTarget': array([10.,  9.]), 'currentState': array([11.12365298,  7.50959313,  1.35844111]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 1.866523146217123}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6742208313465421
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'dynamicTrap': False, 'previousTarget': array([10.,  9.]), 'currentState': array([10.27318237,  8.82966134,  2.94366646]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.3219376747898829}
episode index:1170
target Thresh 75.80301489758676
target distance 32.0
model initialize at round 1170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.36175616, 10.78673722]), 'dynamicTrap': True, 'previousTarget': array([79.34255626, 10.6857707 ]), 'currentState': array([99.      ,  7.      ,  4.556384], dtype=float32), 'targetState': array([67, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6831340992665341
running average episode reward sum: 0.6742284430185489
{'scaleFactor': 20, 'currentTarget': array([67., 13.]), 'dynamicTrap': False, 'previousTarget': array([67., 13.]), 'currentState': array([66.19138396, 13.5824066 ,  4.01632986]), 'targetState': array([67, 13], dtype=int32), 'currentDistance': 0.9965226292800371}
episode index:1171
target Thresh 75.80399736488378
target distance 41.0
model initialize at round 1171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.985399 , 19.5691631]), 'dynamicTrap': False, 'previousTarget': array([71.02375298, 18.97445107]), 'currentState': array([89.98075333, 19.13811158,  2.93159246]), 'targetState': array([50, 20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6743371479217234
{'scaleFactor': 20, 'currentTarget': array([50., 20.]), 'dynamicTrap': False, 'previousTarget': array([50., 20.]), 'currentState': array([49.68522094, 19.75185647,  3.74510711]), 'targetState': array([50, 20], dtype=int32), 'currentDistance': 0.4008254829882777}
episode index:1172
target Thresh 75.80497493210471
target distance 16.0
model initialize at round 1172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 18.]), 'dynamicTrap': False, 'previousTarget': array([47., 18.]), 'currentState': array([49.06688874,  3.68164027,  0.52104139]), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 14.466770702381863}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6745489190611148
{'scaleFactor': 20, 'currentTarget': array([47., 18.]), 'dynamicTrap': False, 'previousTarget': array([47., 18.]), 'currentState': array([47.39052342, 18.71033919,  0.65087636]), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 0.8106110686951442}
episode index:1173
target Thresh 75.8059476236888
target distance 21.0
model initialize at round 1173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.91706338,  3.51179115]), 'dynamicTrap': False, 'previousTarget': array([19.36486284,  4.07722123]), 'currentState': array([ 1.95990996, 12.31750754,  5.69856572]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6747293585437852
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'dynamicTrap': False, 'previousTarget': array([23.,  2.]), 'currentState': array([22.40112575,  1.96814041,  6.21449538]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.599721100103389}
episode index:1174
target Thresh 75.80691546395335
target distance 47.0
model initialize at round 1174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.80445742, 18.10287456]), 'dynamicTrap': False, 'previousTarget': array([36.98191681, 18.14970567]), 'currentState': array([16.82088303, 18.91327853,  3.69348903]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5347754061661248
running average episode reward sum: 0.6746102487970808
{'scaleFactor': 20, 'currentTarget': array([64., 17.]), 'dynamicTrap': False, 'previousTarget': array([64., 17.]), 'currentState': array([64.97524502, 16.75838747,  0.38265774]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 1.0047285563144366}
episode index:1175
target Thresh 75.80787847709445
target distance 15.0
model initialize at round 1175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'dynamicTrap': False, 'previousTarget': array([17., 22.]), 'currentState': array([13.66421678,  8.80554886,  1.16045056]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 13.609591858561913}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6748134010068483
{'scaleFactor': 20, 'currentTarget': array([17., 22.]), 'dynamicTrap': False, 'previousTarget': array([17., 22.]), 'currentState': array([16.21444881, 22.74819514,  0.40524934]), 'targetState': array([17, 22], dtype=int32), 'currentDistance': 1.084844059766035}
episode index:1176
target Thresh 75.80883668718748
target distance 14.0
model initialize at round 1176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'dynamicTrap': False, 'previousTarget': array([26.,  9.]), 'currentState': array([20.85786549, 22.87389349,  3.91454087]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 14.79616396222777}
done in step count: 12
reward sum = 0.8487209106937801
running average episode reward sum: 0.6749611559003802
{'scaleFactor': 20, 'currentTarget': array([26.,  9.]), 'dynamicTrap': False, 'previousTarget': array([26.,  9.]), 'currentState': array([26.0775741 ,  9.50027733,  0.09426277]), 'targetState': array([26,  9], dtype=int32), 'currentDistance': 0.5062560108540343}
episode index:1177
target Thresh 75.80979011818772
target distance 18.0
model initialize at round 1177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.8643985, 21.9580525]), 'dynamicTrap': False, 'previousTarget': array([97.29018892, 21.21358457]), 'currentState': array([109.63413097,   6.5653941 ,   2.18582444]), 'targetState': array([96, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6751331082493602
{'scaleFactor': 20, 'currentTarget': array([96., 23.]), 'dynamicTrap': False, 'previousTarget': array([96., 23.]), 'currentState': array([95.69776683, 22.8815466 ,  3.02587362]), 'targetState': array([96, 23], dtype=int32), 'currentDistance': 0.32461684296136367}
episode index:1178
target Thresh 75.81073879393102
target distance 7.0
model initialize at round 1178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([110.97953155,   7.24152562,   5.52929354]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 7.982457713335438}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6753752311516085
{'scaleFactor': 20, 'currentTarget': array([117.,   2.]), 'dynamicTrap': False, 'previousTarget': array([117.,   2.]), 'currentState': array([116.1578625 ,   2.01964473,   5.77517635]), 'targetState': array([117,   2], dtype=int32), 'currentDistance': 0.8423665957042977}
episode index:1179
target Thresh 75.81168273813431
target distance 18.0
model initialize at round 1179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.0378597 , 16.98277874]), 'dynamicTrap': False, 'previousTarget': array([ 8.51685448, 16.71285862]), 'currentState': array([26.24295935,  8.70181583,  2.37691283]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 21
reward sum = 0.7169217814135858
running average episode reward sum: 0.6754104400925085
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([ 7.9055915 , 17.21796441,  4.4630316 ]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.23753199611498782}
episode index:1180
target Thresh 75.81262197439625
target distance 58.0
model initialize at round 1180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.35417962, 15.73947029]), 'dynamicTrap': False, 'previousTarget': array([78.44164417, 16.30718934]), 'currentState': array([60.8997642 , 20.37905221,  5.90345904]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6305659354891061
running average episode reward sum: 0.6753724684544024
{'scaleFactor': 20, 'currentTarget': array([117.,   7.]), 'dynamicTrap': False, 'previousTarget': array([117.,   7.]), 'currentState': array([116.56495212,   6.85563425,   0.72164226]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 0.4583755308199292}
episode index:1181
target Thresh 75.81355652619777
target distance 25.0
model initialize at round 1181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.37598475,  9.62012971]), 'dynamicTrap': False, 'previousTarget': array([11.95151706,  9.09551454]), 'currentState': array([29.45016425,  3.60550784,  2.44788361]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7727609963413657
running average episode reward sum: 0.6754548614559988
{'scaleFactor': 20, 'currentTarget': array([ 6., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 11.]), 'currentState': array([ 5.42013437, 11.66581901,  4.23684378]), 'targetState': array([ 6, 11], dtype=int32), 'currentDistance': 0.8829264431970549}
episode index:1182
target Thresh 75.81448641690275
target distance 49.0
model initialize at round 1182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.73276552, 19.74147811]), 'dynamicTrap': True, 'previousTarget': array([80.59608118, 19.00079976]), 'currentState': array([61.      , 23.      ,  4.456402], dtype=float32), 'targetState': array([110,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4824875410372299
running average episode reward sum: 0.6752917445325679
{'scaleFactor': 20, 'currentTarget': array([110.,  13.]), 'dynamicTrap': False, 'previousTarget': array([110.,  13.]), 'currentState': array([110.06468684,  12.84215526,   4.95714624]), 'targetState': array([110,  13], dtype=int32), 'currentDistance': 0.17058531537285404}
episode index:1183
target Thresh 75.81541166975849
target distance 63.0
model initialize at round 1183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.47581009, 10.03138989]), 'dynamicTrap': False, 'previousTarget': array([53.35322867,  9.74224216]), 'currentState': array([71.11659801,  6.25787032,  2.45082581]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4691970995614348
running average episode reward sum: 0.6751176781094503
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([ 9.59048736, 18.46501586,  4.05288622]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.6196292092005417}
episode index:1184
target Thresh 75.81633230789636
target distance 65.0
model initialize at round 1184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.25190356, 13.28620806]), 'dynamicTrap': False, 'previousTarget': array([54.0377626 , 12.22844538]), 'currentState': array([73.23692176, 12.51222637,  3.02974892]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5064393513495007
running average episode reward sum: 0.674975333529906
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.08050937, 14.22451633,  4.32938328]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.7796516386692797}
episode index:1185
target Thresh 75.81724835433235
target distance 35.0
model initialize at round 1185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.19736048, 19.72626305]), 'dynamicTrap': False, 'previousTarget': array([71.92693298, 20.29197717]), 'currentState': array([51.21844877, 20.64446064,  5.19938231]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.5695802138335839
running average episode reward sum: 0.6748864674930626
{'scaleFactor': 20, 'currentTarget': array([87., 19.]), 'dynamicTrap': False, 'previousTarget': array([87., 19.]), 'currentState': array([86.26849406, 18.83082901,  5.56894809]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 0.7508127372393355}
episode index:1186
target Thresh 75.81815983196769
target distance 48.0
model initialize at round 1186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.87653685,  6.16321894]), 'dynamicTrap': False, 'previousTarget': array([51.15444247,  5.51930531]), 'currentState': array([69.6408252 ,  9.2247391 ,  3.13994265]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5931204451186264
running average episode reward sum: 0.6748175828912307
{'scaleFactor': 20, 'currentTarget': array([23.,  2.]), 'dynamicTrap': False, 'previousTarget': array([23.,  2.]), 'currentState': array([22.47023236,  1.5310848 ,  5.39611511]), 'targetState': array([23,  2], dtype=int32), 'currentDistance': 0.7074851316267189}
episode index:1187
target Thresh 75.81906676358936
target distance 13.0
model initialize at round 1187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.,  16.]), 'dynamicTrap': False, 'previousTarget': array([100.,  16.]), 'currentState': array([87.83790567, 21.21918253,  4.8389321 ]), 'targetState': array([100,  16], dtype=int32), 'currentDistance': 13.234666776321896}
done in step count: 11
reward sum = 0.8489120692997701
running average episode reward sum: 0.6749641270717093
{'scaleFactor': 20, 'currentTarget': array([98.28460941, 16.97662779]), 'dynamicTrap': True, 'previousTarget': array([100.,  16.]), 'currentState': array([97.63239947, 17.5608026 ,  5.48815438]), 'targetState': array([100,  16], dtype=int32), 'currentDistance': 0.8755786722993331}
episode index:1188
target Thresh 75.8199691718707
target distance 12.0
model initialize at round 1188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 18.]), 'currentState': array([5.81309064, 7.77602464, 1.52086073]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 10.292639409438923}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6751962767124396
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 18.]), 'currentState': array([ 6.97294669, 17.04277447,  1.93819834]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.9576077441491003}
episode index:1189
target Thresh 75.82086707937195
target distance 44.0
model initialize at round 1189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.37599237, 19.76890912]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([47.37492585, 19.56236761,  2.39465189]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7447539985458662
running average episode reward sum: 0.6752547285795266
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.06363098, 20.04784505,  3.48261497]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.0796118733419688}
episode index:1190
target Thresh 75.82176050854086
target distance 30.0
model initialize at round 1190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.36051306, 15.32902522]), 'dynamicTrap': False, 'previousTarget': array([78.14985851, 14.28991511]), 'currentState': array([6.20084052e+01, 5.38395850e+00, 4.09379005e-02]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7743008829730669
running average episode reward sum: 0.6753378907578588
{'scaleFactor': 20, 'currentTarget': array([91., 22.]), 'dynamicTrap': False, 'previousTarget': array([91., 22.]), 'currentState': array([91.10281056, 21.54291443,  0.56088346]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 0.46850531356289427}
episode index:1191
target Thresh 75.8226494817132
target distance 20.0
model initialize at round 1191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 6.43046618, 13.42781353]), 'currentState': array([23.35012473,  6.59485836,  1.92615163]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 19.78795594162915}
done in step count: 15
reward sum = 0.8128710917431029
running average episode reward sum: 0.6754532709600276
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 5.99478373, 14.12059638,  2.68813765]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 1.00206694087904}
episode index:1192
target Thresh 75.82353402111333
target distance 65.0
model initialize at round 1192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.40072024, 12.82437428]), 'dynamicTrap': False, 'previousTarget': array([66.99763356, 13.6923441 ]), 'currentState': array([48.4008825 , 12.74381134,  6.13689244]), 'targetState': array([112,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5020526531692558
running average episode reward sum: 0.6753079225796497
{'scaleFactor': 20, 'currentTarget': array([112.,  13.]), 'dynamicTrap': False, 'previousTarget': array([112.,  13.]), 'currentState': array([112.49153951,  13.0806056 ,   2.13198594]), 'targetState': array([112,  13], dtype=int32), 'currentDistance': 0.4981047606671872}
episode index:1193
target Thresh 75.82441414885481
target distance 72.0
model initialize at round 1193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.80359002, 14.2039631 ]), 'dynamicTrap': True, 'previousTarget': array([58.80984546, 14.24863257]), 'currentState': array([39.        , 17.        ,  0.92963487], dtype=float32), 'targetState': array([111,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.3291344031800287
running average episode reward sum: 0.6750179950089632
{'scaleFactor': 20, 'currentTarget': array([111.,   7.]), 'dynamicTrap': False, 'previousTarget': array([111.,   7.]), 'currentState': array([110.00155238,   6.07962818,   0.87681261]), 'targetState': array([111,   7], dtype=int32), 'currentDistance': 1.3579329651892522}
episode index:1194
target Thresh 75.82528988694088
target distance 20.0
model initialize at round 1194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.31756347,  8.98994885]), 'dynamicTrap': False, 'previousTarget': array([40.49998867,  9.22501076]), 'currentState': array([27.63196094, 23.57433377,  5.69598252]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8146978379118469
running average episode reward sum: 0.6751348819067898
{'scaleFactor': 20, 'currentTarget': array([46.,  4.]), 'dynamicTrap': False, 'previousTarget': array([46.,  4.]), 'currentState': array([46.46775092,  4.12385939,  6.24740764]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 0.4838719612329555}
episode index:1195
target Thresh 75.82616125726501
target distance 12.0
model initialize at round 1195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87., 17.]), 'currentState': array([75.0453698 ,  7.17916949,  1.30341209]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 15.471324929797909}
done in step count: 10
reward sum = 0.8852662144098045
running average episode reward sum: 0.675310577000856
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87., 17.]), 'currentState': array([87.27999829, 16.84964843,  1.47159341]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 0.3178122637365255}
episode index:1196
target Thresh 75.82702828161152
target distance 31.0
model initialize at round 1196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.74176093, 15.68339819]), 'dynamicTrap': False, 'previousTarget': array([96.63445353, 16.00243962]), 'currentState': array([114.02363345,  20.99464963,   2.87491506]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7554886943967624
running average episode reward sum: 0.6753775595550714
{'scaleFactor': 20, 'currentTarget': array([85.76297555, 14.70784564]), 'dynamicTrap': True, 'previousTarget': array([85., 13.]), 'currentState': array([85.444689  , 14.70018578,  3.66906611]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 0.3183787055531873}
episode index:1197
target Thresh 75.82789098165605
target distance 10.0
model initialize at round 1197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96., 23.]), 'dynamicTrap': False, 'previousTarget': array([96., 23.]), 'currentState': array([106.02265077,  14.83177714,   0.83184648]), 'targetState': array([96, 23], dtype=int32), 'currentDistance': 12.92955502571324}
done in step count: 10
reward sum = 0.8752721249088045
running average episode reward sum: 0.675544416454365
{'scaleFactor': 20, 'currentTarget': array([96., 23.]), 'dynamicTrap': False, 'previousTarget': array([96., 23.]), 'currentState': array([96.04729113, 23.50604468,  3.05036366]), 'targetState': array([96, 23], dtype=int32), 'currentDistance': 0.508249615664941}
episode index:1198
target Thresh 75.82874937896618
target distance 13.0
model initialize at round 1198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([45.99459054, 18.64981121,  3.06305718]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 12.28379834579212}
done in step count: 11
reward sum = 0.8482594917423573
running average episode reward sum: 0.6756884657248304
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([33.77914195, 16.72756232,  3.65118572]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 0.7603454553684789}
episode index:1199
target Thresh 75.82960349500185
target distance 45.0
model initialize at round 1199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.9999917 ,  5.01821581]), 'dynamicTrap': True, 'previousTarget': array([36.,  5.]), 'currentState': array([16.      ,  5.      ,  6.101666], dtype=float32), 'targetState': array([61,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6312477800819158
running average episode reward sum: 0.675651431820128
{'scaleFactor': 20, 'currentTarget': array([61.,  5.]), 'dynamicTrap': False, 'previousTarget': array([61.,  5.]), 'currentState': array([61.22934359,  4.08177983,  1.23309073]), 'targetState': array([61,  5], dtype=int32), 'currentDistance': 0.9464284201647949}
episode index:1200
target Thresh 75.83045335111602
target distance 30.0
model initialize at round 1200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.81925546,  4.31725646]), 'dynamicTrap': True, 'previousTarget': array([55.82455801,  4.3567256 ]), 'currentState': array([36.        ,  7.        ,  0.27798468], dtype=float32), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7725398265471557
running average episode reward sum: 0.6757321049214826
{'scaleFactor': 20, 'currentTarget': array([66.,  3.]), 'dynamicTrap': False, 'previousTarget': array([66.,  3.]), 'currentState': array([66.94988318,  3.14600629,  0.86226506]), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 0.9610389678634541}
episode index:1201
target Thresh 75.83129896855515
target distance 35.0
model initialize at round 1201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.68058989, 21.94901702]), 'dynamicTrap': False, 'previousTarget': array([67.12934655, 21.27093182]), 'currentState': array([85.62183222, 20.41707239,  2.6096468 ]), 'targetState': array([52, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.504085184549433
running average episode reward sum: 0.6755893038230034
{'scaleFactor': 20, 'currentTarget': array([52., 23.]), 'dynamicTrap': False, 'previousTarget': array([52., 23.]), 'currentState': array([52.1905482 , 23.74914   ,  3.21257882]), 'targetState': array([52, 23], dtype=int32), 'currentDistance': 0.7729937573257715}
episode index:1202
target Thresh 75.83214036845969
target distance 27.0
model initialize at round 1202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.62111403,  8.53974945]), 'dynamicTrap': True, 'previousTarget': array([98.20583067,  7.80395219]), 'currentState': array([81.      , 18.      ,  4.686083], dtype=float32), 'targetState': array([108,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7624216394390458
running average episode reward sum: 0.6756614836531082
{'scaleFactor': 20, 'currentTarget': array([108.,   2.]), 'dynamicTrap': False, 'previousTarget': array([108.,   2.]), 'currentState': array([107.77583808,   1.16495883,   5.86778621]), 'targetState': array([108,   2], dtype=int32), 'currentDistance': 0.8646052944329985}
episode index:1203
target Thresh 75.8329775718647
target distance 48.0
model initialize at round 1203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.36422253, 12.42805333]), 'dynamicTrap': False, 'previousTarget': array([87.97366596, 13.67544468]), 'currentState': array([69.17795859, 18.0749353 ,  5.25877422]), 'targetState': array([117,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7209649184152003
running average episode reward sum: 0.6756991110906183
{'scaleFactor': 20, 'currentTarget': array([117.,   4.]), 'dynamicTrap': False, 'previousTarget': array([117.,   4.]), 'currentState': array([116.50752775,   4.04482922,   0.25383118]), 'targetState': array([117,   4], dtype=int32), 'currentDistance': 0.4945084226750986}
episode index:1204
target Thresh 75.83381059970031
target distance 22.0
model initialize at round 1204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.83151134, 17.34862268]), 'dynamicTrap': False, 'previousTarget': array([43.55791146, 17.57704261]), 'currentState': array([26.9559032 ,  6.61485121,  0.04326027]), 'targetState': array([48, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6758521063134819
{'scaleFactor': 20, 'currentTarget': array([48., 20.]), 'dynamicTrap': False, 'previousTarget': array([48., 20.]), 'currentState': array([48.85235487, 20.23973652,  1.61194974]), 'targetState': array([48, 20], dtype=int32), 'currentDistance': 0.8854278171131579}
episode index:1205
target Thresh 75.83463947279228
target distance 5.0
model initialize at round 1205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.,  4.]), 'dynamicTrap': False, 'previousTarget': array([67.,  4.]), 'currentState': array([62.50845705,  3.45748732,  0.22513866]), 'targetState': array([67,  4], dtype=int32), 'currentDistance': 4.524188109445796}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6761043848322933
{'scaleFactor': 20, 'currentTarget': array([67.,  4.]), 'dynamicTrap': False, 'previousTarget': array([67.,  4.]), 'currentState': array([66.13449477,  3.54581205,  0.14446977]), 'targetState': array([67,  4], dtype=int32), 'currentDistance': 0.977438484971061}
episode index:1206
target Thresh 75.83546421186243
target distance 70.0
model initialize at round 1206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.44893279, 14.98027801]), 'dynamicTrap': False, 'previousTarget': array([85.00815827, 15.42880452]), 'currentState': array([103.44484021,  15.38485965,   2.52020121]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5641288853043128
running average episode reward sum: 0.6760116130845485
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([34.20050836, 14.66419849,  0.57778103]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 1.0393971896192653}
episode index:1207
target Thresh 75.8362848375293
target distance 66.0
model initialize at round 1207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.2844425 ,  7.64682751]), 'dynamicTrap': False, 'previousTarget': array([63.08213587,  7.81071492]), 'currentState': array([81.18850778,  5.69025259,  2.59748244]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.34502066710868956
running average episode reward sum: 0.6757376139570851
{'scaleFactor': 20, 'currentTarget': array([17., 12.]), 'dynamicTrap': False, 'previousTarget': array([17., 12.]), 'currentState': array([16.89661257, 12.91094895,  4.32961343]), 'targetState': array([17, 12], dtype=int32), 'currentDistance': 0.9167971157711109}
episode index:1208
target Thresh 75.83710137030862
target distance 22.0
model initialize at round 1208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.96015611, 10.20909578]), 'dynamicTrap': False, 'previousTarget': array([85.79267045, 10.27605889]), 'currentState': array([104.18334257,   1.96801117,   6.21144838]), 'targetState': array([82, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.740021755119052
running average episode reward sum: 0.6757907852897252
{'scaleFactor': 20, 'currentTarget': array([82., 12.]), 'dynamicTrap': False, 'previousTarget': array([82., 12.]), 'currentState': array([81.09236547, 12.85257477,  1.33044986]), 'targetState': array([82, 12], dtype=int32), 'currentDistance': 1.24526469775128}
episode index:1209
target Thresh 75.83791383061367
target distance 56.0
model initialize at round 1209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.55159329, 23.53461095]), 'dynamicTrap': False, 'previousTarget': array([70., 23.]), 'currentState': array([50.55386739, 23.83620428,  0.41680026]), 'targetState': array([106,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7151793359578534
running average episode reward sum: 0.6758233378109385
{'scaleFactor': 20, 'currentTarget': array([106.,  23.]), 'dynamicTrap': False, 'previousTarget': array([106.,  23.]), 'currentState': array([106.24340559,  23.37405696,   1.44054136]), 'targetState': array([106,  23], dtype=int32), 'currentDistance': 0.44627893609923197}
episode index:1210
target Thresh 75.83872223875606
target distance 65.0
model initialize at round 1210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.58010415, 11.7318303 ]), 'dynamicTrap': False, 'previousTarget': array([28.61161351, 12.0776773 ]), 'currentState': array([10.97265712, 15.67492819,  6.16739907]), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5254759437738518
running average episode reward sum: 0.6756991863707758
{'scaleFactor': 20, 'currentTarget': array([74.,  3.]), 'dynamicTrap': False, 'previousTarget': array([74.,  3.]), 'currentState': array([73.68341302,  2.76169021,  1.36973297]), 'targetState': array([74,  3], dtype=int32), 'currentDistance': 0.39625606850398726}
episode index:1211
target Thresh 75.83952661494604
target distance 17.0
model initialize at round 1211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'dynamicTrap': False, 'previousTarget': array([13.,  8.]), 'currentState': array([28.73381916, 19.20217459,  3.2815249 ]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 19.31428955427504}
done in step count: 19
reward sum = 0.7445944116693712
running average episode reward sum: 0.6757560306160716
{'scaleFactor': 20, 'currentTarget': array([13.,  8.]), 'dynamicTrap': False, 'previousTarget': array([13.,  8.]), 'currentState': array([12.7261251 ,  8.20200321,  3.76764489]), 'targetState': array([13,  8], dtype=int32), 'currentDistance': 0.34031273169569354}
episode index:1212
target Thresh 75.84032697929301
target distance 30.0
model initialize at round 1212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.2130372 , 13.94666005]), 'dynamicTrap': False, 'previousTarget': array([83.35294118, 13.58823529]), 'currentState': array([100.59762116,  23.83484698,   2.78011036]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 33
reward sum = 0.5813688169886172
running average episode reward sum: 0.6756782175792807
{'scaleFactor': 20, 'currentTarget': array([71.,  7.]), 'dynamicTrap': False, 'previousTarget': array([71.,  7.]), 'currentState': array([71.50993724,  7.23298331,  3.2065212 ]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 0.5606399995319834}
episode index:1213
target Thresh 75.84112335180615
target distance 65.0
model initialize at round 1213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.18159524, 10.63399451]), 'dynamicTrap': False, 'previousTarget': array([97.14977978, 10.55689597]), 'currentState': array([115.01350847,  13.22150638,   2.97001386]), 'targetState': array([52,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4989917751812383
running average episode reward sum: 0.6755326768524289
{'scaleFactor': 20, 'currentTarget': array([52.,  5.]), 'dynamicTrap': False, 'previousTarget': array([52.,  5.]), 'currentState': array([52.37730563,  4.62504589,  3.10156977]), 'targetState': array([52,  5], dtype=int32), 'currentDistance': 0.5319305630756913}
episode index:1214
target Thresh 75.84191575239481
target distance 45.0
model initialize at round 1214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.0255768 , 18.30925264]), 'dynamicTrap': False, 'previousTarget': array([65.95570316, 17.33038021]), 'currentState': array([46.03322216, 17.75630027,  0.67226982]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6414046370319476
running average episode reward sum: 0.675504587930766
{'scaleFactor': 20, 'currentTarget': array([91., 19.]), 'dynamicTrap': False, 'previousTarget': array([91., 19.]), 'currentState': array([91.34188423, 18.86157871,  5.70840062]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 0.3688431626559869}
episode index:1215
target Thresh 75.84270420086905
target distance 25.0
model initialize at round 1215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.96889776,  6.84154915]), 'dynamicTrap': False, 'previousTarget': array([12.95151706,  6.90448546]), 'currentState': array([30.72237701, 13.79187403,  3.54131794]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6756492862721837
{'scaleFactor': 20, 'currentTarget': array([7., 5.]), 'dynamicTrap': False, 'previousTarget': array([7., 5.]), 'currentState': array([7.32344001, 5.4952398 , 4.91679082]), 'targetState': array([7, 5], dtype=int32), 'currentDistance': 0.591503085468842}
episode index:1216
target Thresh 75.84348871694012
target distance 29.0
model initialize at round 1216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.89209179,  5.33055504]), 'dynamicTrap': False, 'previousTarget': array([90.58520839,  5.94788792]), 'currentState': array([72.15607883,  8.56934992,  6.10191536]), 'targetState': array([100,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8168799414798344
running average episode reward sum: 0.6757653344687389
{'scaleFactor': 20, 'currentTarget': array([100.,   4.]), 'dynamicTrap': False, 'previousTarget': array([100.,   4.]), 'currentState': array([99.83473509,  4.36233087,  6.2387299 ]), 'targetState': array([100,   4], dtype=int32), 'currentDistance': 0.3982413229758809}
episode index:1217
target Thresh 75.84426932022096
target distance 12.0
model initialize at round 1217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.,  6.]), 'dynamicTrap': False, 'previousTarget': array([70.,  6.]), 'currentState': array([59.67120409, 17.31252511,  5.48724479]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 15.31852633854112}
done in step count: 10
reward sum = 0.8848780850088044
running average episode reward sum: 0.6759370198140099
{'scaleFactor': 20, 'currentTarget': array([70.,  6.]), 'dynamicTrap': False, 'previousTarget': array([70.,  6.]), 'currentState': array([70.31944307,  5.6081141 ,  0.26291965]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 0.5055872124374193}
episode index:1218
target Thresh 75.8450460302267
target distance 10.0
model initialize at round 1218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([47.31382941, 10.0248026 ,  4.87897661]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 8.030936783923286}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6761705382637113
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([46.86577507,  2.32030796,  5.1219137 ]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 0.34729457125295116}
episode index:1219
target Thresh 75.84581886637513
target distance 13.0
model initialize at round 1219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.99848408,  9.61140689]), 'dynamicTrap': True, 'previousTarget': array([87.,  8.]), 'currentState': array([74.      , 13.      ,  5.211865], dtype=float32), 'targetState': array([87,  8], dtype=int32), 'currentDistance': 12.467805885301871}
done in step count: 9
reward sum = 0.8842082973836408
running average episode reward sum: 0.6763410610170884
{'scaleFactor': 20, 'currentTarget': array([87.,  8.]), 'dynamicTrap': False, 'previousTarget': array([87.,  8.]), 'currentState': array([86.33051481,  7.89702223,  0.33695099]), 'targetState': array([87,  8], dtype=int32), 'currentDistance': 0.6773587226434961}
episode index:1220
target Thresh 75.84658784798717
target distance 61.0
model initialize at round 1220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.24849837, 12.05506174]), 'dynamicTrap': False, 'previousTarget': array([91.21419273, 13.08078253]), 'currentState': array([110.09259616,  14.54740664,   3.19494104]), 'targetState': array([50,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6763405597787945
{'scaleFactor': 20, 'currentTarget': array([50.,  7.]), 'dynamicTrap': False, 'previousTarget': array([50.,  7.]), 'currentState': array([49.52516381,  6.97791519,  3.04546229]), 'targetState': array([50,  7], dtype=int32), 'currentDistance': 0.47534950164828893}
episode index:1221
target Thresh 75.84735299428743
target distance 63.0
model initialize at round 1221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.99651646,  9.37326873]), 'dynamicTrap': True, 'previousTarget': array([73.98992951,  9.63460094]), 'currentState': array([54.      ,  9.      ,  1.520209], dtype=float32), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5383537360259206
running average episode reward sum: 0.6762276409377529
{'scaleFactor': 20, 'currentTarget': array([117.,  11.]), 'dynamicTrap': False, 'previousTarget': array([117.,  11.]), 'currentState': array([117.30160975,  11.04398454,   1.20895665]), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 0.3048000636161664}
episode index:1222
target Thresh 75.8481143244046
target distance 61.0
model initialize at round 1222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.39123599,  5.38831563]), 'dynamicTrap': False, 'previousTarget': array([89.06684958,  5.36613521]), 'currentState': array([107.31765482,   7.10232623,   2.07501662]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5474236235459912
running average episode reward sum: 0.6761223228532135
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([48.86322269,  2.42277791,  2.64377922]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 0.9611943457727363}
episode index:1223
target Thresh 75.84887185737196
target distance 18.0
model initialize at round 1223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.48425766,  3.84736192]), 'dynamicTrap': False, 'previousTarget': array([42.64100589,  3.90599608]), 'currentState': array([25.02700399, 13.60668025,  5.08487523]), 'targetState': array([44,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8416134370014666
running average episode reward sum: 0.6762575280118314
{'scaleFactor': 20, 'currentTarget': array([44.,  3.]), 'dynamicTrap': False, 'previousTarget': array([44.,  3.]), 'currentState': array([43.56317113,  3.51669086,  6.00589606]), 'targetState': array([44,  3], dtype=int32), 'currentDistance': 0.6766009953011278}
episode index:1224
target Thresh 75.8496256121279
target distance 32.0
model initialize at round 1224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.81128064, 20.13954189]), 'dynamicTrap': False, 'previousTarget': array([64.99024152, 20.37530495]), 'currentState': array([46.81315611, 20.41343068,  0.22828871]), 'targetState': array([77, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8334332928849267
running average episode reward sum: 0.6763858347586665
{'scaleFactor': 20, 'currentTarget': array([77., 20.]), 'dynamicTrap': False, 'previousTarget': array([77., 20.]), 'currentState': array([76.04701105, 19.12227974,  0.44425418]), 'targetState': array([77, 20], dtype=int32), 'currentDistance': 1.2956005570100617}
episode index:1225
target Thresh 75.8503756075163
target distance 39.0
model initialize at round 1225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.22944148, 13.41874497]), 'dynamicTrap': False, 'previousTarget': array([52.84081231, 12.38116355]), 'currentState': array([69.49337169,  5.26835521,  2.74843562]), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 28
reward sum = 0.6910933818118832
running average episode reward sum: 0.6763978311265729
{'scaleFactor': 20, 'currentTarget': array([32., 22.]), 'dynamicTrap': False, 'previousTarget': array([32., 22.]), 'currentState': array([32.13078176, 22.28275464,  2.3761385 ]), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 0.3115349988956982}
episode index:1226
target Thresh 75.85112186228709
target distance 23.0
model initialize at round 1226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.29018399, 11.09790549]), 'dynamicTrap': False, 'previousTarget': array([53.11006407, 10.5704125 ]), 'currentState': array([71.57834208,  5.80952599,  2.20436954]), 'targetState': array([49, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7712419624066346
running average episode reward sum: 0.6764751287070783
{'scaleFactor': 20, 'currentTarget': array([49., 12.]), 'dynamicTrap': False, 'previousTarget': array([49., 12.]), 'currentState': array([48.95413613, 11.60130808,  3.22898096]), 'targetState': array([49, 12], dtype=int32), 'currentDistance': 0.40132124190223684}
episode index:1227
target Thresh 75.85186439509668
target distance 31.0
model initialize at round 1227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.55748492,  9.83679111]), 'dynamicTrap': False, 'previousTarget': array([78.70422107,  9.95728965]), 'currentState': array([95.87280677, 19.84576854,  3.97135692]), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7344459975665499
running average episode reward sum: 0.6765223362550095
{'scaleFactor': 20, 'currentTarget': array([65.,  2.]), 'dynamicTrap': False, 'previousTarget': array([65.,  2.]), 'currentState': array([64.44569296,  2.55124049,  3.92385129]), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 0.7817431652339749}
episode index:1228
target Thresh 75.85260322450844
target distance 28.0
model initialize at round 1228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.869375  , 14.47522934]), 'dynamicTrap': False, 'previousTarget': array([80.76952105, 14.50557744]), 'currentState': array([100.13327014,  19.85131938,   5.34218483]), 'targetState': array([72, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.758690279258537
running average episode reward sum: 0.6765891938164444
{'scaleFactor': 20, 'currentTarget': array([72., 12.]), 'dynamicTrap': False, 'previousTarget': array([72., 12.]), 'currentState': array([71.7854594 , 12.17516092,  4.20666918]), 'targetState': array([72, 12], dtype=int32), 'currentDistance': 0.2769639319969518}
episode index:1229
target Thresh 75.85333836899314
target distance 44.0
model initialize at round 1229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.45437537, 17.96486003]), 'dynamicTrap': False, 'previousTarget': array([60.95367385, 17.63952224]), 'currentState': array([42.52989734, 19.7012861 ,  5.7029925 ]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6995150800639126
running average episode reward sum: 0.6766078327483529
{'scaleFactor': 20, 'currentTarget': array([85., 16.]), 'dynamicTrap': False, 'previousTarget': array([85., 16.]), 'currentState': array([85.32222333, 15.27925724,  5.61712836]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 0.7894922420253724}
episode index:1230
target Thresh 75.85406984692942
target distance 53.0
model initialize at round 1230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.40139396, 12.82342606]), 'dynamicTrap': False, 'previousTarget': array([98.22401827, 12.01494615]), 'currentState': array([117.08599451,  16.36129286,   3.15510285]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5982296675004722
running average episode reward sum: 0.6765441624272742
{'scaleFactor': 20, 'currentTarget': array([65.,  7.]), 'dynamicTrap': False, 'previousTarget': array([65.,  7.]), 'currentState': array([64.6179884 ,  7.46520543,  3.1238565 ]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 0.6019542786364198}
episode index:1231
target Thresh 75.85479767660428
target distance 25.0
model initialize at round 1231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.15345117,  18.62404544]), 'dynamicTrap': False, 'previousTarget': array([107.85753677,  18.38290441]), 'currentState': array([89.24829962, 16.6785528 ,  5.77108747]), 'targetState': array([113,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8132241190018655
running average episode reward sum: 0.6766551039504679
{'scaleFactor': 20, 'currentTarget': array([113.,  19.]), 'dynamicTrap': False, 'previousTarget': array([113.,  19.]), 'currentState': array([1.1238246e+02, 1.9579338e+01, 4.2674531e-02]), 'targetState': array([113,  19], dtype=int32), 'currentDistance': 0.8467517951570322}
episode index:1232
target Thresh 75.85552187621347
target distance 19.0
model initialize at round 1232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,   2.]), 'dynamicTrap': False, 'previousTarget': array([115.,   2.]), 'currentState': array([108.49939915,  19.09901555,   4.13049805]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 18.29300807417154}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.676832462547636
{'scaleFactor': 20, 'currentTarget': array([115.,   2.]), 'dynamicTrap': False, 'previousTarget': array([115.,   2.]), 'currentState': array([115.03717124,   1.7445165 ,   0.116784  ]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 0.2581734276806303}
episode index:1233
target Thresh 75.85624246386207
target distance 28.0
model initialize at round 1233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.1854597 , 21.10361207]), 'dynamicTrap': False, 'previousTarget': array([22.98725709, 20.71383061]), 'currentState': array([ 3.18721745, 21.36876639,  0.43553543]), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6769739741428931
{'scaleFactor': 20, 'currentTarget': array([31., 21.]), 'dynamicTrap': False, 'previousTarget': array([31., 21.]), 'currentState': array([30.2977164 , 21.53544986,  0.0635918 ]), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 0.8831244591281552}
episode index:1234
target Thresh 75.85695945756476
target distance 22.0
model initialize at round 1234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.04143648, 11.62834305]), 'dynamicTrap': False, 'previousTarget': array([ 9.44208854, 11.57704261]), 'currentState': array([28.13889433,  3.11470578,  1.78466528]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6128512267197571
running average episode reward sum: 0.6769220528899188
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.6379526 , 13.51890948,  0.85662212]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.602101659715937}
episode index:1235
target Thresh 75.85767287524645
target distance 12.0
model initialize at round 1235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'dynamicTrap': False, 'previousTarget': array([15., 11.]), 'currentState': array([25.51989012,  1.40120121,  2.52012873]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 14.240962970391344}
done in step count: 15
reward sum = 0.7944745139624244
running average episode reward sum: 0.6770171600590713
{'scaleFactor': 20, 'currentTarget': array([15., 11.]), 'dynamicTrap': False, 'previousTarget': array([15., 11.]), 'currentState': array([14.4700317 , 11.20194457,  2.66927486]), 'targetState': array([15, 11], dtype=int32), 'currentDistance': 0.5671402030844662}
episode index:1236
target Thresh 75.8583827347426
target distance 55.0
model initialize at round 1236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.74520608, 17.43266659]), 'dynamicTrap': False, 'previousTarget': array([47.98678996, 18.27320764]), 'currentState': array([28.74680127, 17.68526311,  6.23101079]), 'targetState': array([83, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3016097177980945
running average episode reward sum: 0.6767136778907116
{'scaleFactor': 20, 'currentTarget': array([83., 17.]), 'dynamicTrap': False, 'previousTarget': array([83., 17.]), 'currentState': array([82.63373487, 16.67865075,  1.47691269]), 'targetState': array([83, 17], dtype=int32), 'currentDistance': 0.48725299978371406}
episode index:1237
target Thresh 75.85908905379975
target distance 12.0
model initialize at round 1237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.05308424, 18.17734872]), 'dynamicTrap': True, 'previousTarget': array([93., 18.]), 'currentState': array([101.       ,   6.       ,   4.5167975], dtype=float32), 'targetState': array([93, 18], dtype=int32), 'currentDistance': 14.541021004259175}
done in step count: 18
reward sum = 0.7480310089337284
running average episode reward sum: 0.6767712847816995
{'scaleFactor': 20, 'currentTarget': array([93., 18.]), 'dynamicTrap': False, 'previousTarget': array([93., 18.]), 'currentState': array([92.83115317, 18.65405137,  5.89445214]), 'targetState': array([93, 18], dtype=int32), 'currentDistance': 0.6754942264256244}
episode index:1238
target Thresh 75.85979185007591
target distance 64.0
model initialize at round 1238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.41297651, 15.41115633]), 'dynamicTrap': False, 'previousTarget': array([69.19486842, 14.78509663]), 'currentState': array([90.26322035, 12.96824423,  1.6560145 ]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5894432571002044
running average episode reward sum: 0.6767008021120615
{'scaleFactor': 20, 'currentTarget': array([25., 21.]), 'dynamicTrap': False, 'previousTarget': array([25., 21.]), 'currentState': array([24.8674403 , 20.8423892 ,  2.96884517]), 'targetState': array([25, 21], dtype=int32), 'currentDistance': 0.20594474492920295}
episode index:1239
target Thresh 75.86049114114101
target distance 27.0
model initialize at round 1239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.45593794, 12.07177882]), 'dynamicTrap': False, 'previousTarget': array([34.64006204, 12.01924318]), 'currentState': array([53.81890049,  7.06421642,  2.81959646]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7464794458607324
running average episode reward sum: 0.6767570752118588
{'scaleFactor': 20, 'currentTarget': array([27., 14.]), 'dynamicTrap': False, 'previousTarget': array([27., 14.]), 'currentState': array([27.96008224, 13.01157607,  2.28805847]), 'targetState': array([27, 14], dtype=int32), 'currentDistance': 1.37794767245926}
episode index:1240
target Thresh 75.86118694447737
target distance 53.0
model initialize at round 1240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.4682571 , 14.19491351]), 'dynamicTrap': False, 'previousTarget': array([22.33675644, 14.89217754]), 'currentState': array([ 2.00871867, 18.81296116,  5.02661633]), 'targetState': array([56,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5632221894727434
running average episode reward sum: 0.6766655885996596
{'scaleFactor': 20, 'currentTarget': array([56.,  6.]), 'dynamicTrap': False, 'previousTarget': array([56.,  6.]), 'currentState': array([56.22245689,  6.30982161,  0.06245718]), 'targetState': array([56,  6], dtype=int32), 'currentDistance': 0.3814138099645076}
episode index:1241
target Thresh 75.86187927748011
target distance 52.0
model initialize at round 1241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.00000494, 10.98594954]), 'dynamicTrap': True, 'previousTarget': array([79., 11.]), 'currentState': array([99.      , 11.      ,  2.865997], dtype=float32), 'targetState': array([47, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6307597396053835
running average episode reward sum: 0.6766286273685854
{'scaleFactor': 20, 'currentTarget': array([47., 11.]), 'dynamicTrap': False, 'previousTarget': array([47., 11.]), 'currentState': array([46.85039325, 10.27539396,  2.99204152]), 'targetState': array([47, 11], dtype=int32), 'currentDistance': 0.7398892437286535}
episode index:1242
target Thresh 75.8625681574576
target distance 8.0
model initialize at round 1242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'dynamicTrap': False, 'previousTarget': array([27., 11.]), 'currentState': array([19.98327733,  4.47423768,  0.13240755]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 9.582273779326655}
done in step count: 8
reward sum = 0.89334070442792
running average episode reward sum: 0.6768029733678287
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'dynamicTrap': False, 'previousTarget': array([27., 11.]), 'currentState': array([27.36110618, 10.26512646,  0.06770703]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.8188020447350757}
episode index:1243
target Thresh 75.86325360163187
target distance 63.0
model initialize at round 1243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.49852375, 15.79887133]), 'dynamicTrap': False, 'previousTarget': array([48.04019095, 14.73271054]), 'currentState': array([67.41909671, 17.57953774,  2.93321151]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 76
reward sum = 0.14195164375844838
running average episode reward sum: 0.6763730285691073
{'scaleFactor': 20, 'currentTarget': array([ 5., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 12.]), 'currentState': array([ 4.96253531, 11.28398378,  4.12716169]), 'targetState': array([ 5, 12], dtype=int32), 'currentDistance': 0.7169956986809182}
episode index:1244
target Thresh 75.86393562713906
target distance 6.0
model initialize at round 1244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  9.]), 'dynamicTrap': False, 'previousTarget': array([33.,  9.]), 'currentState': array([29.16430239,  4.67493069,  0.46301413]), 'targetState': array([33,  9], dtype=int32), 'currentDistance': 5.780899642638855}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6766091136867225
{'scaleFactor': 20, 'currentTarget': array([33.,  9.]), 'dynamicTrap': False, 'previousTarget': array([33.,  9.]), 'currentState': array([32.99177645,  8.80254808,  1.15051138]), 'targetState': array([33,  9], dtype=int32), 'currentDistance': 0.1976230989377231}
episode index:1245
target Thresh 75.86461425102985
target distance 29.0
model initialize at round 1245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.79457691, 11.5237927 ]), 'dynamicTrap': False, 'previousTarget': array([31.51966335, 10.64703585]), 'currentState': array([48.39431078,  4.171929  ,  2.70390379]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7012716813952702
running average episode reward sum: 0.676628907079747
{'scaleFactor': 20, 'currentTarget': array([21., 15.]), 'dynamicTrap': False, 'previousTarget': array([21., 15.]), 'currentState': array([21.68666513, 15.64099622,  4.25887979]), 'targetState': array([21, 15], dtype=int32), 'currentDistance': 0.9393535868968924}
episode index:1246
target Thresh 75.86528949026984
target distance 22.0
model initialize at round 1246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.74797421,  4.2844372 ]), 'dynamicTrap': False, 'previousTarget': array([49.79586847,  4.16513874]), 'currentState': array([29.60242534, 10.0678582 ,  4.73980594]), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 19
reward sum = 0.7979232683635068
running average episode reward sum: 0.6767261760142167
{'scaleFactor': 20, 'currentTarget': array([53.,  3.]), 'dynamicTrap': False, 'previousTarget': array([53.,  3.]), 'currentState': array([52.76279883,  2.5558057 ,  5.96188885]), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 0.5035602935328753}
episode index:1247
target Thresh 75.86596136174009
target distance 12.0
model initialize at round 1247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66., 11.]), 'dynamicTrap': False, 'previousTarget': array([66., 11.]), 'currentState': array([55.21560199, 20.38387073,  5.61453337]), 'targetState': array([66, 11], dtype=int32), 'currentDistance': 14.295463274205845}
done in step count: 16
reward sum = 0.7779887441938436
running average episode reward sum: 0.6768073158925657
{'scaleFactor': 20, 'currentTarget': array([66., 11.]), 'dynamicTrap': False, 'previousTarget': array([66., 11.]), 'currentState': array([65.66008137, 10.49430529,  5.82238531]), 'targetState': array([66, 11], dtype=int32), 'currentDistance': 0.6093207825811402}
episode index:1248
target Thresh 75.86662988223739
target distance 5.0
model initialize at round 1248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 22.]), 'dynamicTrap': False, 'previousTarget': array([46., 22.]), 'currentState': array([52.60434998, 21.62454412,  0.82908928]), 'targetState': array([46, 22], dtype=int32), 'currentDistance': 6.615013664152375}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6770116858141145
{'scaleFactor': 20, 'currentTarget': array([46., 22.]), 'dynamicTrap': False, 'previousTarget': array([46., 22.]), 'currentState': array([45.8041558 , 21.9665606 ,  2.91787876]), 'targetState': array([46, 22], dtype=int32), 'currentDistance': 0.1986784953604647}
episode index:1249
target Thresh 75.86729506847482
target distance 32.0
model initialize at round 1249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.95403529, 13.26598586]), 'dynamicTrap': False, 'previousTarget': array([20.33768561, 12.61689548]), 'currentState': array([37.16152723, 23.45922976,  3.10232961]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6770986189781091
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'dynamicTrap': False, 'previousTarget': array([6., 5.]), 'currentState': array([5.51444804, 5.7196877 , 5.66292893]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.8681653574202064}
episode index:1250
target Thresh 75.86795693708203
target distance 21.0
model initialize at round 1250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.06801115,  8.01567665]), 'dynamicTrap': False, 'previousTarget': array([49.71663071,  8.71986011]), 'currentState': array([33.36526949, 17.32228386,  5.05101756]), 'targetState': array([53,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6612075892336544
running average episode reward sum: 0.6770859163164428
{'scaleFactor': 20, 'currentTarget': array([53.,  7.]), 'dynamicTrap': False, 'previousTarget': array([53.,  7.]), 'currentState': array([52.33304065,  6.33648882,  0.83709519]), 'targetState': array([53,  7], dtype=int32), 'currentDistance': 0.9407878922967992}
episode index:1251
target Thresh 75.86861550460578
target distance 43.0
model initialize at round 1251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.01920191, 13.20914115]), 'dynamicTrap': False, 'previousTarget': array([24.37605409, 12.956665  ]), 'currentState': array([6.67910608, 8.11397742, 5.34096116]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = -0.0559937442388293
running average episode reward sum: 0.6765003894310152
{'scaleFactor': 20, 'currentTarget': array([48., 19.]), 'dynamicTrap': False, 'previousTarget': array([48., 19.]), 'currentState': array([47.07989084, 18.88443693,  1.47153414]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 0.9273379551241536}
episode index:1252
target Thresh 75.86927078751033
target distance 56.0
model initialize at round 1252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.19774652,  9.82823194]), 'dynamicTrap': False, 'previousTarget': array([70.88618308, 10.86933753]), 'currentState': array([50.25654454, 11.3607018 ,  3.83936763]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6172621159635634
running average episode reward sum: 0.6764531122774099
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([107.40357287,   6.10356277,   2.99901784]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 0.9830924515040214}
episode index:1253
target Thresh 75.86992280217774
target distance 14.0
model initialize at round 1253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 22.]), 'dynamicTrap': False, 'previousTarget': array([91., 22.]), 'currentState': array([84.17854039,  7.90989519,  5.83426021]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 15.654499824224052}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6766495170478648
{'scaleFactor': 20, 'currentTarget': array([91., 22.]), 'dynamicTrap': False, 'previousTarget': array([91., 22.]), 'currentState': array([90.12165442, 21.34139081,  1.28157201]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 1.097841983746995}
episode index:1254
target Thresh 75.87057156490845
target distance 39.0
model initialize at round 1254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.14679139, 15.33109394]), 'dynamicTrap': False, 'previousTarget': array([59.02633404, 15.67544468]), 'currentState': array([76.03053032, 21.91929582,  3.48048681]), 'targetState': array([39,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6641233236560466
running average episode reward sum: 0.6766395360172737
{'scaleFactor': 20, 'currentTarget': array([39.,  9.]), 'dynamicTrap': False, 'previousTarget': array([39.,  9.]), 'currentState': array([38.5641722 ,  9.52305265,  2.20953806]), 'targetState': array([39,  9], dtype=int32), 'currentDistance': 0.6808303358683836}
episode index:1255
target Thresh 75.87121709192152
target distance 53.0
model initialize at round 1255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.78984275, 15.42731401]), 'dynamicTrap': False, 'previousTarget': array([68.41732325, 14.93567086]), 'currentState': array([86.26535329, 19.97753256,  2.79336208]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6344476485326767
running average episode reward sum: 0.6766059437501681
{'scaleFactor': 20, 'currentTarget': array([36.56264477,  8.67209235]), 'dynamicTrap': True, 'previousTarget': array([35.,  8.]), 'currentState': array([36.5758391 ,  9.08888013,  3.94795781]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 0.41699657609673696}
episode index:1256
target Thresh 75.87185939935519
target distance 45.0
model initialize at round 1256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.24909072,  7.06904522]), 'dynamicTrap': False, 'previousTarget': array([92.82455801,  6.6432744 ]), 'currentState': array([74.39965841,  4.61954791,  5.68990368]), 'targetState': array([118,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6773844430954769
running average episode reward sum: 0.6766065630813894
{'scaleFactor': 20, 'currentTarget': array([118.,  10.]), 'dynamicTrap': False, 'previousTarget': array([118.,  10.]), 'currentState': array([117.49441048,   9.27950161,   0.33744961]), 'targetState': array([118,  10], dtype=int32), 'currentDistance': 0.8801924182530564}
episode index:1257
target Thresh 75.87249850326717
target distance 22.0
model initialize at round 1257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.56379606,   9.48586031]), 'dynamicTrap': False, 'previousTarget': array([105.79880147,   9.54654412]), 'currentState': array([92.47610926, 23.68223593,  5.79958183]), 'targetState': array([113,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.663595421468028
running average episode reward sum: 0.676596220361506
{'scaleFactor': 20, 'currentTarget': array([113.,   3.]), 'dynamicTrap': False, 'previousTarget': array([113.,   3.]), 'currentState': array([112.58851902,   3.14368947,   5.45929785]), 'targetState': array([113,   3], dtype=int32), 'currentDistance': 0.43584774828890116}
episode index:1258
target Thresh 75.87313441963508
target distance 16.0
model initialize at round 1258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.17364872, 13.90111659]), 'dynamicTrap': True, 'previousTarget': array([42., 14.]), 'currentState': array([58.      , 23.      ,  2.553115], dtype=float32), 'targetState': array([42, 14], dtype=int32), 'currentDistance': 18.255494356180158}
done in step count: 15
reward sum = 0.8032785708648859
running average episode reward sum: 0.6766968417677836
{'scaleFactor': 20, 'currentTarget': array([41.60577605, 15.59166346]), 'dynamicTrap': True, 'previousTarget': array([42., 14.]), 'currentState': array([42.47568471, 15.30489286,  5.61345119]), 'targetState': array([42, 14], dtype=int32), 'currentDistance': 0.9159576707327713}
episode index:1259
target Thresh 75.87376716435689
target distance 68.0
model initialize at round 1259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.21532298, 20.38550579]), 'dynamicTrap': False, 'previousTarget': array([66.78718271, 20.09012019]), 'currentState': array([48.45996295, 23.50411931,  5.60577554]), 'targetState': array([115,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5444074079929858
running average episode reward sum: 0.6765918501536765
{'scaleFactor': 20, 'currentTarget': array([115.,  13.]), 'dynamicTrap': False, 'previousTarget': array([115.,  13.]), 'currentState': array([115.60374926,  12.94365397,   5.92998629]), 'targetState': array([115,  13], dtype=int32), 'currentDistance': 0.6063728594060033}
episode index:1260
target Thresh 75.87439675325122
target distance 67.0
model initialize at round 1260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.57085118,  9.69062009]), 'dynamicTrap': False, 'previousTarget': array([25.78088734,  8.95237125]), 'currentState': array([7.7609924 , 6.93934853, 0.3774682 ]), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6102152008667333
running average episode reward sum: 0.6765392120495631
{'scaleFactor': 20, 'currentTarget': array([73., 16.]), 'dynamicTrap': False, 'previousTarget': array([73., 16.]), 'currentState': array([72.91007029, 15.55197366,  1.62876193]), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 0.4569627497138402}
episode index:1261
target Thresh 75.87502320205785
target distance 13.0
model initialize at round 1261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.,  4.]), 'dynamicTrap': False, 'previousTarget': array([81.,  4.]), 'currentState': array([69.37885686, 10.87404809,  4.76404393]), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 13.501981520758527}
done in step count: 13
reward sum = 0.8503886472314562
running average episode reward sum: 0.6766769691297391
{'scaleFactor': 20, 'currentTarget': array([81.,  4.]), 'dynamicTrap': False, 'previousTarget': array([81.,  4.]), 'currentState': array([80.02833553,  3.66510386,  5.32620399]), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 1.027758372386223}
episode index:1262
target Thresh 75.87564652643802
target distance 15.0
model initialize at round 1262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.15008935,  2.1054038 ]), 'dynamicTrap': True, 'previousTarget': array([12.,  2.]), 'currentState': array([24.       , 17.       ,  3.7275305], dtype=float32), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 19.033375379806557}
done in step count: 18
reward sum = 0.7690325691679899
running average episode reward sum: 0.6767500931202682
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'dynamicTrap': False, 'previousTarget': array([12.,  2.]), 'currentState': array([11.06207013,  1.81808352,  4.08543284]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.9554088384178251}
episode index:1263
target Thresh 75.87626674197487
target distance 53.0
model initialize at round 1263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.59130534, 12.17272628]), 'dynamicTrap': True, 'previousTarget': array([39.57578312, 12.23556944]), 'currentState': array([59.       , 17.       ,  2.2437618], dtype=float32), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.633485953780995
running average episode reward sum: 0.6767158651619303
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'dynamicTrap': False, 'previousTarget': array([6., 4.]), 'currentState': array([6.24097538, 3.27311749, 3.63008371]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.765785428018411}
episode index:1264
target Thresh 75.87688386417383
target distance 26.0
model initialize at round 1264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.90530042, 15.87268485]), 'dynamicTrap': True, 'previousTarget': array([19.85217755, 15.80033179]), 'currentState': array([36.       ,  4.       ,  4.9071727], dtype=float32), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 34
reward sum = 0.6225108014883701
running average episode reward sum: 0.6766730153092239
{'scaleFactor': 20, 'currentTarget': array([10., 23.]), 'dynamicTrap': False, 'previousTarget': array([10., 23.]), 'currentState': array([10.22966643, 22.71881618,  2.83612319]), 'targetState': array([10, 23], dtype=int32), 'currentDistance': 0.3630578566124274}
episode index:1265
target Thresh 75.87749790846298
target distance 10.0
model initialize at round 1265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 15.]), 'dynamicTrap': False, 'previousTarget': array([32., 15.]), 'currentState': array([23.30093768, 10.85957054,  1.40131622]), 'targetState': array([32, 15], dtype=int32), 'currentDistance': 9.634149746772174}
done in step count: 10
reward sum = 0.8749780850088044
running average episode reward sum: 0.676829654384816
{'scaleFactor': 20, 'currentTarget': array([32., 15.]), 'dynamicTrap': False, 'previousTarget': array([32., 15.]), 'currentState': array([32.28474495, 15.07283778,  1.77560666]), 'targetState': array([32, 15], dtype=int32), 'currentDistance': 0.29391330477563404}
episode index:1266
target Thresh 75.87810889019346
target distance 43.0
model initialize at round 1266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.01545593, 18.10383581]), 'dynamicTrap': False, 'previousTarget': array([70.00540614, 17.53500945]), 'currentState': array([88.99036391, 19.10536032,  3.20448267]), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7304683303085159
running average episode reward sum: 0.6768719895670763
{'scaleFactor': 20, 'currentTarget': array([47., 17.]), 'dynamicTrap': False, 'previousTarget': array([47., 17.]), 'currentState': array([47.20089832, 17.72052549,  3.16773847]), 'targetState': array([47, 17], dtype=int32), 'currentDistance': 0.7480087639510729}
episode index:1267
target Thresh 75.87871682463985
target distance 73.0
model initialize at round 1267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.72581551, 20.05240777]), 'dynamicTrap': False, 'previousTarget': array([32.99249812, 20.54773967]), 'currentState': array([14.74022766, 19.2932777 ,  0.23036092]), 'targetState': array([86, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.38922884767129806
running average episode reward sum: 0.676645141663373
{'scaleFactor': 20, 'currentTarget': array([86., 22.]), 'dynamicTrap': False, 'previousTarget': array([86., 22.]), 'currentState': array([86.24843732, 22.77995696,  5.96407231]), 'targetState': array([86, 22], dtype=int32), 'currentDistance': 0.8185682385878956}
episode index:1268
target Thresh 75.87932172700053
target distance 64.0
model initialize at round 1268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.54348219, 12.82190763]), 'dynamicTrap': False, 'previousTarget': array([56.76024068, 11.91246239]), 'currentState': array([37.85979474, 16.36485109,  2.01864181]), 'targetState': array([101,   5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 81
reward sum = 0.29062345262078093
running average episode reward sum: 0.6763409480549865
{'scaleFactor': 20, 'currentTarget': array([101.,   5.]), 'dynamicTrap': False, 'previousTarget': array([101.,   5.]), 'currentState': array([100.81519   ,   5.14097038,   0.11376403]), 'targetState': array([101,   5], dtype=int32), 'currentDistance': 0.23243791443159145}
episode index:1269
target Thresh 75.8799236123981
target distance 17.0
model initialize at round 1269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([35.55898996, 20.63725325,  4.09165192]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 16.59596512552541}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6765205079974698
{'scaleFactor': 20, 'currentTarget': array([30.,  5.]), 'dynamicTrap': False, 'previousTarget': array([30.,  5.]), 'currentState': array([29.69728576,  5.04848948,  3.88080838]), 'targetState': array([30,  5], dtype=int32), 'currentDistance': 0.30657322643324336}
episode index:1270
target Thresh 75.88052249587975
target distance 46.0
model initialize at round 1270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.48143053, 13.8711328 ]), 'dynamicTrap': False, 'previousTarget': array([91.54842217, 13.34853574]), 'currentState': array([109.79169959,  19.07819616,   2.92670727]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5923328100001556
running average episode reward sum: 0.6764542706268974
{'scaleFactor': 20, 'currentTarget': array([65.58128223,  8.82929584]), 'dynamicTrap': True, 'previousTarget': array([65.,  7.]), 'currentState': array([65.40810615,  8.11389969,  4.88437158]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 0.7360581573291981}
episode index:1271
target Thresh 75.88111839241755
target distance 22.0
model initialize at round 1271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.21045773,  8.77813855]), 'dynamicTrap': False, 'previousTarget': array([27.29527642,  8.26234812]), 'currentState': array([9.95618535, 3.36768615, 0.35696332]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8305121606759682
running average episode reward sum: 0.6765753853203323
{'scaleFactor': 20, 'currentTarget': array([30.,  9.]), 'dynamicTrap': False, 'previousTarget': array([30.,  9.]), 'currentState': array([29.67064537,  8.67419648,  0.53228831]), 'targetState': array([30,  9], dtype=int32), 'currentDistance': 0.4632735746001429}
episode index:1272
target Thresh 75.88171131690898
target distance 5.0
model initialize at round 1272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'dynamicTrap': False, 'previousTarget': array([23.,  3.]), 'currentState': array([27.70868043,  8.65756478,  2.75477111]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 7.3606868288142735}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6767984965730265
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'dynamicTrap': False, 'previousTarget': array([23.,  3.]), 'currentState': array([22.38701633,  3.38120152,  3.80138224]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.7218473391855834}
episode index:1273
target Thresh 75.88230128417716
target distance 5.0
model initialize at round 1273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'dynamicTrap': False, 'previousTarget': array([12., 12.]), 'currentState': array([ 8.77449929, 11.92787798,  5.3999604 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 3.2263069348463476}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6770365668268938
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'dynamicTrap': False, 'previousTarget': array([12., 12.]), 'currentState': array([12.23301247, 11.26879008,  0.82620245]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.7674390904992114}
episode index:1274
target Thresh 75.88288830897132
target distance 58.0
model initialize at round 1274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.28989379, 15.40019729]), 'dynamicTrap': False, 'previousTarget': array([30.95260657, 14.62395817]), 'currentState': array([12.37513576, 17.24476036,  1.40968459]), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5327096668773927
running average episode reward sum: 0.676923369258306
{'scaleFactor': 20, 'currentTarget': array([69., 12.]), 'dynamicTrap': False, 'previousTarget': array([69., 12.]), 'currentState': array([69.12587389, 11.03850792,  6.04534919]), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 0.969696477186593}
episode index:1275
target Thresh 75.88347240596711
target distance 29.0
model initialize at round 1275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.88761044,  11.00564404]), 'dynamicTrap': False, 'previousTarget': array([104.89383588,  10.94201698]), 'currentState': array([85.00830256, 13.19952603,  1.43315812]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7993588371738817
running average episode reward sum: 0.6770193218193683
{'scaleFactor': 20, 'currentTarget': array([114.,  10.]), 'dynamicTrap': False, 'previousTarget': array([114.,  10.]), 'currentState': array([1.13384474e+02, 9.27814374e+00, 3.59771520e-02]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 0.9486562466725842}
episode index:1276
target Thresh 75.88405358976698
target distance 22.0
model initialize at round 1276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.37790285, 17.4380366 ]), 'dynamicTrap': False, 'previousTarget': array([87.52454686, 17.26673649]), 'currentState': array([72.53620228,  5.2298212 ,  0.48427873]), 'targetState': array([93, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7874349989337284
running average episode reward sum: 0.6771057867192228
{'scaleFactor': 20, 'currentTarget': array([93., 21.]), 'dynamicTrap': False, 'previousTarget': array([93., 21.]), 'currentState': array([93.32129697, 20.01096571,  1.11191581]), 'targetState': array([93, 21], dtype=int32), 'currentDistance': 1.0399137287155513}
episode index:1277
target Thresh 75.88463187490053
target distance 29.0
model initialize at round 1277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.5132184 , 14.70631079]), 'dynamicTrap': True, 'previousTarget': array([40.48844632, 14.66154686]), 'currentState': array([58.      ,  5.      ,  5.434356], dtype=float32), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6325969738516468
running average episode reward sum: 0.6770709597920963
{'scaleFactor': 20, 'currentTarget': array([29., 21.]), 'dynamicTrap': False, 'previousTarget': array([29., 21.]), 'currentState': array([29.62904621, 20.72220444,  3.03631774]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 0.6876550763856587}
episode index:1278
target Thresh 75.88520727582497
target distance 73.0
model initialize at round 1278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.86298646, 14.96273608]), 'dynamicTrap': False, 'previousTarget': array([73.18505209, 13.71437643]), 'currentState': array([92.73381881, 12.69337017,  2.65498835]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5426814444990008
running average episode reward sum: 0.676965885894291
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'dynamicTrap': False, 'previousTarget': array([20., 21.]), 'currentState': array([19.53596601, 21.0557065 ,  3.3849492 ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 0.46736576339878977}
episode index:1279
target Thresh 75.88577980692533
target distance 36.0
model initialize at round 1279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.68235521, 13.76255407]), 'dynamicTrap': False, 'previousTarget': array([44.88854382, 12.94427191]), 'currentState': array([28.82835424,  4.74952742,  0.66043054]), 'targetState': array([63, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5394150483042698
running average episode reward sum: 0.6768584243024238
{'scaleFactor': 20, 'currentTarget': array([63., 22.]), 'dynamicTrap': False, 'previousTarget': array([63., 22.]), 'currentState': array([62.79383508, 21.85230101,  2.36084936]), 'targetState': array([63, 22], dtype=int32), 'currentDistance': 0.25361183696122075}
episode index:1280
target Thresh 75.8863494825149
target distance 59.0
model initialize at round 1280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.93216373, 10.08822551]), 'dynamicTrap': False, 'previousTarget': array([71.30280656, 10.2346594 ]), 'currentState': array([53.70639438,  4.57734599,  5.8623482 ]), 'targetState': array([111,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5192155751121871
running average episode reward sum: 0.6767353619689419
{'scaleFactor': 20, 'currentTarget': array([111.,  21.]), 'dynamicTrap': False, 'previousTarget': array([111.,  21.]), 'currentState': array([111.93797279,  20.68026276,   0.25554603]), 'targetState': array([111,  21], dtype=int32), 'currentDistance': 0.9909716719906819}
episode index:1281
target Thresh 75.88691631683562
target distance 53.0
model initialize at round 1281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.9972813 ,  9.26014139]), 'dynamicTrap': False, 'previousTarget': array([62.99644096,  9.37729134]), 'currentState': array([43.00230512,  8.81189179,  4.82202841]), 'targetState': array([96, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6863875683540906
running average episode reward sum: 0.6767428909910833
{'scaleFactor': 20, 'currentTarget': array([96., 10.]), 'dynamicTrap': False, 'previousTarget': array([96., 10.]), 'currentState': array([96.53238802,  9.24419897,  5.9665514 ]), 'targetState': array([96, 10], dtype=int32), 'currentDistance': 0.9244848303989822}
episode index:1282
target Thresh 75.88748032405837
target distance 34.0
model initialize at round 1282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.98454122, 11.32892243]), 'dynamicTrap': False, 'previousTarget': array([46.97110963, 10.84359429]), 'currentState': array([64.67363866, 18.45055403,  2.63711232]), 'targetState': array([32,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.20061171285263574
running average episode reward sum: 0.6763717832918328
{'scaleFactor': 20, 'currentTarget': array([32.,  6.]), 'dynamicTrap': False, 'previousTarget': array([32.,  6.]), 'currentState': array([32.70354787,  6.03945096,  4.57069291]), 'targetState': array([32,  6], dtype=int32), 'currentDistance': 0.7046530958027493}
episode index:1283
target Thresh 75.88804151828336
target distance 46.0
model initialize at round 1283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.25339533, 14.91873254]), 'dynamicTrap': False, 'previousTarget': array([96.98540321, 14.799588  ]), 'currentState': array([114.11184941,  21.57896599,   3.12163544]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 87
reward sum = 0.03790832927441734
running average episode reward sum: 0.6758745376111338
{'scaleFactor': 20, 'currentTarget': array([70.,  6.]), 'dynamicTrap': False, 'previousTarget': array([70.,  6.]), 'currentState': array([69.74398374,  6.53868594,  5.40740032]), 'targetState': array([70,  6], dtype=int32), 'currentDistance': 0.5964284210594009}
episode index:1284
target Thresh 75.88859991354047
target distance 14.0
model initialize at round 1284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.82085603,  9.97006862]), 'dynamicTrap': True, 'previousTarget': array([36., 10.]), 'currentState': array([22.        ,  8.        ,  0.17588913], dtype=float32), 'targetState': array([36, 10], dtype=int32), 'currentDistance': 13.960559865407166}
done in step count: 18
reward sum = 0.7595673937762467
running average episode reward sum: 0.6759396682384997
{'scaleFactor': 20, 'currentTarget': array([36., 10.]), 'dynamicTrap': False, 'previousTarget': array([36., 10.]), 'currentState': array([36.45359602, 10.07214927,  4.3790726 ]), 'targetState': array([36, 10], dtype=int32), 'currentDistance': 0.45929823553694415}
episode index:1285
target Thresh 75.88915552378964
target distance 38.0
model initialize at round 1285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.326708  , 10.19121131]), 'dynamicTrap': False, 'previousTarget': array([30.00692161,  9.47386636]), 'currentState': array([49.27960926, 11.56297343,  2.88431704]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6235991594004022
running average episode reward sum: 0.6758989679983457
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'dynamicTrap': False, 'previousTarget': array([12.,  9.]), 'currentState': array([11.47953725,  9.50205191,  3.76160755]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.7231442427225948}
episode index:1286
target Thresh 75.88970836292111
target distance 36.0
model initialize at round 1286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.16378704, 13.49120148]), 'dynamicTrap': False, 'previousTarget': array([46.72376903, 13.12276932]), 'currentState': array([63.30217305,  5.06511384,  2.1205622 ]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.3749421746497504
running average episode reward sum: 0.6756651243360702
{'scaleFactor': 20, 'currentTarget': array([29., 21.]), 'dynamicTrap': False, 'previousTarget': array([29., 21.]), 'currentState': array([28.0909129 , 21.75686039,  4.93444535]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 1.1829103966338141}
episode index:1287
target Thresh 75.89025844475591
target distance 72.0
model initialize at round 1287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.30750182, 18.68494165]), 'dynamicTrap': False, 'previousTarget': array([82.12232531, 17.79136948]), 'currentState': array([103.15207254,  21.17351817,   1.80461949]), 'targetState': array([30, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.49935054707646137
running average episode reward sum: 0.6755282341363344
{'scaleFactor': 20, 'currentTarget': array([30., 12.]), 'dynamicTrap': False, 'previousTarget': array([30., 12.]), 'currentState': array([29.94043201, 11.66275118,  2.70744922]), 'targetState': array([30, 12], dtype=int32), 'currentDistance': 0.3424691439506264}
episode index:1288
target Thresh 75.89080578304612
target distance 65.0
model initialize at round 1288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.84624721, 17.52482894]), 'dynamicTrap': True, 'previousTarget': array([61.85022022, 17.55689597]), 'currentState': array([42.       , 20.       ,  0.6084957], dtype=float32), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.40087731225220813
running average episode reward sum: 0.6753151612721885
{'scaleFactor': 20, 'currentTarget': array([107.,  12.]), 'dynamicTrap': False, 'previousTarget': array([107.,  12.]), 'currentState': array([107.08907926,  12.73761967,   1.65323683]), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 0.7429790665427325}
episode index:1289
target Thresh 75.89135039147521
target distance 18.0
model initialize at round 1289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.,  4.]), 'dynamicTrap': False, 'previousTarget': array([76.,  4.]), 'currentState': array([93.28790552,  4.47494231,  3.26579082]), 'targetState': array([76,  4], dtype=int32), 'currentDistance': 17.29442822429475}
done in step count: 11
reward sum = 0.8766027992856364
running average episode reward sum: 0.675471198200881
{'scaleFactor': 20, 'currentTarget': array([76.,  4.]), 'dynamicTrap': False, 'previousTarget': array([76.,  4.]), 'currentState': array([76.51899674,  4.52565826,  4.28535946]), 'targetState': array([76,  4], dtype=int32), 'currentDistance': 0.7386976572075973}
episode index:1290
target Thresh 75.89189228365845
target distance 73.0
model initialize at round 1290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.8573338 , 11.79923989]), 'dynamicTrap': False, 'previousTarget': array([28.84971261, 10.55277516]), 'currentState': array([ 9.06930195, 14.7033397 ,  0.55719948]), 'targetState': array([82,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 83
reward sum = 0.37518035630216073
running average episode reward sum: 0.6752385949151345
{'scaleFactor': 20, 'currentTarget': array([82.,  4.]), 'dynamicTrap': False, 'previousTarget': array([82.,  4.]), 'currentState': array([82.18374341,  3.51122806,  1.67579321]), 'targetState': array([82,  4], dtype=int32), 'currentDistance': 0.5221682207568963}
episode index:1291
target Thresh 75.89243147314313
target distance 14.0
model initialize at round 1291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'dynamicTrap': False, 'previousTarget': array([15., 18.]), 'currentState': array([28.58846949,  4.57695881,  1.3465479 ]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 19.100380572044845}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6753426887799225
{'scaleFactor': 20, 'currentTarget': array([15., 18.]), 'dynamicTrap': False, 'previousTarget': array([15., 18.]), 'currentState': array([14.47963844, 18.20054893,  5.45439538]), 'targetState': array([15, 18], dtype=int32), 'currentDistance': 0.5576701728014243}
episode index:1292
target Thresh 75.89296797340904
target distance 26.0
model initialize at round 1292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.66738185,  9.08670929]), 'dynamicTrap': False, 'previousTarget': array([93.05891029,  9.46607002]), 'currentState': array([112.66504145,   9.39266765,   3.49693537]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6934255695293183
running average episode reward sum: 0.6753566739931858
{'scaleFactor': 20, 'currentTarget': array([87.,  9.]), 'dynamicTrap': False, 'previousTarget': array([87.,  9.]), 'currentState': array([87.95720503,  8.25829617,  1.65896644]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 1.2109360232562862}
episode index:1293
target Thresh 75.89350179786871
target distance 23.0
model initialize at round 1293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.85971675, 15.56435716]), 'dynamicTrap': False, 'previousTarget': array([34.83200822, 15.58678368]), 'currentState': array([15.0494352, 12.8161314,  4.8807799]), 'targetState': array([38, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.548242528106123
running average episode reward sum: 0.6752584404955915
{'scaleFactor': 20, 'currentTarget': array([38., 16.]), 'dynamicTrap': False, 'previousTarget': array([38., 16.]), 'currentState': array([37.84466266, 16.58836289,  4.82219739]), 'targetState': array([38, 16], dtype=int32), 'currentDistance': 0.6085232783114743}
episode index:1294
target Thresh 75.89403295986779
target distance 55.0
model initialize at round 1294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.8830293 , 13.83967596]), 'dynamicTrap': False, 'previousTarget': array([90.3226018 , 13.42229124]), 'currentState': array([108.48759372,  17.79708091,   3.62108922]), 'targetState': array([55,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.47762938552184864
running average episode reward sum: 0.6751058311867315
{'scaleFactor': 20, 'currentTarget': array([55.,  7.]), 'dynamicTrap': False, 'previousTarget': array([55.,  7.]), 'currentState': array([55.31502557,  7.73162797,  3.31587454]), 'targetState': array([55,  7], dtype=int32), 'currentDistance': 0.7965680079179617}
episode index:1295
target Thresh 75.89456147268534
target distance 19.0
model initialize at round 1295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.82916288, 17.57651021]), 'dynamicTrap': False, 'previousTarget': array([14.09517373, 16.33589719]), 'currentState': array([28.01829189,  4.56534731,  1.82526547]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6546662220127024
running average episode reward sum: 0.6750900598833565
{'scaleFactor': 20, 'currentTarget': array([10., 20.]), 'dynamicTrap': False, 'previousTarget': array([10., 20.]), 'currentState': array([10.70681385, 20.50778768,  2.25263448]), 'targetState': array([10, 20], dtype=int32), 'currentDistance': 0.8703069329699926}
episode index:1296
target Thresh 75.89508734953421
target distance 56.0
model initialize at round 1296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.07570643, 16.99022261]), 'dynamicTrap': True, 'previousTarget': array([32.04057123, 16.87981639]), 'currentState': array([13.      , 23.      ,  5.849738], dtype=float32), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.483120220027219
running average episode reward sum: 0.6749420492126887
{'scaleFactor': 20, 'currentTarget': array([69.,  5.]), 'dynamicTrap': False, 'previousTarget': array([69.,  5.]), 'currentState': array([68.61249501,  5.48707992,  4.1996659 ]), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 0.6224202505290636}
episode index:1297
target Thresh 75.89561060356135
target distance 39.0
model initialize at round 1297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.5786725 , 14.02038465]), 'dynamicTrap': False, 'previousTarget': array([25.89562877, 14.95942269]), 'currentState': array([ 6.60928424, 15.12651868,  5.330181  ]), 'targetState': array([45, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6418659595033075
running average episode reward sum: 0.6749165668631437
{'scaleFactor': 20, 'currentTarget': array([44.07891911, 11.24038396]), 'dynamicTrap': True, 'previousTarget': array([45., 13.]), 'currentState': array([44.27964136, 11.39149721,  0.88625907]), 'targetState': array([45, 13], dtype=int32), 'currentDistance': 0.25124616593393484}
episode index:1298
target Thresh 75.89613124784816
target distance 15.0
model initialize at round 1298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'dynamicTrap': False, 'previousTarget': array([12.35363499, 21.62110536]), 'currentState': array([25.37027951,  8.8765635 ,  2.14602172]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 18.734699352823103}
done in step count: 27
reward sum = 0.6783118377694206
running average episode reward sum: 0.6749191806205774
{'scaleFactor': 20, 'currentTarget': array([12., 22.]), 'dynamicTrap': False, 'previousTarget': array([12., 22.]), 'currentState': array([12.75573111, 21.77343003,  3.49384434]), 'targetState': array([12, 22], dtype=int32), 'currentDistance': 0.7889635386040204}
episode index:1299
target Thresh 75.89664929541074
target distance 10.0
model initialize at round 1299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97., 17.]), 'dynamicTrap': False, 'previousTarget': array([97., 17.]), 'currentState': array([88.76649275, 18.17235978,  6.21239186]), 'targetState': array([97, 17], dtype=int32), 'currentDistance': 8.31655391668264}
done in step count: 6
reward sum = 0.92245938780699
running average episode reward sum: 0.675109596164567
{'scaleFactor': 20, 'currentTarget': array([95.22788753, 17.31793651]), 'dynamicTrap': True, 'previousTarget': array([97., 17.]), 'currentState': array([95.61905762, 17.18828064,  0.31059102]), 'targetState': array([97, 17], dtype=int32), 'currentDistance': 0.4120979102585887}
episode index:1300
target Thresh 75.89716475920034
target distance 12.0
model initialize at round 1300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.47563762,   4.3728944 ]), 'dynamicTrap': True, 'previousTarget': array([106.,   4.]), 'currentState': array([94.        , 13.        ,  0.54164714], dtype=float32), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 13.570775018151812}
done in step count: 15
reward sum = 0.7937170287737768
running average episode reward sum: 0.6752007625232213
{'scaleFactor': 20, 'currentTarget': array([106.,   4.]), 'dynamicTrap': False, 'previousTarget': array([106.,   4.]), 'currentState': array([105.05913509,   3.13348296,   0.84813527]), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 1.2790928658790315}
episode index:1301
target Thresh 75.89767765210355
target distance 13.0
model initialize at round 1301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 22.]), 'dynamicTrap': False, 'previousTarget': array([73., 22.]), 'currentState': array([60.69687666, 20.5224301 ,  5.87964272]), 'targetState': array([73, 22], dtype=int32), 'currentDistance': 12.391531650111931}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6753980471510123
{'scaleFactor': 20, 'currentTarget': array([73., 22.]), 'dynamicTrap': False, 'previousTarget': array([73., 22.]), 'currentState': array([73.66447169, 21.67375848,  0.64387046]), 'targetState': array([73, 22], dtype=int32), 'currentDistance': 0.7402406091735909}
episode index:1302
target Thresh 75.89818798694273
target distance 20.0
model initialize at round 1302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.41652782,  2.69472765]), 'dynamicTrap': False, 'previousTarget': array([96.43046618,  2.57218647]), 'currentState': array([114.37318203,  11.50146199,   2.92472303]), 'targetState': array([95,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5150732979846044
running average episode reward sum: 0.6752750043657733
{'scaleFactor': 20, 'currentTarget': array([95.,  2.]), 'dynamicTrap': False, 'previousTarget': array([95.,  2.]), 'currentState': array([94.28460357,  2.18025454,  2.82001836]), 'targetState': array([95,  2], dtype=int32), 'currentDistance': 0.7377558871252831}
episode index:1303
target Thresh 75.8986957764763
target distance 34.0
model initialize at round 1303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.52529399,  8.06878199]), 'dynamicTrap': False, 'previousTarget': array([98.70164397,  8.92719587]), 'currentState': array([115.97934694,  15.77903036,   3.00611639]), 'targetState': array([83,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 27
reward sum = 0.7090403937192419
running average episode reward sum: 0.6753008980692652
{'scaleFactor': 20, 'currentTarget': array([83.,  2.]), 'dynamicTrap': False, 'previousTarget': array([83.,  2.]), 'currentState': array([83.09557515,  2.12290744,  4.16018926]), 'targetState': array([83,  2], dtype=int32), 'currentDistance': 0.1556947279238162}
episode index:1304
target Thresh 75.89920103339898
target distance 40.0
model initialize at round 1304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.01129496,  8.02145373]), 'dynamicTrap': False, 'previousTarget': array([25.84555753,  7.51930531]), 'currentState': array([ 7.25977331, 11.16428566,  6.08172459]), 'targetState': array([46,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.5890447854702081
running average episode reward sum: 0.6752348014312584
{'scaleFactor': 20, 'currentTarget': array([46.,  5.]), 'dynamicTrap': False, 'previousTarget': array([46.,  5.]), 'currentState': array([46.25695223,  4.17132462,  1.72927505]), 'targetState': array([46,  5], dtype=int32), 'currentDistance': 0.8675986031383716}
episode index:1305
target Thresh 75.89970377034226
target distance 43.0
model initialize at round 1305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.19400333,  3.99959555]), 'dynamicTrap': False, 'previousTarget': array([61.02159828,  4.92922799]), 'currentState': array([81.12003096,  2.28104296,  3.84138894]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5129512459944386
running average episode reward sum: 0.6751105414347524
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([37.18452309,  5.58150691,  5.3118359 ]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 0.9165909948276643}
episode index:1306
target Thresh 75.90020399987458
target distance 71.0
model initialize at round 1306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.7246521 ,  8.22334075]), 'dynamicTrap': False, 'previousTarget': array([63.17868512,  7.67256884]), 'currentState': array([45.53628777,  2.58360014,  5.63622469]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 81
reward sum = 0.3815061559781726
running average episode reward sum: 0.6748859015070886
{'scaleFactor': 20, 'currentTarget': array([115.,  23.]), 'dynamicTrap': False, 'previousTarget': array([115.,  23.]), 'currentState': array([115.78918433,  23.82573418,   2.10825754]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 1.1422122573657354}
episode index:1307
target Thresh 75.9007017345017
target distance 16.0
model initialize at round 1307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  9.]), 'dynamicTrap': False, 'previousTarget': array([18.,  9.]), 'currentState': array([34.60298983, 19.35666121,  4.17298174]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 19.56833418527187}
done in step count: 15
reward sum = 0.815456092728609
running average episode reward sum: 0.6749933710722427
{'scaleFactor': 20, 'currentTarget': array([19.92244321,  9.474325  ]), 'dynamicTrap': True, 'previousTarget': array([18.,  9.]), 'currentState': array([20.51225172, 10.14061143,  3.62579539]), 'targetState': array([18,  9], dtype=int32), 'currentDistance': 0.8898380110866043}
episode index:1308
target Thresh 75.90119698666699
target distance 21.0
model initialize at round 1308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,   3.]), 'dynamicTrap': False, 'previousTarget': array([109.90990945,   3.10381815]), 'currentState': array([91.62839426,  4.76073346,  5.14067823]), 'targetState': array([111,   3], dtype=int32), 'currentDistance': 19.451459870510917}
done in step count: 31
reward sum = 0.7134585489043094
running average episode reward sum: 0.675022756234834
{'scaleFactor': 20, 'currentTarget': array([111.,   3.]), 'dynamicTrap': False, 'previousTarget': array([111.,   3.]), 'currentState': array([110.51689622,   2.52814571,   1.23252761]), 'targetState': array([111,   3], dtype=int32), 'currentDistance': 0.675304179747009}
episode index:1309
target Thresh 75.90168976875182
target distance 2.0
model initialize at round 1309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,   6.]), 'dynamicTrap': False, 'previousTarget': array([101.,   6.]), 'currentState': array([102.98445911,   4.19873982,   1.70292276]), 'targetState': array([101,   6], dtype=int32), 'currentDistance': 2.680040333001639}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6752631968789296
{'scaleFactor': 20, 'currentTarget': array([101.,   6.]), 'dynamicTrap': False, 'previousTarget': array([101.,   6.]), 'currentState': array([101.66394481,   5.40633243,   3.11283904]), 'targetState': array([101,   6], dtype=int32), 'currentDistance': 0.8906536327876808}
episode index:1310
target Thresh 75.90218009307574
target distance 65.0
model initialize at round 1310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.76220317, 17.92504865]), 'dynamicTrap': True, 'previousTarget': array([59.76743395, 17.95885631]), 'currentState': array([40.      , 21.      ,  0.519635], dtype=float32), 'targetState': array([105,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.29678303225760316
running average episode reward sum: 0.6749745011011864
{'scaleFactor': 20, 'currentTarget': array([105.,  11.]), 'dynamicTrap': False, 'previousTarget': array([105.,  11.]), 'currentState': array([105.24096552,  10.12179596,   6.27488752]), 'targetState': array([105,  11], dtype=int32), 'currentDistance': 0.910662793284717}
episode index:1311
target Thresh 75.9026679718969
target distance 20.0
model initialize at round 1311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.98063135,  5.60402864]), 'dynamicTrap': False, 'previousTarget': array([96.9529684 ,  6.76121364]), 'currentState': array([85.13679124, 20.93490504,  5.21732652]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7984513925758382
running average episode reward sum: 0.6750686145855421
{'scaleFactor': 20, 'currentTarget': array([101.,   2.]), 'dynamicTrap': False, 'previousTarget': array([101.,   2.]), 'currentState': array([100.68958544,   1.16327853,   5.83208087]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 0.8924460840222366}
episode index:1312
target Thresh 75.90315341741227
target distance 30.0
model initialize at round 1312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.35685167, 18.37549488]), 'dynamicTrap': False, 'previousTarget': array([68.1565257 , 17.74695771]), 'currentState': array([48.93852614, 13.58710959,  0.59951019]), 'targetState': array([79, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7488428489289928
running average episode reward sum: 0.6751248021212188
{'scaleFactor': 20, 'currentTarget': array([79., 21.]), 'dynamicTrap': False, 'previousTarget': array([79., 21.]), 'currentState': array([79.15580344, 20.32805987,  6.19343679]), 'targetState': array([79, 21], dtype=int32), 'currentDistance': 0.6897668042469302}
episode index:1313
target Thresh 75.90363644175805
target distance 4.0
model initialize at round 1313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'dynamicTrap': False, 'previousTarget': array([13., 23.]), 'currentState': array([10.92088065, 20.29717252,  0.51938504]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 3.4099873408234025}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6753568989232575
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'dynamicTrap': False, 'previousTarget': array([13., 23.]), 'currentState': array([13.96254541, 22.3656958 ,  5.86154115]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 1.152751267287908}
episode index:1314
target Thresh 75.90411705700986
target distance 6.0
model initialize at round 1314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 18.]), 'dynamicTrap': False, 'previousTarget': array([17., 18.]), 'currentState': array([11.59038043, 17.3756949 ,  5.99258196]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 5.445524833712128}
done in step count: 6
reward sum = 0.9029553978069901
running average episode reward sum: 0.675529977629633
{'scaleFactor': 20, 'currentTarget': array([15.47092015, 17.64603557]), 'dynamicTrap': True, 'previousTarget': array([17., 18.]), 'currentState': array([15.79788208, 17.73223798,  1.08640509]), 'targetState': array([17, 18], dtype=int32), 'currentDistance': 0.3381345188884276}
episode index:1315
target Thresh 75.90459527518308
target distance 15.0
model initialize at round 1315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  3.]), 'dynamicTrap': False, 'previousTarget': array([16.,  3.]), 'currentState': array([11.27208247, 16.19005711,  4.63368821]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 14.01180968963813}
done in step count: 8
reward sum = 0.9135172474836408
running average episode reward sum: 0.6757108190200996
{'scaleFactor': 20, 'currentTarget': array([14.39250544,  3.83923174]), 'dynamicTrap': True, 'previousTarget': array([16.,  3.]), 'currentState': array([13.88135256,  3.35912387,  5.05048799]), 'targetState': array([16,  3], dtype=int32), 'currentDistance': 0.7012708735264886}
episode index:1316
target Thresh 75.90507110823323
target distance 61.0
model initialize at round 1316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.66775113,  7.40465069]), 'dynamicTrap': False, 'previousTarget': array([51.98925886,  6.34461446]), 'currentState': array([32.70320333,  8.59495782,  0.25083899]), 'targetState': array([93,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5314618167694876
running average episode reward sum: 0.6756012905445865
{'scaleFactor': 20, 'currentTarget': array([93.,  5.]), 'dynamicTrap': False, 'previousTarget': array([93.,  5.]), 'currentState': array([93.74032471,  5.4445077 ,  0.38180794]), 'targetState': array([93,  5], dtype=int32), 'currentDistance': 0.8635205683310649}
episode index:1317
target Thresh 75.90554456805611
target distance 28.0
model initialize at round 1317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.73711211,  13.57490177]), 'dynamicTrap': False, 'previousTarget': array([108.40285  ,  14.1492875]), 'currentState': array([89.09078597, 17.31948044,  5.77635908]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7887823609374012
running average episode reward sum: 0.6756871638908633
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([116.81544302,  11.25822696,   0.29186572]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.7643876720465496}
episode index:1318
target Thresh 75.90601566648832
target distance 29.0
model initialize at round 1318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.71099665, 18.27191889]), 'dynamicTrap': False, 'previousTarget': array([45.95260657, 18.62395817]), 'currentState': array([27.72489904, 19.01750743,  5.55153796]), 'targetState': array([55, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7467983571947094
running average episode reward sum: 0.6757410768501535
{'scaleFactor': 20, 'currentTarget': array([55., 18.]), 'dynamicTrap': False, 'previousTarget': array([55., 18.]), 'currentState': array([54.18010935, 18.32748083,  0.16739071]), 'targetState': array([55, 18], dtype=int32), 'currentDistance': 0.8828727930460295}
episode index:1319
target Thresh 75.90648441530726
target distance 20.0
model initialize at round 1319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 16.]), 'dynamicTrap': False, 'previousTarget': array([68.8434743 , 16.25304229]), 'currentState': array([86.28111463, 21.92513219,  2.23974907]), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 19.217344862427655}
done in step count: 25
reward sum = 0.7427312121398577
running average episode reward sum: 0.6757918269526457
{'scaleFactor': 20, 'currentTarget': array([68., 16.]), 'dynamicTrap': False, 'previousTarget': array([68., 16.]), 'currentState': array([68.10618691, 15.35275614,  5.27389376]), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 0.65589654365029}
episode index:1320
target Thresh 75.90695082623172
target distance 44.0
model initialize at round 1320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.30438882, 14.60772202]), 'dynamicTrap': False, 'previousTarget': array([53.81964584, 14.66692282]), 'currentState': array([71.36827517,  8.56055432,  2.44600701]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6003254326859159
running average episode reward sum: 0.6757346987208012
{'scaleFactor': 20, 'currentTarget': array([29., 22.]), 'dynamicTrap': False, 'previousTarget': array([29., 22.]), 'currentState': array([29.57976069, 21.49294502,  3.73764036]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 0.7702124476320343}
episode index:1321
target Thresh 75.907414910922
target distance 63.0
model initialize at round 1321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.98126209, 19.05322336]), 'dynamicTrap': False, 'previousTarget': array([60.97736275, 19.95130299]), 'currentState': array([42.03026367, 17.65405862,  5.86263186]), 'targetState': array([104,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.505930660218734
running average episode reward sum: 0.6756062539110417
{'scaleFactor': 20, 'currentTarget': array([104.,  22.]), 'dynamicTrap': False, 'previousTarget': array([104.,  22.]), 'currentState': array([104.94000615,  22.00540654,   2.78421882]), 'targetState': array([104,  22], dtype=int32), 'currentDistance': 0.9400217016874692}
episode index:1322
target Thresh 75.90787668098022
target distance 16.0
model initialize at round 1322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96., 16.]), 'dynamicTrap': False, 'previousTarget': array([96., 16.]), 'currentState': array([110.09847205,   9.60423199,   2.71293139]), 'targetState': array([96, 16], dtype=int32), 'currentDistance': 15.481368243015408}
done in step count: 11
reward sum = 0.8584162921970137
running average episode reward sum: 0.6757444323224445
{'scaleFactor': 20, 'currentTarget': array([97.8971682 , 15.81165123]), 'dynamicTrap': True, 'previousTarget': array([96., 16.]), 'currentState': array([98.3493805 , 16.38788466,  3.11422552]), 'targetState': array([96, 16], dtype=int32), 'currentDistance': 0.7324895470411019}
episode index:1323
target Thresh 75.90833614795066
target distance 60.0
model initialize at round 1323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.34868366, 17.25425346]), 'dynamicTrap': False, 'previousTarget': array([52.27212152, 16.28797975]), 'currentState': array([71.13881171, 14.36447294,  3.13197672]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.386394010976019
running average episode reward sum: 0.6755258897081345
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'dynamicTrap': False, 'previousTarget': array([12., 23.]), 'currentState': array([12.59691824, 23.30843206,  4.07415892]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.6718941317768622}
episode index:1324
target Thresh 75.90879332332004
target distance 21.0
model initialize at round 1324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.11726281,  5.24403536]), 'dynamicTrap': False, 'previousTarget': array([43.54489741,  6.41603543]), 'currentState': array([29.76922065, 18.06735793,  5.80338025]), 'targetState': array([49,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.03844028700499014
running average episode reward sum: 0.6750450703853397
{'scaleFactor': 20, 'currentTarget': array([49.,  2.]), 'dynamicTrap': False, 'previousTarget': array([49.,  2.]), 'currentState': array([48.57165946,  2.02580682,  5.69481123]), 'targetState': array([49,  2], dtype=int32), 'currentDistance': 0.4291172447772869}
episode index:1325
target Thresh 75.90924821851776
target distance 18.0
model initialize at round 1325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.12926778, 10.44012799]), 'dynamicTrap': True, 'previousTarget': array([88.,  9.]), 'currentState': array([106.       ,  12.       ,   3.7776165], dtype=float32), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 16.94269182995659}
done in step count: 99
reward sum = -0.6339676587267706
running average episode reward sum: 0.6740578812985282
{'scaleFactor': 20, 'currentTarget': array([93.38225334, 16.99501528]), 'dynamicTrap': True, 'previousTarget': array([93.19527534, 17.0655966 ]), 'currentState': array([106.       ,  12.       ,   3.7776165], dtype=float32), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 13.570471933328704}
episode index:1326
target Thresh 75.9097008449162
target distance 24.0
model initialize at round 1326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.07835858,  6.47466292]), 'dynamicTrap': False, 'previousTarget': array([65.27557802,  6.92257949]), 'currentState': array([47.2041359 , 15.44751995,  5.14064085]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6689717585696803
running average episode reward sum: 0.6740540485006918
{'scaleFactor': 20, 'currentTarget': array([72.,  3.]), 'dynamicTrap': False, 'previousTarget': array([72.,  3.]), 'currentState': array([72.04866341,  2.44698388,  5.9514357 ]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 0.5551530952521561}
episode index:1327
target Thresh 75.91015121383106
target distance 38.0
model initialize at round 1327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.43247439, 15.07268927]), 'dynamicTrap': False, 'previousTarget': array([87.93796297, 15.57404971]), 'currentState': array([69.56644433, 12.76166143,  5.7131492 ]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5362860010523152
running average episode reward sum: 0.6739503075011072
{'scaleFactor': 20, 'currentTarget': array([106.,  17.]), 'dynamicTrap': False, 'previousTarget': array([106.,  17.]), 'currentState': array([106.33700863,  17.3938494 ,   0.52573463]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 0.5183552523110058}
episode index:1328
target Thresh 75.9105993365216
target distance 50.0
model initialize at round 1328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.19144415, 16.76063674]), 'dynamicTrap': True, 'previousTarget': array([61.19316507, 16.77295689]), 'currentState': array([81.       , 14.       ,  2.9221222], dtype=float32), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5616180361677561
running average episode reward sum: 0.673865783594912
{'scaleFactor': 20, 'currentTarget': array([31., 21.]), 'dynamicTrap': False, 'previousTarget': array([31., 21.]), 'currentState': array([31.70481872, 21.14551492,  6.0609956 ]), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 0.7196832793630111}
episode index:1329
target Thresh 75.91104522419087
target distance 9.0
model initialize at round 1329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 14.]), 'dynamicTrap': False, 'previousTarget': array([72., 14.]), 'currentState': array([62.9336039 , 21.18830405,  1.80878596]), 'targetState': array([72, 14], dtype=int32), 'currentDistance': 11.57027455946507}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6740599186056729
{'scaleFactor': 20, 'currentTarget': array([72., 14.]), 'dynamicTrap': False, 'previousTarget': array([72., 14.]), 'currentState': array([72.15539112, 14.03831614,  5.55624469]), 'targetState': array([72, 14], dtype=int32), 'currentDistance': 0.1600453912704244}
episode index:1330
target Thresh 75.91148888798614
target distance 73.0
model initialize at round 1330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.99755128,  3.31295788]), 'dynamicTrap': True, 'previousTarget': array([37.99812374,  3.2739469 ]), 'currentState': array([18.       ,  3.       ,  5.4777813], dtype=float32), 'targetState': array([91,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.3848466252762266
running average episode reward sum: 0.6738426283777771
{'scaleFactor': 20, 'currentTarget': array([91.,  4.]), 'dynamicTrap': False, 'previousTarget': array([91.,  4.]), 'currentState': array([91.29248473,  4.58398672,  4.32688462]), 'targetState': array([91,  4], dtype=int32), 'currentDistance': 0.653136897154358}
episode index:1331
target Thresh 75.91193033899899
target distance 15.0
model initialize at round 1331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 10.]), 'dynamicTrap': False, 'previousTarget': array([47., 10.]), 'currentState': array([31.61976117,  7.63945312,  0.78869724]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 15.560331872470336}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6740157060404129
{'scaleFactor': 20, 'currentTarget': array([47., 10.]), 'dynamicTrap': False, 'previousTarget': array([47., 10.]), 'currentState': array([4.70939546e+01, 1.02182268e+01, 6.14728034e-03]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 0.23759298004855633}
episode index:1332
target Thresh 75.91236958826572
target distance 45.0
model initialize at round 1332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.18397316, 11.32224723]), 'dynamicTrap': False, 'previousTarget': array([67.04429684, 10.33038021]), 'currentState': array([86.17612385, 10.76196974,  2.37589204]), 'targetState': array([42, 12], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 45
reward sum = 0.5766595271182262
running average episode reward sum: 0.673942670647373
{'scaleFactor': 20, 'currentTarget': array([42., 12.]), 'dynamicTrap': False, 'previousTarget': array([42.91694653, 10.52445022]), 'currentState': array([42.82980293, 12.15053128,  3.22062753]), 'targetState': array([42, 12], dtype=int32), 'currentDistance': 0.8433460581910797}
episode index:1333
target Thresh 75.9128066467676
target distance 21.0
model initialize at round 1333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 10.]), 'dynamicTrap': False, 'previousTarget': array([51.23047895, 10.50557744]), 'currentState': array([33.9927353 , 15.89191447,  6.11376771]), 'targetState': array([53, 10], dtype=int32), 'currentDistance': 19.899516767808304}
done in step count: 19
reward sum = 0.7421274413912502
running average episode reward sum: 0.6739937836689202
{'scaleFactor': 20, 'currentTarget': array([53., 10.]), 'dynamicTrap': False, 'previousTarget': array([53., 10.]), 'currentState': array([52.50367274,  9.87976071,  4.99882237]), 'targetState': array([53, 10], dtype=int32), 'currentDistance': 0.5106840881576927}
episode index:1334
target Thresh 75.9132415254311
target distance 8.0
model initialize at round 1334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.01823584, 18.19883237]), 'dynamicTrap': True, 'previousTarget': array([18., 18.]), 'currentState': array([15.     , 10.     ,  4.52193], dtype=float32), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 8.736738511524432}
done in step count: 8
reward sum = 0.90313873432792
running average episode reward sum: 0.6741654278267172
{'scaleFactor': 20, 'currentTarget': array([18., 18.]), 'dynamicTrap': False, 'previousTarget': array([18., 18.]), 'currentState': array([18.3415495 , 18.39456734,  0.66299727]), 'targetState': array([18, 18], dtype=int32), 'currentDistance': 0.5218615217176233}
episode index:1335
target Thresh 75.91367423512823
target distance 21.0
model initialize at round 1335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.76459268,  8.95582358]), 'dynamicTrap': False, 'previousTarget': array([25.45612429,  8.63241055]), 'currentState': array([7.107717  , 5.26702452, 6.12556726]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8070527632365867
running average episode reward sum: 0.6742648943951377
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'dynamicTrap': False, 'previousTarget': array([27.,  9.]), 'currentState': array([27.47127595,  8.65829999,  1.85443772]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.5821167558435947}
episode index:1336
target Thresh 75.91410478667673
target distance 16.0
model initialize at round 1336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54., 10.]), 'dynamicTrap': False, 'previousTarget': array([54., 10.]), 'currentState': array([39.88803184,  2.23978623,  5.86117596]), 'targetState': array([54, 10], dtype=int32), 'currentDistance': 16.10492356653497}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6744103550670703
{'scaleFactor': 20, 'currentTarget': array([54., 10.]), 'dynamicTrap': False, 'previousTarget': array([54., 10.]), 'currentState': array([53.90978849,  9.30562571,  3.08973794]), 'targetState': array([54, 10], dtype=int32), 'currentDistance': 0.7002098065254538}
episode index:1337
target Thresh 75.91453319084043
target distance 35.0
model initialize at round 1337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.69209481, 13.07167926]), 'dynamicTrap': False, 'previousTarget': array([89.50282987, 13.54350397]), 'currentState': array([107.20704602,  17.44965588,   3.14867318]), 'targetState': array([74, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6951052018018955
running average episode reward sum: 0.6744258220676195
{'scaleFactor': 20, 'currentTarget': array([74., 10.]), 'dynamicTrap': False, 'previousTarget': array([74., 10.]), 'currentState': array([74.57535702,  9.22182895,  1.68894487]), 'targetState': array([74, 10], dtype=int32), 'currentDistance': 0.9677736738644764}
episode index:1338
target Thresh 75.91495945832945
target distance 47.0
model initialize at round 1338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.53980392,  7.2270526 ]), 'dynamicTrap': False, 'previousTarget': array([87.64310384,  7.23855458]), 'currentState': array([69.94842397, 11.24922011,  6.07895223]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5759517958503906
running average episode reward sum: 0.6743522791055454
{'scaleFactor': 20, 'currentTarget': array([115.,   2.]), 'dynamicTrap': False, 'previousTarget': array([115.,   2.]), 'currentState': array([114.88495134,   1.27501382,   0.30855579]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 0.7340580105126409}
episode index:1339
target Thresh 75.9153835998005
target distance 1.0
model initialize at round 1339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  2.]), 'dynamicTrap': False, 'previousTarget': array([28.,  2.]), 'currentState': array([27.95355137,  1.89743963,  4.85305071]), 'targetState': array([28,  2], dtype=int32), 'currentDistance': 0.11258820601259333}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6745952997927801
{'scaleFactor': 20, 'currentTarget': array([28.,  2.]), 'dynamicTrap': False, 'previousTarget': array([28.,  2.]), 'currentState': array([27.95355137,  1.89743963,  4.85305071]), 'targetState': array([28,  2], dtype=int32), 'currentDistance': 0.11258820601259333}
episode index:1340
target Thresh 75.91580562585713
target distance 2.0
model initialize at round 1340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92., 10.]), 'dynamicTrap': False, 'previousTarget': array([92., 10.]), 'currentState': array([92.34644037,  9.61588758,  3.08942443]), 'targetState': array([92, 10], dtype=int32), 'currentDistance': 0.5172651952537073}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6748379580330539
{'scaleFactor': 20, 'currentTarget': array([92., 10.]), 'dynamicTrap': False, 'previousTarget': array([92., 10.]), 'currentState': array([92.34644037,  9.61588758,  3.08942443]), 'targetState': array([92, 10], dtype=int32), 'currentDistance': 0.5172651952537073}
episode index:1341
target Thresh 75.91622554705003
target distance 14.0
model initialize at round 1341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.,  4.]), 'dynamicTrap': False, 'previousTarget': array([65.,  4.]), 'currentState': array([52.29391794,  5.8183964 ,  5.75659442]), 'targetState': array([65,  4], dtype=int32), 'currentDistance': 12.835539991721442}
done in step count: 33
reward sum = 0.6920661515277476
running average episode reward sum: 0.6748507957331245
{'scaleFactor': 20, 'currentTarget': array([65.,  4.]), 'dynamicTrap': False, 'previousTarget': array([66.58093979,  4.6152288 ]), 'currentState': array([65.52300909,  3.00650805,  2.45830789]), 'targetState': array([65,  4], dtype=int32), 'currentDistance': 1.1227487552888331}
episode index:1342
target Thresh 75.91664337387724
target distance 12.0
model initialize at round 1342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.,  9.]), 'dynamicTrap': False, 'previousTarget': array([70.,  9.]), 'currentState': array([66.59615682, 20.72118642,  5.23250741]), 'targetState': array([70,  9], dtype=int32), 'currentDistance': 12.20542336502716}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6750423181100224
{'scaleFactor': 20, 'currentTarget': array([70.,  9.]), 'dynamicTrap': False, 'previousTarget': array([70.,  9.]), 'currentState': array([70.44846006,  9.00028803,  3.95663299]), 'targetState': array([70,  9], dtype=int32), 'currentDistance': 0.4484601552562453}
episode index:1343
target Thresh 75.91705911678446
target distance 74.0
model initialize at round 1343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.99569248, 17.58493088]), 'dynamicTrap': True, 'previousTarget': array([36.98358488, 17.18985467]), 'currentState': array([17.       , 18.       ,  4.2547154], dtype=float32), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.38389053069788215
running average episode reward sum: 0.6748256873158168
{'scaleFactor': 20, 'currentTarget': array([91., 15.]), 'dynamicTrap': False, 'previousTarget': array([91., 15.]), 'currentState': array([90.88362825, 15.96417264,  4.41373307]), 'targetState': array([91, 15], dtype=int32), 'currentDistance': 0.9711700444275169}
episode index:1344
target Thresh 75.91747278616528
target distance 27.0
model initialize at round 1344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.48327983,  6.76838768]), 'dynamicTrap': False, 'previousTarget': array([85.,  7.]), 'currentState': array([66.50088287,  5.92945273,  5.79036647]), 'targetState': array([92,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7533017594133601
running average episode reward sum: 0.6748840338378225
{'scaleFactor': 20, 'currentTarget': array([92.,  7.]), 'dynamicTrap': False, 'previousTarget': array([92.,  7.]), 'currentState': array([92.4042503 ,  6.97112089,  5.04940292]), 'targetState': array([92,  7], dtype=int32), 'currentDistance': 0.4052805269213495}
episode index:1345
target Thresh 75.91788439236144
target distance 34.0
model initialize at round 1345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.69518135,  5.80445857]), 'dynamicTrap': False, 'previousTarget': array([97.03451254,  6.17444044]), 'currentState': array([115.60708165,   3.92929813,   2.91782355]), 'targetState': array([83,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8345137614500875
running average episode reward sum: 0.6750026294749787
{'scaleFactor': 20, 'currentTarget': array([84.79337507,  6.32141095]), 'dynamicTrap': True, 'previousTarget': array([83.,  7.]), 'currentState': array([84.76343842,  6.77989454,  3.27559822]), 'targetState': array([83,  7], dtype=int32), 'currentDistance': 0.4594599049820264}
episode index:1346
target Thresh 75.91829394566315
target distance 40.0
model initialize at round 1346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.73214114,  5.769971  ]), 'dynamicTrap': False, 'previousTarget': array([84.29939066,  5.44760664]), 'currentState': array([105.51482885,   2.82968755,   1.42444664]), 'targetState': array([64,  9], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 75
reward sum = 0.08755807596237786
running average episode reward sum: 0.6745665162207006
{'scaleFactor': 20, 'currentTarget': array([64.,  9.]), 'dynamicTrap': False, 'previousTarget': array([64.,  9.]), 'currentState': array([63.96424905,  8.92755952,  1.94769922]), 'targetState': array([64,  9], dtype=int32), 'currentDistance': 0.08078213815667117}
episode index:1347
target Thresh 75.91870145630924
target distance 32.0
model initialize at round 1347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.32875109, 16.07960373]), 'dynamicTrap': False, 'previousTarget': array([45.7469812 , 15.41491154]), 'currentState': array([64.79020499, 11.46963496,  2.72259146]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6890913701234385
running average episode reward sum: 0.6745772913348719
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([32.70619062, 18.41135398,  5.29813518]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 0.6578967155212856}
episode index:1348
target Thresh 75.9191069344875
target distance 43.0
model initialize at round 1348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.800035  ,  6.54287333]), 'dynamicTrap': False, 'previousTarget': array([78.86614761,  6.31001716]), 'currentState': array([60.93303021,  4.24023993,  6.00594211]), 'targetState': array([102,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6927890710528669
running average episode reward sum: 0.674590791542224
{'scaleFactor': 20, 'currentTarget': array([102.,   9.]), 'dynamicTrap': False, 'previousTarget': array([102.,   9.]), 'currentState': array([101.8115237 ,   9.55529873,   0.96611628]), 'targetState': array([102,   9], dtype=int32), 'currentDistance': 0.5864128221561744}
episode index:1349
target Thresh 75.9195103903349
target distance 11.0
model initialize at round 1349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'dynamicTrap': False, 'previousTarget': array([17.,  3.]), 'currentState': array([ 7.55847473, 10.92271615,  5.10571969]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 12.325251749381975}
done in step count: 11
reward sum = 0.8859234527647064
running average episode reward sum: 0.6747473342542407
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'dynamicTrap': False, 'previousTarget': array([17.,  3.]), 'currentState': array([16.34577303,  3.21154438,  0.8308918 ]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.6875783237832711}
episode index:1350
target Thresh 75.91991183393787
target distance 62.0
model initialize at round 1350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.65771269, 10.31566399]), 'dynamicTrap': True, 'previousTarget': array([38.74482241, 10.81535122]), 'currentState': array([19.       , 14.       ,  2.0316586], dtype=float32), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4640325772448782
running average episode reward sum: 0.674591364781991
{'scaleFactor': 20, 'currentTarget': array([81.,  4.]), 'dynamicTrap': False, 'previousTarget': array([81.,  4.]), 'currentState': array([81.39056193,  4.67090055,  4.01835826]), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 0.7763028886739846}
episode index:1351
target Thresh 75.92031127533254
target distance 23.0
model initialize at round 1351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.87783436, 19.84591435]), 'dynamicTrap': False, 'previousTarget': array([15.58874323, 18.84114513]), 'currentState': array([31.36149048, 10.13397486,  2.38602963]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7092186529965263
running average episode reward sum: 0.674616976681558
{'scaleFactor': 20, 'currentTarget': array([10., 22.]), 'dynamicTrap': False, 'previousTarget': array([10., 22.]), 'currentState': array([10.47253514, 22.42481429,  4.32369433]), 'targetState': array([10, 22], dtype=int32), 'currentDistance': 0.6354184759908013}
episode index:1352
target Thresh 75.92070872450492
target distance 15.0
model initialize at round 1352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.,  8.]), 'dynamicTrap': False, 'previousTarget': array([66.,  8.]), 'currentState': array([51.26327988,  9.66224902,  0.40371346]), 'targetState': array([66,  8], dtype=int32), 'currentDistance': 14.830171664747438}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6747801114026054
{'scaleFactor': 20, 'currentTarget': array([66.,  8.]), 'dynamicTrap': False, 'previousTarget': array([66.,  8.]), 'currentState': array([65.40396944,  8.94687144,  2.76089271]), 'targetState': array([66,  8], dtype=int32), 'currentDistance': 1.11884671255584}
episode index:1353
target Thresh 75.9211041913913
target distance 10.0
model initialize at round 1353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 23.]), 'currentState': array([14.09003964, 21.99353752,  1.74911564]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 11.13561609799637}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6749770833656765
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.42924027, 23.83918425,  2.62708632]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.942590795476001}
episode index:1354
target Thresh 75.92149768587834
target distance 26.0
model initialize at round 1354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.59657041,  8.08574874]), 'dynamicTrap': False, 'previousTarget': array([89.73939228,  8.05501274]), 'currentState': array([72.80612558, 18.95225519,  3.48405452]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6750825666529323
{'scaleFactor': 20, 'currentTarget': array([99.,  2.]), 'dynamicTrap': False, 'previousTarget': array([99.,  2.]), 'currentState': array([99.58537271,  2.58204443,  5.9352615 ]), 'targetState': array([99,  2], dtype=int32), 'currentDistance': 0.8254919350495138}
episode index:1355
target Thresh 75.92188921780345
target distance 29.0
model initialize at round 1355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.69660917, 14.7985671 ]), 'dynamicTrap': False, 'previousTarget': array([14.51966335, 15.35296415]), 'currentState': array([31.02455028, 22.8039762 ,  3.45391533]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7018490578162003
running average episode reward sum: 0.6751023059532002
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.80995885, 10.69388815,  1.10963012]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.36030556463875646}
episode index:1356
target Thresh 75.92227879695493
target distance 45.0
model initialize at round 1356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.53151756, 15.05522573]), 'dynamicTrap': False, 'previousTarget': array([85.61161351, 14.0776773 ]), 'currentState': array([66.07389687, 19.68134688,  0.5168736 ]), 'targetState': array([111,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6845152074356066
running average episode reward sum: 0.6751092425055086
{'scaleFactor': 20, 'currentTarget': array([111.,   9.]), 'dynamicTrap': False, 'previousTarget': array([111.,   9.]), 'currentState': array([111.656538  ,   9.67450724,   1.24943172]), 'targetState': array([111,   9], dtype=int32), 'currentDistance': 0.9412768792939727}
episode index:1357
target Thresh 75.92266643307228
target distance 24.0
model initialize at round 1357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.78858324, 18.91037802]), 'dynamicTrap': False, 'previousTarget': array([ 9.15444247, 18.51930531]), 'currentState': array([29.43666042, 22.6457579 ,  2.22256663]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.6751096989168155
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.13599041, 18.28011819,  2.01395331]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.311383355630407}
episode index:1358
target Thresh 75.92305213584645
target distance 6.0
model initialize at round 1358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([1.70520696, 9.71054896, 0.91726482]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 4.480611482550252}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6753269095872225
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.55968155, 14.58072553,  1.1017921 ]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.7287815013535893}
episode index:1359
target Thresh 75.92343591491998
target distance 19.0
model initialize at round 1359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,   7.]), 'dynamicTrap': False, 'previousTarget': array([110.,   7.]), 'currentState': array([91.25805561, 10.53048535,  5.89622259]), 'targetState': array([110,   7], dtype=int32), 'currentDistance': 19.071570632009696}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6754564175736252
{'scaleFactor': 20, 'currentTarget': array([110.,   7.]), 'dynamicTrap': False, 'previousTarget': array([110.,   7.]), 'currentState': array([110.17604206,   7.14141068,   2.15089063]), 'targetState': array([110,   7], dtype=int32), 'currentDistance': 0.22580475576122794}
episode index:1360
target Thresh 75.92381777988741
target distance 63.0
model initialize at round 1360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.71071261,  8.10800598]), 'dynamicTrap': False, 'previousTarget': array([24.75271057,  8.13535088]), 'currentState': array([6.98364407, 4.81517031, 6.03603957]), 'targetState': array([68, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5294208243419927
running average episode reward sum: 0.675349117358172
{'scaleFactor': 20, 'currentTarget': array([68., 15.]), 'dynamicTrap': False, 'previousTarget': array([68., 15.]), 'currentState': array([68.83035597, 14.08451341,  1.82804942]), 'targetState': array([68, 15], dtype=int32), 'currentDistance': 1.2359638919095532}
episode index:1361
target Thresh 75.92419774029537
target distance 45.0
model initialize at round 1361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.51236759, 10.96069329]), 'dynamicTrap': False, 'previousTarget': array([91.99506356, 11.55566525]), 'currentState': array([73.5123956 , 10.92722321,  5.39401805]), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4529478377345876
running average episode reward sum: 0.6751858271381841
{'scaleFactor': 20, 'currentTarget': array([117.,  11.]), 'dynamicTrap': False, 'previousTarget': array([117.,  11.]), 'currentState': array([116.40630073,  11.71852525,   5.36239369]), 'targetState': array([117,  11], dtype=int32), 'currentDistance': 0.9320715361402689}
episode index:1362
target Thresh 75.9245758056429
target distance 18.0
model initialize at round 1362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.27112091, 19.16112104]), 'dynamicTrap': True, 'previousTarget': array([70.42900019, 19.06563667]), 'currentState': array([60.     ,  2.     ,  5.69923], dtype=float32), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8587458127689782
running average episode reward sum: 0.6753205006419485
{'scaleFactor': 20, 'currentTarget': array([71., 20.]), 'dynamicTrap': False, 'previousTarget': array([71., 20.]), 'currentState': array([70.36812895, 19.98572525,  1.95861501]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 0.6320322673985698}
episode index:1363
target Thresh 75.92495198538163
target distance 72.0
model initialize at round 1363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.08658026, 21.42721933]), 'dynamicTrap': False, 'previousTarget': array([58.00771159, 21.55534134]), 'currentState': array([78.0774688 , 20.82358433,  5.03777056]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6622820409839835
running average episode reward sum: 0.6753109416539295
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.78037619, 23.80453837,  3.06169508]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.1208340560649839}
episode index:1364
target Thresh 75.92532628891608
target distance 16.0
model initialize at round 1364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81., 18.]), 'dynamicTrap': False, 'previousTarget': array([81., 18.]), 'currentState': array([85.90868023,  3.92671142,  1.28368068]), 'targetState': array([81, 18], dtype=int32), 'currentDistance': 14.904784231344284}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6754922118024819
{'scaleFactor': 20, 'currentTarget': array([81., 18.]), 'dynamicTrap': False, 'previousTarget': array([81., 18.]), 'currentState': array([81.12713504, 17.70110079,  3.17543739]), 'targetState': array([81, 18], dtype=int32), 'currentDistance': 0.3248138828414839}
episode index:1365
target Thresh 75.92569872560387
target distance 71.0
model initialize at round 1365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.91891685,  9.25441851]), 'dynamicTrap': False, 'previousTarget': array([78.48927723,  8.39678259]), 'currentState': array([96.47105099,  5.04561223,  3.22179902]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6327696491514
running average episode reward sum: 0.6754609361343625
{'scaleFactor': 20, 'currentTarget': array([27., 20.]), 'dynamicTrap': False, 'previousTarget': array([27., 20.]), 'currentState': array([27.38804525, 20.46329306,  4.37693543]), 'targetState': array([27, 20], dtype=int32), 'currentDistance': 0.6043339917731142}
episode index:1366
target Thresh 75.92606930475593
target distance 15.0
model initialize at round 1366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 20.]), 'dynamicTrap': False, 'previousTarget': array([87., 20.]), 'currentState': array([90.50793274,  5.65499081,  1.7179389 ]), 'targetState': array([87, 20], dtype=int32), 'currentDistance': 14.767697202604078}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6756418313489152
{'scaleFactor': 20, 'currentTarget': array([87., 20.]), 'dynamicTrap': False, 'previousTarget': array([87., 20.]), 'currentState': array([86.59737174, 19.86794021,  2.63904703]), 'targetState': array([87, 20], dtype=int32), 'currentDistance': 0.42373258370469047}
episode index:1367
target Thresh 75.92643803563676
target distance 7.0
model initialize at round 1367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 14.]), 'dynamicTrap': False, 'previousTarget': array([68., 14.]), 'currentState': array([71.63034542,  7.57313459,  1.2698304 ]), 'targetState': array([68, 14], dtype=int32), 'currentDistance': 7.381328257513661}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6758501311871105
{'scaleFactor': 20, 'currentTarget': array([68., 14.]), 'dynamicTrap': False, 'previousTarget': array([68., 14.]), 'currentState': array([68.71079396, 13.74696863,  1.24223789]), 'targetState': array([68, 14], dtype=int32), 'currentDistance': 0.754488524834368}
episode index:1368
target Thresh 75.92680492746464
target distance 45.0
model initialize at round 1368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.00667439,  2.48334611]), 'dynamicTrap': True, 'previousTarget': array([37.00493644,  2.55566525]), 'currentState': array([57.       ,  3.       ,  1.6987975], dtype=float32), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5223475650952631
running average episode reward sum: 0.6757380036735299
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'dynamicTrap': False, 'previousTarget': array([12.,  2.]), 'currentState': array([11.65509284,  1.9977924 ,  4.18154096]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.3449142229845467}
episode index:1369
target Thresh 75.92716998941191
target distance 25.0
model initialize at round 1369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.98208564, 11.06834768]), 'dynamicTrap': False, 'previousTarget': array([21.85753677, 11.38290441]), 'currentState': array([1.21753289, 8.00853489, 4.89555585]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.724304814632452
running average episode reward sum: 0.6757734539005072
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'dynamicTrap': False, 'previousTarget': array([27., 12.]), 'currentState': array([26.5282621 , 12.43627559,  6.0993907 ]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 0.6425519750538901}
episode index:1370
target Thresh 75.92753323060512
target distance 18.0
model initialize at round 1370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  21.]), 'dynamicTrap': False, 'previousTarget': array([109.,  21.]), 'currentState': array([109.75001071,   4.58420738,   0.71730661]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 16.432917073006134}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6759336032807831
{'scaleFactor': 20, 'currentTarget': array([109.,  21.]), 'dynamicTrap': False, 'previousTarget': array([109.,  21.]), 'currentState': array([108.54728238,  21.97428466,   3.2962984 ]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 1.074329480673157}
episode index:1371
target Thresh 75.92789466012532
target distance 10.0
model initialize at round 1371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,   6.]), 'dynamicTrap': False, 'previousTarget': array([112.,   6.]), 'currentState': array([112.41874116,  16.74736919,   5.87257892]), 'targetState': array([112,   6], dtype=int32), 'currentDistance': 10.755523637586558}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6761202882258459
{'scaleFactor': 20, 'currentTarget': array([112.,   6.]), 'dynamicTrap': False, 'previousTarget': array([112.,   6.]), 'currentState': array([112.66915359,   5.46074969,   3.80652589]), 'targetState': array([112,   6], dtype=int32), 'currentDistance': 0.859393637151358}
episode index:1372
target Thresh 75.92825428700826
target distance 6.0
model initialize at round 1372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.,  2.]), 'dynamicTrap': False, 'previousTarget': array([40.,  2.]), 'currentState': array([44.14818055,  2.39683826,  3.4717983 ]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 4.16711920246994}
done in step count: 5
reward sum = 0.9215860598999999
running average episode reward sum: 0.676299068831581
{'scaleFactor': 20, 'currentTarget': array([40.,  2.]), 'dynamicTrap': False, 'previousTarget': array([40.,  2.]), 'currentState': array([39.61720847,  2.03262842,  2.88249752]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 0.38417960915191707}
episode index:1373
target Thresh 75.92861212024465
target distance 37.0
model initialize at round 1373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.0280113 , 10.37154082]), 'dynamicTrap': False, 'previousTarget': array([66.25789045, 11.20142317]), 'currentState': array([85.58884803,  6.20336779,  3.78598547]), 'targetState': array([49, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6779868012809391
running average episode reward sum: 0.6763002971666969
{'scaleFactor': 20, 'currentTarget': array([49., 14.]), 'dynamicTrap': False, 'previousTarget': array([49., 14.]), 'currentState': array([48.66079709, 13.23304321,  1.47442579]), 'targetState': array([49, 14], dtype=int32), 'currentDistance': 0.8386187047062461}
episode index:1374
target Thresh 75.92896816878032
target distance 34.0
model initialize at round 1374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.47521979, 11.45694202]), 'dynamicTrap': False, 'previousTarget': array([47.41085913, 11.03305841]), 'currentState': array([65.07219461,  7.46212316,  2.83279975]), 'targetState': array([33, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6254775143798663
running average episode reward sum: 0.676263335142852
{'scaleFactor': 20, 'currentTarget': array([33., 14.]), 'dynamicTrap': False, 'previousTarget': array([33., 14.]), 'currentState': array([32.47249618, 13.9137234 ,  2.06899992]), 'targetState': array([33, 14], dtype=int32), 'currentDistance': 0.5345127998972093}
episode index:1375
target Thresh 75.9293224415165
target distance 10.0
model initialize at round 1375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,   6.]), 'dynamicTrap': False, 'previousTarget': array([116.,   6.]), 'currentState': array([111.83862691,  14.13660833,   5.26216996]), 'targetState': array([116,   6], dtype=int32), 'currentDistance': 9.13900547158447}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6764291191107779
{'scaleFactor': 20, 'currentTarget': array([116.,   6.]), 'dynamicTrap': False, 'previousTarget': array([116.,   6.]), 'currentState': array([116.96028099,   5.65782086,   1.43238172]), 'targetState': array([116,   6], dtype=int32), 'currentDistance': 1.0194244200124205}
episode index:1376
target Thresh 75.92967494731005
target distance 25.0
model initialize at round 1376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.46033317, 15.94778535]), 'dynamicTrap': False, 'previousTarget': array([93.96953885, 15.65462135]), 'currentState': array([112.97104606,   8.37457544,   1.96576565]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6812981261742941
running average episode reward sum: 0.6764326550636199
{'scaleFactor': 20, 'currentTarget': array([87., 19.]), 'dynamicTrap': False, 'previousTarget': array([87., 19.]), 'currentState': array([87.61675959, 18.67410151,  0.86260852]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 0.6975687912711577}
episode index:1377
target Thresh 75.93002569497361
target distance 19.0
model initialize at round 1377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([29.3131289 ,  7.61883048,  1.09605348]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 19.690560784809772}
done in step count: 19
reward sum = 0.761563445651716
running average episode reward sum: 0.6764944335763835
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([49.89763601,  7.97841992,  4.30793769]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 0.8978953748101719}
episode index:1378
target Thresh 75.93037469327591
target distance 32.0
model initialize at round 1378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.92302439,  8.47449982]), 'dynamicTrap': True, 'previousTarget': array([75.08959956,  7.96549986]), 'currentState': array([56.        ,  2.        ,  0.07449216], dtype=float32), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6441868072732339
running average episode reward sum: 0.6764710052759462
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([88.31212312, 12.07753432,  1.12242149]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 0.3216090960749567}
episode index:1379
target Thresh 75.93072195094192
target distance 35.0
model initialize at round 1379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.95999318, 12.30720137]), 'dynamicTrap': False, 'previousTarget': array([20.78575605, 13.14696025]), 'currentState': array([3.57567402, 2.41854892, 6.07149553]), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7177152307772048
running average episode reward sum: 0.6765008923958746
{'scaleFactor': 20, 'currentTarget': array([38., 22.]), 'dynamicTrap': False, 'previousTarget': array([38., 22.]), 'currentState': array([37.10124535, 21.77712985,  1.53911993]), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 0.9259757176468392}
episode index:1380
target Thresh 75.93106747665308
target distance 16.0
model initialize at round 1380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 23.]), 'dynamicTrap': False, 'previousTarget': array([46.82990784, 22.05153389]), 'currentState': array([58.61016141,  7.53156526,  1.77562523]), 'targetState': array([46, 23], dtype=int32), 'currentDistance': 19.957170244890847}
done in step count: 12
reward sum = 0.8668808817161292
running average episode reward sum: 0.6766387490137749
{'scaleFactor': 20, 'currentTarget': array([46., 23.]), 'dynamicTrap': False, 'previousTarget': array([46., 23.]), 'currentState': array([46.46838062, 22.01150621,  2.12059914]), 'targetState': array([46, 23], dtype=int32), 'currentDistance': 1.0938465972913272}
episode index:1381
target Thresh 75.93141127904758
target distance 26.0
model initialize at round 1381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.66327787,  5.63395982]), 'dynamicTrap': False, 'previousTarget': array([43.41934502,  5.20720018]), 'currentState': array([25.74124644, 14.5109407 ,  0.10470819]), 'targetState': array([51,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7478976322950015
running average episode reward sum: 0.6766903111579726
{'scaleFactor': 20, 'currentTarget': array([51.,  2.]), 'dynamicTrap': False, 'previousTarget': array([51.,  2.]), 'currentState': array([50.579425  ,  2.64877396,  6.26067254]), 'targetState': array([51,  2], dtype=int32), 'currentDistance': 0.7731694411452367}
episode index:1382
target Thresh 75.93175336672047
target distance 36.0
model initialize at round 1382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.27321014, 11.14534949]), 'dynamicTrap': False, 'previousTarget': array([25.18892201, 11.20711072]), 'currentState': array([44.11620763, 17.84918764,  5.27070471]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6224414654172032
running average episode reward sum: 0.6766510856729828
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'dynamicTrap': False, 'previousTarget': array([8., 5.]), 'currentState': array([7.50129932, 4.93240425, 2.11714235]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.5032609220675213}
episode index:1383
target Thresh 75.93209374822398
target distance 51.0
model initialize at round 1383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.72209196, 17.91781752]), 'dynamicTrap': False, 'previousTarget': array([66., 18.]), 'currentState': array([47.72217076, 17.86167815,  5.27047736]), 'targetState': array([97, 18], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.626586281603689
running average episode reward sum: 0.6766149116816031
{'scaleFactor': 20, 'currentTarget': array([97., 18.]), 'dynamicTrap': False, 'previousTarget': array([97., 18.]), 'currentState': array([96.9567053 , 17.47698251,  4.98073387]), 'targetState': array([97, 18], dtype=int32), 'currentDistance': 0.524806371184549}
episode index:1384
target Thresh 75.93243243206767
target distance 46.0
model initialize at round 1384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.9496815 ,  8.86697808]), 'dynamicTrap': False, 'previousTarget': array([73.11711074,  8.16118362]), 'currentState': array([92.88245622,  7.22853618,  2.67618388]), 'targetState': array([47, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7319221597942888
running average episode reward sum: 0.6766548447127314
{'scaleFactor': 20, 'currentTarget': array([47., 11.]), 'dynamicTrap': False, 'previousTarget': array([47., 11.]), 'currentState': array([46.38087995, 10.16985247,  2.76983886]), 'targetState': array([47, 11], dtype=int32), 'currentDistance': 1.0355938165723866}
episode index:1385
target Thresh 75.93276942671862
target distance 75.0
model initialize at round 1385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.98567473, 21.21495437]), 'dynamicTrap': False, 'previousTarget': array([33., 20.]), 'currentState': array([12.99055012, 21.65653325,  0.64643812]), 'targetState': array([88, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6208572728963223
running average episode reward sum: 0.6766145867244079
{'scaleFactor': 20, 'currentTarget': array([88., 20.]), 'dynamicTrap': False, 'previousTarget': array([88., 20.]), 'currentState': array([87.35754856, 20.14489986,  0.63858085]), 'targetState': array([88, 20], dtype=int32), 'currentDistance': 0.6585892693511216}
episode index:1386
target Thresh 75.93310474060176
target distance 20.0
model initialize at round 1386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.577206  ,  8.09453098]), 'dynamicTrap': False, 'previousTarget': array([47.59715  ,  8.1492875]), 'currentState': array([67.3142665 , 11.32693609,  3.92094767]), 'targetState': array([47,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6767594363540218
{'scaleFactor': 20, 'currentTarget': array([47.,  8.]), 'dynamicTrap': False, 'previousTarget': array([47.,  8.]), 'currentState': array([46.23746731,  7.35860218,  3.38631742]), 'targetState': array([47,  8], dtype=int32), 'currentDistance': 0.9964172166443901}
episode index:1387
target Thresh 75.93343838209992
target distance 47.0
model initialize at round 1387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.009567  , 13.38146253]), 'dynamicTrap': True, 'previousTarget': array([76.04061834, 12.72599692]), 'currentState': array([96.       , 14.       ,  3.9153996], dtype=float32), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6131103283039795
running average episode reward sum: 0.6767135796479339
{'scaleFactor': 20, 'currentTarget': array([49., 11.]), 'dynamicTrap': False, 'previousTarget': array([49., 11.]), 'currentState': array([49.98693492, 10.08592475,  3.2285581 ]), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 1.3452041105793393}
episode index:1388
target Thresh 75.93377035955416
target distance 58.0
model initialize at round 1388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.2782396 , 10.71003575]), 'dynamicTrap': False, 'previousTarget': array([72.18757742, 10.73274794]), 'currentState': array([90.06894692,  7.82422498,  2.8655076 ]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.2721454166309245
running average episode reward sum: 0.6764223138718237
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([34.51010043, 16.7897012 ,  4.73141795]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 0.9401225616550692}
episode index:1389
target Thresh 75.93410068126394
target distance 5.0
model initialize at round 1389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  12.]), 'dynamicTrap': False, 'previousTarget': array([109.,  12.]), 'currentState': array([112.00151712,  11.04026527,   3.06311642]), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 3.1512213112672187}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6766407870273116
{'scaleFactor': 20, 'currentTarget': array([109.,  12.]), 'dynamicTrap': False, 'previousTarget': array([109.,  12.]), 'currentState': array([108.23826559,  12.20650187,   2.82047272]), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 0.7892289479638085}
episode index:1390
target Thresh 75.93442935548732
target distance 17.0
model initialize at round 1390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'dynamicTrap': False, 'previousTarget': array([24., 20.]), 'currentState': array([41.07329302, 17.8315446 ,  5.02637227]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 17.21044838115509}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6768045118928626
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'dynamicTrap': False, 'previousTarget': array([24., 20.]), 'currentState': array([24.12261112, 19.17836206,  3.16778236]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.8307360538055385}
episode index:1391
target Thresh 75.9347563904412
target distance 30.0
model initialize at round 1391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.64744927, 15.24554149]), 'dynamicTrap': False, 'previousTarget': array([71.38838649, 14.9223227 ]), 'currentState': array([89.24810363, 11.26881565,  2.28085911]), 'targetState': array([61, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5723230553237245
running average episode reward sum: 0.6767294533752124
{'scaleFactor': 20, 'currentTarget': array([61., 17.]), 'dynamicTrap': False, 'previousTarget': array([61., 17.]), 'currentState': array([60.2429121 , 16.97518733,  1.88857409]), 'targetState': array([61, 17], dtype=int32), 'currentDistance': 0.7574943983042185}
episode index:1392
target Thresh 75.9350817943014
target distance 59.0
model initialize at round 1392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.85488241, 16.30933491]), 'dynamicTrap': False, 'previousTarget': array([71.99712788, 16.66106563]), 'currentState': array([53.85557589, 16.47588395,  0.07670325]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5034837619718951
running average episode reward sum: 0.6766050846089502
{'scaleFactor': 20, 'currentTarget': array([111.,  16.]), 'dynamicTrap': False, 'previousTarget': array([111.,  16.]), 'currentState': array([111.50665387,  16.058042  ,   5.77190972]), 'targetState': array([111,  16], dtype=int32), 'currentDistance': 0.5099676628349727}
episode index:1393
target Thresh 75.93540557520309
target distance 9.0
model initialize at round 1393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([49.3279435 ,  6.97176391,  1.34664362]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 7.06029478164405}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6768088083717846
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([50.04910297, 14.34935644,  2.09178194]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 0.3527903440238326}
episode index:1394
target Thresh 75.93572774124081
target distance 6.0
model initialize at round 1394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([108.65551936,  10.8782001 ,   5.27125174]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 5.823654179139521}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6770191956059266
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([112.37578147,   7.91231208,   4.26113326]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 1.105423949095093}
episode index:1395
target Thresh 75.93604830046871
target distance 26.0
model initialize at round 1395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.91106004, 16.80740339]), 'dynamicTrap': False, 'previousTarget': array([39.35987106, 17.22305213]), 'currentState': array([58.72705714, 19.51410967,  3.53128135]), 'targetState': array([33, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8338080209090903
running average episode reward sum: 0.6771315085180349
{'scaleFactor': 20, 'currentTarget': array([33., 16.]), 'dynamicTrap': False, 'previousTarget': array([33., 16.]), 'currentState': array([33.52442332, 15.11259075,  4.54606809]), 'targetState': array([33, 16], dtype=int32), 'currentDistance': 1.0307836754451025}
episode index:1396
target Thresh 75.93636726090078
target distance 69.0
model initialize at round 1396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.38054759,  9.47154017]), 'dynamicTrap': False, 'previousTarget': array([67.99160369, 10.57946677]), 'currentState': array([48.40753817,  8.43284167,  5.97680306]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5638798764219487
running average episode reward sum: 0.6770504407785245
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([116.27270619,  12.63468064,   1.39466282]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.9652853475338763}
episode index:1397
target Thresh 75.93668463051107
target distance 47.0
model initialize at round 1397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.38269737, 11.99924742]), 'dynamicTrap': False, 'previousTarget': array([67.72377759, 12.33172109]), 'currentState': array([85.52197783,  6.19511582,  2.63025618]), 'targetState': array([40, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3872015822128755
running average episode reward sum: 0.676843109692283
{'scaleFactor': 20, 'currentTarget': array([40., 20.]), 'dynamicTrap': False, 'previousTarget': array([40., 20.]), 'currentState': array([40.49041464, 20.55277017,  4.02536461]), 'targetState': array([40, 20], dtype=int32), 'currentDistance': 0.7389596567987743}
episode index:1398
target Thresh 75.93700041723382
target distance 23.0
model initialize at round 1398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.61324132, 10.28974788]), 'dynamicTrap': False, 'previousTarget': array([89.13347761, 10.17676768]), 'currentState': array([70.92266811, 17.40750547,  6.2637257 ]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6769865535188068
{'scaleFactor': 20, 'currentTarget': array([93.,  9.]), 'dynamicTrap': False, 'previousTarget': array([93.,  9.]), 'currentState': array([93.11149746,  8.95613555,  5.90641866]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 0.11981558466680897}
episode index:1399
target Thresh 75.93731462896372
target distance 24.0
model initialize at round 1399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.11243733,  9.29839345]), 'dynamicTrap': False, 'previousTarget': array([12.63557441, 10.19631201]), 'currentState': array([27.91012083, 21.56351456,  3.63990164]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.754076283966966
running average episode reward sum: 0.677041617611984
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'dynamicTrap': False, 'previousTarget': array([4., 3.]), 'currentState': array([4.29227184, 2.38151849, 4.73663766]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.6840630117484371}
episode index:1400
target Thresh 75.93762727355607
target distance 19.0
model initialize at round 1400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'dynamicTrap': False, 'previousTarget': array([15., 13.]), 'currentState': array([32.45172458, 15.10657039,  3.06978023]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 17.578405211338012}
done in step count: 55
reward sum = 0.22445581618804833
running average episode reward sum: 0.6767185727858427
{'scaleFactor': 20, 'currentTarget': array([15., 13.]), 'dynamicTrap': False, 'previousTarget': array([15., 13.]), 'currentState': array([14.99431942, 12.81509493,  1.38589148]), 'targetState': array([15, 13], dtype=int32), 'currentDistance': 0.18499230606384368}
episode index:1401
target Thresh 75.93793835882703
target distance 9.0
model initialize at round 1401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.50506619,  9.92462281]), 'dynamicTrap': True, 'previousTarget': array([53., 11.]), 'currentState': array([61.       ,  2.       ,  2.6245382], dtype=float32), 'targetState': array([53, 11], dtype=int32), 'currentDistance': 10.246160834504426}
done in step count: 20
reward sum = 0.6693647086921064
running average episode reward sum: 0.676713327519014
{'scaleFactor': 20, 'currentTarget': array([53., 11.]), 'dynamicTrap': False, 'previousTarget': array([53., 11.]), 'currentState': array([52.18843306, 11.08004673,  1.55389091]), 'targetState': array([53, 11], dtype=int32), 'currentDistance': 0.8155049797405267}
episode index:1402
target Thresh 75.9382478925537
target distance 47.0
model initialize at round 1402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.6287998,  5.9750668]), 'dynamicTrap': False, 'previousTarget': array([86.9954746 ,  6.57456437]), 'currentState': array([68.62880946,  5.95541209,  5.69845547]), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 28
reward sum = 0.7463741495891317
running average episode reward sum: 0.6767629788533477
{'scaleFactor': 20, 'currentTarget': array([114.,   6.]), 'dynamicTrap': False, 'previousTarget': array([114.,   6.]), 'currentState': array([113.33953167,   6.37426535,   0.32083646]), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 0.7591396190327578}
episode index:1403
target Thresh 75.93855588247449
target distance 30.0
model initialize at round 1403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.27181225, 12.12587174]), 'dynamicTrap': False, 'previousTarget': array([47.11145618, 12.05572809]), 'currentState': array([65.16620498, 21.05843626,  0.43909193]), 'targetState': array([35,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.661131905047459
running average episode reward sum: 0.6767518456098962
{'scaleFactor': 20, 'currentTarget': array([34.0990676 ,  4.29787022]), 'dynamicTrap': True, 'previousTarget': array([35.,  6.]), 'currentState': array([33.87468139,  4.53375597,  0.66977183]), 'targetState': array([35,  6], dtype=int32), 'currentDistance': 0.32556299060791133}
episode index:1404
target Thresh 75.93886233628913
target distance 52.0
model initialize at round 1404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.01572423,  4.20708008]), 'dynamicTrap': True, 'previousTarget': array([84.01477651,  4.23133756]), 'currentState': array([104.       ,   5.       ,   2.8559194], dtype=float32), 'targetState': array([52,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 73
reward sum = 0.15662156712238084
running average episode reward sum: 0.6763816461234282
{'scaleFactor': 20, 'currentTarget': array([52.,  3.]), 'dynamicTrap': False, 'previousTarget': array([52.,  3.]), 'currentState': array([52.62089434,  2.42503822,  3.84268041]), 'targetState': array([52,  3], dtype=int32), 'currentDistance': 0.8462214992261171}
episode index:1405
target Thresh 75.93916726165901
target distance 9.0
model initialize at round 1405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   5.]), 'dynamicTrap': False, 'previousTarget': array([118.,   5.]), 'currentState': array([109.91497999,   9.75028694,   4.39401293]), 'targetState': array([118,   5], dtype=int32), 'currentDistance': 9.37724771110126}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6765701941342942
{'scaleFactor': 20, 'currentTarget': array([118.,   5.]), 'dynamicTrap': False, 'previousTarget': array([118.,   5.]), 'currentState': array([118.10180362,   4.72839024,   0.18090263]), 'targetState': array([118,   5], dtype=int32), 'currentDistance': 0.29006178767009494}
episode index:1406
target Thresh 75.93947066620724
target distance 50.0
model initialize at round 1406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.10487397, 13.63464936]), 'dynamicTrap': False, 'previousTarget': array([70.74071961, 14.60740149]), 'currentState': array([90.52834629, 18.40211443,  4.02172565]), 'targetState': array([40,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6492569323347186
running average episode reward sum: 0.6765507817236335
{'scaleFactor': 20, 'currentTarget': array([40.,  6.]), 'dynamicTrap': False, 'previousTarget': array([40.,  6.]), 'currentState': array([39.39126942,  5.70024521,  2.68866149]), 'targetState': array([40,  6], dtype=int32), 'currentDistance': 0.6785321309813944}
episode index:1407
target Thresh 75.93977255751899
target distance 53.0
model initialize at round 1407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.2123104 , 18.21893166]), 'dynamicTrap': False, 'previousTarget': array([32.87305937, 19.24978031]), 'currentState': array([12.40958449, 15.41678132,  5.35406291]), 'targetState': array([66, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4608185894154399
running average episode reward sum: 0.676397562837051
{'scaleFactor': 20, 'currentTarget': array([66., 23.]), 'dynamicTrap': False, 'previousTarget': array([66., 23.]), 'currentState': array([66.40159764, 23.14641357,  5.58388143]), 'targetState': array([66, 23], dtype=int32), 'currentDistance': 0.4274547852715621}
episode index:1408
target Thresh 75.94007294314153
target distance 11.0
model initialize at round 1408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  7.]), 'dynamicTrap': False, 'previousTarget': array([89.,  7.]), 'currentState': array([98.12016021,  5.37945897,  3.2102586 ]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 9.263016546999172}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.676592447497848
{'scaleFactor': 20, 'currentTarget': array([89.,  7.]), 'dynamicTrap': False, 'previousTarget': array([89.,  7.]), 'currentState': array([88.68095548,  6.43618492,  2.5746727 ]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 0.6478247124653957}
episode index:1409
target Thresh 75.94037183058454
target distance 11.0
model initialize at round 1409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([96.0778399 , 22.70642997,  2.87908089]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 9.10528531184044}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.676780311116219
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([86.75097505, 21.9222331 ,  3.99840155]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 0.26088525283502867}
episode index:1410
target Thresh 75.9406692273202
target distance 11.0
model initialize at round 1410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,  11.]), 'dynamicTrap': False, 'previousTarget': array([103.,  11.]), 'currentState': array([112.15766094,  20.22631397,   3.4649432 ]), 'targetState': array([103,  11], dtype=int32), 'currentDistance': 12.999523964684105}
done in step count: 14
reward sum = 0.8056922020458639
running average episode reward sum: 0.6768716731934192
{'scaleFactor': 20, 'currentTarget': array([104.74668967,  10.73918268]), 'dynamicTrap': True, 'previousTarget': array([104.63791347,  10.87083896]), 'currentState': array([103.92336672,   9.80059442,   3.95266869]), 'targetState': array([103,  11], dtype=int32), 'currentDistance': 1.2485225640565203}
episode index:1411
target Thresh 75.94096514078346
target distance 19.0
model initialize at round 1411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.25358901, 18.71217398]), 'dynamicTrap': False, 'previousTarget': array([56., 19.]), 'currentState': array([36.5929233 , 11.51637277,  5.45459986]), 'targetState': array([56, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7346831382449319
running average episode reward sum: 0.6769126161573368
{'scaleFactor': 20, 'currentTarget': array([56., 19.]), 'dynamicTrap': False, 'previousTarget': array([56., 19.]), 'currentState': array([56.54133639, 19.81951003,  0.299645  ]), 'targetState': array([56, 19], dtype=int32), 'currentDistance': 0.9821617847338084}
episode index:1412
target Thresh 75.94125957837215
target distance 21.0
model initialize at round 1412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.85517415, 18.82395677]), 'dynamicTrap': False, 'previousTarget': array([96.76952105, 18.49442256]), 'currentState': array([115.44441472,  14.79138269,   2.49457192]), 'targetState': array([95, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6770608626227004
{'scaleFactor': 20, 'currentTarget': array([95., 19.]), 'dynamicTrap': False, 'previousTarget': array([95., 19.]), 'currentState': array([95.98331732, 19.78206259,  4.04823871]), 'targetState': array([95, 19], dtype=int32), 'currentDistance': 1.256397567440891}
episode index:1413
target Thresh 75.94155254744726
target distance 67.0
model initialize at round 1413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.67291359, 17.12074143]), 'dynamicTrap': False, 'previousTarget': array([59.85893586, 17.62878378]), 'currentState': array([41.79933396, 19.36592132,  5.8882584 ]), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4631001995480246
running average episode reward sum: 0.6769095467365089
{'scaleFactor': 20, 'currentTarget': array([107.,  12.]), 'dynamicTrap': False, 'previousTarget': array([107.,  12.]), 'currentState': array([106.49334249,  12.3750874 ,   0.79544227]), 'targetState': array([107,  12], dtype=int32), 'currentDistance': 0.6303906689082144}
episode index:1414
target Thresh 75.941844055333
target distance 64.0
model initialize at round 1414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.31807897,  9.36551104]), 'dynamicTrap': False, 'previousTarget': array([60.19486842,  8.78509663]), 'currentState': array([78.14312281,  6.72588294,  2.63018116]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.03826917762679166
running average episode reward sum: 0.6764582107866082
{'scaleFactor': 20, 'currentTarget': array([17.20389552, 16.48617619]), 'dynamicTrap': True, 'previousTarget': array([16., 15.]), 'currentState': array([17.97681568, 16.37193585,  4.54184627]), 'targetState': array([16, 15], dtype=int32), 'currentDistance': 0.781317115944859}
episode index:1415
target Thresh 75.9421341093171
target distance 24.0
model initialize at round 1415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.3784436 , 21.12086435]), 'dynamicTrap': False, 'previousTarget': array([13.01733854, 20.83261089]), 'currentState': array([33.37082792, 21.67274231,  2.31060749]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6765940071156917
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 21.]), 'currentState': array([ 9.48093292, 20.49896521,  3.97848357]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.6945014997299138}
episode index:1416
target Thresh 75.94242271665091
target distance 40.0
model initialize at round 1416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.40561231, 14.35406941]), 'dynamicTrap': False, 'previousTarget': array([73.97504678, 13.99875234]), 'currentState': array([55.41766864, 13.65972956,  5.72926885]), 'targetState': array([94, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6766438152224074
{'scaleFactor': 20, 'currentTarget': array([94., 15.]), 'dynamicTrap': False, 'previousTarget': array([94., 15.]), 'currentState': array([93.7152838 , 14.48838487,  2.06970375]), 'targetState': array([94, 15], dtype=int32), 'currentDistance': 0.5855026525211401}
episode index:1417
target Thresh 75.94270988454964
target distance 14.0
model initialize at round 1417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.,  4.]), 'dynamicTrap': False, 'previousTarget': array([43.,  4.]), 'currentState': array([50.79577075, 16.41020078,  3.60064089]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 14.65561752149242}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.676817370144273
{'scaleFactor': 20, 'currentTarget': array([43.,  4.]), 'dynamicTrap': False, 'previousTarget': array([43.,  4.]), 'currentState': array([42.61203176,  4.05393336,  2.89810991]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 0.3916990766605165}
episode index:1418
target Thresh 75.9429956201925
target distance 25.0
model initialize at round 1418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.76934588,   5.93249392]), 'dynamicTrap': False, 'previousTarget': array([112.85753677,   6.38290441]), 'currentState': array([92.05658155,  2.55508035,  5.07023478]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6769465040304583
{'scaleFactor': 20, 'currentTarget': array([118.,   7.]), 'dynamicTrap': False, 'previousTarget': array([118.,   7.]), 'currentState': array([117.78634736,   7.38305151,   1.22099873]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 0.43860678248103396}
episode index:1419
target Thresh 75.94327993072291
target distance 4.0
model initialize at round 1419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.12969254, 20.84780217]), 'dynamicTrap': True, 'previousTarget': array([54., 21.]), 'currentState': array([58.       , 17.       ,  2.2425873], dtype=float32), 'targetState': array([54, 21], dtype=int32), 'currentDistance': 5.457550859110489}
done in step count: 7
reward sum = 0.8830553978069899
running average episode reward sum: 0.6770916511387517
{'scaleFactor': 20, 'currentTarget': array([54., 21.]), 'dynamicTrap': False, 'previousTarget': array([54., 21.]), 'currentState': array([54.58791148, 20.64192768,  1.51368105]), 'targetState': array([54, 21], dtype=int32), 'currentDistance': 0.688371767076113}
episode index:1420
target Thresh 75.94356282324861
target distance 31.0
model initialize at round 1420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.74449604, 13.31120786]), 'dynamicTrap': False, 'previousTarget': array([42.05202505, 12.45124116]), 'currentState': array([26.80263575,  2.68231469,  6.18990082]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7404544693782666
running average episode reward sum: 0.6771362414401165
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([55.92320435, 20.68388663,  5.76152126]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 0.3253079043723959}
episode index:1421
target Thresh 75.94384430484196
target distance 52.0
model initialize at round 1421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.05557239, 16.60469951]), 'dynamicTrap': False, 'previousTarget': array([81.13182128, 16.70751784]), 'currentState': array([100.93030562,  18.83964451,   4.20139484]), 'targetState': array([49, 13], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6771750368889311
{'scaleFactor': 20, 'currentTarget': array([49., 13.]), 'dynamicTrap': False, 'previousTarget': array([49., 13.]), 'currentState': array([49.74569182, 12.73799357,  4.5365137 ]), 'targetState': array([49, 13], dtype=int32), 'currentDistance': 0.790381967825987}
episode index:1422
target Thresh 75.94412438254001
target distance 18.0
model initialize at round 1422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 23.]), 'dynamicTrap': False, 'previousTarget': array([93., 23.]), 'currentState': array([98.46824844,  6.3792247 ,  1.71635631]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 17.49719727324491}
done in step count: 19
reward sum = 0.7434844726909839
running average episode reward sum: 0.677221635227513
{'scaleFactor': 20, 'currentTarget': array([93., 23.]), 'dynamicTrap': False, 'previousTarget': array([93., 23.]), 'currentState': array([93.61672963, 22.82179277,  0.50503221]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 0.6419604725012704}
episode index:1423
target Thresh 75.94440306334472
target distance 9.0
model initialize at round 1423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([47., 12.]), 'currentState': array([54.87487409,  4.65695397,  2.63099128]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 10.767263670523311}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.677407210026792
{'scaleFactor': 20, 'currentTarget': array([47., 12.]), 'dynamicTrap': False, 'previousTarget': array([47., 12.]), 'currentState': array([47.14433136, 12.78160217,  1.53893145]), 'targetState': array([47, 12], dtype=int32), 'currentDistance': 0.7948166393325147}
episode index:1424
target Thresh 75.94468035422311
target distance 9.0
model initialize at round 1424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'dynamicTrap': False, 'previousTarget': array([14., 10.]), 'currentState': array([ 6.41828684, 15.47815131,  5.59835166]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 9.353743434039623}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6775991979845979
{'scaleFactor': 20, 'currentTarget': array([14., 10.]), 'dynamicTrap': False, 'previousTarget': array([14., 10.]), 'currentState': array([13.88828248,  9.92419009,  5.20233253]), 'targetState': array([14, 10], dtype=int32), 'currentDistance': 0.13501091855587277}
episode index:1425
target Thresh 75.94495626210748
target distance 52.0
model initialize at round 1425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.96636192, 13.25652422]), 'dynamicTrap': False, 'previousTarget': array([67.86817872, 14.29248216]), 'currentState': array([48.18209142, 10.32690981,  5.83079767]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.48396712034400047
running average episode reward sum: 0.6774634111138822
{'scaleFactor': 20, 'currentTarget': array([100.,  18.]), 'dynamicTrap': False, 'previousTarget': array([100.,  18.]), 'currentState': array([99.11232551, 18.51136052,  4.79765892]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 1.0244293911946551}
episode index:1426
target Thresh 75.94523079389553
target distance 8.0
model initialize at round 1426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  14.]), 'dynamicTrap': False, 'previousTarget': array([110.,  14.]), 'currentState': array([103.68880912,  15.62264226,   0.70781737]), 'targetState': array([110,  14], dtype=int32), 'currentDistance': 6.516448282875943}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.677661822185281
{'scaleFactor': 20, 'currentTarget': array([110.,  14.]), 'dynamicTrap': False, 'previousTarget': array([110.,  14.]), 'currentState': array([110.73450249,  13.92145705,   0.53601271]), 'targetState': array([110,  14], dtype=int32), 'currentDistance': 0.7386899905093839}
episode index:1427
target Thresh 75.94550395645058
target distance 64.0
model initialize at round 1427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.82918397, 17.45877557]), 'dynamicTrap': False, 'previousTarget': array([57.99685072, 16.76459164]), 'currentState': array([77.78010351, 23.85116546,  2.05566223]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 94
reward sum = 0.04820794494960695
running average episode reward sum: 0.6772210281536034
{'scaleFactor': 20, 'currentTarget': array([13.,  2.]), 'dynamicTrap': False, 'previousTarget': array([13.,  2.]), 'currentState': array([12.69224765,  1.80904482,  4.21414486]), 'targetState': array([13,  2], dtype=int32), 'currentDistance': 0.36218142871514886}
episode index:1428
target Thresh 75.94577575660169
target distance 16.0
model initialize at round 1428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.99399063, 17.19969157]), 'dynamicTrap': True, 'previousTarget': array([69., 17.]), 'currentState': array([85.       , 18.       ,  4.8225565], dtype=float32), 'targetState': array([69, 17], dtype=int32), 'currentDistance': 16.026004794818103}
done in step count: 15
reward sum = 0.8020237025482785
running average episode reward sum: 0.6773083638249783
{'scaleFactor': 20, 'currentTarget': array([69., 17.]), 'dynamicTrap': False, 'previousTarget': array([69., 17.]), 'currentState': array([68.45166542, 16.67936572,  3.44689199]), 'targetState': array([69, 17], dtype=int32), 'currentDistance': 0.6351985183901608}
episode index:1429
target Thresh 75.9460462011439
target distance 17.0
model initialize at round 1429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 20.]), 'currentState': array([8.32202881, 2.69172884, 2.39354861]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 17.311266654910877}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.677467156630002
{'scaleFactor': 20, 'currentTarget': array([ 8., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 20.]), 'currentState': array([ 8.65633383, 19.66744157,  0.09609993]), 'targetState': array([ 8, 20], dtype=int32), 'currentDistance': 0.7357779551967876}
episode index:1430
target Thresh 75.94631529683834
target distance 44.0
model initialize at round 1430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.8238453 ,  8.71088849]), 'dynamicTrap': False, 'previousTarget': array([31.9414844 ,  8.06407315]), 'currentState': array([49.94872101,  2.85946737,  3.12513387]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5259302410874298
running average episode reward sum: 0.6773612608120128
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.748381  , 15.34624253,  2.78716781]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.7005076361116175}
episode index:1431
target Thresh 75.94658305041239
target distance 55.0
model initialize at round 1431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.9999841 ,  8.02521864]), 'dynamicTrap': True, 'previousTarget': array([55.,  8.]), 'currentState': array([35.       ,  8.       ,  3.3999603], dtype=float32), 'targetState': array([90,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.48377650982707393
running average episode reward sum: 0.67722607593004
{'scaleFactor': 20, 'currentTarget': array([90.,  8.]), 'dynamicTrap': False, 'previousTarget': array([90.,  8.]), 'currentState': array([89.26296402,  7.8683079 ,  1.84762109]), 'targetState': array([90,  8], dtype=int32), 'currentDistance': 0.7487087796337213}
episode index:1432
target Thresh 75.94684946855992
target distance 62.0
model initialize at round 1432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.18628456,  9.0894221 ]), 'dynamicTrap': False, 'previousTarget': array([31.28807928, 10.2886669 ]), 'currentState': array([12.0379596 ,  3.31519909,  5.74112284]), 'targetState': array([74, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5036350092100739
running average episode reward sum: 0.6771049377118125
{'scaleFactor': 20, 'currentTarget': array([74., 22.]), 'dynamicTrap': False, 'previousTarget': array([74., 22.]), 'currentState': array([73.85726427, 22.83396863,  6.15317914]), 'targetState': array([74, 22], dtype=int32), 'currentDistance': 0.8460952450314148}
episode index:1433
target Thresh 75.94711455794139
target distance 52.0
model initialize at round 1433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.68137815, 18.34377201]), 'dynamicTrap': False, 'previousTarget': array([55.68768483, 17.80053053]), 'currentState': array([74.91088767, 23.84158645,  2.5876798 ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6473033312442303
running average episode reward sum: 0.6770841555594641
{'scaleFactor': 20, 'currentTarget': array([23.,  9.]), 'dynamicTrap': False, 'previousTarget': array([23.,  9.]), 'currentState': array([23.53592071,  9.19542904,  5.4549573 ]), 'targetState': array([23,  9], dtype=int32), 'currentDistance': 0.5704415074076704}
episode index:1434
target Thresh 75.94737832518406
target distance 7.0
model initialize at round 1434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 20.]), 'dynamicTrap': False, 'previousTarget': array([49., 20.]), 'currentState': array([55.85391329, 12.86389148,  3.79064636]), 'targetState': array([49, 20], dtype=int32), 'currentDistance': 9.89445158503152}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6772618428015181
{'scaleFactor': 20, 'currentTarget': array([49., 20.]), 'dynamicTrap': False, 'previousTarget': array([49., 20.]), 'currentState': array([48.8800327 , 20.39304339,  4.87332382]), 'targetState': array([49, 20], dtype=int32), 'currentDistance': 0.410944352360935}
episode index:1435
target Thresh 75.94764077688212
target distance 35.0
model initialize at round 1435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.4905692 , 17.62226022]), 'dynamicTrap': True, 'previousTarget': array([51.74850544, 16.96373059]), 'currentState': array([33.       , 10.       ,  4.7598486], dtype=float32), 'targetState': array([68, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6743217792620151
running average episode reward sum: 0.6772597954035101
{'scaleFactor': 20, 'currentTarget': array([68., 23.]), 'dynamicTrap': False, 'previousTarget': array([68., 23.]), 'currentState': array([67.18803191, 22.37884203,  0.14577469]), 'targetState': array([68, 23], dtype=int32), 'currentDistance': 1.022315704204118}
episode index:1436
target Thresh 75.94790191959689
target distance 16.0
model initialize at round 1436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 10.]), 'dynamicTrap': False, 'previousTarget': array([72., 10.]), 'currentState': array([57.97614808,  5.71912056,  6.03145102]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 14.662685686727247}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6774306269268395
{'scaleFactor': 20, 'currentTarget': array([72., 10.]), 'dynamicTrap': False, 'previousTarget': array([72., 10.]), 'currentState': array([71.96705531, 10.65596313,  1.15690566]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 0.6567899022664322}
episode index:1437
target Thresh 75.94816175985692
target distance 62.0
model initialize at round 1437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.97664158,  8.03367125]), 'dynamicTrap': True, 'previousTarget': array([45.95850618,  7.71235444]), 'currentState': array([26.       ,  9.       ,  2.7392979], dtype=float32), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5689277502654304
running average episode reward sum: 0.6773551729096897
{'scaleFactor': 20, 'currentTarget': array([88.,  5.]), 'dynamicTrap': False, 'previousTarget': array([88.,  5.]), 'currentState': array([88.13925441,  5.25184725,  1.67809735]), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 0.2877826106087805}
episode index:1438
target Thresh 75.94842030415825
target distance 19.0
model initialize at round 1438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 23.]), 'dynamicTrap': False, 'previousTarget': array([53.23313766, 22.91410718]), 'currentState': array([70.89663365, 17.22658566,  1.7185992 ]), 'targetState': array([53, 23], dtype=int32), 'currentDistance': 18.804834728120785}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6775004332980195
{'scaleFactor': 20, 'currentTarget': array([53., 23.]), 'dynamicTrap': False, 'previousTarget': array([53., 23.]), 'currentState': array([53.13859608, 23.25125522,  0.61963082]), 'targetState': array([53, 23], dtype=int32), 'currentDistance': 0.28694609495657325}
episode index:1439
target Thresh 75.94867755896449
target distance 11.0
model initialize at round 1439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.03333942,  9.80309647]), 'dynamicTrap': True, 'previousTarget': array([13., 10.]), 'currentState': array([21.      , 21.      ,  1.645536], dtype=float32), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 13.741845932519444}
done in step count: 15
reward sum = 0.8206543646412885
running average episode reward sum: 0.6775998457503412
{'scaleFactor': 20, 'currentTarget': array([13., 10.]), 'dynamicTrap': False, 'previousTarget': array([13., 10.]), 'currentState': array([12.07203874, 10.41852753,  4.30740695]), 'targetState': array([13, 10], dtype=int32), 'currentDistance': 1.0179771117821301}
episode index:1440
target Thresh 75.94893353070704
target distance 6.0
model initialize at round 1440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61., 11.]), 'dynamicTrap': False, 'previousTarget': array([61., 11.]), 'currentState': array([67.26653548, 13.41730046,  2.2567361 ]), 'targetState': array([61, 11], dtype=int32), 'currentDistance': 6.716606911470929}
done in step count: 4
reward sum = 0.9509900498999999
running average episode reward sum: 0.677789568307003
{'scaleFactor': 20, 'currentTarget': array([61.10096235, 12.95580797]), 'dynamicTrap': True, 'previousTarget': array([61., 11.]), 'currentState': array([62.08343233, 13.01667616,  4.31000952]), 'targetState': array([61, 11], dtype=int32), 'currentDistance': 0.9843536969887594}
episode index:1441
target Thresh 75.94918822578519
target distance 14.0
model initialize at round 1441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80., 17.]), 'dynamicTrap': False, 'previousTarget': array([80., 17.]), 'currentState': array([85.6147039 ,  3.47446748,  1.29579752]), 'targetState': array([80, 17], dtype=int32), 'currentDistance': 14.644621197455706}
done in step count: 14
reward sum = 0.8041294480120442
running average episode reward sum: 0.6778771826479912
{'scaleFactor': 20, 'currentTarget': array([80., 17.]), 'dynamicTrap': False, 'previousTarget': array([80., 17.]), 'currentState': array([79.43643928, 16.58981486,  2.81076517]), 'targetState': array([80, 17], dtype=int32), 'currentDistance': 0.6970312287361474}
episode index:1442
target Thresh 75.94944165056634
target distance 13.0
model initialize at round 1442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.8019936 ,  12.02781886]), 'dynamicTrap': True, 'previousTarget': array([109.,  12.]), 'currentState': array([96.       , 16.       ,  6.1056633], dtype=float32), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 13.404076364358463}
done in step count: 53
reward sum = 0.19815402147034972
running average episode reward sum: 0.6775447341648467
{'scaleFactor': 20, 'currentTarget': array([109.,  12.]), 'dynamicTrap': False, 'previousTarget': array([109.,  12.]), 'currentState': array([109.30426874,  12.32550535,   4.27693639]), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 0.4455706468506293}
episode index:1443
target Thresh 75.94969381138613
target distance 4.0
model initialize at round 1443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 19.]), 'dynamicTrap': False, 'previousTarget': array([68., 19.]), 'currentState': array([67.4609485 , 16.93575387,  1.16088736]), 'targetState': array([68, 19], dtype=int32), 'currentDistance': 2.133468677227072}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6777611159278905
{'scaleFactor': 20, 'currentTarget': array([68., 19.]), 'dynamicTrap': False, 'previousTarget': array([68., 19.]), 'currentState': array([67.3239705 , 18.85303789,  2.13307232]), 'targetState': array([68, 19], dtype=int32), 'currentDistance': 0.6918191604136269}
episode index:1444
target Thresh 75.94994471454858
target distance 28.0
model initialize at round 1444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.76005599, 10.52659818]), 'dynamicTrap': False, 'previousTarget': array([89.85982065,  9.57777387]), 'currentState': array([108.55061421,  19.66421484,   2.13777599]), 'targetState': array([80,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 38
reward sum = 0.5546193073428397
running average episode reward sum: 0.6776758966831948
{'scaleFactor': 20, 'currentTarget': array([80.,  5.]), 'dynamicTrap': False, 'previousTarget': array([80.,  5.]), 'currentState': array([79.87018344,  5.88125776,  4.29812018]), 'targetState': array([80,  5], dtype=int32), 'currentDistance': 0.890767971451534}
episode index:1445
target Thresh 75.95019436632629
target distance 15.0
model initialize at round 1445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  7.]), 'dynamicTrap': False, 'previousTarget': array([33.,  7.]), 'currentState': array([25.99115757, 20.82063427,  5.01476192]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 15.496251278290945}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6778389958192947
{'scaleFactor': 20, 'currentTarget': array([33.,  7.]), 'dynamicTrap': False, 'previousTarget': array([33.,  7.]), 'currentState': array([32.33330384,  7.06876331,  3.99859203]), 'targetState': array([33,  7], dtype=int32), 'currentDistance': 0.6702329209671221}
episode index:1446
target Thresh 75.95044277296054
target distance 60.0
model initialize at round 1446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.72417484, 18.67405382]), 'dynamicTrap': False, 'previousTarget': array([64.61161351, 18.0776773 ]), 'currentState': array([46.19477405, 22.98711593,  6.05165476]), 'targetState': array([105,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5202084995410023
running average episode reward sum: 0.6777300597472296
{'scaleFactor': 20, 'currentTarget': array([105.,  10.]), 'dynamicTrap': False, 'previousTarget': array([105.,  10.]), 'currentState': array([105.86143821,   9.08586579,   3.78172831]), 'targetState': array([105,  10], dtype=int32), 'currentDistance': 1.2560721061706166}
episode index:1447
target Thresh 75.95068994066156
target distance 9.0
model initialize at round 1447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.65402642, 19.54489382,  4.47764874]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 8.551894999306734}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6779187752100423
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.20588364, 11.66938072,  5.17518754]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 1.0386006636892626}
episode index:1448
target Thresh 75.95093587560852
target distance 13.0
model initialize at round 1448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.21588114, 19.28485237]), 'dynamicTrap': True, 'previousTarget': array([29., 18.]), 'currentState': array([16.       ,  9.       ,  4.2761374], dtype=float32), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 15.96890541773182}
done in step count: 18
reward sum = 0.7671411919309719
running average episode reward sum: 0.6779803503768614
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([29.77792949, 17.18044385,  6.2231757 ]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 1.1299763578506234}
episode index:1449
target Thresh 75.95118058394982
target distance 59.0
model initialize at round 1449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.53388184, 15.33110582]), 'dynamicTrap': False, 'previousTarget': array([73.33879402, 15.33435143]), 'currentState': array([91.16296378, 19.16505185,  2.43143332]), 'targetState': array([34,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5434467099196731
running average episode reward sum: 0.6778875685558565
{'scaleFactor': 20, 'currentTarget': array([34.,  8.]), 'dynamicTrap': False, 'previousTarget': array([34.,  8.]), 'currentState': array([33.28170965,  7.71936425,  5.35187983]), 'targetState': array([34,  8], dtype=int32), 'currentDistance': 0.7711662916954495}
episode index:1450
target Thresh 75.95142407180317
target distance 38.0
model initialize at round 1450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.11370918, 18.26066867]), 'dynamicTrap': False, 'previousTarget': array([44.33093623, 18.62324859]), 'currentState': array([62.65273208, 13.99140829,  2.77420998]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6498375746426523
running average episode reward sum: 0.677868237064531
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'dynamicTrap': False, 'previousTarget': array([26., 22.]), 'currentState': array([25.4585366 , 21.63065808,  3.95955018]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 0.6554357817601033}
episode index:1451
target Thresh 75.95166634525579
target distance 66.0
model initialize at round 1451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.40595111,  9.7797958 ]), 'dynamicTrap': False, 'previousTarget': array([93.,  9.]), 'currentState': array([111.40286807,  10.13095396,   2.88935411]), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 42
reward sum = 0.6024764060613967
running average episode reward sum: 0.6778163143159063
{'scaleFactor': 20, 'currentTarget': array([47.,  9.]), 'dynamicTrap': False, 'previousTarget': array([47.,  9.]), 'currentState': array([47.7257159 ,  9.72050697,  3.54677575]), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 1.0226406277303397}
episode index:1452
target Thresh 75.95190741036453
target distance 57.0
model initialize at round 1452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.44221391, 10.33745902]), 'dynamicTrap': False, 'previousTarget': array([69.19412046,  9.22022743]), 'currentState': array([89.16173766, 13.67518029,  2.48454526]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5085600173891918
running average episode reward sum: 0.6776998268438301
{'scaleFactor': 20, 'currentTarget': array([32.,  4.]), 'dynamicTrap': False, 'previousTarget': array([32.,  4.]), 'currentState': array([32.0456107 ,  4.55144941,  1.56134459]), 'targetState': array([32,  4], dtype=int32), 'currentDistance': 0.5533324399821631}
episode index:1453
target Thresh 75.952147273156
target distance 44.0
model initialize at round 1453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.46086751, 17.1329362 ]), 'dynamicTrap': False, 'previousTarget': array([67.91786413, 16.18928508]), 'currentState': array([47.62190046, 19.66579746,  0.82698333]), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 37
reward sum = 0.6465239951301786
running average episode reward sum: 0.6776783854189927
{'scaleFactor': 20, 'currentTarget': array([92., 14.]), 'dynamicTrap': False, 'previousTarget': array([92., 14.]), 'currentState': array([91.73742669, 13.49664678,  0.70348   ]), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 0.5677228207558455}
episode index:1454
target Thresh 75.95238593962682
target distance 26.0
model initialize at round 1454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.39804536,  3.56509106]), 'dynamicTrap': False, 'previousTarget': array([44.88441983,  4.11828302]), 'currentState': array([63.8252377 ,  8.31737449,  3.61358297]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7796716320614839
running average episode reward sum: 0.6777484838702934
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([38.18391529,  2.59181608,  2.00758324]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 0.6197347066569444}
episode index:1455
target Thresh 75.95262341574364
target distance 30.0
model initialize at round 1455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.76006448, 10.91133498]), 'dynamicTrap': True, 'previousTarget': array([34.9007438 , 12.00992562]), 'currentState': array([15.       , 14.       ,  0.5839073], dtype=float32), 'targetState': array([45, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.5792059009053865
running average episode reward sum: 0.6776808035248505
{'scaleFactor': 20, 'currentTarget': array([45., 11.]), 'dynamicTrap': False, 'previousTarget': array([45., 11.]), 'currentState': array([45.09093899, 11.1138105 ,  1.20032684]), 'targetState': array([45, 11], dtype=int32), 'currentDistance': 0.1456802327528905}
episode index:1456
target Thresh 75.95285970744341
target distance 41.0
model initialize at round 1456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.28478157,  8.85688663]), 'dynamicTrap': False, 'previousTarget': array([32.,  8.]), 'currentState': array([52.26859403,  9.66139861,  2.42051569]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.582340361515844
running average episode reward sum: 0.6776153673944394
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'dynamicTrap': False, 'previousTarget': array([11.,  8.]), 'currentState': array([10.32043861,  7.95548423,  4.2932361 ]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.6810178644043985}
episode index:1457
target Thresh 75.9530948206334
target distance 18.0
model initialize at round 1457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'dynamicTrap': False, 'previousTarget': array([19.,  4.]), 'currentState': array([ 9.47238743, 21.24869781,  4.51540112]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 19.70515104158041}
done in step count: 14
reward sum = 0.8416134370014666
running average episode reward sum: 0.6777278489236623
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'dynamicTrap': False, 'previousTarget': array([19.,  4.]), 'currentState': array([18.95668639,  4.4866169 ,  5.33152169]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.48854076306766625}
episode index:1458
target Thresh 75.95332876119147
target distance 7.0
model initialize at round 1458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([8.32302669, 9.35880369, 3.886024  ]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 5.3351057374720146}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6779283774713499
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([3.4378005 , 9.12383966, 2.18142724]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.45497861769951964}
episode index:1459
target Thresh 75.95356153496613
target distance 23.0
model initialize at round 1459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.40405631, 12.41590203]), 'dynamicTrap': False, 'previousTarget': array([51.95731557, 12.37089005]), 'currentState': array([68.93900444, 22.03492454,  3.37863278]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8596106402941418
running average episode reward sum: 0.678052817377393
{'scaleFactor': 20, 'currentTarget': array([47., 10.]), 'dynamicTrap': False, 'previousTarget': array([47., 10.]), 'currentState': array([47.77029895, 10.59766636,  2.87747209]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 0.974969510397237}
episode index:1460
target Thresh 75.95379314777674
target distance 63.0
model initialize at round 1460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.24266967, 10.00546645]), 'dynamicTrap': False, 'previousTarget': array([51.79898987, 11.17157288]), 'currentState': array([32.37832344, 12.33092311,  5.10650635]), 'targetState': array([95,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.611438667346053
running average episode reward sum: 0.6780072224766186
{'scaleFactor': 20, 'currentTarget': array([95.,  5.]), 'dynamicTrap': False, 'previousTarget': array([95.,  5.]), 'currentState': array([95.3733555 ,  4.4143668 ,  2.74518441]), 'targetState': array([95,  5], dtype=int32), 'currentDistance': 0.6945218341586795}
episode index:1461
target Thresh 75.95402360541365
target distance 62.0
model initialize at round 1461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.39218423, 15.41983786]), 'dynamicTrap': False, 'previousTarget': array([68.94289151, 16.58385933]), 'currentState': array([51.35773946, 21.55904892,  5.71679211]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.5144669110282452
running average episode reward sum: 0.6778953617984733
{'scaleFactor': 20, 'currentTarget': array([112.,   2.]), 'dynamicTrap': False, 'previousTarget': array([112.,   2.]), 'currentState': array([112.50946311,   2.08956342,   4.3380594 ]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 0.5172758145629619}
episode index:1462
target Thresh 75.95425291363829
target distance 49.0
model initialize at round 1462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.32904769, 10.06731496]), 'dynamicTrap': False, 'previousTarget': array([84.51432504, 10.61923315]), 'currentState': array([66.79315648, 14.35088293,  0.18989175]), 'targetState': array([114,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6601210374239553
running average episode reward sum: 0.6778832125678688
{'scaleFactor': 20, 'currentTarget': array([114.,   4.]), 'dynamicTrap': False, 'previousTarget': array([114.,   4.]), 'currentState': array([113.20598484,   4.40769802,   5.42329669]), 'targetState': array([114,   4], dtype=int32), 'currentDistance': 0.8925680617106289}
episode index:1463
target Thresh 75.95448107818339
target distance 6.0
model initialize at round 1463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.06837201, 22.84420288]), 'dynamicTrap': True, 'previousTarget': array([41., 23.]), 'currentState': array([42.       , 17.       ,  1.9906656], dtype=float32), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 5.917992733085996}
done in step count: 18
reward sum = 0.6859715325449631
running average episode reward sum: 0.6778887373765963
{'scaleFactor': 20, 'currentTarget': array([41., 23.]), 'dynamicTrap': False, 'previousTarget': array([41., 23.]), 'currentState': array([41.00216536, 23.36064648,  0.76204861]), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 0.36065297641531285}
episode index:1464
target Thresh 75.95470810475308
target distance 11.0
model initialize at round 1464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'dynamicTrap': False, 'previousTarget': array([19., 17.]), 'currentState': array([28.29210124, 23.09840459,  3.83121705]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 11.1145707973905}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6780686632551112
{'scaleFactor': 20, 'currentTarget': array([19., 17.]), 'dynamicTrap': False, 'previousTarget': array([19., 17.]), 'currentState': array([18.68510734, 17.34702975,  3.56706862]), 'targetState': array([19, 17], dtype=int32), 'currentDistance': 0.46860114982648465}
episode index:1465
target Thresh 75.95493399902303
target distance 72.0
model initialize at round 1465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.13243538, 14.37162347]), 'dynamicTrap': False, 'previousTarget': array([94.00192873, 15.27775099]), 'currentState': array([113.12230125,  13.73502112,   3.09580612]), 'targetState': array([42, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5673680099168474
running average episode reward sum: 0.6779931512132706
{'scaleFactor': 20, 'currentTarget': array([42., 16.]), 'dynamicTrap': False, 'previousTarget': array([42., 16.]), 'currentState': array([42.96394529, 16.53915799,  4.90840971]), 'targetState': array([42, 16], dtype=int32), 'currentDistance': 1.104482623076938}
episode index:1466
target Thresh 75.95515876664061
target distance 8.0
model initialize at round 1466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.63854322, 12.38414088]), 'dynamicTrap': True, 'previousTarget': array([69., 12.]), 'currentState': array([63.       , 20.       ,  2.3818743], dtype=float32), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 10.786503260888326}
done in step count: 9
reward sum = 0.8936172474836408
running average episode reward sum: 0.6781401342373132
{'scaleFactor': 20, 'currentTarget': array([69., 12.]), 'dynamicTrap': False, 'previousTarget': array([69., 12.]), 'currentState': array([69.65410485, 11.7553429 ,  5.95253768]), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 0.6983625541081524}
episode index:1467
target Thresh 75.95538241322501
target distance 47.0
model initialize at round 1467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.44057899,  6.21534898]), 'dynamicTrap': False, 'previousTarget': array([53.,  7.]), 'currentState': array([72.43177814,  5.62208984,  3.31125641]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6719231141156307
running average episode reward sum: 0.6781358992099824
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([25.59573922,  7.83188815,  4.03592649]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.9249133283241996}
episode index:1468
target Thresh 75.95560494436742
target distance 61.0
model initialize at round 1468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.82618167,  6.63106828]), 'dynamicTrap': True, 'previousTarget': array([75.83019061,  6.60068074]), 'currentState': array([56.       ,  4.       ,  3.6761825], dtype=float32), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.6500940540582194
running average episode reward sum: 0.6781168101390825
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([117.1465918 ,  11.59572332,   1.02254359]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.43003347844347034}
episode index:1469
target Thresh 75.95582636563113
target distance 9.0
model initialize at round 1469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 13.]), 'currentState': array([12.26888129, 23.28065561,  3.04648781]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 10.78783873846096}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6782959688732743
{'scaleFactor': 20, 'currentTarget': array([ 9., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 13.]), 'currentState': array([ 9.52989382, 13.03019135,  5.24346898]), 'targetState': array([ 9, 13], dtype=int32), 'currentDistance': 0.5307532155620543}
episode index:1470
target Thresh 75.95604668255169
target distance 4.0
model initialize at round 1470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([4.70263263, 6.76807495, 5.23718745]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 4.413194491585004}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6785011381670382
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([6.09862786, 3.40816612, 4.02739394]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.9894803218737627}
episode index:1471
target Thresh 75.956265900637
target distance 53.0
model initialize at round 1471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.82135638, 14.09138879]), 'dynamicTrap': False, 'previousTarget': array([21.50626598, 13.58348695]), 'currentState': array([ 3.42515178, 18.96860594,  5.93942637]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4947315576088861
running average episode reward sum: 0.6783762947019851
{'scaleFactor': 20, 'currentTarget': array([53.4797333 ,  5.27448093]), 'dynamicTrap': True, 'previousTarget': array([55.,  6.]), 'currentState': array([53.11894453,  5.5867736 ,  0.3755085 ]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 0.47717422620369326}
episode index:1472
target Thresh 75.95648402536757
target distance 44.0
model initialize at round 1472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.21645288, 14.92916971]), 'dynamicTrap': False, 'previousTarget': array([68.81964584, 14.66692282]), 'currentState': array([86.3487931 ,  9.1022016 ,  2.71203864]), 'targetState': array([44, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 32
reward sum = 0.6434611584035211
running average episode reward sum: 0.6783525912829095
{'scaleFactor': 20, 'currentTarget': array([44., 22.]), 'dynamicTrap': False, 'previousTarget': array([44., 22.]), 'currentState': array([44.85507688, 21.66510765,  3.06742135]), 'targetState': array([44, 22], dtype=int32), 'currentDistance': 0.9183187694753308}
episode index:1473
target Thresh 75.9567010621965
target distance 40.0
model initialize at round 1473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.4160675 ,  6.27181685]), 'dynamicTrap': False, 'previousTarget': array([88.9007438 ,  7.00992562]), 'currentState': array([70.46273898,  7.6373502 ,  5.58554569]), 'targetState': array([109,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7683251363843665
running average episode reward sum: 0.6784136310014316
{'scaleFactor': 20, 'currentTarget': array([109.,   5.]), 'dynamicTrap': False, 'previousTarget': array([109.,   5.]), 'currentState': array([108.52476405,   5.19784132,   0.57568462]), 'targetState': array([109,   5], dtype=int32), 'currentDistance': 0.5147721848642766}
episode index:1474
target Thresh 75.95691701654972
target distance 49.0
model initialize at round 1474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.87551889, 12.61171587]), 'dynamicTrap': True, 'previousTarget': array([81.89513224, 12.55545404]), 'currentState': array([63.       ,  6.       ,  4.2726192], dtype=float32), 'targetState': array([112,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.615905585963236
running average episode reward sum: 0.6783712526658124
{'scaleFactor': 20, 'currentTarget': array([112.,  23.]), 'dynamicTrap': False, 'previousTarget': array([112.,  23.]), 'currentState': array([111.53888903,  22.82267873,   0.56258207]), 'targetState': array([112,  23], dtype=int32), 'currentDistance': 0.494030526586443}
episode index:1475
target Thresh 75.95713189382613
target distance 19.0
model initialize at round 1475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.50567838,  7.07038766]), 'dynamicTrap': False, 'previousTarget': array([84.10111675,  8.13601924]), 'currentState': array([69.24653972, 18.71686396,  5.93540025]), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8231290838879497
running average episode reward sum: 0.6784693270772095
{'scaleFactor': 20, 'currentTarget': array([87.,  6.]), 'dynamicTrap': False, 'previousTarget': array([87.,  6.]), 'currentState': array([86.1707657 ,  6.78725684,  5.47288433]), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 1.1434171820642942}
episode index:1476
target Thresh 75.95734569939764
target distance 52.0
model initialize at round 1476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.61902639, 12.76909152]), 'dynamicTrap': False, 'previousTarget': array([30.89972147, 13.45778872]), 'currentState': array([13.70193925, 19.26091824,  6.13814276]), 'targetState': array([64,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6926942768115789
running average episode reward sum: 0.6784789580519789
{'scaleFactor': 20, 'currentTarget': array([64.,  2.]), 'dynamicTrap': False, 'previousTarget': array([64.,  2.]), 'currentState': array([64.67053807,  2.43772778,  5.76059579]), 'targetState': array([64,  2], dtype=int32), 'currentDistance': 0.8007664523163747}
episode index:1477
target Thresh 75.95755843860941
target distance 44.0
model initialize at round 1477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.99294727, 11.53109259]), 'dynamicTrap': True, 'previousTarget': array([81.99483671, 11.45442811]), 'currentState': array([62.       , 11.       ,  5.3111725], dtype=float32), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.540852991017049
running average episode reward sum: 0.6783858417008051
{'scaleFactor': 20, 'currentTarget': array([106.,  12.]), 'dynamicTrap': False, 'previousTarget': array([106.,  12.]), 'currentState': array([105.48976814,  12.17633588,   1.62233859]), 'targetState': array([106,  12], dtype=int32), 'currentDistance': 0.5398433942497315}
episode index:1478
target Thresh 75.95777011677995
target distance 10.0
model initialize at round 1478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 10.]), 'dynamicTrap': False, 'previousTarget': array([93., 10.]), 'currentState': array([100.82305698,  18.55473371,   4.22006845]), 'targetState': array([93, 10], dtype=int32), 'currentDistance': 11.592397909855586}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6785637283185875
{'scaleFactor': 20, 'currentTarget': array([93., 10.]), 'dynamicTrap': False, 'previousTarget': array([93., 10.]), 'currentState': array([92.75225134, 10.78518525,  5.13744265]), 'targetState': array([93, 10], dtype=int32), 'currentDistance': 0.8233439625274128}
episode index:1479
target Thresh 75.9579807392012
target distance 23.0
model initialize at round 1479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.30681407, 15.88726118]), 'dynamicTrap': False, 'previousTarget': array([32.37514441, 15.28798697]), 'currentState': array([50.26240246,  9.5087293 ,  3.03551376]), 'targetState': array([28, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7894682623337329
running average episode reward sum: 0.6786386638145436
{'scaleFactor': 20, 'currentTarget': array([28., 17.]), 'dynamicTrap': False, 'previousTarget': array([28., 17.]), 'currentState': array([27.98767041, 17.89559994,  1.50144591]), 'targetState': array([28, 17], dtype=int32), 'currentDistance': 0.8956848042269917}
episode index:1480
target Thresh 75.95819031113874
target distance 24.0
model initialize at round 1480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.7176878 , 17.44762912]), 'dynamicTrap': False, 'previousTarget': array([42.41416067, 16.52566297]), 'currentState': array([58.3082772 ,  7.93074073,  3.10105872]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6787670278584021
{'scaleFactor': 20, 'currentTarget': array([36., 20.]), 'dynamicTrap': False, 'previousTarget': array([36., 20.]), 'currentState': array([35.40042399, 20.23489191,  3.21360102]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 0.6439453451021884}
episode index:1481
target Thresh 75.95839883783188
target distance 29.0
model initialize at round 1481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.91669862, 12.95350231]), 'dynamicTrap': False, 'previousTarget': array([98.74981351, 13.18111808]), 'currentState': array([115.74141663,   3.88270148,   2.85759449]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6925080244048604
running average episode reward sum: 0.6787762997858965
{'scaleFactor': 20, 'currentTarget': array([88., 18.]), 'dynamicTrap': False, 'previousTarget': array([88., 18.]), 'currentState': array([87.27434049, 17.99005517,  3.13234669]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 0.7257276559867576}
episode index:1482
target Thresh 75.9586063244938
target distance 14.0
model initialize at round 1482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 11.]), 'dynamicTrap': False, 'previousTarget': array([49., 11.]), 'currentState': array([62.24816089,  3.55748701,  1.64930803]), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 15.1955508774189}
done in step count: 13
reward sum = 0.8398570619766187
running average episode reward sum: 0.6788849179667399
{'scaleFactor': 20, 'currentTarget': array([49., 11.]), 'dynamicTrap': False, 'previousTarget': array([49., 11.]), 'currentState': array([48.61573328, 10.62180569,  2.09953922]), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 0.5391584638890685}
episode index:1483
target Thresh 75.95881277631167
target distance 11.0
model initialize at round 1483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 22.]), 'dynamicTrap': False, 'previousTarget': array([37., 22.]), 'currentState': array([25.26565568, 12.56718524,  1.08178186]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 15.055657767307355}
done in step count: 13
reward sum = 0.8313791278566843
running average episode reward sum: 0.678987676868283
{'scaleFactor': 20, 'currentTarget': array([37., 22.]), 'dynamicTrap': False, 'previousTarget': array([37., 22.]), 'currentState': array([36.6380797 , 22.58300909,  0.8931697 ]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 0.6862112682281937}
episode index:1484
target Thresh 75.95901819844681
target distance 48.0
model initialize at round 1484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.9802466 , 11.88867634]), 'dynamicTrap': True, 'previousTarget': array([85.98266146, 11.83261089]), 'currentState': array([66.       , 11.       ,  2.7681725], dtype=float32), 'targetState': array([114,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7130935495354235
running average episode reward sum: 0.6790106437859039
{'scaleFactor': 20, 'currentTarget': array([114.,  13.]), 'dynamicTrap': False, 'previousTarget': array([114.,  13.]), 'currentState': array([113.15517751,  12.03244838,   4.81255597]), 'targetState': array([114,  13], dtype=int32), 'currentDistance': 1.2844770082691654}
episode index:1485
target Thresh 75.95922259603479
target distance 9.0
model initialize at round 1485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 23.]), 'currentState': array([15.3862912 , 20.10591339,  2.26701748]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 7.933034409604553}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6791936716500454
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.62919024, 22.68947709,  3.48579662]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.701644382544379}
episode index:1486
target Thresh 75.95942597418554
target distance 35.0
model initialize at round 1486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.91195473, 11.47841483]), 'dynamicTrap': False, 'previousTarget': array([41.43046618, 10.57218647]), 'currentState': array([61.36796452, 19.18397792,  1.48899644]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.4680639287774866
running average episode reward sum: 0.6790516879628413
{'scaleFactor': 20, 'currentTarget': array([26.22561645,  2.89432438]), 'dynamicTrap': True, 'previousTarget': array([25.,  4.]), 'currentState': array([25.48992806,  2.8566743 ,  3.23034844]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.7366511725053501}
episode index:1487
target Thresh 75.95962833798352
target distance 23.0
model initialize at round 1487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.632791  ,  21.05668042]), 'dynamicTrap': False, 'previousTarget': array([105.98112317,  20.86874449]), 'currentState': array([86.63852168, 21.53542289,  0.28082442]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7802051540739152
running average episode reward sum: 0.6791196674427546
{'scaleFactor': 20, 'currentTarget': array([109.,  21.]), 'dynamicTrap': False, 'previousTarget': array([109.,  21.]), 'currentState': array([108.34850281,  21.01768823,   0.59830697]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 0.6517372679115041}
episode index:1488
target Thresh 75.95982969248786
target distance 13.0
model initialize at round 1488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.54945291, 21.37442876]), 'dynamicTrap': True, 'previousTarget': array([29., 21.]), 'currentState': array([16.       ,  8.       ,  5.0405703], dtype=float32), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 17.67102733684629}
done in step count: 16
reward sum = 0.78456554809669
running average episode reward sum: 0.6791904840180762
{'scaleFactor': 20, 'currentTarget': array([29., 21.]), 'dynamicTrap': False, 'previousTarget': array([29., 21.]), 'currentState': array([29.6746012 , 21.2863198 ,  1.34447061]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 0.7328477409864533}
episode index:1489
target Thresh 75.96003004273241
target distance 54.0
model initialize at round 1489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.56652561,  4.13509874]), 'dynamicTrap': False, 'previousTarget': array([21.83405013,  4.57108057]), 'currentState': array([3.78778783, 1.1683612 , 5.93376452]), 'targetState': array([56,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7240606403494187
running average episode reward sum: 0.6792205982169564
{'scaleFactor': 20, 'currentTarget': array([56.,  9.]), 'dynamicTrap': False, 'previousTarget': array([56.,  9.]), 'currentState': array([55.99583279,  9.79127613,  5.73445156]), 'targetState': array([56,  9], dtype=int32), 'currentDistance': 0.7912871000730844}
episode index:1490
target Thresh 75.96022939372595
target distance 37.0
model initialize at round 1490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.13957675, 19.35872596]), 'dynamicTrap': True, 'previousTarget': array([33.25789045, 20.20142317]), 'currentState': array([53.        , 17.        ,  0.78829837], dtype=float32), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7479948679090332
running average episode reward sum: 0.679266724487709
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'dynamicTrap': False, 'previousTarget': array([16., 23.]), 'currentState': array([15.0417011 , 22.24408992,  1.84820764]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 1.220547755286117}
episode index:1491
target Thresh 75.96042775045225
target distance 45.0
model initialize at round 1491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.16493937,  5.57052619]), 'dynamicTrap': False, 'previousTarget': array([69.95570316,  4.66961979]), 'currentState': array([50.27121683,  7.62960451,  0.39587688]), 'targetState': array([95,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6960417082869271
running average episode reward sum: 0.6792779677744377
{'scaleFactor': 20, 'currentTarget': array([95.,  3.]), 'dynamicTrap': False, 'previousTarget': array([95.,  3.]), 'currentState': array([94.13201658,  3.03025834,  3.24982023]), 'targetState': array([95,  3], dtype=int32), 'currentDistance': 0.8685106672322686}
episode index:1492
target Thresh 75.96062511787026
target distance 10.0
model initialize at round 1492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.1982238 , 11.99588278]), 'dynamicTrap': True, 'previousTarget': array([29., 12.]), 'currentState': array([39.       ,  4.       ,  3.1194413], dtype=float32), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 12.64946474262832}
done in step count: 11
reward sum = 0.8559342642587164
running average episode reward sum: 0.6793962908129403
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([29., 12.]), 'currentState': array([28.51083446, 11.46728654,  2.59439495]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 0.723233401672608}
episode index:1493
target Thresh 75.96082150091418
target distance 66.0
model initialize at round 1493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.70585531, 11.44832217]), 'dynamicTrap': False, 'previousTarget': array([97.27212152, 11.28797975]), 'currentState': array([115.42648294,   8.1171293 ,   2.06421065]), 'targetState': array([51, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6517149622541738
running average episode reward sum: 0.6793777624805716
{'scaleFactor': 20, 'currentTarget': array([51., 19.]), 'dynamicTrap': False, 'previousTarget': array([51., 19.]), 'currentState': array([51.79343833, 19.47666937,  3.00119352]), 'targetState': array([51, 19], dtype=int32), 'currentDistance': 0.9256122679495478}
episode index:1494
target Thresh 75.96101690449356
target distance 55.0
model initialize at round 1494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.43134435, 15.00358085]), 'dynamicTrap': False, 'previousTarget': array([35.46369226, 14.39949092]), 'currentState': array([17.07704084, 20.0445069 ,  6.04324967]), 'targetState': array([71,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.591082230393426
running average episode reward sum: 0.6793187019239916
{'scaleFactor': 20, 'currentTarget': array([71.,  6.]), 'dynamicTrap': False, 'previousTarget': array([71.,  6.]), 'currentState': array([70.02214722,  6.12753935,  0.47380504]), 'targetState': array([71,  6], dtype=int32), 'currentDistance': 0.986135054869425}
episode index:1495
target Thresh 75.96121133349354
target distance 7.0
model initialize at round 1495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.,  9.]), 'dynamicTrap': False, 'previousTarget': array([86.,  9.]), 'currentState': array([79.28328939,  6.52529959,  5.91217661]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 7.158096364601437}
done in step count: 13
reward sum = 0.8300284779815442
running average episode reward sum: 0.6794194437529071
{'scaleFactor': 20, 'currentTarget': array([86.,  9.]), 'dynamicTrap': False, 'previousTarget': array([86.,  9.]), 'currentState': array([86.08315133,  8.53144438,  0.36626334]), 'targetState': array([86,  9], dtype=int32), 'currentDistance': 0.4758765777435666}
episode index:1496
target Thresh 75.96140479277483
target distance 16.0
model initialize at round 1496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'dynamicTrap': False, 'previousTarget': array([21., 23.]), 'currentState': array([ 6.66129886, 19.26921028,  5.43383688]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 14.816110901522073}
done in step count: 11
reward sum = 0.8574869386866365
running average episode reward sum: 0.6795383933153211
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'dynamicTrap': False, 'previousTarget': array([21., 23.]), 'currentState': array([20.0770967 , 22.05599378,  5.82035097]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.320188712404276}
episode index:1497
target Thresh 75.96159728717393
target distance 6.0
model initialize at round 1497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([111.47771397,  17.22785657,   2.09363627]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 5.904900285610079}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6797324925187154
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([108.92488051,  21.77564127,   1.22037816]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 0.9517041596509653}
episode index:1498
target Thresh 75.9617888215032
target distance 72.0
model initialize at round 1498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.6094793, 12.264353 ]), 'dynamicTrap': False, 'previousTarget': array([58.31824278, 11.55365061]), 'currentState': array([79.3491268 ,  9.04778394,  1.6272642 ]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 52
reward sum = 0.5052832551834334
running average episode reward sum: 0.679616115442441
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.36453125, 21.42840396,  2.58860375]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.562505987565643}
episode index:1499
target Thresh 75.96197940055103
target distance 4.0
model initialize at round 1499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  12.]), 'dynamicTrap': False, 'previousTarget': array([110.,  12.]), 'currentState': array([111.9695643 ,   9.65744546,   2.51312476]), 'targetState': array([110,  12], dtype=int32), 'currentDistance': 3.060513895878922}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6797906914650801
{'scaleFactor': 20, 'currentTarget': array([110.,  12.]), 'dynamicTrap': False, 'previousTarget': array([110.,  12.]), 'currentState': array([110.64143164,  12.00021132,   2.73534464]), 'targetState': array([110,  12], dtype=int32), 'currentDistance': 0.6414316748396581}
episode index:1500
target Thresh 75.96216902908189
target distance 32.0
model initialize at round 1500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.81177356, 17.6256744 ]), 'dynamicTrap': False, 'previousTarget': array([15.00975848, 17.37530495]), 'currentState': array([33.77836838, 18.78113583,  3.58269095]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7820445953230729
running average episode reward sum: 0.6798588153184165
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.27256255, 17.74210037,  2.76634953]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 1.0391718852687455}
episode index:1501
target Thresh 75.9623577118365
target distance 65.0
model initialize at round 1501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.75923796, 15.0107292 ]), 'dynamicTrap': False, 'previousTarget': array([53.28038229, 14.66283393]), 'currentState': array([71.43230921, 18.61215494,  3.77931833]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5528983388011477
running average episode reward sum: 0.6797742877042239
{'scaleFactor': 20, 'currentTarget': array([8., 7.]), 'dynamicTrap': False, 'previousTarget': array([8., 7.]), 'currentState': array([7.72238298, 6.80780532, 5.87562152]), 'targetState': array([8, 7], dtype=int32), 'currentDistance': 0.33765367956632686}
episode index:1502
target Thresh 75.96254545353194
target distance 30.0
model initialize at round 1502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.70270826, 20.71220846]), 'dynamicTrap': False, 'previousTarget': array([40.9007438 , 21.00992562]), 'currentState': array([19.74233363, 21.97055851,  4.83753252]), 'targetState': array([51, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6907655953699098
running average episode reward sum: 0.6797816006168425
{'scaleFactor': 20, 'currentTarget': array([51., 20.]), 'dynamicTrap': False, 'previousTarget': array([51., 20.]), 'currentState': array([51.00752778, 19.64261959,  5.86338729]), 'targetState': array([51, 20], dtype=int32), 'currentDistance': 0.3574596842714397}
episode index:1503
target Thresh 75.96273225886179
target distance 75.0
model initialize at round 1503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.79869141,  7.06801444]), 'dynamicTrap': False, 'previousTarget': array([69.44014327,  8.17276944]), 'currentState': array([88.26400719,  2.47437952,  3.27131867]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5689658224888073
running average episode reward sum: 0.6797079199132999
{'scaleFactor': 20, 'currentTarget': array([14., 20.]), 'dynamicTrap': False, 'previousTarget': array([14., 20.]), 'currentState': array([14.59834545, 19.50612463,  2.79202183]), 'targetState': array([14, 20], dtype=int32), 'currentDistance': 0.7758415802426147}
episode index:1504
target Thresh 75.96291813249616
target distance 75.0
model initialize at round 1504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.96194908, 19.23312156]), 'dynamicTrap': True, 'previousTarget': array([57.95570316, 19.33038021]), 'currentState': array([38.       , 18.       ,  1.2280124], dtype=float32), 'targetState': array([113,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6055964408273565
running average episode reward sum: 0.6796586764056016
{'scaleFactor': 20, 'currentTarget': array([113.,  23.]), 'dynamicTrap': False, 'previousTarget': array([113.,  23.]), 'currentState': array([113.06780622,  22.04580742,   1.55330982]), 'targetState': array([113,  23], dtype=int32), 'currentDistance': 0.9565987479875191}
episode index:1505
target Thresh 75.9631030790819
target distance 15.0
model initialize at round 1505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85., 20.]), 'dynamicTrap': False, 'previousTarget': array([85., 20.]), 'currentState': array([89.49314425,  6.85315588,  1.58232278]), 'targetState': array([85, 20], dtype=int32), 'currentDistance': 13.893446501206983}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.67982627711709
{'scaleFactor': 20, 'currentTarget': array([85., 20.]), 'dynamicTrap': False, 'previousTarget': array([85., 20.]), 'currentState': array([85.57887959, 19.48140346,  1.037136  ]), 'targetState': array([85, 20], dtype=int32), 'currentDistance': 0.777202641151924}
episode index:1506
target Thresh 75.96328710324272
target distance 21.0
model initialize at round 1506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.3758765 ,  7.59694671]), 'dynamicTrap': False, 'previousTarget': array([98.98417253,  7.81486795]), 'currentState': array([113.23027807,  22.02107779,   4.65619001]), 'targetState': array([94,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7465353118541751
running average episode reward sum: 0.6798705432317131
{'scaleFactor': 20, 'currentTarget': array([94.,  2.]), 'dynamicTrap': False, 'previousTarget': array([94.,  2.]), 'currentState': array([93.21907338,  2.7792048 ,  5.43906855]), 'targetState': array([94,  2], dtype=int32), 'currentDistance': 1.1031801774548653}
episode index:1507
target Thresh 75.96347020957919
target distance 8.0
model initialize at round 1507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 16.]), 'currentState': array([11.07819159, 23.02879447,  3.39217901]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 9.292382081352587}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6800503307029785
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 16.]), 'currentState': array([ 4.11199116, 16.56486303,  4.68072331]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 1.0524399974416516}
episode index:1508
target Thresh 75.963652402669
target distance 15.0
model initialize at round 1508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.,  8.]), 'dynamicTrap': False, 'previousTarget': array([74.,  8.]), 'currentState': array([76.19335142, 22.33647623,  3.97657543]), 'targetState': array([74,  8], dtype=int32), 'currentDistance': 14.503287254863203}
done in step count: 19
reward sum = 0.7414241850808717
running average episode reward sum: 0.6800910025746669
{'scaleFactor': 20, 'currentTarget': array([73.91374138,  6.31029831]), 'dynamicTrap': True, 'previousTarget': array([74.,  8.]), 'currentState': array([73.34840795,  5.42639703,  0.54456049]), 'targetState': array([74,  8], dtype=int32), 'currentDistance': 1.0492298870654804}
episode index:1509
target Thresh 75.963833687067
target distance 45.0
model initialize at round 1509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.23147129,  8.47217882]), 'dynamicTrap': False, 'previousTarget': array([50.78571634,  9.55079306]), 'currentState': array([70.25159991,  2.28875685,  3.94632053]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5653079868447874
running average episode reward sum: 0.6800149873324618
{'scaleFactor': 20, 'currentTarget': array([25., 17.]), 'dynamicTrap': False, 'previousTarget': array([25., 17.]), 'currentState': array([25.17763375, 17.12820748,  2.21280553]), 'targetState': array([25, 17], dtype=int32), 'currentDistance': 0.21906826956781913}
episode index:1510
target Thresh 75.96401406730527
target distance 29.0
model initialize at round 1510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.17501016, 14.31740503]), 'dynamicTrap': False, 'previousTarget': array([99.00719668, 14.54725724]), 'currentState': array([114.65600323,   2.98705343,   2.77744627]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7590856993174806
running average episode reward sum: 0.6800673173867205
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([87.53079531, 21.21691735,  4.63633317]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 0.946024360631509}
episode index:1511
target Thresh 75.96419354789334
target distance 12.0
model initialize at round 1511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.00709538, 16.19148857]), 'dynamicTrap': True, 'previousTarget': array([68., 16.]), 'currentState': array([56.      ,  5.      ,  4.576352], dtype=float32), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 16.414010962440297}
done in step count: 13
reward sum = 0.8478200229989679
running average episode reward sum: 0.6801782649433424
{'scaleFactor': 20, 'currentTarget': array([68., 16.]), 'dynamicTrap': False, 'previousTarget': array([68., 16.]), 'currentState': array([67.93891816, 15.40315959,  0.30436428]), 'targetState': array([68, 16], dtype=int32), 'currentDistance': 0.5999578827140255}
episode index:1512
target Thresh 75.96437213331826
target distance 16.0
model initialize at round 1512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 23.]), 'dynamicTrap': False, 'previousTarget': array([73., 23.]), 'currentState': array([87.79528151, 21.21024237,  2.53546685]), 'targetState': array([73, 23], dtype=int32), 'currentDistance': 14.903140187725926}
done in step count: 9
reward sum = 0.9037162474836409
running average episode reward sum: 0.680326009809529
{'scaleFactor': 20, 'currentTarget': array([73., 23.]), 'dynamicTrap': False, 'previousTarget': array([73., 23.]), 'currentState': array([73.68381519, 22.56975675,  1.81417863]), 'targetState': array([73, 23], dtype=int32), 'currentDistance': 0.8079062248091857}
episode index:1513
target Thresh 75.96454982804467
target distance 9.0
model initialize at round 1513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67., 17.]), 'dynamicTrap': False, 'previousTarget': array([67., 17.]), 'currentState': array([70.3456408 ,  9.64709457,  2.37394896]), 'targetState': array([67, 17], dtype=int32), 'currentDistance': 8.078275226220796}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.680504783944331
{'scaleFactor': 20, 'currentTarget': array([67., 17.]), 'dynamicTrap': False, 'previousTarget': array([67., 17.]), 'currentState': array([67.0998654 , 17.63631115,  2.63608363]), 'targetState': array([67, 17], dtype=int32), 'currentDistance': 0.6441001281708364}
episode index:1514
target Thresh 75.96472663651491
target distance 61.0
model initialize at round 1514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.96317697, 11.52915301]), 'dynamicTrap': False, 'previousTarget': array([91.06684958, 10.63386479]), 'currentState': array([109.92505909,  10.29494792,   3.27362549]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.17562716125289035
running average episode reward sum: 0.680171531388099
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([49.78199412, 14.13760436,  5.61808094]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 0.2578013307792894}
episode index:1515
target Thresh 75.96490256314924
target distance 34.0
model initialize at round 1515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.49118855,  9.43516354]), 'dynamicTrap': False, 'previousTarget': array([98.077403  ,  8.75787621]), 'currentState': array([117.47368304,   8.59855514,   2.89886326]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7398677588413776
running average episode reward sum: 0.6802109088468413
{'scaleFactor': 20, 'currentTarget': array([84., 10.]), 'dynamicTrap': False, 'previousTarget': array([84., 10.]), 'currentState': array([84.90456451, 10.28441198,  4.30388879]), 'targetState': array([84, 10], dtype=int32), 'currentDistance': 0.948223137020089}
episode index:1516
target Thresh 75.96507761234581
target distance 26.0
model initialize at round 1516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.82164212, 11.15915504]), 'dynamicTrap': False, 'previousTarget': array([90.98522349, 10.76866244]), 'currentState': array([70.82827464, 11.67418592,  0.66301131]), 'targetState': array([97, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5688515244388925
running average episode reward sum: 0.6801375012104486
{'scaleFactor': 20, 'currentTarget': array([97., 11.]), 'dynamicTrap': False, 'previousTarget': array([97., 11.]), 'currentState': array([96.50258898, 10.05812387,  5.65020153]), 'targetState': array([97, 11], dtype=int32), 'currentDistance': 1.065151803134445}
episode index:1517
target Thresh 75.96525178848088
target distance 12.0
model initialize at round 1517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,  10.]), 'dynamicTrap': False, 'previousTarget': array([102.,  10.]), 'currentState': array([91.68638781, 10.15673135,  0.13356703]), 'targetState': array([102,  10], dtype=int32), 'currentDistance': 10.314803004842247}
done in step count: 65
reward sum = 0.17290095169061748
running average episode reward sum: 0.6798033532858636
{'scaleFactor': 20, 'currentTarget': array([102.,  10.]), 'dynamicTrap': False, 'previousTarget': array([102.,  10.]), 'currentState': array([1.02747314e+02, 1.08037242e+01, 8.21189622e-02]), 'targetState': array([102,  10], dtype=int32), 'currentDistance': 1.0974744047361042}
episode index:1518
target Thresh 75.96542509590884
target distance 1.0
model initialize at round 1518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([89.5310231 , 12.30117502,  4.8449862 ]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 1.5603647397242983}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6800075643765247
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([88.25845292, 11.19983346,  2.8449862 ]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 0.8408712181712452}
episode index:1519
target Thresh 75.96559753896241
target distance 55.0
model initialize at round 1519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.75141015, 12.460223  ]), 'dynamicTrap': False, 'previousTarget': array([63.46369226, 11.60050908]), 'currentState': array([45.21912983,  8.16022004,  6.03382975]), 'targetState': array([99, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5126669741193042
running average episode reward sum: 0.6798974718829345
{'scaleFactor': 20, 'currentTarget': array([98.36562375, 18.51004973]), 'dynamicTrap': True, 'previousTarget': array([99., 20.]), 'currentState': array([98.58898393, 18.90614671,  2.13331514]), 'targetState': array([99, 20], dtype=int32), 'currentDistance': 0.45473353156497925}
episode index:1520
target Thresh 75.96576912195266
target distance 70.0
model initialize at round 1520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.50747173, 10.11298896]), 'dynamicTrap': False, 'previousTarget': array([72.01834208, 11.14364323]), 'currentState': array([91.50241959, 10.56250002,  3.36915445]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5619694459395884
running average episode reward sum: 0.6798199386640368
{'scaleFactor': 20, 'currentTarget': array([20.92537989, 10.44157175]), 'dynamicTrap': True, 'previousTarget': array([22.,  9.]), 'currentState': array([21.89306959, 11.38675852,  4.57018286]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 1.3527015107968752}
episode index:1521
target Thresh 75.96593984916917
target distance 4.0
model initialize at round 1521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  19.]), 'dynamicTrap': False, 'previousTarget': array([116.,  19.]), 'currentState': array([113.39805507,  18.93692588,   5.86360509]), 'targetState': array([116,  19], dtype=int32), 'currentDistance': 2.602709310149572}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6800237363390276
{'scaleFactor': 20, 'currentTarget': array([116.,  19.]), 'dynamicTrap': False, 'previousTarget': array([116.,  19.]), 'currentState': array([115.15171887,  19.46599902,   1.01999986]), 'targetState': array([116,  19], dtype=int32), 'currentDistance': 0.9678512081645253}
episode index:1522
target Thresh 75.96610972488014
target distance 6.0
model initialize at round 1522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 11.]), 'dynamicTrap': False, 'previousTarget': array([44., 11.]), 'currentState': array([46.23434953,  3.93017018,  0.15038305]), 'targetState': array([44, 11], dtype=int32), 'currentDistance': 7.414500086356497}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6802079597623113
{'scaleFactor': 20, 'currentTarget': array([44., 11.]), 'dynamicTrap': False, 'previousTarget': array([44., 11.]), 'currentState': array([44.39982845, 10.44422739,  2.7117556 ]), 'targetState': array([44, 11], dtype=int32), 'currentDistance': 0.6846502648644921}
episode index:1523
target Thresh 75.96627875333247
target distance 36.0
model initialize at round 1523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.71408694,  6.85526223]), 'dynamicTrap': False, 'previousTarget': array([27.4762588 ,  7.66139084]), 'currentState': array([46.39189153, 10.43073515,  3.33269453]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.27257637201236967
running average episode reward sum: 0.6799404849671997
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'dynamicTrap': False, 'previousTarget': array([11.,  4.]), 'currentState': array([10.53109631,  4.64144385,  3.28991948]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.7945570315293047}
episode index:1524
target Thresh 75.96644693875187
target distance 61.0
model initialize at round 1524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.33812619, 13.48192463]), 'dynamicTrap': False, 'previousTarget': array([46.49319579, 14.52615179]), 'currentState': array([27.7595472 , 17.56594771,  6.20947504]), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4730721754580442
running average episode reward sum: 0.679804833616702
{'scaleFactor': 20, 'currentTarget': array([88.,  5.]), 'dynamicTrap': False, 'previousTarget': array([88.,  5.]), 'currentState': array([88.95047101,  4.67831156,  4.09181533]), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 1.0034334060391807}
episode index:1525
target Thresh 75.966614285343
target distance 71.0
model initialize at round 1525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.34456179,  5.42114367]), 'dynamicTrap': False, 'previousTarget': array([73.04940963,  5.59502885]), 'currentState': array([91.29666551,  6.80445747,  2.55283499]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4462808502677612
running average episode reward sum: 0.6796518034834458
{'scaleFactor': 20, 'currentTarget': array([22.,  2.]), 'dynamicTrap': False, 'previousTarget': array([22.,  2.]), 'currentState': array([21.97880576,  2.88219371,  4.34921182]), 'targetState': array([22,  2], dtype=int32), 'currentDistance': 0.8824482591035883}
episode index:1526
target Thresh 75.96678079728952
target distance 30.0
model initialize at round 1526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.13867744, 20.38810137]), 'dynamicTrap': False, 'previousTarget': array([23.32469879, 20.15325301]), 'currentState': array([ 5.95466852, 14.73356321,  0.15394717]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6243509672354006
running average episode reward sum: 0.6796155881355427
{'scaleFactor': 20, 'currentTarget': array([34., 23.]), 'dynamicTrap': False, 'previousTarget': array([34., 23.]), 'currentState': array([33.73936262, 22.62311427,  3.22873208]), 'targetState': array([34, 23], dtype=int32), 'currentDistance': 0.4582299662586468}
episode index:1527
target Thresh 75.96694647875424
target distance 63.0
model initialize at round 1527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.37262941, 16.51077013]), 'dynamicTrap': False, 'previousTarget': array([72.02263725, 16.04869701]), 'currentState': array([90.33590202, 17.72227612,  3.49733198]), 'targetState': array([29, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4828112106976651
running average episode reward sum: 0.679486789459209
{'scaleFactor': 20, 'currentTarget': array([29., 14.]), 'dynamicTrap': False, 'previousTarget': array([29., 14.]), 'currentState': array([29.59802182, 13.41687024,  4.08427344]), 'targetState': array([29, 14], dtype=int32), 'currentDistance': 0.8352666770114147}
episode index:1528
target Thresh 75.9671113338792
target distance 60.0
model initialize at round 1528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.46875241,  8.43937484]), 'dynamicTrap': False, 'previousTarget': array([28.98889814,  9.3337034 ]), 'currentState': array([10.47005258,  8.66742159,  5.3262307 ]), 'targetState': array([69,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6639044926778768
running average episode reward sum: 0.6794765982906142
{'scaleFactor': 20, 'currentTarget': array([69.,  8.]), 'dynamicTrap': False, 'previousTarget': array([69.,  8.]), 'currentState': array([68.54187242,  7.46361395,  1.76618897]), 'targetState': array([69,  8], dtype=int32), 'currentDistance': 0.7054012129249528}
episode index:1529
target Thresh 75.9672753667858
target distance 6.0
model initialize at round 1529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([25.33507284,  4.41919377,  0.54713004]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 6.614313174710625}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6796603364682021
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([25.05047626, 10.77533708,  1.36622911]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.9757401108541203}
episode index:1530
target Thresh 75.96743858157488
target distance 20.0
model initialize at round 1530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'dynamicTrap': False, 'previousTarget': array([24.59715  , 15.8507125]), 'currentState': array([43.52464165, 12.41107437,  2.83196944]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 19.851751029618434}
done in step count: 18
reward sum = 0.7675409094327283
running average episode reward sum: 0.6797177372343448
{'scaleFactor': 20, 'currentTarget': array([24., 16.]), 'dynamicTrap': False, 'previousTarget': array([24., 16.]), 'currentState': array([23.92791968, 16.76817054,  3.05913264]), 'targetState': array([24, 16], dtype=int32), 'currentDistance': 0.7715449088043691}
episode index:1531
target Thresh 75.96760098232679
target distance 57.0
model initialize at round 1531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.92379316,  9.74426663]), 'dynamicTrap': True, 'previousTarget': array([75.95093522,  9.40006563]), 'currentState': array([56.       ,  8.       ,  4.9720497], dtype=float32), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5950844946464602
running average episode reward sum: 0.6796624936034128
{'scaleFactor': 20, 'currentTarget': array([113.,  12.]), 'dynamicTrap': False, 'previousTarget': array([113.,  12.]), 'currentState': array([112.78756265,  12.46202861,   2.24055624]), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 0.5085273530935469}
episode index:1532
target Thresh 75.96776257310157
target distance 37.0
model initialize at round 1532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.02382352, 21.97589616]), 'dynamicTrap': True, 'previousTarget': array([36.02915453, 22.07950516]), 'currentState': array([56.       , 21.       ,  1.1488196], dtype=float32), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7481203593991467
running average episode reward sum: 0.6797071497454843
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'dynamicTrap': False, 'previousTarget': array([19., 23.]), 'currentState': array([18.17781261, 22.93358763,  3.32160065]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 0.824865269791796}
episode index:1533
target Thresh 75.967923357939
target distance 51.0
model initialize at round 1533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.90634246, 15.9332692 ]), 'dynamicTrap': True, 'previousTarget': array([76.9045705 , 15.95142848]), 'currentState': array([57.      , 14.      ,  2.903632], dtype=float32), 'targetState': array([108,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.28908353611566767
running average episode reward sum: 0.6794525059295587
{'scaleFactor': 20, 'currentTarget': array([108.,  19.]), 'dynamicTrap': False, 'previousTarget': array([108.,  19.]), 'currentState': array([107.61297346,  19.18352417,   3.80799577]), 'targetState': array([108,  19], dtype=int32), 'currentDistance': 0.42833475698730283}
episode index:1534
target Thresh 75.96808334085871
target distance 27.0
model initialize at round 1534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.62219463,  7.58042972]), 'dynamicTrap': False, 'previousTarget': array([27.94535509,  7.47743371]), 'currentState': array([9.68278758, 6.0247798 , 5.28790969]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7202776018926411
running average episode reward sum: 0.6794791020832806
{'scaleFactor': 20, 'currentTarget': array([35.,  8.]), 'dynamicTrap': False, 'previousTarget': array([35.,  8.]), 'currentState': array([34.75303844,  8.5433598 ,  5.70772262]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 0.5968499698811215}
episode index:1535
target Thresh 75.96824252586028
target distance 16.0
model initialize at round 1535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.0604656,  8.0050506]), 'dynamicTrap': False, 'previousTarget': array([86.85786438,  8.85786438]), 'currentState': array([100.57679229,  21.76282147,   3.37279177]), 'targetState': array([85,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7869594558780076
running average episode reward sum: 0.679549076271949
{'scaleFactor': 20, 'currentTarget': array([86.098868  ,  8.40988732]), 'dynamicTrap': True, 'previousTarget': array([85.,  7.]), 'currentState': array([85.75798049,  8.68834834,  3.24678179]), 'targetState': array([85,  7], dtype=int32), 'currentDistance': 0.44016454714204717}
episode index:1536
target Thresh 75.96840091692333
target distance 20.0
model initialize at round 1536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'dynamicTrap': False, 'previousTarget': array([10.38838649, 18.9223227 ]), 'currentState': array([28.93886681, 16.60419301,  2.64223528]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 19.08980270628}
done in step count: 14
reward sum = 0.8405004572968984
running average episode reward sum: 0.679653794151601
{'scaleFactor': 20, 'currentTarget': array([10., 19.]), 'dynamicTrap': False, 'previousTarget': array([10., 19.]), 'currentState': array([10.86707615, 18.90266179,  5.09889602]), 'targetState': array([10, 19], dtype=int32), 'currentDistance': 0.8725226527751019}
episode index:1537
target Thresh 75.96855851800767
target distance 56.0
model initialize at round 1537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.567966  ,  8.13457455]), 'dynamicTrap': True, 'previousTarget': array([79.55604828,  8.19058177]), 'currentState': array([60.       ,  4.       ,  2.4783504], dtype=float32), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.611114779471304
running average episode reward sum: 0.6796092304229402
{'scaleFactor': 20, 'currentTarget': array([116.,  16.]), 'dynamicTrap': False, 'previousTarget': array([116.,  16.]), 'currentState': array([115.85814063,  15.96151596,   2.26975089]), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 0.1469867376663532}
episode index:1538
target Thresh 75.96871533305332
target distance 11.0
model initialize at round 1538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,  21.]), 'dynamicTrap': False, 'previousTarget': array([103.,  21.]), 'currentState': array([93.41581029, 20.90987324,  5.84437758]), 'targetState': array([103,  21], dtype=int32), 'currentDistance': 9.584613464335307}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.679785566238065
{'scaleFactor': 20, 'currentTarget': array([103.,  21.]), 'dynamicTrap': False, 'previousTarget': array([103.,  21.]), 'currentState': array([102.01451208,  20.8569909 ,   5.35760352]), 'targetState': array([103,  21], dtype=int32), 'currentDistance': 0.9958102415636503}
episode index:1539
target Thresh 75.96887136598066
target distance 12.0
model initialize at round 1539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.,  5.]), 'dynamicTrap': False, 'previousTarget': array([70.,  5.]), 'currentState': array([70.95687177, 16.00885564,  4.87851202]), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 11.050362261588884}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6799554977855734
{'scaleFactor': 20, 'currentTarget': array([70.,  5.]), 'dynamicTrap': False, 'previousTarget': array([70.,  5.]), 'currentState': array([69.67961863,  4.71584897,  5.18848667]), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 0.428235949622646}
episode index:1540
target Thresh 75.96902662069054
target distance 3.0
model initialize at round 1540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 11.]), 'dynamicTrap': False, 'previousTarget': array([31., 11.]), 'currentState': array([33.9845197 , 10.65711514,  2.50372058]), 'targetState': array([31, 11], dtype=int32), 'currentDistance': 3.0041517736503853}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6801502703373025
{'scaleFactor': 20, 'currentTarget': array([31., 11.]), 'dynamicTrap': False, 'previousTarget': array([31., 11.]), 'currentState': array([30.56465006, 10.67764752,  3.63176116]), 'targetState': array([31, 11], dtype=int32), 'currentDistance': 0.5417016615332823}
episode index:1541
target Thresh 75.96918110106432
target distance 16.0
model initialize at round 1541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.10945909, 19.14233301]), 'dynamicTrap': True, 'previousTarget': array([ 6., 19.]), 'currentState': array([22.      , 14.      ,  3.958878], dtype=float32), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 16.701882508524754}
done in step count: 15
reward sum = 0.8123943936189393
running average episode reward sum: 0.6802360317661491
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.8412981 , 19.35726114,  2.9007803 ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.3909243105803764}
episode index:1542
target Thresh 75.96933481096401
target distance 46.0
model initialize at round 1542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.9878292 , 10.02880762]), 'dynamicTrap': False, 'previousTarget': array([32.11006407, 10.5704125 ]), 'currentState': array([49.66054345,  2.86432979,  2.96426558]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4407507442903467
running average episode reward sum: 0.6800808241916347
{'scaleFactor': 20, 'currentTarget': array([ 5., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 20.]), 'currentState': array([ 5.33065247, 20.75548891,  3.05310792]), 'targetState': array([ 5, 20], dtype=int32), 'currentDistance': 0.8246784546882858}
episode index:1543
target Thresh 75.9694877542324
target distance 30.0
model initialize at round 1543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.29616473,  9.79644236]), 'dynamicTrap': False, 'previousTarget': array([23.97366596, 10.67544468]), 'currentState': array([ 6.07824802, 15.33464399,  5.50892833]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6161271615845111
running average episode reward sum: 0.6800394034256975
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([34.84477269,  7.23120674,  0.73462954]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 0.27848172851297687}
episode index:1544
target Thresh 75.96963993469303
target distance 2.0
model initialize at round 1544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.,  8.]), 'dynamicTrap': False, 'previousTarget': array([86.,  8.]), 'currentState': array([85.51636645, 10.93281884,  3.46758807]), 'targetState': array([86,  8], dtype=int32), 'currentDistance': 2.972427926115219}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6801905217713661
{'scaleFactor': 20, 'currentTarget': array([86.,  8.]), 'dynamicTrap': False, 'previousTarget': array([86.,  8.]), 'currentState': array([86.72435116,  7.51865899,  4.25497412]), 'targetState': array([86,  8], dtype=int32), 'currentDistance': 0.8696975168317731}
episode index:1545
target Thresh 75.96979135615047
target distance 44.0
model initialize at round 1545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.22709134,  8.19491663]), 'dynamicTrap': False, 'previousTarget': array([26.29527642,  7.26234812]), 'currentState': array([6.7545788 , 3.63188529, 0.78875995]), 'targetState': array([51, 14], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 30
reward sum = 0.7120171004900947
running average episode reward sum: 0.6802111081741595
{'scaleFactor': 20, 'currentTarget': array([51., 14.]), 'dynamicTrap': False, 'previousTarget': array([51., 14.]), 'currentState': array([51.00413264, 13.50654926,  0.08728047]), 'targetState': array([51, 14], dtype=int32), 'currentDistance': 0.4934680413829157}
episode index:1546
target Thresh 75.96994202239023
target distance 26.0
model initialize at round 1546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.57283288, 12.02250317]), 'dynamicTrap': False, 'previousTarget': array([41.11558017, 11.88171698]), 'currentState': array([23.78139594,  5.17546747,  6.18731385]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6665645175194818
running average episode reward sum: 0.6802022868485909
{'scaleFactor': 20, 'currentTarget': array([48., 14.]), 'dynamicTrap': False, 'previousTarget': array([48., 14.]), 'currentState': array([48.68469079, 14.14519715,  5.83089229]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 0.6999169195318091}
episode index:1547
target Thresh 75.97009193717899
target distance 15.0
model initialize at round 1547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'dynamicTrap': False, 'previousTarget': array([22.,  6.]), 'currentState': array([13.70513605, 19.09849499,  5.04288018]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 15.504042667876359}
done in step count: 11
reward sum = 0.8758342642587164
running average episode reward sum: 0.6803286640949797
{'scaleFactor': 20, 'currentTarget': array([22.,  6.]), 'dynamicTrap': False, 'previousTarget': array([22.,  6.]), 'currentState': array([22.3186372 ,  6.17744885,  0.46049505]), 'targetState': array([22,  6], dtype=int32), 'currentDistance': 0.3647159985136105}
episode index:1548
target Thresh 75.97024110426462
target distance 28.0
model initialize at round 1548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.56201424, 14.4150197 ]), 'dynamicTrap': False, 'previousTarget': array([11.31144849, 14.48418723]), 'currentState': array([29.11262958, 18.63087549,  2.644086  ]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8084921361032208
running average episode reward sum: 0.6804114035862698
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 13.]), 'currentState': array([ 3.00212231, 12.52475766,  4.87416563]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.47524707512872133}
episode index:1549
target Thresh 75.9703895273763
target distance 26.0
model initialize at round 1549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.20338634, 18.26926298]), 'dynamicTrap': False, 'previousTarget': array([62.88854382, 17.94427191]), 'currentState': array([46.6710132 ,  8.64554788,  5.6211575 ]), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.7588005978051366
running average episode reward sum: 0.6804619772599595
{'scaleFactor': 20, 'currentTarget': array([71., 22.]), 'dynamicTrap': False, 'previousTarget': array([71., 22.]), 'currentState': array([71.63107429, 22.9639491 ,  4.55219105]), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 1.1521513010473248}
episode index:1550
target Thresh 75.97053721022462
target distance 70.0
model initialize at round 1550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.74061976,  9.74834938]), 'dynamicTrap': False, 'previousTarget': array([90.56492592, 10.71994656]), 'currentState': array([109.06652857,   4.59963606,   3.11443806]), 'targetState': array([40, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4579188850666832
running average episode reward sum: 0.6803184936415241
{'scaleFactor': 20, 'currentTarget': array([40., 23.]), 'dynamicTrap': False, 'previousTarget': array([40., 23.]), 'currentState': array([40.05453136, 23.28727397,  2.26278445]), 'targetState': array([40, 23], dtype=int32), 'currentDistance': 0.2924038391658754}
episode index:1551
target Thresh 75.97068415650168
target distance 52.0
model initialize at round 1551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.46334135, 11.92476505]), 'dynamicTrap': False, 'previousTarget': array([89.29298988, 12.58917133]), 'currentState': array([107.20700885,  15.11656606,   3.53923942]), 'targetState': array([57,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6142231778583627
running average episode reward sum: 0.6802759064535194
{'scaleFactor': 20, 'currentTarget': array([58.6784808 ,  7.77078513]), 'dynamicTrap': True, 'previousTarget': array([57.,  7.]), 'currentState': array([58.95562447,  8.24905888,  3.92140435]), 'targetState': array([57,  7], dtype=int32), 'currentDistance': 0.5527697431810847}
episode index:1552
target Thresh 75.9708303698811
target distance 68.0
model initialize at round 1552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.00393761, 13.60315145]), 'dynamicTrap': True, 'previousTarget': array([79., 14.]), 'currentState': array([99.       , 14.       ,  6.2571883], dtype=float32), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.624564735088593
running average episode reward sum: 0.6802400331944306
{'scaleFactor': 20, 'currentTarget': array([31., 14.]), 'dynamicTrap': False, 'previousTarget': array([31., 14.]), 'currentState': array([31.92146412, 14.79550026,  4.47497905]), 'targetState': array([31, 14], dtype=int32), 'currentDistance': 1.2173400498983462}
episode index:1553
target Thresh 75.97097585401826
target distance 69.0
model initialize at round 1553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.36798926,  7.76785909]), 'dynamicTrap': False, 'previousTarget': array([63.34578843,  6.7029674 ]), 'currentState': array([82.08446041,  4.41215274,  3.15599597]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 67
reward sum = 0.29288004303313153
running average episode reward sum: 0.6799907667914955
{'scaleFactor': 20, 'currentTarget': array([14., 16.]), 'dynamicTrap': False, 'previousTarget': array([14., 16.]), 'currentState': array([14.7210189 , 15.34156311,  3.75250997]), 'targetState': array([14, 16], dtype=int32), 'currentDistance': 0.976425821084675}
episode index:1554
target Thresh 75.97112061255027
target distance 8.0
model initialize at round 1554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 21.]), 'currentState': array([ 2.24532113, 14.43499216,  1.05978227]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 9.419395656043449}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6801650428578031
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.85419098, 20.3796269 ,  5.63503194]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.6372778446389342}
episode index:1555
target Thresh 75.97126464909607
target distance 27.0
model initialize at round 1555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.89954626,  9.87167998]), 'dynamicTrap': False, 'previousTarget': array([83.5218472 ,  9.45406225]), 'currentState': array([65.85473801, 18.49634782,  0.21604514]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6678488424169283
running average episode reward sum: 0.6801571275618901
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([92.31154294,  5.5879717 ,  0.88114683]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 0.5165523461054987}
episode index:1556
target Thresh 75.97140796725661
target distance 24.0
model initialize at round 1556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.64152792, 14.89549172]), 'dynamicTrap': False, 'previousTarget': array([31.57960839, 14.92091492]), 'currentState': array([13.31668468, 20.04820301,  5.94553918]), 'targetState': array([36, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7232133887320096
running average episode reward sum: 0.6801847809088202
{'scaleFactor': 20, 'currentTarget': array([36., 14.]), 'dynamicTrap': False, 'previousTarget': array([36., 14.]), 'currentState': array([35.48507457, 13.51094285,  1.88078543]), 'targetState': array([36, 14], dtype=int32), 'currentDistance': 0.7101585048325119}
episode index:1557
target Thresh 75.97155057061484
target distance 36.0
model initialize at round 1557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.53060919,  9.6244905 ]), 'dynamicTrap': False, 'previousTarget': array([89.40285  ,  8.8507125]), 'currentState': array([70.99039123,  5.36070324,  6.21480626]), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.574245696060917
running average episode reward sum: 0.6801167840636032
{'scaleFactor': 20, 'currentTarget': array([106.,  13.]), 'dynamicTrap': False, 'previousTarget': array([106.,  13.]), 'currentState': array([106.69614364,  13.31894641,   1.46915698]), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 0.7657302289383355}
episode index:1558
target Thresh 75.97169246273586
target distance 75.0
model initialize at round 1558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.94593008, 20.40987468]), 'dynamicTrap': False, 'previousTarget': array([64.06369443, 19.40509555]), 'currentState': array([83.84968917, 22.36956199,  2.69010985]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5199368872719605
running average episode reward sum: 0.6800140387802218
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.30347925, 14.80750701,  4.77423094]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.3593789165349927}
episode index:1559
target Thresh 75.97183364716696
target distance 12.0
model initialize at round 1559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  10.]), 'dynamicTrap': False, 'previousTarget': array([113.,  10.]), 'currentState': array([117.27283871,  23.45672767,   3.04378331]), 'targetState': array([113,  10], dtype=int32), 'currentDistance': 14.118805554757149}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6801696353543549
{'scaleFactor': 20, 'currentTarget': array([113.,  10.]), 'dynamicTrap': False, 'previousTarget': array([113.,  10.]), 'currentState': array([113.60667545,  10.12333557,   3.81156936]), 'targetState': array([113,  10], dtype=int32), 'currentDistance': 0.6190854206285721}
episode index:1560
target Thresh 75.97197412743779
target distance 39.0
model initialize at round 1560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.92074114, 14.80512527]), 'dynamicTrap': False, 'previousTarget': array([65.83763329, 15.45671368]), 'currentState': array([47.01969234, 16.79214732,  6.28008711]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5556105023982619
running average episode reward sum: 0.6800898409065931
{'scaleFactor': 20, 'currentTarget': array([85., 13.]), 'dynamicTrap': False, 'previousTarget': array([85., 13.]), 'currentState': array([85.02437279, 12.19212656,  4.16426579]), 'targetState': array([85, 13], dtype=int32), 'currentDistance': 0.8082410132470378}
episode index:1561
target Thresh 75.97211390706033
target distance 17.0
model initialize at round 1561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'dynamicTrap': False, 'previousTarget': array([18.,  4.]), 'currentState': array([33.27144801,  6.33470586,  4.18933678]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 15.448882675457549}
done in step count: 11
reward sum = 0.8766027992856364
running average episode reward sum: 0.6802156494586924
{'scaleFactor': 20, 'currentTarget': array([18.,  4.]), 'dynamicTrap': False, 'previousTarget': array([18.,  4.]), 'currentState': array([17.60493555,  3.51983027,  2.57219969]), 'targetState': array([18,  4], dtype=int32), 'currentDistance': 0.6218029337223747}
episode index:1562
target Thresh 75.9722529895291
target distance 48.0
model initialize at round 1562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.44769955, 21.60694992]), 'dynamicTrap': False, 'previousTarget': array([95.06908484, 20.6609096 ]), 'currentState': array([115.42376314,  20.6287458 ,   2.32626194]), 'targetState': array([67, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7082536422403579
running average episode reward sum: 0.6802335880337286
{'scaleFactor': 20, 'currentTarget': array([67., 23.]), 'dynamicTrap': False, 'previousTarget': array([67., 23.]), 'currentState': array([67.36120781, 22.12402127,  4.00910429]), 'targetState': array([67, 23], dtype=int32), 'currentDistance': 0.9475282626321017}
episode index:1563
target Thresh 75.97239137832116
target distance 58.0
model initialize at round 1563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.76555568, 10.58151116]), 'dynamicTrap': False, 'previousTarget': array([95.63706177, 10.00765644]), 'currentState': array([113.13991993,   5.61824514,   3.17106229]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5841095218857755
running average episode reward sum: 0.6801721276333782
{'scaleFactor': 20, 'currentTarget': array([57., 20.]), 'dynamicTrap': False, 'previousTarget': array([57., 20.]), 'currentState': array([56.6685818 , 20.53502172,  1.91744669]), 'targetState': array([57, 20], dtype=int32), 'currentDistance': 0.6293538460391263}
episode index:1564
target Thresh 75.97252907689625
target distance 68.0
model initialize at round 1564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.08666072, 11.14018318]), 'dynamicTrap': True, 'previousTarget': array([66.13698791, 10.66317505]), 'currentState': array([86.      , 13.      ,  5.574299], dtype=float32), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.3596629143173752
running average episode reward sum: 0.679967329414007
{'scaleFactor': 20, 'currentTarget': array([16.70421522,  5.76986304]), 'dynamicTrap': True, 'previousTarget': array([18.,  5.]), 'currentState': array([16.61580912,  5.59711246,  4.75709406]), 'targetState': array([18,  5], dtype=int32), 'currentDistance': 0.1940577278241226}
episode index:1565
target Thresh 75.97266608869683
target distance 14.0
model initialize at round 1565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.,  9.]), 'dynamicTrap': False, 'previousTarget': array([87.,  9.]), 'currentState': array([99.3786786 ,  4.78164902,  2.30467963]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 13.077697383966509}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6801283115458671
{'scaleFactor': 20, 'currentTarget': array([87.,  9.]), 'dynamicTrap': False, 'previousTarget': array([87.,  9.]), 'currentState': array([87.23413154,  9.31312611,  2.21540728]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 0.39098022505213864}
episode index:1566
target Thresh 75.9728024171482
target distance 28.0
model initialize at round 1566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.72116289,  9.20122841]), 'dynamicTrap': False, 'previousTarget': array([99.16516177,  9.27327206]), 'currentState': array([116.19619935,  16.86106053,   2.75732949]), 'targetState': array([90,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7753040724642051
running average episode reward sum: 0.680189049108674
{'scaleFactor': 20, 'currentTarget': array([90.,  6.]), 'dynamicTrap': False, 'previousTarget': array([90.,  6.]), 'currentState': array([89.16596817,  6.51849154,  3.81196542]), 'targetState': array([90,  6], dtype=int32), 'currentDistance': 0.9820603760221747}
episode index:1567
target Thresh 75.97293806565858
target distance 23.0
model initialize at round 1567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.02030117,   8.46510552]), 'dynamicTrap': False, 'previousTarget': array([107.92481176,   8.26740767]), 'currentState': array([88.25958287, 11.54958476,  0.39481974]), 'targetState': array([111,   8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 19
reward sum = 0.8074331688625067
running average episode reward sum: 0.6802701996952516
{'scaleFactor': 20, 'currentTarget': array([111.,   8.]), 'dynamicTrap': False, 'previousTarget': array([111.,   8.]), 'currentState': array([111.67180217,   7.58617433,   5.98191966]), 'targetState': array([111,   8], dtype=int32), 'currentDistance': 0.7890309508951636}
episode index:1568
target Thresh 75.97307303761919
target distance 27.0
model initialize at round 1568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.94536367, 11.91393687]), 'dynamicTrap': False, 'previousTarget': array([40.79416933, 11.80395219]), 'currentState': array([58.13571894, 22.13605435,  0.88477703]), 'targetState': array([31,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.776990682679529
running average episode reward sum: 0.6803318443625456
{'scaleFactor': 20, 'currentTarget': array([31.,  6.]), 'dynamicTrap': False, 'previousTarget': array([31.,  6.]), 'currentState': array([30.4032116 ,  6.34818496,  3.15437463]), 'targetState': array([31,  6], dtype=int32), 'currentDistance': 0.6909335421145881}
episode index:1569
target Thresh 75.97320733640434
target distance 21.0
model initialize at round 1569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.30312113,  11.69556494]), 'dynamicTrap': False, 'previousTarget': array([108.6897547 ,  11.11990655]), 'currentState': array([91.97563661,  3.68911061,  6.13971224]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8402151506759682
running average episode reward sum: 0.6804336808633821
{'scaleFactor': 20, 'currentTarget': array([111.,  12.]), 'dynamicTrap': False, 'previousTarget': array([111.,  12.]), 'currentState': array([110.78564479,  11.62168735,   1.23954492]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 0.4348202146106234}
episode index:1570
target Thresh 75.9733409653715
target distance 44.0
model initialize at round 1570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.03275128, 13.85554082]), 'dynamicTrap': True, 'previousTarget': array([91.18035416, 14.33307718]), 'currentState': array([72.        , 20.        ,  0.38220748], dtype=float32), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5918565353397479
running average episode reward sum: 0.6803772982118712
{'scaleFactor': 20, 'currentTarget': array([116.,   7.]), 'dynamicTrap': False, 'previousTarget': array([116.,   7.]), 'currentState': array([1.16297325e+02, 6.56689084e+00, 1.03233927e-01]), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 0.5253432185277523}
episode index:1571
target Thresh 75.97347392786143
target distance 13.0
model initialize at round 1571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'dynamicTrap': False, 'previousTarget': array([24., 21.]), 'currentState': array([30.29423969,  9.64445787,  2.30814058]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 12.983288893508943}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.680531475944833
{'scaleFactor': 20, 'currentTarget': array([24., 21.]), 'dynamicTrap': False, 'previousTarget': array([24., 21.]), 'currentState': array([23.76143395, 21.61186388,  2.35267775]), 'targetState': array([24, 21], dtype=int32), 'currentDistance': 0.6567276166811483}
episode index:1572
target Thresh 75.97360622719815
target distance 62.0
model initialize at round 1572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.24975248, 12.22159313]), 'dynamicTrap': False, 'previousTarget': array([88.49117996, 11.59478257]), 'currentState': array([106.66262036,  17.03205682,   2.59000582]), 'targetState': array([46,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.21522708023313047
running average episode reward sum: 0.6802356689545521
{'scaleFactor': 20, 'currentTarget': array([46.,  2.]), 'dynamicTrap': False, 'previousTarget': array([46.,  2.]), 'currentState': array([46.42659116,  2.3518387 ,  5.81818555]), 'targetState': array([46,  2], dtype=int32), 'currentDistance': 0.5529651800867658}
episode index:1573
target Thresh 75.97373786668918
target distance 7.0
model initialize at round 1573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   6.]), 'dynamicTrap': False, 'previousTarget': array([118.,   6.]), 'currentState': array([113.28558656,  13.20675364,   3.95571208]), 'targetState': array([118,   6], dtype=int32), 'currentDistance': 8.611793779575288}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6804076857150003
{'scaleFactor': 20, 'currentTarget': array([118.,   6.]), 'dynamicTrap': False, 'previousTarget': array([118.,   6.]), 'currentState': array([117.5446654 ,   5.57008002,   6.07265311]), 'targetState': array([118,   6], dtype=int32), 'currentDistance': 0.6262274252938911}
episode index:1574
target Thresh 75.97386884962552
target distance 30.0
model initialize at round 1574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.64070268,  9.23811132]), 'dynamicTrap': False, 'previousTarget': array([33.11145618, 10.05572809]), 'currentState': array([50.8792529 , 17.44513792,  3.68053389]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6945545310651814
running average episode reward sum: 0.6804166678390322
{'scaleFactor': 20, 'currentTarget': array([20.79811889,  5.99001854]), 'dynamicTrap': True, 'previousTarget': array([20.71209076,  5.8276607 ]), 'currentState': array([21.75752002,  5.42905361,  5.76700835]), 'targetState': array([21,  4], dtype=int32), 'currentDistance': 1.1113650089239175}
episode index:1575
target Thresh 75.97399917928172
target distance 56.0
model initialize at round 1575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.41030187,  5.7803981 ]), 'dynamicTrap': False, 'previousTarget': array([25.99681199,  6.35708593]), 'currentState': array([7.42272234, 5.07565387, 0.03418582]), 'targetState': array([62,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.624248525856328
running average episode reward sum: 0.680381028155033
{'scaleFactor': 20, 'currentTarget': array([62.,  7.]), 'dynamicTrap': False, 'previousTarget': array([62.,  7.]), 'currentState': array([62.53148353,  7.01022989,  5.53416711]), 'targetState': array([62,  7], dtype=int32), 'currentDistance': 0.5315819733566913}
episode index:1576
target Thresh 75.97412885891606
target distance 46.0
model initialize at round 1576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.10920208, 10.74792019]), 'dynamicTrap': False, 'previousTarget': array([29.75381194, 11.5608599 ]), 'currentState': array([47.42388068, 15.93860325,  3.17733443]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6554130429065592
running average episode reward sum: 0.6803651955708551
{'scaleFactor': 20, 'currentTarget': array([3., 4.]), 'dynamicTrap': False, 'previousTarget': array([3., 4.]), 'currentState': array([2.29910172, 4.26517005, 3.77131956]), 'targetState': array([3, 4], dtype=int32), 'currentDistance': 0.7493821155533126}
episode index:1577
target Thresh 75.97425789177053
target distance 50.0
model initialize at round 1577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.70954885,  9.05133087]), 'dynamicTrap': False, 'previousTarget': array([64.38838649,  9.9223227 ]), 'currentState': array([83.18398557,  4.49651861,  3.2524426 ]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5872483334361519
running average episode reward sum: 0.6803061861525188
{'scaleFactor': 20, 'currentTarget': array([35.30879735, 17.51101842]), 'dynamicTrap': True, 'previousTarget': array([34., 16.]), 'currentState': array([36.23144136, 18.36869867,  3.94521383]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 1.259717188363437}
episode index:1578
target Thresh 75.97438628107095
target distance 38.0
model initialize at round 1578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.83586958, 10.52203799]), 'dynamicTrap': False, 'previousTarget': array([90.46160575, 11.39067232]), 'currentState': array([71.20156429, 14.32914396,  5.84244418]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7594197758527338
running average episode reward sum: 0.6803562897558755
{'scaleFactor': 20, 'currentTarget': array([109.,   7.]), 'dynamicTrap': False, 'previousTarget': array([109.,   7.]), 'currentState': array([109.34683635,   6.86197069,   5.67874731]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 0.3732928403973096}
episode index:1579
target Thresh 75.97451403002707
target distance 14.0
model initialize at round 1579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([12.85608814, 12.53656104,  6.23059511]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 16.800177792396795}
done in step count: 16
reward sum = 0.78495570670268
running average episode reward sum: 0.6804224919185
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([26.75471025, 23.1576215 ,  0.36027989]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.7709942232823072}
episode index:1580
target Thresh 75.97464114183259
target distance 25.0
model initialize at round 1580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.75780931, 18.72333256]), 'dynamicTrap': False, 'previousTarget': array([90.85753677, 18.38290441]), 'currentState': array([70.78560544, 17.66925824,  0.68853402]), 'targetState': array([96, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7017640864126337
running average episode reward sum: 0.6804359907132465
{'scaleFactor': 20, 'currentTarget': array([94.12118008, 18.95077735]), 'dynamicTrap': True, 'previousTarget': array([96., 19.]), 'currentState': array([9.33272114e+01, 1.85238516e+01, 3.71405214e-02]), 'targetState': array([96, 19], dtype=int32), 'currentDistance': 0.9014720600062053}
episode index:1581
target Thresh 75.97476761966536
target distance 32.0
model initialize at round 1581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.27026533,  9.8895339 ]), 'dynamicTrap': False, 'previousTarget': array([31.03894843,  9.75243428]), 'currentState': array([49.19566744, 11.61532385,  2.63441998]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6734299471800164
running average episode reward sum: 0.6804315621143
{'scaleFactor': 20, 'currentTarget': array([19.,  9.]), 'dynamicTrap': False, 'previousTarget': array([20.60581369, 10.98040706]), 'currentState': array([19.32787511,  9.93536987,  3.90791524]), 'targetState': array([19,  9], dtype=int32), 'currentDistance': 0.9911704564541737}
episode index:1582
target Thresh 75.97489346668729
target distance 21.0
model initialize at round 1582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.,  6.]), 'dynamicTrap': False, 'previousTarget': array([85.97736275,  5.95130299]), 'currentState': array([67.1299591 ,  6.17399836,  6.2109533 ]), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 19.870802722235204}
done in step count: 21
reward sum = 0.7197054899151341
running average episode reward sum: 0.6804563719233973
{'scaleFactor': 20, 'currentTarget': array([87.,  6.]), 'dynamicTrap': False, 'previousTarget': array([87.,  6.]), 'currentState': array([86.64447618,  5.60528767,  1.88266425]), 'targetState': array([87,  6], dtype=int32), 'currentDistance': 0.5312203027225053}
episode index:1583
target Thresh 75.97501868604459
target distance 40.0
model initialize at round 1583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.23167086, 15.04240803]), 'dynamicTrap': False, 'previousTarget': array([66.0992562 , 15.00992562]), 'currentState': array([84.10734322, 17.26898578,  2.47698474]), 'targetState': array([46, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6911032110485675
running average episode reward sum: 0.680463093412744
{'scaleFactor': 20, 'currentTarget': array([46., 13.]), 'dynamicTrap': False, 'previousTarget': array([46., 13.]), 'currentState': array([45.56296754, 13.19104786,  4.94297815]), 'targetState': array([46, 13], dtype=int32), 'currentDistance': 0.47696609385144006}
episode index:1584
target Thresh 75.97514328086774
target distance 17.0
model initialize at round 1584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([90.43731085, 12.76413481,  3.59805524]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 17.751637255871234}
done in step count: 17
reward sum = 0.7874620011018292
running average episode reward sum: 0.6805306006100241
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([74.10546991,  4.59063164,  4.7730329 ]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 1.0719280815265215}
episode index:1585
target Thresh 75.97526725427161
target distance 53.0
model initialize at round 1585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.23274823, 16.89463485]), 'dynamicTrap': False, 'previousTarget': array([52.96803691, 16.13026624]), 'currentState': array([34.24484468, 16.19914088,  6.11865551]), 'targetState': array([86, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.25272836412125194
running average episode reward sum: 0.6802608640170299
{'scaleFactor': 20, 'currentTarget': array([86., 18.]), 'dynamicTrap': False, 'previousTarget': array([86., 18.]), 'currentState': array([85.61472277, 17.60693252,  2.98027012]), 'targetState': array([86, 18], dtype=int32), 'currentDistance': 0.5504003932344868}
episode index:1586
target Thresh 75.97539060935556
target distance 56.0
model initialize at round 1586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.2124593 ,  6.87336031]), 'dynamicTrap': False, 'previousTarget': array([64.15444247,  6.48069469]), 'currentState': array([82.06854067,  4.47835874,  2.72211581]), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7028527958142405
running average episode reward sum: 0.6802750996388304
{'scaleFactor': 20, 'currentTarget': array([28., 11.]), 'dynamicTrap': False, 'previousTarget': array([28., 11.]), 'currentState': array([28.01667299, 11.30339664,  5.56323874]), 'targetState': array([28, 11], dtype=int32), 'currentDistance': 0.30385442270134855}
episode index:1587
target Thresh 75.97551334920344
target distance 67.0
model initialize at round 1587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.50456644, 11.88534613]), 'dynamicTrap': False, 'previousTarget': array([49.63383135, 13.19045063]), 'currentState': array([30.78612827, 15.22947475,  5.22379035]), 'targetState': array([97,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.559469302641879
running average episode reward sum: 0.6801990254593613
{'scaleFactor': 20, 'currentTarget': array([95.31394441,  3.24869202]), 'dynamicTrap': True, 'previousTarget': array([97.,  4.]), 'currentState': array([94.77848307,  3.29372458,  1.10042888]), 'targetState': array([97,  4], dtype=int32), 'currentDistance': 0.5373516365887865}
episode index:1588
target Thresh 75.9756354768838
target distance 10.0
model initialize at round 1588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'dynamicTrap': False, 'previousTarget': array([2., 7.]), 'currentState': array([ 1.74894842, 15.44043937,  4.08127426]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 8.444172173300833}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6803694414596385
{'scaleFactor': 20, 'currentTarget': array([2., 7.]), 'dynamicTrap': False, 'previousTarget': array([2., 7.]), 'currentState': array([1.91173477, 6.83662095, 3.51633211]), 'targetState': array([2, 7], dtype=int32), 'currentDistance': 0.18569724366704046}
episode index:1589
target Thresh 75.9757569954498
target distance 28.0
model initialize at round 1589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.10060237,  9.56336818]), 'dynamicTrap': False, 'previousTarget': array([17.38497013,  9.31304745]), 'currentState': array([34.51222769,  1.75234692,  2.29653931]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7744982137715343
running average episode reward sum: 0.6804286419453692
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([ 7.36385997, 13.50799431,  4.07262855]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.8140837495085104}
episode index:1590
target Thresh 75.97587790793943
target distance 8.0
model initialize at round 1590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  14.]), 'dynamicTrap': False, 'previousTarget': array([101.,  14.]), 'currentState': array([102.27548127,  22.04292271,   4.05402517]), 'targetState': array([101,  14], dtype=int32), 'currentDistance': 8.143430369846804}
done in step count: 11
reward sum = 0.8389312297665209
running average episode reward sum: 0.6805282664505994
{'scaleFactor': 20, 'currentTarget': array([101.,  14.]), 'dynamicTrap': False, 'previousTarget': array([101.,  14.]), 'currentState': array([101.2990205 ,  13.96867221,   1.14750752]), 'targetState': array([101,  14], dtype=int32), 'currentDistance': 0.30065709690816933}
episode index:1591
target Thresh 75.9759982173755
target distance 37.0
model initialize at round 1591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.82996139, 18.60242794]), 'dynamicTrap': True, 'previousTarget': array([79.88414095, 18.14963686]), 'currentState': array([60.       , 16.       ,  4.8487887], dtype=float32), 'targetState': array([97, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5644336798770997
running average episode reward sum: 0.6804553427153146
{'scaleFactor': 20, 'currentTarget': array([97., 20.]), 'dynamicTrap': False, 'previousTarget': array([97., 20.]), 'currentState': array([97.63196285, 19.83844715,  1.64487754]), 'targetState': array([97, 20], dtype=int32), 'currentDistance': 0.65228549914012}
episode index:1592
target Thresh 75.97611792676575
target distance 31.0
model initialize at round 1592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.39825308,  11.58219018]), 'dynamicTrap': False, 'previousTarget': array([102.00318503,  10.71121856]), 'currentState': array([84.02437126,  3.68279355,  0.54631472]), 'targetState': array([115,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7149478681202843
running average episode reward sum: 0.6804769952736354
{'scaleFactor': 20, 'currentTarget': array([115.,  17.]), 'dynamicTrap': False, 'previousTarget': array([115.,  17.]), 'currentState': array([115.80585517,  16.50161983,   0.72987545]), 'targetState': array([115,  17], dtype=int32), 'currentDistance': 0.947515359570743}
episode index:1593
target Thresh 75.97623703910293
target distance 56.0
model initialize at round 1593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.2911406 , 14.50395773]), 'dynamicTrap': False, 'previousTarget': array([63.481944  , 14.47740586]), 'currentState': array([43.80712438, 19.0176161 ,  3.15151138]), 'targetState': array([100,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5291613639364918
running average episode reward sum: 0.6803820670231102
{'scaleFactor': 20, 'currentTarget': array([100.,   6.]), 'dynamicTrap': False, 'previousTarget': array([100.,   6.]), 'currentState': array([99.68760567,  6.41782389,  0.18657977]), 'targetState': array([100,   6], dtype=int32), 'currentDistance': 0.5216962912446489}
episode index:1594
target Thresh 75.97635555736484
target distance 12.0
model initialize at round 1594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.,  8.]), 'dynamicTrap': False, 'previousTarget': array([45.,  8.]), 'currentState': array([51.6118128 , 21.41493823,  2.84855741]), 'targetState': array([45,  8], dtype=int32), 'currentDistance': 14.95582282302483}
done in step count: 13
reward sum = 0.8503886472314562
running average episode reward sum: 0.680488654220733
{'scaleFactor': 20, 'currentTarget': array([45.,  8.]), 'dynamicTrap': False, 'previousTarget': array([45.,  8.]), 'currentState': array([45.34784532,  7.28561925,  0.68173692]), 'targetState': array([45,  8], dtype=int32), 'currentDistance': 0.7945666867788288}
episode index:1595
target Thresh 75.97647348451447
target distance 25.0
model initialize at round 1595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.81837402, 16.5496783 ]), 'dynamicTrap': False, 'previousTarget': array([75.06369443, 16.40509555]), 'currentState': array([93.61430668, 19.39942393,  2.04452395]), 'targetState': array([70, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6805747559020465
{'scaleFactor': 20, 'currentTarget': array([70., 16.]), 'dynamicTrap': False, 'previousTarget': array([70., 16.]), 'currentState': array([69.94586007, 15.98032283,  3.9041275 ]), 'targetState': array([70, 16], dtype=int32), 'currentDistance': 0.05760488708654843}
episode index:1596
target Thresh 75.97659082349998
target distance 61.0
model initialize at round 1596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.96579205, 19.23800948]), 'dynamicTrap': False, 'previousTarget': array([52.01074114, 18.34461446]), 'currentState': array([70.93450771, 20.35622015,  3.17465246]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6322446649214067
running average episode reward sum: 0.6805444928519648
{'scaleFactor': 20, 'currentTarget': array([11., 17.]), 'dynamicTrap': False, 'previousTarget': array([11., 17.]), 'currentState': array([10.6421282 , 16.40535275,  3.96024855]), 'targetState': array([11, 17], dtype=int32), 'currentDistance': 0.6940299564446563}
episode index:1597
target Thresh 75.97670757725484
target distance 13.0
model initialize at round 1597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 14.]), 'dynamicTrap': False, 'previousTarget': array([91., 14.]), 'currentState': array([79.55571379, 19.07075987,  0.23098069]), 'targetState': array([91, 14], dtype=int32), 'currentDistance': 12.51735964275703}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6807018901329754
{'scaleFactor': 20, 'currentTarget': array([91., 14.]), 'dynamicTrap': False, 'previousTarget': array([91., 14.]), 'currentState': array([90.78547926, 14.04626991,  5.1003421 ]), 'targetState': array([91, 14], dtype=int32), 'currentDistance': 0.2194539829039992}
episode index:1598
target Thresh 75.97682374869794
target distance 42.0
model initialize at round 1598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.76343509, 21.51449528]), 'dynamicTrap': False, 'previousTarget': array([53.02263725, 21.95130299]), 'currentState': array([71.71244493, 20.08725801,  2.74826384]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6807675413216397
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([31.19171772, 22.48879304,  3.12821668]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 0.5459745780957685}
episode index:1599
target Thresh 75.97693934073354
target distance 31.0
model initialize at round 1599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.7339215 ,  8.44777203]), 'dynamicTrap': False, 'previousTarget': array([43.74482241,  7.81535122]), 'currentState': array([24.18989904, 12.69408644,  2.41763335]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6303261291703075
running average episode reward sum: 0.6807360154390452
{'scaleFactor': 20, 'currentTarget': array([55.,  6.]), 'dynamicTrap': False, 'previousTarget': array([55.,  6.]), 'currentState': array([54.04529122,  5.89297269,  0.50696128]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 0.9606891838255482}
episode index:1600
target Thresh 75.97705435625147
target distance 28.0
model initialize at round 1600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.72144536, 9.7931425 ]), 'dynamicTrap': False, 'previousTarget': array([11.050826,  9.424941]), 'currentState': array([29.71198062,  9.17791883,  2.81466901]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6812057354931946
running average episode reward sum: 0.6807363088307092
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 10.]), 'currentState': array([2.93033952, 9.41135537, 2.98258366]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.5927521261417932}
episode index:1601
target Thresh 75.9771687981271
target distance 41.0
model initialize at round 1601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.41629765, 12.99796421]), 'dynamicTrap': False, 'previousTarget': array([28.21754296, 13.12836937]), 'currentState': array([45.03837295, 20.29305239,  2.76758736]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6916285252792821
running average episode reward sum: 0.6807431079670692
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'dynamicTrap': False, 'previousTarget': array([6., 5.]), 'currentState': array([5.31384422, 4.67931629, 1.80266306]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.7573953990570151}
episode index:1602
target Thresh 75.9772826692215
target distance 44.0
model initialize at round 1602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.42465476, 11.86289405]), 'dynamicTrap': False, 'previousTarget': array([67.0585156 , 12.06407315]), 'currentState': array([49.51909309,  5.33757886,  0.52029055]), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5886521132284019
running average episode reward sum: 0.6806856588125223
{'scaleFactor': 20, 'currentTarget': array([92., 20.]), 'dynamicTrap': False, 'previousTarget': array([92., 20.]), 'currentState': array([92.24406161, 19.00641893,  1.04395096]), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 1.0231174917286046}
episode index:1603
target Thresh 75.97739597238144
target distance 63.0
model initialize at round 1603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.58547713, 15.12238857]), 'dynamicTrap': False, 'previousTarget': array([65.90990945, 16.10381815]), 'currentState': array([46.63945104, 16.59073572,  6.14835548]), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5544120790847257
running average episode reward sum: 0.6806069346356346
{'scaleFactor': 20, 'currentTarget': array([109.,  12.]), 'dynamicTrap': False, 'previousTarget': array([109.,  12.]), 'currentState': array([109.68898673,  12.61454239,   1.25390842]), 'targetState': array([109,  12], dtype=int32), 'currentDistance': 0.9232361872166513}
episode index:1604
target Thresh 75.97750871043952
target distance 11.0
model initialize at round 1604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 17.]), 'dynamicTrap': False, 'previousTarget': array([44., 17.]), 'currentState': array([53.21255511,  7.88074927,  2.53376341]), 'targetState': array([44, 17], dtype=int32), 'currentDistance': 12.962712122255311}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6807636065442149
{'scaleFactor': 20, 'currentTarget': array([44., 17.]), 'dynamicTrap': False, 'previousTarget': array([44., 17.]), 'currentState': array([44.42588913, 17.46468113,  3.26430565]), 'targetState': array([44, 17], dtype=int32), 'currentDistance': 0.6303253966075353}
episode index:1605
target Thresh 75.97762088621418
target distance 32.0
model initialize at round 1605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.37803374, 21.03009317]), 'dynamicTrap': False, 'previousTarget': array([43.08731548, 20.86681417]), 'currentState': array([61.29125842, 19.16904964,  2.09155488]), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7484767976977886
running average episode reward sum: 0.6808057691788061
{'scaleFactor': 20, 'currentTarget': array([31., 22.]), 'dynamicTrap': False, 'previousTarget': array([31., 22.]), 'currentState': array([31.79740208, 21.35086447,  3.42596724]), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 1.0282154524958749}
episode index:1606
target Thresh 75.97773250250985
target distance 8.0
model initialize at round 1606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41., 23.]), 'dynamicTrap': False, 'previousTarget': array([41., 23.]), 'currentState': array([49.06108122, 21.64162688,  2.54360574]), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 8.174729843961025}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6809738987872199
{'scaleFactor': 20, 'currentTarget': array([41., 23.]), 'dynamicTrap': False, 'previousTarget': array([41., 23.]), 'currentState': array([40.32702306, 23.33306926,  2.70229411]), 'targetState': array([41, 23], dtype=int32), 'currentDistance': 0.7508882024954936}
episode index:1607
target Thresh 75.9778435621169
target distance 57.0
model initialize at round 1607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.22795898, 17.00570647]), 'dynamicTrap': False, 'previousTarget': array([50.5709957 , 17.87979038]), 'currentState': array([32.60081226, 20.84954721,  5.9569031 ]), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5881477099433304
running average episode reward sum: 0.6809161710578394
{'scaleFactor': 20, 'currentTarget': array([88., 10.]), 'dynamicTrap': False, 'previousTarget': array([88., 10.]), 'currentState': array([87.44198649, 10.5068186 ,  1.66382782]), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 0.7538197186507212}
episode index:1608
target Thresh 75.97795406781185
target distance 43.0
model initialize at round 1608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.65961081,  8.99383278]), 'dynamicTrap': False, 'previousTarget': array([48.97840172,  9.07077201]), 'currentState': array([30.68126372,  9.92423457,  5.22815121]), 'targetState': array([72,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7103367285640845
running average episode reward sum: 0.6809344560531819
{'scaleFactor': 20, 'currentTarget': array([72.,  8.]), 'dynamicTrap': False, 'previousTarget': array([72.,  8.]), 'currentState': array([71.99687363,  8.40161491,  1.02136044]), 'targetState': array([72,  8], dtype=int32), 'currentDistance': 0.4016270740965161}
episode index:1609
target Thresh 75.97806402235736
target distance 25.0
model initialize at round 1609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.90632274, 10.27414145]), 'dynamicTrap': False, 'previousTarget': array([15.06369443, 10.59490445]), 'currentState': array([34.69097854,  7.34713023,  3.6042763 ]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8067560340134267
running average episode reward sum: 0.6810126061016045
{'scaleFactor': 20, 'currentTarget': array([10., 11.]), 'dynamicTrap': False, 'previousTarget': array([10., 11.]), 'currentState': array([ 9.84718314, 10.69271447,  3.16234208]), 'targetState': array([10, 11], dtype=int32), 'currentDistance': 0.3431871062936774}
episode index:1610
target Thresh 75.97817342850229
target distance 36.0
model initialize at round 1610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.66350728, 16.15830756]), 'dynamicTrap': False, 'previousTarget': array([85.36769221, 15.81739318]), 'currentState': array([106.40960401,  12.98156962,   1.55862504]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7046274092186171
running average episode reward sum: 0.6810272645765375
{'scaleFactor': 20, 'currentTarget': array([69., 19.]), 'dynamicTrap': False, 'previousTarget': array([69., 19.]), 'currentState': array([69.37797225, 19.86810522,  4.19817151]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 0.9468208333172256}
episode index:1611
target Thresh 75.97828228898177
target distance 24.0
model initialize at round 1611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.19271856, 15.13301342]), 'dynamicTrap': False, 'previousTarget': array([13.63557441, 14.80368799]), 'currentState': array([30.2157406 ,  3.16377097,  1.77356118]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6515617844451861
running average episode reward sum: 0.6810089857427091
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.18219146, 21.95760832,  3.04264364]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.18705823044354947}
episode index:1612
target Thresh 75.97839060651738
target distance 27.0
model initialize at round 1612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9.04156669, 6.81729192]), 'dynamicTrap': False, 'previousTarget': array([11.01370332,  6.74023321]), 'currentState': array([29.02844599,  6.09296068,  3.37851256]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7624181368183667
running average episode reward sum: 0.6810594563881373
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.24113105, 7.2058329 , 2.31857951]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.7862882805708946}
episode index:1613
target Thresh 75.97849838381701
target distance 40.0
model initialize at round 1613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.77777107, 19.8825952 ]), 'dynamicTrap': False, 'previousTarget': array([32.9007438 , 19.00992562]), 'currentState': array([12.97791927, 22.70498229,  0.61146343]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7700431458051552
running average episode reward sum: 0.6811145887855455
{'scaleFactor': 20, 'currentTarget': array([52.23398499, 15.19054905]), 'dynamicTrap': True, 'previousTarget': array([53., 17.]), 'currentState': array([52.82739527, 15.59657702,  1.49238353]), 'targetState': array([53, 17], dtype=int32), 'currentDistance': 0.719023275887758}
episode index:1614
target Thresh 75.9786056235751
target distance 1.0
model initialize at round 1614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.12062173, 16.15944272]), 'dynamicTrap': True, 'previousTarget': array([80., 16.]), 'currentState': array([81.       , 17.       ,  4.0190134], dtype=float32), 'targetState': array([80, 16], dtype=int32), 'currentDistance': 1.2164878504100765}
done in step count: 0
reward sum = 0.99
running average episode reward sum: 0.681305849102087
{'scaleFactor': 20, 'currentTarget': array([80.12062173, 16.15944272]), 'dynamicTrap': True, 'previousTarget': array([80., 16.]), 'currentState': array([81.       , 17.       ,  4.0190134], dtype=float32), 'targetState': array([80, 16], dtype=int32), 'currentDistance': 1.2164878504100765}
episode index:1615
target Thresh 75.97871232847268
target distance 16.0
model initialize at round 1615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.,  5.]), 'dynamicTrap': False, 'previousTarget': array([44.,  5.]), 'currentState': array([40.57447291, 21.98691377,  3.45763624]), 'targetState': array([44,  5], dtype=int32), 'currentDistance': 17.32886249322073}
done in step count: 19
reward sum = 0.7434350822685504
running average episode reward sum: 0.6813442954097396
{'scaleFactor': 20, 'currentTarget': array([44.,  5.]), 'dynamicTrap': False, 'previousTarget': array([44.,  5.]), 'currentState': array([43.02524034,  4.49964314,  0.0525746 ]), 'targetState': array([44,  5], dtype=int32), 'currentDistance': 1.0956794130330634}
episode index:1616
target Thresh 75.97881850117736
target distance 14.0
model initialize at round 1616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([92.8972583 , 16.40029373,  3.6892277 ]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 12.734260493573267}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6814993486271157
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([89.27428413,  3.56110054,  4.25016339]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 0.8481133491845216}
episode index:1617
target Thresh 75.97892414434347
target distance 32.0
model initialize at round 1617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.44523862,  12.25690599]), 'dynamicTrap': False, 'previousTarget': array([105.96105157,  12.75243428]), 'currentState': array([87.45116049, 12.74356771,  6.06898481]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7729895323537468
running average episode reward sum: 0.6815558938580962
{'scaleFactor': 20, 'currentTarget': array([118.,  12.]), 'dynamicTrap': False, 'previousTarget': array([118.,  12.]), 'currentState': array([117.70154747,  11.17910114,   0.85325194]), 'targetState': array([118,  12], dtype=int32), 'currentDistance': 0.8734694331575558}
episode index:1618
target Thresh 75.9790292606121
target distance 46.0
model initialize at round 1618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.81947871, 11.91832626]), 'dynamicTrap': False, 'previousTarget': array([50.51477147, 11.63520309]), 'currentState': array([67.19820598,  4.03020963,  2.33444643]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4424376337553963
running average episode reward sum: 0.6814081988240612
{'scaleFactor': 20, 'currentTarget': array([23., 23.]), 'dynamicTrap': False, 'previousTarget': array([23., 23.]), 'currentState': array([22.58320251, 23.8417468 ,  2.33227051]), 'targetState': array([23, 23], dtype=int32), 'currentDistance': 0.939285806806048}
episode index:1619
target Thresh 75.97913385261114
target distance 46.0
model initialize at round 1619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.29957689,  2.801479  ]), 'dynamicTrap': False, 'previousTarget': array([29.04239788,  3.69841725]), 'currentState': array([49.29029611,  3.41069571,  3.95132971]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6506243789529839
running average episode reward sum: 0.6813891964661161
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([3.25467249, 2.72759194, 2.56577081]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7708748945251609}
episode index:1620
target Thresh 75.9792379229554
target distance 21.0
model initialize at round 1620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.84294337,  6.23926056]), 'dynamicTrap': False, 'previousTarget': array([68.9086344 ,  5.87913569]), 'currentState': array([51.36995785, 17.58125105,  4.40304971]), 'targetState': array([74,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7910458855873943
running average episode reward sum: 0.6814568440226376
{'scaleFactor': 20, 'currentTarget': array([74.,  2.]), 'dynamicTrap': False, 'previousTarget': array([74.,  2.]), 'currentState': array([74.34833327,  1.96932702,  6.04296874]), 'targetState': array([74,  2], dtype=int32), 'currentDistance': 0.34968113755321056}
episode index:1621
target Thresh 75.97934147424667
target distance 17.0
model initialize at round 1621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([24.53291763, 21.79026724,  3.65749288]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 18.124962582253875}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6815887067909705
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([27.32399995,  3.92850775,  4.44211701]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 0.679769964583875}
episode index:1622
target Thresh 75.97944450907372
target distance 38.0
model initialize at round 1622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.54120548,  3.72829335]), 'dynamicTrap': False, 'previousTarget': array([50.02764341,  3.94882334]), 'currentState': array([68.52184806,  4.6080218 ,  2.90218878]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6519931230088954
running average episode reward sum: 0.6815704716808152
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([32.68178478,  3.13344182,  4.88283513]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.6947209574346506}
episode index:1623
target Thresh 75.97954703001241
target distance 59.0
model initialize at round 1623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.0050838 , 14.14742426]), 'dynamicTrap': False, 'previousTarget': array([88.02580461, 15.01563705]), 'currentState': array([106.94898408,  12.65048015,   3.08776474]), 'targetState': array([49, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5559914936994863
running average episode reward sum: 0.6814931447239302
{'scaleFactor': 20, 'currentTarget': array([49., 17.]), 'dynamicTrap': False, 'previousTarget': array([49., 17.]), 'currentState': array([48.89832573, 16.88837304,  4.83071613]), 'targetState': array([49, 17], dtype=int32), 'currentDistance': 0.1509908434780505}
episode index:1624
target Thresh 75.9796490396258
target distance 8.0
model initialize at round 1624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'dynamicTrap': False, 'previousTarget': array([2., 9.]), 'currentState': array([10.22154964, 12.46896224,  3.84609699]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 8.923428577614693}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.681647343002812
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'dynamicTrap': False, 'previousTarget': array([2., 9.]), 'currentState': array([1.16255222, 8.56750867, 4.25575644]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.942532509525225}
episode index:1625
target Thresh 75.97975054046411
target distance 43.0
model initialize at round 1625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.95213481, 12.27874887]), 'dynamicTrap': False, 'previousTarget': array([89.26392603, 12.62402064]), 'currentState': array([68.55234708, 17.14169405,  4.68677139]), 'targetState': array([113,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6268144650228926
running average episode reward sum: 0.6816136204456287
{'scaleFactor': 20, 'currentTarget': array([113.,   6.]), 'dynamicTrap': False, 'previousTarget': array([113.,   6.]), 'currentState': array([112.44944505,   5.3369954 ,   0.46465073]), 'targetState': array([113,   6], dtype=int32), 'currentDistance': 0.8617922351324343}
episode index:1626
target Thresh 75.97985153506488
target distance 15.0
model initialize at round 1626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'dynamicTrap': False, 'previousTarget': array([4.85786438, 6.85786438]), 'currentState': array([17.106347  , 20.56328365,  2.95010149]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 19.592487398185252}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.681744981621912
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'dynamicTrap': False, 'previousTarget': array([4., 6.]), 'currentState': array([4.00752656, 6.29655389, 5.4620104 ]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.29664938740278846}
episode index:1627
target Thresh 75.97995202595298
target distance 23.0
model initialize at round 1627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.7307525 ,  8.08675946]), 'dynamicTrap': False, 'previousTarget': array([34.11006407,  8.4295875 ]), 'currentState': array([51.31326119, 15.48205331,  3.74723017]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8325330691018655
running average episode reward sum: 0.6818376032972683
{'scaleFactor': 20, 'currentTarget': array([30.,  7.]), 'dynamicTrap': False, 'previousTarget': array([30.,  7.]), 'currentState': array([30.17467887,  7.31664798,  3.48988872]), 'targetState': array([30,  7], dtype=int32), 'currentDistance': 0.3616333044455961}
episode index:1628
target Thresh 75.98005201564068
target distance 16.0
model initialize at round 1628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5.14626244, 13.86406257]), 'dynamicTrap': True, 'previousTarget': array([ 5., 14.]), 'currentState': array([21.      , 17.      ,  2.369253], dtype=float32), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 16.160912666107684}
done in step count: 17
reward sum = 0.8035392033839268
running average episode reward sum: 0.6819123126895866
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.59527934, 14.50987165,  5.05380417]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.6509745883334506}
episode index:1629
target Thresh 75.98015150662775
target distance 3.0
model initialize at round 1629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 12.]), 'dynamicTrap': False, 'previousTarget': array([48., 12.]), 'currentState': array([45.92011411, 13.6002792 ,  5.66537809]), 'targetState': array([48, 12], dtype=int32), 'currentDistance': 2.6242749191177643}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6821013235406973
{'scaleFactor': 20, 'currentTarget': array([48., 12.]), 'dynamicTrap': False, 'previousTarget': array([48., 12.]), 'currentState': array([47.19648117, 12.2750916 ,  5.28890404]), 'targetState': array([48, 12], dtype=int32), 'currentDistance': 0.8493043616007713}
episode index:1630
target Thresh 75.98025050140143
target distance 24.0
model initialize at round 1630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.21052419,  6.00362341]), 'dynamicTrap': False, 'previousTarget': array([73.8,  6.4]), 'currentState': array([91.29955728, 11.97093569,  2.19403207]), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7792107414805874
running average episode reward sum: 0.6821608633432356
{'scaleFactor': 20, 'currentTarget': array([69.,  5.]), 'dynamicTrap': False, 'previousTarget': array([69.,  5.]), 'currentState': array([68.63658189,  5.21735913,  3.6231868 ]), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 0.42345921780955126}
episode index:1631
target Thresh 75.98034900243663
target distance 19.0
model initialize at round 1631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91.92524322, 12.43827311]), 'currentState': array([109.20915807,  19.57179015,   3.25586283]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 19.720685674949888}
done in step count: 14
reward sum = 0.8405004572968984
running average episode reward sum: 0.6822578851532561
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91., 12.]), 'currentState': array([91.81770747, 11.2074711 ,  3.56968467]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 1.1387482402759928}
episode index:1632
target Thresh 75.98044701219585
target distance 56.0
model initialize at round 1632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.07099007,  3.68361615]), 'dynamicTrap': True, 'previousTarget': array([86.07924589,  3.77863876]), 'currentState': array([106.       ,   2.       ,   3.8873124], dtype=float32), 'targetState': array([50,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5299680876847158
running average episode reward sum: 0.6821646274695644
{'scaleFactor': 20, 'currentTarget': array([50.,  7.]), 'dynamicTrap': False, 'previousTarget': array([50.,  7.]), 'currentState': array([49.64013801,  7.64987708,  2.75065932]), 'targetState': array([50,  7], dtype=int32), 'currentDistance': 0.742859922698656}
episode index:1633
target Thresh 75.98054453312938
target distance 44.0
model initialize at round 1633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.88595307, 14.96115899]), 'dynamicTrap': False, 'previousTarget': array([52.3226018 , 14.42229124]), 'currentState': array([70.43196755, 19.19829455,  2.77451503]), 'targetState': array([28, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.23823936846623583
running average episode reward sum: 0.6818929473844951
{'scaleFactor': 20, 'currentTarget': array([28., 10.]), 'dynamicTrap': False, 'previousTarget': array([28., 10.]), 'currentState': array([27.61734142, 10.12956882,  3.79989147]), 'targetState': array([28, 10], dtype=int32), 'currentDistance': 0.4039995891652655}
episode index:1634
target Thresh 75.9806415676752
target distance 19.0
model initialize at round 1634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.27377669,  3.1785523 ]), 'dynamicTrap': False, 'previousTarget': array([51.69147429,  3.97927459]), 'currentState': array([67.02592451, 14.10400622,  3.35581303]), 'targetState': array([50,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8310818517466291
running average episode reward sum: 0.6819841944208023
{'scaleFactor': 20, 'currentTarget': array([50.,  3.]), 'dynamicTrap': False, 'previousTarget': array([50.,  3.]), 'currentState': array([50.36665974,  3.96097477,  4.01596725]), 'targetState': array([50,  3], dtype=int32), 'currentDistance': 1.0285484295709504}
episode index:1635
target Thresh 75.98073811825923
target distance 13.0
model initialize at round 1635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'dynamicTrap': False, 'previousTarget': array([23., 14.]), 'currentState': array([11.62211346, 14.10295517,  0.92185   ]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 11.378352339695015}
done in step count: 14
reward sum = 0.8041406345851075
running average episode reward sum: 0.6820588621715139
{'scaleFactor': 20, 'currentTarget': array([23., 14.]), 'dynamicTrap': False, 'previousTarget': array([23., 14.]), 'currentState': array([22.91282439, 14.39302554,  6.28055493]), 'targetState': array([23, 14], dtype=int32), 'currentDistance': 0.40257751974039707}
episode index:1636
target Thresh 75.9808341872952
target distance 37.0
model initialize at round 1636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.39571066,  9.2416178 ]), 'dynamicTrap': False, 'previousTarget': array([89.74210955,  9.79857683]), 'currentState': array([71.59893521, 12.0855028 ,  6.09247875]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7072474366622328
running average episode reward sum: 0.6820742492054118
{'scaleFactor': 20, 'currentTarget': array([107.,   7.]), 'dynamicTrap': False, 'previousTarget': array([107.,   7.]), 'currentState': array([106.4557555 ,   7.1731179 ,   0.69745789]), 'targetState': array([107,   7], dtype=int32), 'currentDistance': 0.5711145996974336}
episode index:1637
target Thresh 75.98092977718485
target distance 22.0
model initialize at round 1637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.25749733, 16.80100195]), 'dynamicTrap': True, 'previousTarget': array([85.18339664, 17.29773591]), 'currentState': array([105.       ,  20.       ,   3.6525369], dtype=float32), 'targetState': array([83, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8576210229989678
running average episode reward sum: 0.6821814206179841
{'scaleFactor': 20, 'currentTarget': array([83., 17.]), 'dynamicTrap': False, 'previousTarget': array([83., 17.]), 'currentState': array([82.98080808, 17.34160302,  4.0024351 ]), 'targetState': array([83, 17], dtype=int32), 'currentDistance': 0.3421417187796982}
episode index:1638
target Thresh 75.98102489031794
target distance 42.0
model initialize at round 1638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.3887898 , 11.56719351]), 'dynamicTrap': False, 'previousTarget': array([92.85976548, 10.6357422 ]), 'currentState': array([72.63312719, 14.68388965,  2.72558796]), 'targetState': array([115,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6822120014288666
{'scaleFactor': 20, 'currentTarget': array([115.,   8.]), 'dynamicTrap': False, 'previousTarget': array([115.,   8.]), 'currentState': array([114.19637622,   8.3608754 ,   4.96462036]), 'targetState': array([115,   8], dtype=int32), 'currentDistance': 0.8809325947394332}
episode index:1639
target Thresh 75.9811195290723
target distance 39.0
model initialize at round 1639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.79363808, 10.6626819 ]), 'dynamicTrap': False, 'previousTarget': array([70.33391929,  9.99170841]), 'currentState': array([53.44760369,  2.69882465,  5.74938649]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7767247582646316
running average episode reward sum: 0.6822696311586446
{'scaleFactor': 20, 'currentTarget': array([91., 19.]), 'dynamicTrap': False, 'previousTarget': array([91., 19.]), 'currentState': array([91.15356351, 18.04465936,  1.43878185]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 0.9676039972119266}
episode index:1640
target Thresh 75.9812136958139
target distance 29.0
model initialize at round 1640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.01168202, 20.68347957]), 'dynamicTrap': True, 'previousTarget': array([18.01188001, 20.68924552]), 'currentState': array([38.       , 20.       ,  3.1283004], dtype=float32), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 19
reward sum = 0.7975811657078968
running average episode reward sum: 0.6823399002229646
{'scaleFactor': 20, 'currentTarget': array([ 9., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 21.]), 'currentState': array([ 8.90801767, 21.11811585,  3.54542464]), 'targetState': array([ 9, 21], dtype=int32), 'currentDistance': 0.14970672878758312}
episode index:1641
target Thresh 75.98130739289692
target distance 43.0
model initialize at round 1641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.76345021,  7.33344077]), 'dynamicTrap': False, 'previousTarget': array([27.86614761,  7.31001716]), 'currentState': array([9.91927474, 4.84171448, 5.72163278]), 'targetState': array([51, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7296830314169366
running average episode reward sum: 0.682368732824179
{'scaleFactor': 20, 'currentTarget': array([51., 10.]), 'dynamicTrap': False, 'previousTarget': array([51., 10.]), 'currentState': array([50.9271353 , 10.55728747,  1.73538541]), 'targetState': array([51, 10], dtype=int32), 'currentDistance': 0.5620307715702176}
episode index:1642
target Thresh 75.9814006226638
target distance 60.0
model initialize at round 1642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.91800527, 10.43077642]), 'dynamicTrap': False, 'previousTarget': array([28.9972228 ,  9.66671295]), 'currentState': array([ 9.93139445, 11.16247837,  6.21938068]), 'targetState': array([69,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 71
reward sum = 0.2178138445717392
running average episode reward sum: 0.6820859848702823
{'scaleFactor': 20, 'currentTarget': array([69.,  9.]), 'dynamicTrap': False, 'previousTarget': array([69.,  9.]), 'currentState': array([68.04077357,  8.80452075,  0.8524922 ]), 'targetState': array([69,  9], dtype=int32), 'currentDistance': 0.9789420194324894}
episode index:1643
target Thresh 75.98149338744525
target distance 73.0
model initialize at round 1643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.58177624, 14.93133445]), 'dynamicTrap': True, 'previousTarget': array([56.59069837, 14.97451403]), 'currentState': array([37.       , 19.       ,  1.8799487], dtype=float32), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.31083999448748356
running average episode reward sum: 0.6814820152964637
{'scaleFactor': 20, 'currentTarget': array([110.,   4.]), 'dynamicTrap': False, 'previousTarget': array([110.,   4.]), 'currentState': array([105.97004613,   2.34705556,   0.26606791]), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 4.355772441260873}
episode index:1644
target Thresh 75.98158568956045
target distance 64.0
model initialize at round 1644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.04218676,  6.70165842]), 'dynamicTrap': True, 'previousTarget': array([66.06075717,  6.44224665]), 'currentState': array([86.      ,  8.      ,  4.068109], dtype=float32), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4449709217588428
running average episode reward sum: 0.6813382395557113
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'dynamicTrap': False, 'previousTarget': array([22.,  3.]), 'currentState': array([22.48368197,  3.83083037,  2.52010954]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.9613674355083508}
episode index:1645
target Thresh 75.98167753131692
target distance 16.0
model initialize at round 1645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([46.23279297,  5.00920616,  1.69600027]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 18.73231266594602}
done in step count: 19
reward sum = 0.7448698949800352
running average episode reward sum: 0.6813768371592497
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([35.63412903, 20.06671361,  1.96521825]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 0.6376286782331616}
episode index:1646
target Thresh 75.98176891501072
target distance 4.0
model initialize at round 1646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'dynamicTrap': False, 'previousTarget': array([14., 22.]), 'currentState': array([17.12654652, 20.72952148,  2.47765863]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 3.3748198234135827}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6815582112714784
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'dynamicTrap': False, 'previousTarget': array([14., 22.]), 'currentState': array([13.80130649, 22.92355002,  2.67466669]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 0.9446818273785875}
episode index:1647
target Thresh 75.98185984292644
target distance 45.0
model initialize at round 1647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.14579334, 17.41049327]), 'dynamicTrap': True, 'previousTarget': array([62.23767062, 18.07414013]), 'currentState': array([8.200000e+01, 1.500000e+01, 7.812395e-02], dtype=float32), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6468856429897326
running average episode reward sum: 0.6815371720916958
{'scaleFactor': 20, 'currentTarget': array([37., 22.]), 'dynamicTrap': False, 'previousTarget': array([37., 22.]), 'currentState': array([37.8065608 , 22.14371034,  3.15108064]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 0.8192636815530286}
episode index:1648
target Thresh 75.9819503173373
target distance 20.0
model initialize at round 1648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.72478683,  3.66236967]), 'dynamicTrap': False, 'previousTarget': array([51.11145618,  3.05572809]), 'currentState': array([70.98842339, 11.81341744,  6.09634277]), 'targetState': array([49,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7409837124174059
running average episode reward sum: 0.6815732221464719
{'scaleFactor': 20, 'currentTarget': array([49.,  2.]), 'dynamicTrap': False, 'previousTarget': array([49.,  2.]), 'currentState': array([48.11990471,  2.18160432,  4.40491446]), 'targetState': array([49,  2], dtype=int32), 'currentDistance': 0.8986366607516972}
episode index:1649
target Thresh 75.98204034050515
target distance 26.0
model initialize at round 1649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.8313746 , 22.02721593]), 'dynamicTrap': False, 'previousTarget': array([63.64012894, 21.77694787]), 'currentState': array([45.17646116, 18.32797579,  1.184421  ]), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7743439628380961
running average episode reward sum: 0.6816294468378001
{'scaleFactor': 20, 'currentTarget': array([70., 23.]), 'dynamicTrap': False, 'previousTarget': array([70., 23.]), 'currentState': array([69.26072172, 22.44134477,  5.29030267]), 'targetState': array([70, 23], dtype=int32), 'currentDistance': 0.9266218426692644}
episode index:1650
target Thresh 75.98212991468057
target distance 9.0
model initialize at round 1650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.09881507, 13.11344451]), 'dynamicTrap': True, 'previousTarget': array([90., 13.]), 'currentState': array([81.      ,  9.      ,  3.913003], dtype=float32), 'targetState': array([90, 13], dtype=int32), 'currentDistance': 9.985432461827601}
done in step count: 9
reward sum = 0.9035172474836408
running average episode reward sum: 0.6817638428406141
{'scaleFactor': 20, 'currentTarget': array([90., 13.]), 'dynamicTrap': False, 'previousTarget': array([90., 13.]), 'currentState': array([90.45334677, 13.69465349,  0.76371603]), 'targetState': array([90, 13], dtype=int32), 'currentDistance': 0.8294979050969751}
episode index:1651
target Thresh 75.98221904210293
target distance 24.0
model initialize at round 1651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.22344325,  4.95132211]), 'dynamicTrap': False, 'previousTarget': array([88.2,  5.4]), 'currentState': array([69.82930366,  9.83674424,  4.48613286]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8144125312909167
running average episode reward sum: 0.6818441386568673
{'scaleFactor': 20, 'currentTarget': array([93.,  4.]), 'dynamicTrap': False, 'previousTarget': array([93.,  4.]), 'currentState': array([93.8551362 ,  4.52207224,  0.13673694]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 1.0019068588193896}
episode index:1652
target Thresh 75.98230772500041
target distance 41.0
model initialize at round 1652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.64186718, 11.69184012]), 'dynamicTrap': False, 'previousTarget': array([91.85291755, 12.42108751]), 'currentState': array([70.85726512,  8.76446567,  4.97599077]), 'targetState': array([113,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7016559683152003
running average episode reward sum: 0.681856124034761
{'scaleFactor': 20, 'currentTarget': array([113.,  15.]), 'dynamicTrap': False, 'previousTarget': array([113.,  15.]), 'currentState': array([113.58665503,  14.11239438,   2.51920838]), 'targetState': array([113,  15], dtype=int32), 'currentDistance': 1.0639585799473315}
episode index:1653
target Thresh 75.9823959655901
target distance 14.0
model initialize at round 1653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 18.]), 'dynamicTrap': False, 'previousTarget': array([32., 18.]), 'currentState': array([30.65147332,  5.78944872,  1.76666605]), 'targetState': array([32, 18], dtype=int32), 'currentDistance': 12.284790868199504}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6820130914019716
{'scaleFactor': 20, 'currentTarget': array([32., 18.]), 'dynamicTrap': False, 'previousTarget': array([32., 18.]), 'currentState': array([31.43998788, 17.35413525,  2.04648663]), 'targetState': array([32, 18], dtype=int32), 'currentDistance': 0.8548420036441653}
episode index:1654
target Thresh 75.98248376607802
target distance 26.0
model initialize at round 1654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.90603959,  4.03357832]), 'dynamicTrap': False, 'previousTarget': array([30.76743395,  3.95885631]), 'currentState': array([12.30544366,  8.01059217,  6.16184277]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.796830778225163
running average episode reward sum: 0.6820824676477861
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([36.42987359,  2.43383015,  0.27826025]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 0.8034876598632726}
episode index:1655
target Thresh 75.98257112865916
target distance 2.0
model initialize at round 1655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.,  5.]), 'dynamicTrap': False, 'previousTarget': array([91.,  5.]), 'currentState': array([89.42591847,  5.35143269,  0.28347397]), 'targetState': array([91,  5], dtype=int32), 'currentDistance': 1.612835266060661}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6822684081866462
{'scaleFactor': 20, 'currentTarget': array([91.,  5.]), 'dynamicTrap': False, 'previousTarget': array([91.,  5.]), 'currentState': array([91.11629058,  5.76218393,  0.19236398]), 'targetState': array([91,  5], dtype=int32), 'currentDistance': 0.7710044325217978}
episode index:1656
target Thresh 75.98265805551762
target distance 4.0
model initialize at round 1656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.93455658, 20.83501702]), 'dynamicTrap': True, 'previousTarget': array([65., 21.]), 'currentState': array([61.       , 22.       ,  1.2772794], dtype=float32), 'targetState': array([65, 21], dtype=int32), 'currentDistance': 4.1034035681022285}
done in step count: 4
reward sum = 0.9505960099999999
running average episode reward sum: 0.6824303439753084
{'scaleFactor': 20, 'currentTarget': array([65., 21.]), 'dynamicTrap': False, 'previousTarget': array([65., 21.]), 'currentState': array([65.91753996, 21.60732194,  5.02577724]), 'targetState': array([65, 21], dtype=int32), 'currentDistance': 1.1003270011769932}
episode index:1657
target Thresh 75.98274454882657
target distance 47.0
model initialize at round 1657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.68342038, 15.50210723]), 'dynamicTrap': False, 'previousTarget': array([55.43788142, 16.16215289]), 'currentState': array([74.1155457 , 10.77003523,  3.11116207]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6959435469426066
running average episode reward sum: 0.6824384942786663
{'scaleFactor': 20, 'currentTarget': array([28., 22.]), 'dynamicTrap': False, 'previousTarget': array([28., 22.]), 'currentState': array([27.36935789, 21.90837996,  2.98585531]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 0.6372626642430269}
episode index:1658
target Thresh 75.98283061074834
target distance 11.0
model initialize at round 1658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.96714569, 20.19702138]), 'dynamicTrap': True, 'previousTarget': array([92., 20.]), 'currentState': array([81.      , 12.      ,  4.790489], dtype=float32), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 13.69194814274868}
done in step count: 16
reward sum = 0.7649750185785165
running average episode reward sum: 0.6824882450467795
{'scaleFactor': 20, 'currentTarget': array([92., 20.]), 'dynamicTrap': False, 'previousTarget': array([92., 20.]), 'currentState': array([91.59325827, 20.03197764,  6.14629882]), 'targetState': array([92, 20], dtype=int32), 'currentDistance': 0.407996818859954}
episode index:1659
target Thresh 75.98291624343447
target distance 13.0
model initialize at round 1659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,  21.]), 'dynamicTrap': False, 'previousTarget': array([114.,  21.]), 'currentState': array([102.38968814,  14.59294714,   1.41328972]), 'targetState': array([114,  21], dtype=int32), 'currentDistance': 13.260832090443698}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6826329778476115
{'scaleFactor': 20, 'currentTarget': array([114.,  21.]), 'dynamicTrap': False, 'previousTarget': array([114.,  21.]), 'currentState': array([114.88922376,  21.1710557 ,   0.19955699]), 'targetState': array([114,  21], dtype=int32), 'currentDistance': 0.905526885488314}
episode index:1660
target Thresh 75.9830014490258
target distance 45.0
model initialize at round 1660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.82234907, 12.60132573]), 'dynamicTrap': False, 'previousTarget': array([92.69125016, 12.50066669]), 'currentState': array([73.12075536,  9.15934881,  0.8776257 ]), 'targetState': array([118,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6570804574075262
running average episode reward sum: 0.6826175940303688
{'scaleFactor': 20, 'currentTarget': array([118.,  17.]), 'dynamicTrap': False, 'previousTarget': array([118.,  17.]), 'currentState': array([118.33016282,  16.113034  ,   2.46736628]), 'targetState': array([118,  17], dtype=int32), 'currentDistance': 0.9464228303291276}
episode index:1661
target Thresh 75.9830862296525
target distance 17.0
model initialize at round 1661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'dynamicTrap': False, 'previousTarget': array([9., 7.]), 'currentState': array([24.32998857,  3.79155357,  2.25576782]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 15.6621415560258}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6827565228230603
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'dynamicTrap': False, 'previousTarget': array([9., 7.]), 'currentState': array([8.62337561, 7.14102715, 3.1708138 ]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.4021623926028001}
episode index:1662
target Thresh 75.98317058743403
target distance 55.0
model initialize at round 1662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.06114639,  4.5627273 ]), 'dynamicTrap': True, 'previousTarget': array([82.11795647,  5.1689502 ]), 'currentState': array([102.        ,   3.        ,   0.73249996], dtype=float32), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5733538335784848
running average episode reward sum: 0.6826907364795579
{'scaleFactor': 20, 'currentTarget': array([47.,  9.]), 'dynamicTrap': False, 'previousTarget': array([47.,  9.]), 'currentState': array([47.5276649 ,  8.00253203,  2.98496685]), 'targetState': array([47,  9], dtype=int32), 'currentDistance': 1.128438120576653}
episode index:1663
target Thresh 75.98325452447938
target distance 25.0
model initialize at round 1663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.20423125, 13.85088399]), 'dynamicTrap': True, 'previousTarget': array([17.06369443, 12.59490445]), 'currentState': array([37.       , 11.       ,  5.0757337], dtype=float32), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7292087733438901
running average episode reward sum: 0.6827186920305581
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'dynamicTrap': False, 'previousTarget': array([12., 13.]), 'currentState': array([11.99820494, 13.8644552 ,  0.4935146 ]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.8644570602788806}
episode index:1664
target Thresh 75.98333804288697
target distance 38.0
model initialize at round 1664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.47219767,  6.31417676]), 'dynamicTrap': False, 'previousTarget': array([84.97235659,  5.94882334]), 'currentState': array([66.53512278,  7.89943533,  5.97691542]), 'targetState': array([103,   5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 24
reward sum = 0.7499788692658687
running average episode reward sum: 0.6827590885334022
{'scaleFactor': 20, 'currentTarget': array([103.,   5.]), 'dynamicTrap': False, 'previousTarget': array([103.,   5.]), 'currentState': array([102.78969388,   5.71170451,   0.97963608]), 'targetState': array([103,   5], dtype=int32), 'currentDistance': 0.7421266578235938}
episode index:1665
target Thresh 75.98342114474475
target distance 22.0
model initialize at round 1665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.53904261, 21.07368428]), 'dynamicTrap': False, 'previousTarget': array([ 5.08213587, 21.18928508]), 'currentState': array([23.35476661, 23.78238923,  3.2642597 ]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8765838717161293
running average episode reward sum: 0.6828754299398744
{'scaleFactor': 20, 'currentTarget': array([ 3., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 21.]), 'currentState': array([ 3.04507046, 21.57534015,  3.91315086]), 'targetState': array([ 3, 21], dtype=int32), 'currentDistance': 0.5771027931682946}
episode index:1666
target Thresh 75.98350383213031
target distance 47.0
model initialize at round 1666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.35649818, 11.25743503]), 'dynamicTrap': False, 'previousTarget': array([90.64310384, 10.23855458]), 'currentState': array([70.85000866, 15.67296066,  0.65021348]), 'targetState': array([118,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5814803183037386
running average episode reward sum: 0.682814605037873
{'scaleFactor': 20, 'currentTarget': array([118.,   5.]), 'dynamicTrap': False, 'previousTarget': array([118.,   5.]), 'currentState': array([118.85068244,   4.76004268,   0.47546602]), 'targetState': array([118,   5], dtype=int32), 'currentDistance': 0.8838778927514762}
episode index:1667
target Thresh 75.9835861071108
target distance 62.0
model initialize at round 1667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.74889471, 11.73955208]), 'dynamicTrap': False, 'previousTarget': array([73.79255473, 11.87311278]), 'currentState': array([53.96489504,  8.80810925,  4.63244743]), 'targetState': array([116,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.562716595914188
running average episode reward sum: 0.6827426038333624
{'scaleFactor': 20, 'currentTarget': array([116.,  18.]), 'dynamicTrap': False, 'previousTarget': array([116.,  18.]), 'currentState': array([115.8812434 ,  18.25469572,   1.34659964]), 'targetState': array([116,  18], dtype=int32), 'currentDistance': 0.28102141852848567}
episode index:1668
target Thresh 75.98366797174312
target distance 72.0
model initialize at round 1668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.37144085,  9.10724795]), 'dynamicTrap': False, 'previousTarget': array([54.59715  ,  7.8507125]), 'currentState': array([73.87489024,  4.67831333,  2.65520364]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4895556529013054
running average episode reward sum: 0.6826268537129717
{'scaleFactor': 20, 'currentTarget': array([ 2., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 21.]), 'currentState': array([ 2.68421753, 21.8264468 ,  3.51186021]), 'targetState': array([ 2, 21], dtype=int32), 'currentDistance': 1.072924951094259}
episode index:1669
target Thresh 75.98374942807386
target distance 58.0
model initialize at round 1669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.01030184, 15.64184706]), 'dynamicTrap': True, 'previousTarget': array([66.00297199, 15.34477635]), 'currentState': array([86.       , 15.       ,  4.6371384], dtype=float32), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5659606139479507
running average episode reward sum: 0.6825569936891603
{'scaleFactor': 20, 'currentTarget': array([28., 16.]), 'dynamicTrap': False, 'previousTarget': array([28., 16.]), 'currentState': array([27.13199938, 15.55159614,  3.86692808]), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 0.9769806020572827}
episode index:1670
target Thresh 75.98383047813948
target distance 11.0
model initialize at round 1670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 22.]), 'dynamicTrap': False, 'previousTarget': array([68., 22.]), 'currentState': array([77.58496639, 23.8289858 ,  2.87185222]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 9.75790806042537}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6827176358532602
{'scaleFactor': 20, 'currentTarget': array([68., 22.]), 'dynamicTrap': False, 'previousTarget': array([68., 22.]), 'currentState': array([67.77469095, 22.85569927,  3.0663908 ]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 0.884864624031819}
episode index:1671
target Thresh 75.98391112396621
target distance 29.0
model initialize at round 1671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.76413528,  7.90508388]), 'dynamicTrap': False, 'previousTarget': array([11.04739343,  7.37604183]), 'currentState': array([30.76296248,  7.68849536,  2.68267775]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7544248482517584
running average episode reward sum: 0.6827605229420153
{'scaleFactor': 20, 'currentTarget': array([2., 8.]), 'dynamicTrap': False, 'previousTarget': array([2., 8.]), 'currentState': array([2.49797846, 8.33431993, 3.5744141 ]), 'targetState': array([2, 8], dtype=int32), 'currentDistance': 0.599793600752925}
episode index:1672
target Thresh 75.98399136757018
target distance 47.0
model initialize at round 1672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.28419918, 14.30279755]), 'dynamicTrap': True, 'previousTarget': array([57.27622241, 14.33172109]), 'currentState': array([38.       ,  9.       ,  2.9110582], dtype=float32), 'targetState': array([85, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 86
reward sum = 0.05613398740273612
running average episode reward sum: 0.6823859703206528
{'scaleFactor': 20, 'currentTarget': array([85., 22.]), 'dynamicTrap': False, 'previousTarget': array([85., 22.]), 'currentState': array([85.31356861, 21.91662041,  1.68769378]), 'targetState': array([85, 22], dtype=int32), 'currentDistance': 0.3244648340902277}
episode index:1673
target Thresh 75.98407121095752
target distance 8.0
model initialize at round 1673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,   7.]), 'dynamicTrap': False, 'previousTarget': array([116.,   7.]), 'currentState': array([109.8925312 ,   3.28800528,   0.3367488 ]), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 7.147032953363012}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6825521650874864
{'scaleFactor': 20, 'currentTarget': array([116.,   7.]), 'dynamicTrap': False, 'previousTarget': array([116.,   7.]), 'currentState': array([116.10971772,   6.60917528,   1.23372286]), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 0.40593342286486067}
episode index:1674
target Thresh 75.98415065612431
target distance 29.0
model initialize at round 1674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.28253422, 13.95326496]), 'dynamicTrap': True, 'previousTarget': array([22.30004947, 13.90691532]), 'currentState': array([41.       , 21.       ,  4.0246525], dtype=float32), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7898278682212584
running average episode reward sum: 0.6826162102833871
{'scaleFactor': 20, 'currentTarget': array([12., 10.]), 'dynamicTrap': False, 'previousTarget': array([12., 10.]), 'currentState': array([11.11607936, 10.03934849,  3.75296493]), 'targetState': array([12, 10], dtype=int32), 'currentDistance': 0.8847960272317885}
episode index:1675
target Thresh 75.98422970505668
target distance 34.0
model initialize at round 1675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.183836  , 10.50107061]), 'dynamicTrap': False, 'previousTarget': array([49.90362596, 11.48405927]), 'currentState': array([67.56949211, 18.37302396,  3.44371128]), 'targetState': array([34,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6518351209780051
running average episode reward sum: 0.6825978444783123
{'scaleFactor': 20, 'currentTarget': array([34.,  4.]), 'dynamicTrap': False, 'previousTarget': array([34.,  4.]), 'currentState': array([33.52794255,  4.34379791,  5.53427229]), 'targetState': array([34,  4], dtype=int32), 'currentDistance': 0.5839822279157101}
episode index:1676
target Thresh 75.98430835973085
target distance 6.0
model initialize at round 1676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71., 17.]), 'dynamicTrap': False, 'previousTarget': array([71., 17.]), 'currentState': array([67.73585167, 21.50989244,  5.32001358]), 'targetState': array([71, 17], dtype=int32), 'currentDistance': 5.56720703245931}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6827636155966914
{'scaleFactor': 20, 'currentTarget': array([71., 17.]), 'dynamicTrap': False, 'previousTarget': array([71., 17.]), 'currentState': array([71.35814734, 17.35584482,  5.62596643]), 'targetState': array([71, 17], dtype=int32), 'currentDistance': 0.5048713256222579}
episode index:1677
target Thresh 75.98438662211319
target distance 22.0
model initialize at round 1677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.6863198 , 11.11835211]), 'dynamicTrap': False, 'previousTarget': array([65.81660336, 11.29773591]), 'currentState': array([46.76699477, 12.9129245 ,  0.05365008]), 'targetState': array([68, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6828744512326701
{'scaleFactor': 20, 'currentTarget': array([68., 11.]), 'dynamicTrap': False, 'previousTarget': array([68., 11.]), 'currentState': array([68.6163657 , 11.76857328,  0.12777704]), 'targetState': array([68, 11], dtype=int32), 'currentDistance': 0.9851962016060994}
episode index:1678
target Thresh 75.98446449416026
target distance 56.0
model initialize at round 1678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.0106862 , 21.65370775]), 'dynamicTrap': True, 'previousTarget': array([44.01274291, 21.71383061]), 'currentState': array([64.        , 21.        ,  0.87329495], dtype=float32), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6404399018009942
running average episode reward sum: 0.6828491775284226
{'scaleFactor': 20, 'currentTarget': array([ 8., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 23.]), 'currentState': array([ 8.32854252, 22.89055352,  3.03275326]), 'targetState': array([ 8, 23], dtype=int32), 'currentDistance': 0.34629281858103605}
episode index:1679
target Thresh 75.98454197781889
target distance 20.0
model initialize at round 1679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.87930317, 18.60740444]), 'dynamicTrap': False, 'previousTarget': array([73., 18.]), 'currentState': array([87.26164243,  5.82524347,  2.34448767]), 'targetState': array([69, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7539866351697893
running average episode reward sum: 0.6828915212532092
{'scaleFactor': 20, 'currentTarget': array([69., 21.]), 'dynamicTrap': False, 'previousTarget': array([69., 21.]), 'currentState': array([68.96418172, 21.15919682,  3.09542752]), 'targetState': array([69, 21], dtype=int32), 'currentDistance': 0.16317651665832292}
episode index:1680
target Thresh 75.98461907502617
target distance 5.0
model initialize at round 1680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 18.]), 'dynamicTrap': False, 'previousTarget': array([70., 18.]), 'currentState': array([66.15897875, 21.74383091,  6.055004  ]), 'targetState': array([70, 18], dtype=int32), 'currentDistance': 5.363740681764099}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6830624953631121
{'scaleFactor': 20, 'currentTarget': array([70., 18.]), 'dynamicTrap': False, 'previousTarget': array([70., 18.]), 'currentState': array([70.01626778, 17.6474107 ,  5.8506166 ]), 'targetState': array([70, 18], dtype=int32), 'currentDistance': 0.3529643872847805}
episode index:1681
target Thresh 75.98469578770951
target distance 26.0
model initialize at round 1681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.80445566,  12.84482687]), 'dynamicTrap': True, 'previousTarget': array([100.73939228,  12.94498726]), 'currentState': array([84.       ,  2.       ,  1.8640999], dtype=float32), 'targetState': array([110,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7997278682212584
running average episode reward sum: 0.6831318564646924
{'scaleFactor': 20, 'currentTarget': array([110.,  19.]), 'dynamicTrap': False, 'previousTarget': array([110.,  19.]), 'currentState': array([109.74751281,  19.41573187,   1.48442239]), 'targetState': array([110,  19], dtype=int32), 'currentDistance': 0.4863977454618005}
episode index:1682
target Thresh 75.98477211778675
target distance 30.0
model initialize at round 1682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.84904627, 10.3395941 ]), 'dynamicTrap': False, 'previousTarget': array([97.35294118, 10.58823529]), 'currentState': array([113.11701044,  20.4300556 ,   3.37739789]), 'targetState': array([85,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7775808621250063
running average episode reward sum: 0.6831879758976457
{'scaleFactor': 20, 'currentTarget': array([84.15780147,  5.55108341]), 'dynamicTrap': True, 'previousTarget': array([85.,  4.]), 'currentState': array([83.70661299,  5.4728977 ,  5.83667116]), 'targetState': array([85,  4], dtype=int32), 'currentDistance': 0.45791270983052684}
episode index:1683
target Thresh 75.98484806716614
target distance 37.0
model initialize at round 1683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.32054513, 12.54891608]), 'dynamicTrap': False, 'previousTarget': array([64.98470177, 11.74931438]), 'currentState': array([48.27614981,  3.92338435,  5.89955587]), 'targetState': array([84, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6743226881363442
running average episode reward sum: 0.6831827114749845
{'scaleFactor': 20, 'currentTarget': array([84., 21.]), 'dynamicTrap': False, 'previousTarget': array([84., 21.]), 'currentState': array([83.80660924, 21.48285039,  1.50806352]), 'targetState': array([84, 21], dtype=int32), 'currentDistance': 0.5201389101900102}
episode index:1684
target Thresh 75.98492363774642
target distance 21.0
model initialize at round 1684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.86127225, 19.24073131]), 'dynamicTrap': False, 'previousTarget': array([33.00530293, 19.52709229]), 'currentState': array([14.24078564,  9.7793122 ,  2.82516539]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 23
reward sum = 0.704424887667784
running average episode reward sum: 0.6831953181077399
{'scaleFactor': 20, 'currentTarget': array([35.01979809, 21.0585669 ]), 'dynamicTrap': True, 'previousTarget': array([35.20335151, 21.1375723 ]), 'currentState': array([34.0219921 , 20.65989111,  0.55625381]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 1.0745041542888363}
episode index:1685
target Thresh 75.98499883141686
target distance 34.0
model initialize at round 1685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.97487398, 13.00220234]), 'dynamicTrap': True, 'previousTarget': array([99.922597  , 13.75787621]), 'currentState': array([80.       , 12.       ,  6.2189975], dtype=float32), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7377984314910575
running average episode reward sum: 0.6832277042959862
{'scaleFactor': 20, 'currentTarget': array([114.,  15.]), 'dynamicTrap': False, 'previousTarget': array([114.,  15.]), 'currentState': array([113.52402639,  15.00418532,   1.02814521]), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 0.4759920157091117}
episode index:1686
target Thresh 75.9850736500573
target distance 64.0
model initialize at round 1686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.0840531 ,  7.16686582]), 'dynamicTrap': False, 'previousTarget': array([91.40025361,  5.98119849]), 'currentState': array([110.77561346,   3.66794447,   2.71452224]), 'targetState': array([47, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 50
reward sum = 0.5517618143959999
running average episode reward sum: 0.6831497754934374
{'scaleFactor': 20, 'currentTarget': array([47., 15.]), 'dynamicTrap': False, 'previousTarget': array([47., 15.]), 'currentState': array([47.40957863, 14.1831192 ,  3.32899779]), 'targetState': array([47, 15], dtype=int32), 'currentDistance': 0.9138100948614724}
episode index:1687
target Thresh 75.98514809553821
target distance 17.0
model initialize at round 1687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82., 23.]), 'dynamicTrap': False, 'previousTarget': array([82.20859686, 22.86502556]), 'currentState': array([98.01095052, 13.1649629 ,  3.28470755]), 'targetState': array([82, 23], dtype=int32), 'currentDistance': 18.790382954217506}
done in step count: 20
reward sum = 0.7509340855798716
running average episode reward sum: 0.6831899320752421
{'scaleFactor': 20, 'currentTarget': array([82., 23.]), 'dynamicTrap': False, 'previousTarget': array([82., 23.]), 'currentState': array([82.51305561, 23.09885435,  2.96535341]), 'targetState': array([82, 23], dtype=int32), 'currentDistance': 0.5224923394365983}
episode index:1688
target Thresh 75.98522216972074
target distance 52.0
model initialize at round 1688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.20297857, 15.56946564]), 'dynamicTrap': False, 'previousTarget': array([57.89972147, 15.45778872]), 'currentState': array([40.40842267, 22.40793538,  5.57730633]), 'targetState': array([91,  4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 41
reward sum = 0.5849128575709326
running average episode reward sum: 0.6831317455302426
{'scaleFactor': 20, 'currentTarget': array([91.,  4.]), 'dynamicTrap': False, 'previousTarget': array([91.,  4.]), 'currentState': array([91.69834062,  4.43677475,  0.73280601]), 'targetState': array([91,  4], dtype=int32), 'currentDistance': 0.8236818593463698}
episode index:1689
target Thresh 75.98529587445675
target distance 3.0
model initialize at round 1689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,   9.]), 'dynamicTrap': False, 'previousTarget': array([108.,   9.]), 'currentState': array([107.68262703,  12.03397607,   5.29337484]), 'targetState': array([108,   9], dtype=int32), 'currentDistance': 3.0505305116287746}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6833074663908756
{'scaleFactor': 20, 'currentTarget': array([108.,   9.]), 'dynamicTrap': False, 'previousTarget': array([108.,   9.]), 'currentState': array([108.95492406,   8.77514008,   6.15607047]), 'targetState': array([108,   9], dtype=int32), 'currentDistance': 0.981041255668347}
episode index:1690
target Thresh 75.98536921158885
target distance 20.0
model initialize at round 1690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.8747813 ,  3.89237124]), 'dynamicTrap': False, 'previousTarget': array([95.94427191,  4.11145618]), 'currentState': array([85.65310573, 21.08298928,  4.66256094]), 'targetState': array([97,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6834275594750419
{'scaleFactor': 20, 'currentTarget': array([97.,  2.]), 'dynamicTrap': False, 'previousTarget': array([97.,  2.]), 'currentState': array([96.42495288,  2.13420528,  5.77195868]), 'targetState': array([97,  2], dtype=int32), 'currentDistance': 0.5905000019669868}
episode index:1691
target Thresh 75.98544218295048
target distance 6.0
model initialize at round 1691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.38481959,  5.43349397]), 'dynamicTrap': True, 'previousTarget': array([100.,   7.]), 'currentState': array([94.       ,  4.       ,  2.1865914], dtype=float32), 'targetState': array([100,   7], dtype=int32), 'currentDistance': 5.572359189756114}
done in step count: 5
reward sum = 0.9310900498999999
running average episode reward sum: 0.6835739321053166
{'scaleFactor': 20, 'currentTarget': array([100.,   7.]), 'dynamicTrap': False, 'previousTarget': array([100.,   7.]), 'currentState': array([100.43842508,   7.00036019,   1.03715504]), 'targetState': array([100,   7], dtype=int32), 'currentDistance': 0.43842522805652595}
episode index:1692
target Thresh 75.98551479036593
target distance 25.0
model initialize at round 1692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.85527397, 15.73556637]), 'dynamicTrap': False, 'previousTarget': array([68.14985851, 15.28991511]), 'currentState': array([50.13698805,  6.45858711,  1.18479419]), 'targetState': array([76, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6836630873500565
{'scaleFactor': 20, 'currentTarget': array([76., 20.]), 'dynamicTrap': False, 'previousTarget': array([76., 20.]), 'currentState': array([75.46702217, 19.60542605,  1.99284551]), 'targetState': array([76, 20], dtype=int32), 'currentDistance': 0.6631394771003994}
episode index:1693
target Thresh 75.98558703565037
target distance 46.0
model initialize at round 1693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.93564512,  9.70881856]), 'dynamicTrap': False, 'previousTarget': array([27.98112317,  8.86874449]), 'currentState': array([7.93689305, 9.48539981, 0.67561233]), 'targetState': array([54, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7395486671881251
running average episode reward sum: 0.6836960776569267
{'scaleFactor': 20, 'currentTarget': array([54., 10.]), 'dynamicTrap': False, 'previousTarget': array([54., 10.]), 'currentState': array([53.59876904, 10.05507214,  1.83499339]), 'targetState': array([54, 10], dtype=int32), 'currentDistance': 0.40499287126620614}
episode index:1694
target Thresh 75.98565892060996
target distance 5.0
model initialize at round 1694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  3.]), 'dynamicTrap': False, 'previousTarget': array([96.,  3.]), 'currentState': array([90.34056864,  6.63866258,  5.27129936]), 'targetState': array([96,  3], dtype=int32), 'currentDistance': 6.728226270068634}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6838594404488696
{'scaleFactor': 20, 'currentTarget': array([96.,  3.]), 'dynamicTrap': False, 'previousTarget': array([96.,  3.]), 'currentState': array([96.67072416,  3.01578666,  6.00871345]), 'targetState': array([96,  3], dtype=int32), 'currentDistance': 0.6709099206150276}
episode index:1695
target Thresh 75.98573044704182
target distance 4.0
model initialize at round 1695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([115.1217282 ,  10.4683311 ,   3.98182145]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 3.156662946850854}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6840341105901143
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([111.67233534,   9.26784273,   3.08518919]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 0.8021336570402485}
episode index:1696
target Thresh 75.98580161673411
target distance 8.0
model initialize at round 1696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 8.98860237, 21.15019033,  3.30207384]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 8.681303099174345}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6841970816563547
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.54876814, 16.94102762,  3.77095479]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 1.0893481779803107}
episode index:1697
target Thresh 75.98587243146608
target distance 58.0
model initialize at round 1697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.33493528, 12.19719083]), 'dynamicTrap': False, 'previousTarget': array([72.51579154, 13.37422914]), 'currentState': array([52.94830111,  7.28206974,  5.62869227]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5625155763947083
running average episode reward sum: 0.6841254199924786
{'scaleFactor': 20, 'currentTarget': array([111.,  22.]), 'dynamicTrap': False, 'previousTarget': array([111.,  22.]), 'currentState': array([111.48230665,  21.52198323,   1.90125499]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 0.6790579821125831}
episode index:1698
target Thresh 75.9859428930081
target distance 44.0
model initialize at round 1698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.9824098 , 17.32562339]), 'dynamicTrap': False, 'previousTarget': array([91.59715  , 17.1492875]), 'currentState': array([109.2653458 ,  22.63301248,   2.62761399]), 'targetState': array([67, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6425738747907818
running average episode reward sum: 0.684100963520906
{'scaleFactor': 20, 'currentTarget': array([67., 11.]), 'dynamicTrap': False, 'previousTarget': array([67., 11.]), 'currentState': array([67.94525004, 10.79875398,  4.49057869]), 'targetState': array([67, 11], dtype=int32), 'currentDistance': 0.9664355142816901}
episode index:1699
target Thresh 75.98601300312171
target distance 17.0
model initialize at round 1699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  23.]), 'dynamicTrap': False, 'previousTarget': array([107.,  23.]), 'currentState': array([116.83177241,   7.46305937,   2.06384873]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 18.38641544196866}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6842359142761782
{'scaleFactor': 20, 'currentTarget': array([107.,  23.]), 'dynamicTrap': False, 'previousTarget': array([107.,  23.]), 'currentState': array([107.98410941,  22.55501849,   2.05754717]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 1.0800369797043832}
episode index:1700
target Thresh 75.98608276355968
target distance 23.0
model initialize at round 1700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.00282766, 10.445265  ]), 'dynamicTrap': False, 'previousTarget': array([89.7042351, 10.5731765]), 'currentState': array([71.48209014, 14.79736873,  5.76677102]), 'targetState': array([93, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6861458620020944
running average episode reward sum: 0.6842370371143475
{'scaleFactor': 20, 'currentTarget': array([93., 10.]), 'dynamicTrap': False, 'previousTarget': array([93., 10.]), 'currentState': array([92.54042005,  9.88849429,  5.88767739]), 'targetState': array([93, 10], dtype=int32), 'currentDistance': 0.47291357651528626}
episode index:1701
target Thresh 75.986152176066
target distance 27.0
model initialize at round 1701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.11618457, 13.61978342]), 'dynamicTrap': False, 'previousTarget': array([53.64006204, 13.01924318]), 'currentState': array([71.62559201,  9.21716816,  3.1345129 ]), 'targetState': array([46, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7984853509374011
running average episode reward sum: 0.6843041630331624
{'scaleFactor': 20, 'currentTarget': array([46., 15.]), 'dynamicTrap': False, 'previousTarget': array([46., 15.]), 'currentState': array([45.81899682, 15.57539152,  2.45530166]), 'targetState': array([46, 15], dtype=int32), 'currentDistance': 0.6031894850775326}
episode index:1702
target Thresh 75.986221242376
target distance 57.0
model initialize at round 1702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.65753872, 10.24420612]), 'dynamicTrap': False, 'previousTarget': array([53.95093522, 10.59993437]), 'currentState': array([32.69170922, 11.41281751,  4.39969301]), 'targetState': array([91,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6260293625952725
running average episode reward sum: 0.6842699441250957
{'scaleFactor': 20, 'currentTarget': array([91.,  8.]), 'dynamicTrap': False, 'previousTarget': array([91.,  8.]), 'currentState': array([9.05818968e+01, 8.31344901e+00, 6.48852934e-02]), 'targetState': array([91,  8], dtype=int32), 'currentDistance': 0.5225519761460122}
episode index:1703
target Thresh 75.98628996421635
target distance 19.0
model initialize at round 1703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.,  9.]), 'dynamicTrap': False, 'previousTarget': array([85.,  9.]), 'currentState': array([67.08804295, 15.33418079,  6.22638518]), 'targetState': array([85,  9], dtype=int32), 'currentDistance': 18.998948702625245}
done in step count: 24
reward sum = 0.6725314241133774
running average episode reward sum: 0.6842630553222719
{'scaleFactor': 20, 'currentTarget': array([85.,  9.]), 'dynamicTrap': False, 'previousTarget': array([85.,  9.]), 'currentState': array([84.75877435,  8.59613132,  0.24102629]), 'targetState': array([85,  9], dtype=int32), 'currentDistance': 0.4704250494397678}
episode index:1704
target Thresh 75.9863583433051
target distance 12.0
model initialize at round 1704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.,  4.]), 'dynamicTrap': False, 'previousTarget': array([49.,  4.]), 'currentState': array([37.95486464,  4.61413507,  0.04251068]), 'targetState': array([49,  4], dtype=int32), 'currentDistance': 11.06219585115459}
done in step count: 9
reward sum = 0.8841132574836408
running average episode reward sum: 0.684380269517088
{'scaleFactor': 20, 'currentTarget': array([49.,  4.]), 'dynamicTrap': False, 'previousTarget': array([49.,  4.]), 'currentState': array([48.89206348,  3.95271812,  1.15108357]), 'targetState': array([49,  4], dtype=int32), 'currentDistance': 0.11783831741076257}
episode index:1705
target Thresh 75.9864263813517
target distance 27.0
model initialize at round 1705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.93578043, 17.68638779]), 'dynamicTrap': False, 'previousTarget': array([83.75509063, 18.20634329]), 'currentState': array([66.68190755,  7.57185019,  6.16785574]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7339492475082616
running average episode reward sum: 0.6844093251900019
{'scaleFactor': 20, 'currentTarget': array([93., 23.]), 'dynamicTrap': False, 'previousTarget': array([93., 23.]), 'currentState': array([93.26662437, 22.9348262 ,  0.68729279]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 0.27447437335195035}
episode index:1706
target Thresh 75.98649408005714
target distance 6.0
model initialize at round 1706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([11.68928068, 15.51245271,  2.11219977]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 4.540174947993216}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.684582547612269
{'scaleFactor': 20, 'currentTarget': array([11., 20.]), 'dynamicTrap': False, 'previousTarget': array([11., 20.]), 'currentState': array([11.37002146, 19.39779805,  1.34606345]), 'targetState': array([11, 20], dtype=int32), 'currentDistance': 0.7067977589750053}
episode index:1707
target Thresh 75.98656144111388
target distance 16.0
model initialize at round 1707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([11.,  6.]), 'currentState': array([ 7.12395912, 23.85339305,  1.54780531]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 18.26930037460171}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6847006988558895
{'scaleFactor': 20, 'currentTarget': array([11.,  6.]), 'dynamicTrap': False, 'previousTarget': array([11.,  6.]), 'currentState': array([10.36690014,  5.41924087,  5.31532618]), 'targetState': array([11,  6], dtype=int32), 'currentDistance': 0.8591254875885269}
episode index:1708
target Thresh 75.98662846620596
target distance 73.0
model initialize at round 1708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.04680909,  8.24823797]), 'dynamicTrap': False, 'previousTarget': array([63.69021635,  9.50647688]), 'currentState': array([43.43248237,  4.3395058 ,  5.25509226]), 'targetState': array([117,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5122072843332236
running average episode reward sum: 0.6845997664892876
{'scaleFactor': 20, 'currentTarget': array([117.,  19.]), 'dynamicTrap': False, 'previousTarget': array([117.,  19.]), 'currentState': array([117.26142699,  18.78538373,   2.27727293]), 'targetState': array([117,  19], dtype=int32), 'currentDistance': 0.33823691927812893}
episode index:1709
target Thresh 75.98669515700898
target distance 73.0
model initialize at round 1709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.77890509, 14.50042654]), 'dynamicTrap': False, 'previousTarget': array([58.18505209, 13.71437643]), 'currentState': array([76.62317648, 12.00946426,  3.47756612]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 53
reward sum = 0.5211035687791129
running average episode reward sum: 0.6845041546777613
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 21.]), 'currentState': array([ 4.4908905 , 21.12337021,  2.75883364]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.5238441492214277}
episode index:1710
target Thresh 75.98676151519022
target distance 7.0
model initialize at round 1710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.,  5.]), 'dynamicTrap': False, 'previousTarget': array([47.,  5.]), 'currentState': array([52.32852269,  3.15485217,  3.69598484]), 'targetState': array([47,  5], dtype=int32), 'currentDistance': 5.638947117984384}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6846711884856643
{'scaleFactor': 20, 'currentTarget': array([47.,  5.]), 'dynamicTrap': False, 'previousTarget': array([47.,  5.]), 'currentState': array([47.37541975,  4.85590642,  2.47547344]), 'targetState': array([47,  5], dtype=int32), 'currentDistance': 0.40212305313664254}
episode index:1711
target Thresh 75.98682754240868
target distance 42.0
model initialize at round 1711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.81130792, 10.13434945]), 'dynamicTrap': False, 'previousTarget': array([88.54387571,  9.36758945]), 'currentState': array([107.06434534,  15.54919481,   1.94979605]), 'targetState': array([66,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6111164859571039
running average episode reward sum: 0.6846282242902622
{'scaleFactor': 20, 'currentTarget': array([66.,  4.]), 'dynamicTrap': False, 'previousTarget': array([66.,  4.]), 'currentState': array([65.90504619,  4.07453853,  4.18414086]), 'targetState': array([66,  4], dtype=int32), 'currentDistance': 0.1207154426450432}
episode index:1712
target Thresh 75.986893240315
target distance 45.0
model initialize at round 1712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.21396298, 15.82485286]), 'dynamicTrap': False, 'previousTarget': array([47.61161351, 15.9223227 ]), 'currentState': array([29.67116416, 11.57291046,  5.29810101]), 'targetState': array([73, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6749802739852827
running average episode reward sum: 0.6846225920951046
{'scaleFactor': 20, 'currentTarget': array([73., 21.]), 'dynamicTrap': False, 'previousTarget': array([73., 21.]), 'currentState': array([73.48498184, 20.92682581,  1.69996356]), 'targetState': array([73, 21], dtype=int32), 'currentDistance': 0.49047104878000025}
episode index:1713
target Thresh 75.98695861055164
target distance 73.0
model initialize at round 1713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.58687448, 21.28130126]), 'dynamicTrap': False, 'previousTarget': array([67.11902654, 20.82126318]), 'currentState': array([85.44024401, 23.69867964,  2.02603436]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5133386953541891
running average episode reward sum: 0.6845226598332954
{'scaleFactor': 20, 'currentTarget': array([14., 15.]), 'dynamicTrap': False, 'previousTarget': array([14., 15.]), 'currentState': array([13.91513216, 15.74829105,  4.516046  ]), 'targetState': array([14, 15], dtype=int32), 'currentDistance': 0.7530883336285559}
episode index:1714
target Thresh 75.98702365475287
target distance 28.0
model initialize at round 1714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.03400227, 15.66106799]), 'dynamicTrap': False, 'previousTarget': array([21.88618308, 16.13066247]), 'currentState': array([ 2.31066569, 12.34595196,  5.90804744]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7404904233200033
running average episode reward sum: 0.6845552940977191
{'scaleFactor': 20, 'currentTarget': array([30., 17.]), 'dynamicTrap': False, 'previousTarget': array([30., 17.]), 'currentState': array([30.16716577, 17.22124821,  0.55966357]), 'targetState': array([30, 17], dtype=int32), 'currentDistance': 0.27729977249313287}
episode index:1715
target Thresh 75.98708837454478
target distance 8.0
model initialize at round 1715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.87991962, 14.86826339]), 'dynamicTrap': True, 'previousTarget': array([31., 15.]), 'currentState': array([24.        ,  7.        ,  0.88824296], dtype=float32), 'targetState': array([31, 15], dtype=int32), 'currentDistance': 10.45193105254368}
done in step count: 6
reward sum = 0.931480149401
running average episode reward sum: 0.6846991897010427
{'scaleFactor': 20, 'currentTarget': array([31., 15.]), 'dynamicTrap': False, 'previousTarget': array([31., 15.]), 'currentState': array([30.68041758, 14.12119966,  1.11134898]), 'targetState': array([31, 15], dtype=int32), 'currentDistance': 0.9351058534501759}
episode index:1716
target Thresh 75.9871527715454
target distance 28.0
model initialize at round 1716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.96642864, 17.91428358]), 'dynamicTrap': False, 'previousTarget': array([97.88618308, 17.86933753]), 'currentState': array([78.09470616, 20.17584183,  0.98480053]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7189010360149012
running average episode reward sum: 0.684719109238791
{'scaleFactor': 20, 'currentTarget': array([106.,  17.]), 'dynamicTrap': False, 'previousTarget': array([106.,  17.]), 'currentState': array([105.89790318,  16.88327809,   2.3521014 ]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 0.15507341557880688}
episode index:1717
target Thresh 75.98721684736464
target distance 53.0
model initialize at round 1717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.31861613,  7.17600914]), 'dynamicTrap': True, 'previousTarget': array([61.33675644,  7.10782246]), 'currentState': array([42.      ,  2.      ,  5.075669], dtype=float32), 'targetState': array([95, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5729537011666235
running average episode reward sum: 0.6846540537044067
{'scaleFactor': 20, 'currentTarget': array([93.22554429, 15.81537884]), 'dynamicTrap': True, 'previousTarget': array([95., 16.]), 'currentState': array([92.94504219, 14.86542826,  0.15766598]), 'targetState': array([95, 16], dtype=int32), 'currentDistance': 0.9904986326909683}
episode index:1718
target Thresh 75.98728060360439
target distance 28.0
model initialize at round 1718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.89134046,  3.92132985]), 'dynamicTrap': False, 'previousTarget': array([83.88618308,  3.86933753]), 'currentState': array([64.01920547,  6.17926055,  1.36306624]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.782866816211422
running average episode reward sum: 0.6847111873649693
{'scaleFactor': 20, 'currentTarget': array([92.,  3.]), 'dynamicTrap': False, 'previousTarget': array([92.,  3.]), 'currentState': array([9.23984936e+01, 2.64511681e+00, 1.85889788e-02]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 0.5336096175209996}
episode index:1719
target Thresh 75.98734404185856
target distance 11.0
model initialize at round 1719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'dynamicTrap': False, 'previousTarget': array([21., 10.]), 'currentState': array([11.89937189,  2.66736325,  0.29770124]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 11.68712940020088}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6848604716452228
{'scaleFactor': 20, 'currentTarget': array([21., 10.]), 'dynamicTrap': False, 'previousTarget': array([21., 10.]), 'currentState': array([20.95639923,  9.62340525,  1.12258382]), 'targetState': array([21, 10], dtype=int32), 'currentDistance': 0.3791103174274065}
episode index:1720
target Thresh 75.98740716371313
target distance 46.0
model initialize at round 1720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.98987885, 15.93693091]), 'dynamicTrap': False, 'previousTarget': array([37.24011002, 16.06699718]), 'currentState': array([57.90928948, 22.42198306,  5.86381552]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.38963442761719647
running average episode reward sum: 0.6846889283308545
{'scaleFactor': 20, 'currentTarget': array([10.,  6.]), 'dynamicTrap': False, 'previousTarget': array([10.,  6.]), 'currentState': array([9.09757194, 5.40551236, 5.26363374]), 'targetState': array([10,  6], dtype=int32), 'currentDistance': 1.0806442336640452}
episode index:1721
target Thresh 75.98746997074615
target distance 28.0
model initialize at round 1721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.44732543,  6.69160031]), 'dynamicTrap': False, 'previousTarget': array([59.050826,  6.424941]), 'currentState': array([77.42448395,  5.73601809,  3.37565994]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6847857743487197
{'scaleFactor': 20, 'currentTarget': array([51.,  7.]), 'dynamicTrap': False, 'previousTarget': array([51.,  7.]), 'currentState': array([51.53931642,  6.7029196 ,  2.37246054]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 0.615726367830239}
episode index:1722
target Thresh 75.98753246452776
target distance 30.0
model initialize at round 1722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.94419053, 16.73514309]), 'dynamicTrap': False, 'previousTarget': array([87.1565257 , 17.25304229]), 'currentState': array([69.79840498, 22.51778053,  0.39499122]), 'targetState': array([98, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6848775662344047
{'scaleFactor': 20, 'currentTarget': array([98., 14.]), 'dynamicTrap': False, 'previousTarget': array([98., 14.]), 'currentState': array([97.14573313, 13.5726007 ,  5.85136041]), 'targetState': array([98, 14], dtype=int32), 'currentDistance': 0.955218320401366}
episode index:1723
target Thresh 75.98759464662035
target distance 6.0
model initialize at round 1723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97., 17.]), 'dynamicTrap': False, 'previousTarget': array([97., 17.]), 'currentState': array([93.67848242, 11.12282036,  1.08304327]), 'targetState': array([97, 17], dtype=int32), 'currentDistance': 6.750831028321958}
done in step count: 6
reward sum = 0.9030494873079901
running average episode reward sum: 0.6850041160726144
{'scaleFactor': 20, 'currentTarget': array([96.28119905, 14.77488799]), 'dynamicTrap': True, 'previousTarget': array([96.39342459, 14.94024156]), 'currentState': array([95.36949816, 14.88125712,  1.44281311]), 'targetState': array([97, 17], dtype=int32), 'currentDistance': 0.9178850129625267}
episode index:1724
target Thresh 75.98765651857845
target distance 12.0
model initialize at round 1724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.0587251, 18.1909408]), 'dynamicTrap': True, 'previousTarget': array([ 4., 18.]), 'currentState': array([16.       , 13.       ,  4.3313017], dtype=float32), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 13.020749314927462}
done in step count: 11
reward sum = 0.8561293041587164
running average episode reward sum: 0.6851033190802005
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([ 4.65379974, 17.83681044,  2.72862082]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 0.6738582405589792}
episode index:1725
target Thresh 75.98771808194887
target distance 68.0
model initialize at round 1725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.22859983, 16.01525048]), 'dynamicTrap': True, 'previousTarget': array([86.17290468, 15.62417438]), 'currentState': array([106.       ,  13.       ,   2.9704652], dtype=float32), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5667996579139202
running average episode reward sum: 0.6850347769821898
{'scaleFactor': 20, 'currentTarget': array([38., 22.]), 'dynamicTrap': False, 'previousTarget': array([38., 22.]), 'currentState': array([38.62677388, 22.09882775,  4.13863058]), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 0.6345174757845538}
episode index:1726
target Thresh 75.9877793382707
target distance 66.0
model initialize at round 1726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.26293794, 18.05412849]), 'dynamicTrap': False, 'previousTarget': array([94., 19.]), 'currentState': array([113.25857242,  17.63627446,   3.20441246]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6082502157920959
running average episode reward sum: 0.6849903157423578
{'scaleFactor': 20, 'currentTarget': array([48., 19.]), 'dynamicTrap': False, 'previousTarget': array([48., 19.]), 'currentState': array([47.42128996, 18.81949832,  3.58085612]), 'targetState': array([48, 19], dtype=int32), 'currentDistance': 0.606206374392178}
episode index:1727
target Thresh 75.98784028907536
target distance 63.0
model initialize at round 1727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.54752637, 16.21973266]), 'dynamicTrap': False, 'previousTarget': array([93.01007049, 17.36539906]), 'currentState': array([112.54725966,  16.32301939,   3.53290701]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5638245221781117
running average episode reward sum: 0.68492019664886
{'scaleFactor': 20, 'currentTarget': array([50., 16.]), 'dynamicTrap': False, 'previousTarget': array([50., 16.]), 'currentState': array([49.50039862, 16.20542586,  3.59595285]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 0.5401863778672472}
episode index:1728
target Thresh 75.98790093588661
target distance 9.0
model initialize at round 1728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.,  6.]), 'dynamicTrap': False, 'previousTarget': array([97.,  6.]), 'currentState': array([105.47681337,   9.57913527,   2.90072668]), 'targetState': array([97,  6], dtype=int32), 'currentDistance': 9.20144413246758}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.685074083203661
{'scaleFactor': 20, 'currentTarget': array([97.,  6.]), 'dynamicTrap': False, 'previousTarget': array([97.,  6.]), 'currentState': array([96.98714863,  5.37000561,  3.98594499]), 'targetState': array([97,  6], dtype=int32), 'currentDistance': 0.63012545176098}
episode index:1729
target Thresh 75.98796128022063
target distance 6.0
model initialize at round 1729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([10.34542834, 21.32274602,  3.95595491]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 4.336525552479661}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6852446184156821
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([10.06393201, 17.51595564,  4.69113016]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.5199014549815015}
episode index:1730
target Thresh 75.98802132358601
target distance 16.0
model initialize at round 1730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89., 11.]), 'dynamicTrap': False, 'previousTarget': array([89., 11.]), 'currentState': array([72.45507551, 12.26041728,  3.87393057]), 'targetState': array([89, 11], dtype=int32), 'currentDistance': 16.592865274387}
done in step count: 20
reward sum = 0.7343564075404779
running average episode reward sum: 0.6852729903331428
{'scaleFactor': 20, 'currentTarget': array([89., 11.]), 'dynamicTrap': False, 'previousTarget': array([89., 11.]), 'currentState': array([89.88280741, 10.93228146,  5.0338252 ]), 'targetState': array([89, 11], dtype=int32), 'currentDistance': 0.885400886348714}
episode index:1731
target Thresh 75.98808106748388
target distance 15.0
model initialize at round 1731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.94723757,  3.80743057]), 'dynamicTrap': True, 'previousTarget': array([11.,  4.]), 'currentState': array([18.       , 19.       ,  1.2043678], dtype=float32), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 16.74979473359628}
done in step count: 16
reward sum = 0.8024478209948755
running average episode reward sum: 0.6853406432376821
{'scaleFactor': 20, 'currentTarget': array([11.,  4.]), 'dynamicTrap': False, 'previousTarget': array([11.,  4.]), 'currentState': array([11.08971129,  4.51330799,  5.57716863]), 'targetState': array([11,  4], dtype=int32), 'currentDistance': 0.5210884807083407}
episode index:1732
target Thresh 75.9881405134078
target distance 22.0
model initialize at round 1732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.24900118,  6.5660818 ]), 'dynamicTrap': False, 'previousTarget': array([75.52085402,  7.33524419]), 'currentState': array([89.33864457, 19.69249266,  3.37747353]), 'targetState': array([69,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6947463723981147
running average episode reward sum: 0.6853460706636257
{'scaleFactor': 20, 'currentTarget': array([69.,  2.]), 'dynamicTrap': False, 'previousTarget': array([69.,  2.]), 'currentState': array([68.17453435,  2.15513753,  4.97993166]), 'targetState': array([69,  2], dtype=int32), 'currentDistance': 0.8399173691854207}
episode index:1733
target Thresh 75.98819966284394
target distance 16.0
model initialize at round 1733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 20.]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([23.69522674,  3.90009209,  0.9253847 ]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 16.951331926893296}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6854776572707885
{'scaleFactor': 20, 'currentTarget': array([29., 20.]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([29.37684825, 19.08265143,  1.04746946]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 0.9917373655261941}
episode index:1734
target Thresh 75.98825851727103
target distance 6.0
model initialize at round 1734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.,  7.]), 'dynamicTrap': False, 'previousTarget': array([47.,  7.]), 'currentState': array([51.83681029,  5.04685761,  3.41878378]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 5.2162725207586185}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6856418194279811
{'scaleFactor': 20, 'currentTarget': array([47.,  7.]), 'dynamicTrap': False, 'previousTarget': array([47.,  7.]), 'currentState': array([46.60737073,  6.52613508,  1.91938138]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 0.6153906920051716}
episode index:1735
target Thresh 75.98831707816046
target distance 71.0
model initialize at round 1735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.22561658, 20.92077805]), 'dynamicTrap': False, 'previousTarget': array([55.03166438, 20.12497665]), 'currentState': array([76.2097852 , 20.12516231,  1.75917214]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6013234413893204
running average episode reward sum: 0.6855932489337192
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.07900459, 23.45007909,  3.55614817]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.45696051492261525}
episode index:1736
target Thresh 75.98837534697623
target distance 2.0
model initialize at round 1736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  10.]), 'dynamicTrap': False, 'previousTarget': array([109.,  10.]), 'currentState': array([106.71725859,  10.65904954,   0.72959828]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 2.3759744633904782}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.68576849749507
{'scaleFactor': 20, 'currentTarget': array([109.,  10.]), 'dynamicTrap': False, 'previousTarget': array([109.,  10.]), 'currentState': array([108.34349001,  10.22573736,   5.01278359]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 0.694235350768194}
episode index:1737
target Thresh 75.98843332517507
target distance 19.0
model initialize at round 1737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([34.2910587 , 14.67663092,  4.97052729]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 18.316044299297243}
done in step count: 16
reward sum = 0.8034231190018655
running average episode reward sum: 0.6858361929044524
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([52.6085946 , 10.0780007 ,  6.07890756]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 0.6135727326142667}
episode index:1738
target Thresh 75.98849101420642
target distance 45.0
model initialize at round 1738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.68700708,  7.90146611]), 'dynamicTrap': False, 'previousTarget': array([24.92145281,  8.22920419]), 'currentState': array([6.75320141, 9.52731724, 5.46166855]), 'targetState': array([50,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7120234126686941
running average episode reward sum: 0.6858512516852254
{'scaleFactor': 20, 'currentTarget': array([50.,  6.]), 'dynamicTrap': False, 'previousTarget': array([50.,  6.]), 'currentState': array([49.71983646,  5.08157816,  0.91042191]), 'targetState': array([50,  6], dtype=int32), 'currentDistance': 0.960203253716078}
episode index:1739
target Thresh 75.98854841551254
target distance 34.0
model initialize at round 1739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.17051655, 12.39393281]), 'dynamicTrap': True, 'previousTarget': array([81.30432882, 11.52429332]), 'currentState': array([101.       ,  15.       ,   4.3089023], dtype=float32), 'targetState': array([67,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5982577885707703
running average episode reward sum: 0.6858009106144699
{'scaleFactor': 20, 'currentTarget': array([67.,  9.]), 'dynamicTrap': False, 'previousTarget': array([67.,  9.]), 'currentState': array([66.57114212,  8.05828868,  0.31117192]), 'targetState': array([67,  9], dtype=int32), 'currentDistance': 1.034765333741765}
episode index:1740
target Thresh 75.98860553052845
target distance 38.0
model initialize at round 1740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.00073361,  5.1713005 ]), 'dynamicTrap': True, 'previousTarget': array([65.,  5.]), 'currentState': array([85.       ,  5.       ,  3.4091206], dtype=float32), 'targetState': array([47,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7333584089952527
running average episode reward sum: 0.6858282268111274
{'scaleFactor': 20, 'currentTarget': array([47.,  5.]), 'dynamicTrap': False, 'previousTarget': array([47.,  5.]), 'currentState': array([46.25453516,  5.22954148,  2.70855793]), 'targetState': array([47,  5], dtype=int32), 'currentDistance': 0.7800045648979419}
episode index:1741
target Thresh 75.98866236068201
target distance 65.0
model initialize at round 1741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.6923813 ,  3.92778058]), 'dynamicTrap': False, 'previousTarget': array([50.,  3.]), 'currentState': array([68.68787383,  4.35237245,  1.86107826]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.44088803991507963
running average episode reward sum: 0.6856876182078576
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'dynamicTrap': False, 'previousTarget': array([5., 3.]), 'currentState': array([4.22602623, 2.6758823 , 3.37550501]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.8390993305318417}
episode index:1742
target Thresh 75.98871890739403
target distance 37.0
model initialize at round 1742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.03143644,  9.29706864]), 'dynamicTrap': False, 'previousTarget': array([97.93458096,  9.38368262]), 'currentState': array([78.08961161, 10.82141191,  5.10114493]), 'targetState': array([115,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6892374422385862
running average episode reward sum: 0.6856896548252017
{'scaleFactor': 20, 'currentTarget': array([115.,   8.]), 'dynamicTrap': False, 'previousTarget': array([115.,   8.]), 'currentState': array([114.83660622,   8.08906322,   5.90431482]), 'targetState': array([115,   8], dtype=int32), 'currentDistance': 0.18609079983406707}
episode index:1743
target Thresh 75.98877517207812
target distance 61.0
model initialize at round 1743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.00822796, 19.45593312]), 'dynamicTrap': False, 'previousTarget': array([47.43927128, 18.83132011]), 'currentState': array([67.52001697, 23.84798156,  2.16881976]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5239905482000589
running average episode reward sum: 0.6855969374475497
{'scaleFactor': 20, 'currentTarget': array([ 6., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 10.]), 'currentState': array([6.86668254, 9.95282864, 4.468096  ]), 'targetState': array([ 6, 10], dtype=int32), 'currentDistance': 0.8679653003038454}
episode index:1744
target Thresh 75.98883115614095
target distance 8.0
model initialize at round 1744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 21.]), 'dynamicTrap': False, 'previousTarget': array([34., 21.]), 'currentState': array([26.49710201, 16.58311835,  6.05981398]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 8.706452880574133}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6857490251910754
{'scaleFactor': 20, 'currentTarget': array([34., 21.]), 'dynamicTrap': False, 'previousTarget': array([34., 21.]), 'currentState': array([33.73223503, 21.00132957,  1.31080335]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 0.2677682686352046}
episode index:1745
target Thresh 75.98888686098209
target distance 15.0
model initialize at round 1745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85., 16.]), 'dynamicTrap': False, 'previousTarget': array([85., 16.]), 'currentState': array([100.00519541,   8.68706173,   2.57082313]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 16.692362189075617}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6858794766356875
{'scaleFactor': 20, 'currentTarget': array([85., 16.]), 'dynamicTrap': False, 'previousTarget': array([85., 16.]), 'currentState': array([84.44354111, 16.47435713,  2.56878556]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 0.7312052908743824}
episode index:1746
target Thresh 75.98894228799418
target distance 5.0
model initialize at round 1746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70., 11.]), 'dynamicTrap': False, 'previousTarget': array([70., 11.]), 'currentState': array([73.8541131 , 14.65769594,  4.37618482]), 'targetState': array([70, 11], dtype=int32), 'currentDistance': 5.313466610423662}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6860367270840929
{'scaleFactor': 20, 'currentTarget': array([70., 11.]), 'dynamicTrap': False, 'previousTarget': array([70., 11.]), 'currentState': array([69.3530226 , 10.44705241,  2.48454317]), 'targetState': array([70, 11], dtype=int32), 'currentDistance': 0.8510762587710772}
episode index:1747
target Thresh 75.98899743856289
target distance 28.0
model initialize at round 1747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.97511473, 12.89151912]), 'dynamicTrap': False, 'previousTarget': array([55.3704006 , 13.55557175]), 'currentState': array([71.53631368, 22.4625322 ,  4.50350881]), 'targetState': array([45,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6423343375009071
running average episode reward sum: 0.6860117257170545
{'scaleFactor': 20, 'currentTarget': array([46.06911995,  9.53596611]), 'dynamicTrap': True, 'previousTarget': array([45.,  8.]), 'currentState': array([46.32660304,  9.32641854,  4.72529565]), 'targetState': array([45,  8], dtype=int32), 'currentDistance': 0.331975496947963}
episode index:1748
target Thresh 75.98905231406698
target distance 7.0
model initialize at round 1748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'dynamicTrap': False, 'previousTarget': array([16., 12.]), 'currentState': array([10.57378684,  6.07121035,  1.14434832]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 8.037060158917884}
done in step count: 6
reward sum = 0.922171199301
running average episode reward sum: 0.6861467511450612
{'scaleFactor': 20, 'currentTarget': array([16., 12.]), 'dynamicTrap': False, 'previousTarget': array([16., 12.]), 'currentState': array([15.15000671, 12.46408043,  0.9230692 ]), 'targetState': array([16, 12], dtype=int32), 'currentDistance': 0.9684313294623788}
episode index:1749
target Thresh 75.98910691587837
target distance 26.0
model initialize at round 1749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.36533724, 13.27940548]), 'dynamicTrap': False, 'previousTarget': array([27.05891029, 13.53392998]), 'currentState': array([46.18736161, 10.61719777,  3.18420684]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.722734997458638
running average episode reward sum: 0.6861676587143833
{'scaleFactor': 20, 'currentTarget': array([21., 14.]), 'dynamicTrap': False, 'previousTarget': array([21., 14.]), 'currentState': array([21.16585398, 14.27831806,  3.69632559]), 'targetState': array([21, 14], dtype=int32), 'currentDistance': 0.323988400370085}
episode index:1750
target Thresh 75.98916124536208
target distance 57.0
model initialize at round 1750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.58354995, 14.06811435]), 'dynamicTrap': False, 'previousTarget': array([54.01230012, 13.7013228 ]), 'currentState': array([72.57669501, 13.54452059,  1.77460217]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.3752836850818284
running average episode reward sum: 0.6859901121846103
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'dynamicTrap': False, 'previousTarget': array([17., 15.]), 'currentState': array([17.9032882 , 15.88256686,  4.10658692]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 1.2628752210932896}
episode index:1751
target Thresh 75.98921530387636
target distance 12.0
model initialize at round 1751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 10.]), 'dynamicTrap': False, 'previousTarget': array([87., 10.]), 'currentState': array([76.86756566,  2.33285735,  0.74227732]), 'targetState': array([87, 10], dtype=int32), 'currentDistance': 12.706348884460601}
done in step count: 7
reward sum = 0.9225554474079899
running average episode reward sum: 0.6861251380608794
{'scaleFactor': 20, 'currentTarget': array([87., 10.]), 'dynamicTrap': False, 'previousTarget': array([87., 10.]), 'currentState': array([86.30279054,  9.52762373,  0.63042325]), 'targetState': array([87, 10], dtype=int32), 'currentDistance': 0.8421640996500213}
episode index:1752
target Thresh 75.98926909277267
target distance 55.0
model initialize at round 1752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.91803716,  6.80881056]), 'dynamicTrap': True, 'previousTarget': array([76.91786413,  6.81071492]), 'currentState': array([57.       ,  5.       ,  3.1202364], dtype=float32), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.55025321386277
running average episode reward sum: 0.686047629832586
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([112.05034263,   9.22549575,   1.12259313]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 0.7761386587452578}
episode index:1753
target Thresh 75.98932261339576
target distance 42.0
model initialize at round 1753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.94346555,  9.99231957]), 'dynamicTrap': False, 'previousTarget': array([86.54387571,  9.63241055]), 'currentState': array([104.39515674,   5.34132935,   3.68290317]), 'targetState': array([64, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5954077320100731
running average episode reward sum: 0.6859959537220829
{'scaleFactor': 20, 'currentTarget': array([64., 15.]), 'dynamicTrap': False, 'previousTarget': array([64., 15.]), 'currentState': array([63.28809096, 15.28636678,  2.78709397]), 'targetState': array([64, 15], dtype=int32), 'currentDistance': 0.7673463486944417}
episode index:1754
target Thresh 75.98937586708361
target distance 67.0
model initialize at round 1754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.29434575, 11.67021026]), 'dynamicTrap': False, 'previousTarget': array([70.577174  , 12.09075278]), 'currentState': array([52.78636589,  7.26127369,  0.06272697]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.4091253559528456
running average episode reward sum: 0.6858381926977131
{'scaleFactor': 20, 'currentTarget': array([118.,  22.]), 'dynamicTrap': False, 'previousTarget': array([118.,  22.]), 'currentState': array([118.66838078,  22.63421696,   1.34920808]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 0.9213924339837118}
episode index:1755
target Thresh 75.9894288551676
target distance 64.0
model initialize at round 1755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.68369912, 14.54982286]), 'dynamicTrap': False, 'previousTarget': array([91.08731548, 14.13318583]), 'currentState': array([109.57103566,  16.66969205,   1.91291201]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.47659092457913554
running average episode reward sum: 0.6857190313832947
{'scaleFactor': 20, 'currentTarget': array([47., 10.]), 'dynamicTrap': False, 'previousTarget': array([47., 10.]), 'currentState': array([47.53254824,  9.50298171,  4.15865572]), 'targetState': array([47, 10], dtype=int32), 'currentDistance': 0.7284468437406373}
episode index:1756
target Thresh 75.9894815789724
target distance 6.0
model initialize at round 1756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'dynamicTrap': False, 'previousTarget': array([6., 7.]), 'currentState': array([10.43727665,  5.41447954,  3.83423376]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 4.7120376899343155}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6858810006312268
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'dynamicTrap': False, 'previousTarget': array([6., 7.]), 'currentState': array([5.63146274, 6.46435436, 2.77358449]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.6501814864700628}
episode index:1757
target Thresh 75.98953403981614
target distance 13.0
model initialize at round 1757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.0182039 , 23.17356499]), 'dynamicTrap': True, 'previousTarget': array([ 4., 23.]), 'currentState': array([17.       , 12.       ,  4.5088887], dtype=float32), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 17.128210195846986}
done in step count: 17
reward sum = 0.7750085412909168
running average episode reward sum: 0.6859316988909877
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.08753516, 23.84597132,  2.38012338]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.8504880200136773}
episode index:1758
target Thresh 75.98958623901032
target distance 3.0
model initialize at round 1758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.18116246,  6.91605347]), 'dynamicTrap': True, 'previousTarget': array([65.,  7.]), 'currentState': array([64.       ,  4.       ,  2.8066702], dtype=float32), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 3.146190167943078}
done in step count: 3
reward sum = 0.960299
running average episode reward sum: 0.6860876780274908
{'scaleFactor': 20, 'currentTarget': array([65.,  7.]), 'dynamicTrap': False, 'previousTarget': array([65.,  7.]), 'currentState': array([65.83723451,  6.39927671,  0.13293088]), 'targetState': array([65,  7], dtype=int32), 'currentDistance': 1.0304514061718246}
episode index:1759
target Thresh 75.98963817785994
target distance 8.0
model initialize at round 1759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([111.35314343,  13.97110241,   2.14913797]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 8.700963588757837}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.686238190738782
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([108.294295  ,  22.82471145,   3.04379855]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 0.875647490554072}
episode index:1760
target Thresh 75.98968985766348
target distance 29.0
model initialize at round 1760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.37918015,  8.73060733]), 'dynamicTrap': False, 'previousTarget': array([87.18757742,  9.26725206]), 'currentState': array([106.30358442,  10.46787943,   3.27576303]), 'targetState': array([78,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7630107389400459
running average episode reward sum: 0.6862817867343535
{'scaleFactor': 20, 'currentTarget': array([78.,  8.]), 'dynamicTrap': False, 'previousTarget': array([78.,  8.]), 'currentState': array([78.06607053,  7.19944567,  3.71440821]), 'targetState': array([78,  8], dtype=int32), 'currentDistance': 0.803276130883911}
episode index:1761
target Thresh 75.9897412797129
target distance 42.0
model initialize at round 1761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.75062517, 13.39390439]), 'dynamicTrap': False, 'previousTarget': array([93.34744447, 14.06718784]), 'currentState': array([74.57787886,  7.70129522,  4.48184621]), 'targetState': array([116,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.669272523629228
running average episode reward sum: 0.6862721333500712
{'scaleFactor': 20, 'currentTarget': array([116.,  20.]), 'dynamicTrap': False, 'previousTarget': array([116.,  20.]), 'currentState': array([116.77720535,  20.32503682,   1.71553549]), 'targetState': array([116,  20], dtype=int32), 'currentDistance': 0.8424352144396734}
episode index:1762
target Thresh 75.98979244529379
target distance 18.0
model initialize at round 1762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([48.18367556, 19.87947875,  2.82250649]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 16.184124317381375}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6864010301816842
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([31.48039686, 19.72688735,  4.13823454]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 0.5870076203961323}
episode index:1763
target Thresh 75.98984335568528
target distance 11.0
model initialize at round 1763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([117.50068265,  22.60719329,   2.88201916]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 10.622965984533598}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6865510239570347
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([107.7756724 ,  21.5309437 ,   3.67533071]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 0.9399834495521913}
episode index:1764
target Thresh 75.98989401216014
target distance 45.0
model initialize at round 1764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.01187471, 18.89672229]), 'dynamicTrap': False, 'previousTarget': array([76.82455801, 18.3567256 ]), 'currentState': array([58.27064441, 22.1035628 ,  5.98832566]), 'targetState': array([102,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6131197623221967
running average episode reward sum: 0.686509419842794
{'scaleFactor': 20, 'currentTarget': array([102.,  15.]), 'dynamicTrap': False, 'previousTarget': array([102.,  15.]), 'currentState': array([101.31114481,  14.30335771,   1.44820597]), 'targetState': array([102,  15], dtype=int32), 'currentDistance': 0.9797101359084982}
episode index:1765
target Thresh 75.98994441598475
target distance 46.0
model initialize at round 1765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.7770514, 12.6609283]), 'dynamicTrap': False, 'previousTarget': array([65.24618806, 13.5608599 ]), 'currentState': array([47.43996611, 17.76750568,  6.14560032]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6382421208505291
running average episode reward sum: 0.686482088416411
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([92.34045744,  6.17482797,  0.84991203]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 0.38272194073638005}
episode index:1766
target Thresh 75.98999456841926
target distance 46.0
model initialize at round 1766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.61655252, 16.67221334]), 'dynamicTrap': False, 'previousTarget': array([81.7042351, 16.5731765]), 'currentState': array([61.92296853, 20.15972672,  1.91915994]), 'targetState': array([108,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.528103069835316
running average episode reward sum: 0.686392456826948
{'scaleFactor': 20, 'currentTarget': array([108.,  12.]), 'dynamicTrap': False, 'previousTarget': array([108.,  12.]), 'currentState': array([108.21454321,  11.44852953,   6.15174756]), 'targetState': array([108,  12], dtype=int32), 'currentDistance': 0.5917334481464439}
episode index:1767
target Thresh 75.99004447071746
target distance 24.0
model initialize at round 1767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.15843462, 16.47536562]), 'dynamicTrap': False, 'previousTarget': array([94.27212152, 16.28797975]), 'currentState': array([115.0557928 ,  14.45172107,   1.74668826]), 'targetState': array([90, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8069469688992539
running average episode reward sum: 0.6864606437681654
{'scaleFactor': 20, 'currentTarget': array([90., 17.]), 'dynamicTrap': False, 'previousTarget': array([90., 17.]), 'currentState': array([90.71990589, 16.17168276,  4.02194325]), 'targetState': array([90, 17], dtype=int32), 'currentDistance': 1.097439723278376}
episode index:1768
target Thresh 75.9900941241269
target distance 43.0
model initialize at round 1768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.18088404, 11.25790189]), 'dynamicTrap': False, 'previousTarget': array([35.98257137, 11.80827905]), 'currentState': array([53.18915504, 17.47767951,  2.82303286]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5571322755861564
running average episode reward sum: 0.6863875355894306
{'scaleFactor': 20, 'currentTarget': array([12.,  4.]), 'dynamicTrap': False, 'previousTarget': array([12.,  4.]), 'currentState': array([11.44092517,  3.51982809,  4.21491544]), 'targetState': array([12,  4], dtype=int32), 'currentDistance': 0.7369733585475998}
episode index:1769
target Thresh 75.99014352988894
target distance 41.0
model initialize at round 1769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.0460386 , 18.22633994]), 'dynamicTrap': False, 'previousTarget': array([29.53488686, 17.2881459 ]), 'currentState': array([10.36269108, 14.68150845,  0.41239047]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5034286488643049
running average episode reward sum: 0.6862841689867609
{'scaleFactor': 20, 'currentTarget': array([51., 22.]), 'dynamicTrap': False, 'previousTarget': array([51., 22.]), 'currentState': array([51.47588587, 21.38669491,  4.26629976]), 'targetState': array([51, 22], dtype=int32), 'currentDistance': 0.7762799036112756}
episode index:1770
target Thresh 75.9901926892387
target distance 20.0
model initialize at round 1770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.94774342,  6.1595574 ]), 'dynamicTrap': False, 'previousTarget': array([32.0992562 ,  6.00992562]), 'currentState': array([52.67019541,  9.479932  ,  2.13940801]), 'targetState': array([32,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.799036248068075
running average episode reward sum: 0.6863478347569931
{'scaleFactor': 20, 'currentTarget': array([32.,  6.]), 'dynamicTrap': False, 'previousTarget': array([32.,  6.]), 'currentState': array([31.33879586,  5.62220652,  5.04029316]), 'targetState': array([32,  6], dtype=int32), 'currentDistance': 0.7615240148188137}
episode index:1771
target Thresh 75.9902416034052
target distance 1.0
model initialize at round 1771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([114.99614918,   7.35305772,   1.94617241]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 2.0271313017731516}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6865191960240602
{'scaleFactor': 20, 'currentTarget': array([113.,   7.]), 'dynamicTrap': False, 'previousTarget': array([113.,   7.]), 'currentState': array([113.34856302,   7.69634756,   3.94617241]), 'targetState': array([113,   7], dtype=int32), 'currentDistance': 0.7787143883406903}
episode index:1772
target Thresh 75.99029027361125
target distance 70.0
model initialize at round 1772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.70426149, 17.87250754]), 'dynamicTrap': False, 'previousTarget': array([79.12934655, 16.72906818]), 'currentState': array([98.51577945, 20.61180599,  2.87263942]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.36419134554512356
running average episode reward sum: 0.6863373980260462
{'scaleFactor': 20, 'currentTarget': array([29., 11.]), 'dynamicTrap': False, 'previousTarget': array([29., 11.]), 'currentState': array([29.09227416, 10.64617072,  3.05799854]), 'targetState': array([29, 11], dtype=int32), 'currentDistance': 0.36566334005819573}
episode index:1773
target Thresh 75.99033870107364
target distance 14.0
model initialize at round 1773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([16.79049475,  5.17452013,  1.36586189]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 15.006156813069753}
done in step count: 12
reward sum = 0.8473749216161293
running average episode reward sum: 0.6864281745331432
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([ 8.03455686, 17.25776359,  2.76550086]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 1.2177829605011365}
episode index:1774
target Thresh 75.99038688700307
target distance 33.0
model initialize at round 1774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.47856367, 10.43981615]), 'dynamicTrap': False, 'previousTarget': array([28.20732955,  9.72394111]), 'currentState': array([ 8.24226963, 18.65185487,  3.18958449]), 'targetState': array([43,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6864796636513776
{'scaleFactor': 20, 'currentTarget': array([43.,  3.]), 'dynamicTrap': False, 'previousTarget': array([43.,  3.]), 'currentState': array([42.92418919,  3.33218057,  1.27934741]), 'targetState': array([43,  3], dtype=int32), 'currentDistance': 0.3407215989420893}
episode index:1775
target Thresh 75.99043483260417
target distance 12.0
model initialize at round 1775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.,  7.]), 'dynamicTrap': False, 'previousTarget': array([51.,  7.]), 'currentState': array([50.59927087, 17.41582707,  5.21977055]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 10.423532865293984}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6866232450059664
{'scaleFactor': 20, 'currentTarget': array([51.,  7.]), 'dynamicTrap': False, 'previousTarget': array([51.,  7.]), 'currentState': array([50.81854545,  6.35737267,  4.80030002]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 0.6677541769366204}
episode index:1776
target Thresh 75.99048253907559
target distance 8.0
model initialize at round 1776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.,  5.]), 'dynamicTrap': False, 'previousTarget': array([54.,  5.]), 'currentState': array([61.81357377,  5.92758118,  3.50441139]), 'targetState': array([54,  5], dtype=int32), 'currentDistance': 7.868439609200308}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6867774221387711
{'scaleFactor': 20, 'currentTarget': array([54.,  5.]), 'dynamicTrap': False, 'previousTarget': array([54.,  5.]), 'currentState': array([54.05622947,  4.20690546,  3.38145375]), 'targetState': array([54,  5], dtype=int32), 'currentDistance': 0.7950853443449878}
episode index:1777
target Thresh 75.99053000760998
target distance 5.0
model initialize at round 1777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 15.]), 'currentState': array([ 2.41309809, 18.11597692,  5.3976438 ]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 4.049860941087935}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6869423954671521
{'scaleFactor': 20, 'currentTarget': array([ 5., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 15.]), 'currentState': array([ 5.36732932, 15.55552154,  5.23266017]), 'targetState': array([ 5, 15], dtype=int32), 'currentDistance': 0.6659842435387637}
episode index:1778
target Thresh 75.99057723939406
target distance 70.0
model initialize at round 1778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.75548917,  3.20800667]), 'dynamicTrap': False, 'previousTarget': array([65.99184173,  3.42880452]), 'currentState': array([47.76175586,  3.70863488,  5.45669827]), 'targetState': array([116,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5581959290478579
running average episode reward sum: 0.6868700253342576
{'scaleFactor': 20, 'currentTarget': array([116.,   2.]), 'dynamicTrap': False, 'previousTarget': array([116.,   2.]), 'currentState': array([116.35728429,   1.38123206,   6.20889878]), 'targetState': array([116,   2], dtype=int32), 'currentDistance': 0.7145109039796309}
episode index:1779
target Thresh 75.99062423560864
target distance 75.0
model initialize at round 1779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.23762427,  5.25474271]), 'dynamicTrap': False, 'previousTarget': array([32.70616041,  5.41573447]), 'currentState': array([14.55831422,  1.68756515,  5.51406974]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5583322521692415
running average episode reward sum: 0.6867978131021424
{'scaleFactor': 20, 'currentTarget': array([88., 15.]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([88.29017501, 15.25644153,  0.62237704]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 0.3872515885845401}
episode index:1780
target Thresh 75.99067099742864
target distance 18.0
model initialize at round 1780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'dynamicTrap': False, 'previousTarget': array([14.,  2.]), 'currentState': array([32.79576914,  6.21547833,  1.98021545]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 19.262689196031378}
done in step count: 13
reward sum = 0.8681062215049579
running average episode reward sum: 0.6868996145667144
{'scaleFactor': 20, 'currentTarget': array([14.,  2.]), 'dynamicTrap': False, 'previousTarget': array([14.,  2.]), 'currentState': array([13.56412871,  1.2537735 ,  3.09294826]), 'targetState': array([14,  2], dtype=int32), 'currentDistance': 0.8641977596444435}
episode index:1781
target Thresh 75.99071752602306
target distance 7.0
model initialize at round 1781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80., 18.]), 'dynamicTrap': False, 'previousTarget': array([80., 18.]), 'currentState': array([85.42523806, 18.79412869,  3.48976111]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 5.483051013713783}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6870586490142079
{'scaleFactor': 20, 'currentTarget': array([80., 18.]), 'dynamicTrap': False, 'previousTarget': array([80., 18.]), 'currentState': array([80.19209618, 17.61138985,  4.68569781]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 0.4334960140058872}
episode index:1782
target Thresh 75.99076382255517
target distance 40.0
model initialize at round 1782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.70645965,  8.99800288]), 'dynamicTrap': False, 'previousTarget': array([64.1565257 ,  9.74695771]), 'currentState': array([45.80021328,  2.4746718 ,  6.13741136]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6209759606070091
running average episode reward sum: 0.687021586373486
{'scaleFactor': 20, 'currentTarget': array([85., 16.]), 'dynamicTrap': False, 'previousTarget': array([85., 16.]), 'currentState': array([85.15418555, 16.65246665,  1.00630079]), 'targetState': array([85, 16], dtype=int32), 'currentDistance': 0.6704371048164622}
episode index:1783
target Thresh 75.99080988818235
target distance 22.0
model initialize at round 1783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.40631529,   9.94438516]), 'dynamicTrap': False, 'previousTarget': array([111.21853056,  10.82541376]), 'currentState': array([94.81598033, 19.46174383,  6.10813618]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7774409094327284
running average episode reward sum: 0.6870722698505372
{'scaleFactor': 20, 'currentTarget': array([116.,   8.]), 'dynamicTrap': False, 'previousTarget': array([116.,   8.]), 'currentState': array([115.7013948 ,   7.70139541,   5.70745699]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 0.4222910914072306}
episode index:1784
target Thresh 75.99085572405627
target distance 24.0
model initialize at round 1784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.49937129, 11.13621562]), 'dynamicTrap': False, 'previousTarget': array([41., 11.]), 'currentState': array([26.01955531, 23.80015249,  6.20605559]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6767446768904741
running average episode reward sum: 0.6870664840841729
{'scaleFactor': 20, 'currentTarget': array([49.,  5.]), 'dynamicTrap': False, 'previousTarget': array([49.,  5.]), 'currentState': array([48.60108156,  5.53071027,  5.06427108]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 0.6639196590624044}
episode index:1785
target Thresh 75.9909013313228
target distance 22.0
model initialize at round 1785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.05211672,  4.0752655 ]), 'dynamicTrap': False, 'previousTarget': array([41.0206292 ,  4.09184678]), 'currentState': array([61.03867825,  4.80831277,  4.81372824]), 'targetState': array([39,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7894843005415744
running average episode reward sum: 0.6871238288862207
{'scaleFactor': 20, 'currentTarget': array([39.,  4.]), 'dynamicTrap': False, 'previousTarget': array([39.,  4.]), 'currentState': array([39.39677779,  3.01998025,  3.48326421]), 'targetState': array([39,  4], dtype=int32), 'currentDistance': 1.0572943448354608}
episode index:1786
target Thresh 75.99094671112215
target distance 34.0
model initialize at round 1786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.20817083,  16.42809069]), 'dynamicTrap': True, 'previousTarget': array([102.18731003,  16.35667352]), 'currentState': array([83.       , 22.       ,  3.4814765], dtype=float32), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6704026670001773
running average episode reward sum: 0.6871144717726863
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([1.17930031e+02, 1.17331751e+01, 2.06411173e-02]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.9675504981345858}
episode index:1787
target Thresh 75.9909918645888
target distance 42.0
model initialize at round 1787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.2654078 , 16.92111024]), 'dynamicTrap': False, 'previousTarget': array([88.34744447, 15.93281216]), 'currentState': array([69.13237248, 22.74579561,  0.6046524 ]), 'targetState': array([111,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5929042988431891
running average episode reward sum: 0.68706178151937
{'scaleFactor': 20, 'currentTarget': array([111.,  10.]), 'dynamicTrap': False, 'previousTarget': array([111.,  10.]), 'currentState': array([111.92078578,   9.63897134,   4.3664097 ]), 'targetState': array([111,  10], dtype=int32), 'currentDistance': 0.9890339501735332}
episode index:1788
target Thresh 75.9910367928516
target distance 29.0
model initialize at round 1788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.0819539 , 17.74311583]), 'dynamicTrap': False, 'previousTarget': array([51.09254037, 17.51981367]), 'currentState': array([71.27044189, 12.10379595,  1.72532385]), 'targetState': array([41, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6871303483649271
{'scaleFactor': 20, 'currentTarget': array([41., 21.]), 'dynamicTrap': False, 'previousTarget': array([41., 21.]), 'currentState': array([41.77582487, 20.58793651,  2.14972835]), 'targetState': array([41, 21], dtype=int32), 'currentDistance': 0.8784648875654198}
episode index:1789
target Thresh 75.99108149703375
target distance 69.0
model initialize at round 1789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([41.96936904,  4.8935191 ]), 'dynamicTrap': True, 'previousTarget': array([41.96647808,  4.84252301]), 'currentState': array([22.       ,  6.       ,  5.0823913], dtype=float32), 'targetState': array([91,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5510069351617085
running average episode reward sum: 0.6870543017653723
{'scaleFactor': 20, 'currentTarget': array([91.,  2.]), 'dynamicTrap': False, 'previousTarget': array([91.,  2.]), 'currentState': array([90.18835524,  1.16166856,  5.7863528 ]), 'targetState': array([91,  2], dtype=int32), 'currentDistance': 1.1668619585680022}
episode index:1790
target Thresh 75.99112597825287
target distance 27.0
model initialize at round 1790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.01771735, 18.84165308]), 'dynamicTrap': True, 'previousTarget': array([15.01370332, 18.74023321]), 'currentState': array([35.       , 18.       ,  4.1710615], dtype=float32), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.796830488551902
running average episode reward sum: 0.6871155950019924
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.75555924, 18.79514906,  2.75431146]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.31892819011045237}
episode index:1791
target Thresh 75.99117023762098
target distance 31.0
model initialize at round 1791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.83191935,  9.13653004]), 'dynamicTrap': False, 'previousTarget': array([68.20692454,  9.5762039 ]), 'currentState': array([4.99309405e+01, 2.59795227e+00, 2.53483613e-02]), 'targetState': array([80, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7742954004487806
running average episode reward sum: 0.6871642444469962
{'scaleFactor': 20, 'currentTarget': array([80., 13.]), 'dynamicTrap': False, 'previousTarget': array([80., 13.]), 'currentState': array([79.52623012, 13.50962079,  0.81196075]), 'targetState': array([80, 13], dtype=int32), 'currentDistance': 0.6958241505943625}
episode index:1792
target Thresh 75.99121427624456
target distance 10.0
model initialize at round 1792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([80.60876029, 12.40295169,  4.55697179]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 13.067445518935989}
done in step count: 9
reward sum = 0.8940132574836408
running average episode reward sum: 0.6872796092060797
{'scaleFactor': 20, 'currentTarget': array([92.,  6.]), 'dynamicTrap': False, 'previousTarget': array([92.,  6.]), 'currentState': array([91.90614454,  5.87350064,  5.48930893]), 'targetState': array([92,  6], dtype=int32), 'currentDistance': 0.15751486953136248}
episode index:1793
target Thresh 75.99125809522461
target distance 62.0
model initialize at round 1793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.21628844, 18.26172805]), 'dynamicTrap': False, 'previousTarget': array([72.83555733, 19.44057325]), 'currentState': array([53.31951347, 20.29109884,  5.8103025 ]), 'targetState': array([115,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4391669006021316
running average episode reward sum: 0.6871413078077498
{'scaleFactor': 20, 'currentTarget': array([115.,  14.]), 'dynamicTrap': False, 'previousTarget': array([115.,  14.]), 'currentState': array([114.77175137,  13.37327222,   0.25063335]), 'targetState': array([115,  14], dtype=int32), 'currentDistance': 0.6669971069613804}
episode index:1794
target Thresh 75.99130169565657
target distance 13.0
model initialize at round 1794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.,  7.]), 'dynamicTrap': False, 'previousTarget': array([59.,  7.]), 'currentState': array([66.63337279, 20.98220067,  3.52842355]), 'targetState': array([59,  7], dtype=int32), 'currentDistance': 15.930169980316887}
done in step count: 15
reward sum = 0.7954419898843544
running average episode reward sum: 0.6872016424495753
{'scaleFactor': 20, 'currentTarget': array([59.,  7.]), 'dynamicTrap': False, 'previousTarget': array([59.,  7.]), 'currentState': array([59.37767939,  6.97022275,  2.94142715]), 'targetState': array([59,  7], dtype=int32), 'currentDistance': 0.3788514298544488}
episode index:1795
target Thresh 75.99134507863047
target distance 47.0
model initialize at round 1795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.12131908, 11.19955561]), 'dynamicTrap': True, 'previousTarget': array([43.11222174, 11.11572109]), 'currentState': array([63.       ,  9.       ,  4.7591786], dtype=float32), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5082760687122978
running average episode reward sum: 0.6871020179653118
{'scaleFactor': 20, 'currentTarget': array([16.31587446, 15.96724366]), 'dynamicTrap': True, 'previousTarget': array([16., 14.]), 'currentState': array([17.23331708, 15.45256832,  4.70234815]), 'targetState': array([16, 14], dtype=int32), 'currentDistance': 1.051946601493419}
episode index:1796
target Thresh 75.9913882452309
target distance 42.0
model initialize at round 1796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.40256863,  8.77852454]), 'dynamicTrap': False, 'previousTarget': array([46.45612429,  9.36758945]), 'currentState': array([25.80044305, 12.74799209,  4.8698014 ]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6184030196614193
running average episode reward sum: 0.6870637881387653
{'scaleFactor': 20, 'currentTarget': array([69.,  4.]), 'dynamicTrap': False, 'previousTarget': array([69.,  4.]), 'currentState': array([69.02742544,  3.08205113,  0.95028794]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 0.9183584739895079}
episode index:1797
target Thresh 75.99143119653698
target distance 4.0
model initialize at round 1797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 19.]), 'currentState': array([ 9.51186973, 23.1216053 ,  4.04244816]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 4.153268695499617}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6872213160652734
{'scaleFactor': 20, 'currentTarget': array([ 9., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 19.]), 'currentState': array([ 8.64119646, 18.93440888,  3.89526618]), 'targetState': array([ 9, 19], dtype=int32), 'currentDistance': 0.3647494663416333}
episode index:1798
target Thresh 75.99147393362254
target distance 38.0
model initialize at round 1798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.82146348, 17.48370586]), 'dynamicTrap': False, 'previousTarget': array([70.33093623, 17.37675141]), 'currentState': array([88.40588542, 21.53961725,  3.825194  ]), 'targetState': array([52, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6508887443646771
running average episode reward sum: 0.6872011200832274
{'scaleFactor': 20, 'currentTarget': array([52., 14.]), 'dynamicTrap': False, 'previousTarget': array([52., 14.]), 'currentState': array([51.3965682 , 13.24941878,  4.14537176]), 'targetState': array([52, 14], dtype=int32), 'currentDistance': 0.9630691064745771}
episode index:1799
target Thresh 75.99151645755602
target distance 4.0
model initialize at round 1799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 16.]), 'currentState': array([ 7.3531901 , 16.60464336,  2.38516521]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 2.429629031680916}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6873693416831812
{'scaleFactor': 20, 'currentTarget': array([ 5., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 16.]), 'currentState': array([ 5.65156562, 16.32441708,  4.2430321 ]), 'targetState': array([ 5, 16], dtype=int32), 'currentDistance': 0.727862761151516}
episode index:1800
target Thresh 75.99155876940047
target distance 43.0
model initialize at round 1800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.20586375, 10.78239956]), 'dynamicTrap': False, 'previousTarget': array([32.02159828, 10.07077201]), 'currentState': array([53.15186207, 12.25112379,  1.80673509]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 45
reward sum = 0.4944209373128771
running average episode reward sum: 0.6872622076441083
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([9.87752917, 9.00491212, 4.17790287]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.8775429173829865}
episode index:1801
target Thresh 75.99160087021373
target distance 42.0
model initialize at round 1801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.43755282, 17.22081592]), 'dynamicTrap': False, 'previousTarget': array([68.14023452, 17.3642578 ]), 'currentState': array([86.2551615 , 14.52593445,  3.42827083]), 'targetState': array([46, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6466020426678762
running average episode reward sum: 0.6872396437345766
{'scaleFactor': 20, 'currentTarget': array([46., 20.]), 'dynamicTrap': False, 'previousTarget': array([46., 20.]), 'currentState': array([46.04884643, 19.90140564,  2.8363506 ]), 'targetState': array([46, 20], dtype=int32), 'currentDistance': 0.11003100004297584}
episode index:1802
target Thresh 75.9916427610483
target distance 20.0
model initialize at round 1802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  14.]), 'dynamicTrap': False, 'previousTarget': array([108.56953382,  13.42781353]), 'currentState': array([91.80047242,  6.66160899,  6.1318053 ]), 'targetState': array([110,  14], dtype=int32), 'currentDistance': 19.623322517733207}
done in step count: 16
reward sum = 0.8039034655227957
running average episode reward sum: 0.6873043491265833
{'scaleFactor': 20, 'currentTarget': array([110.,  14.]), 'dynamicTrap': False, 'previousTarget': array([110.,  14.]), 'currentState': array([110.71519975,  13.9814942 ,   6.18784413]), 'targetState': array([110,  14], dtype=int32), 'currentDistance': 0.7154391263763161}
episode index:1803
target Thresh 75.99168444295147
target distance 47.0
model initialize at round 1803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.47846699,  8.10902726]), 'dynamicTrap': False, 'previousTarget': array([45.7164234 ,  8.35598696]), 'currentState': array([27.83591364,  4.34470821,  5.54513602]), 'targetState': array([73, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6873545248528986
{'scaleFactor': 20, 'currentTarget': array([73., 13.]), 'dynamicTrap': False, 'previousTarget': array([73., 13.]), 'currentState': array([72.20842061, 13.72195188,  0.5386558 ]), 'targetState': array([73, 13], dtype=int32), 'currentDistance': 1.0713600886033638}
episode index:1804
target Thresh 75.99172591696527
target distance 33.0
model initialize at round 1804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.61856041,  7.48254841]), 'dynamicTrap': False, 'previousTarget': array([47.14048809,  8.19985209]), 'currentState': array([29.36565044, 12.8978469 ,  5.52940691]), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6856989031693479
running average episode reward sum: 0.6873536076109686
{'scaleFactor': 20, 'currentTarget': array([61.,  4.]), 'dynamicTrap': False, 'previousTarget': array([61.,  4.]), 'currentState': array([60.35541973,  4.1651745 ,  3.98515485]), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 0.6654068938607632}
episode index:1805
target Thresh 75.99176718412656
target distance 29.0
model initialize at round 1805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.7734706 ,  8.01639103]), 'dynamicTrap': False, 'previousTarget': array([71.18757742,  7.73274794]), 'currentState': array([89.61525917,  5.50572851,  3.80142784]), 'targetState': array([62,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7474723885013719
running average episode reward sum: 0.6873868959724805
{'scaleFactor': 20, 'currentTarget': array([62.,  9.]), 'dynamicTrap': False, 'previousTarget': array([62.,  9.]), 'currentState': array([62.22418542,  9.11769582,  3.26241089]), 'targetState': array([62,  9], dtype=int32), 'currentDistance': 0.25320231094069323}
episode index:1806
target Thresh 75.99180824546704
target distance 21.0
model initialize at round 1806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.56117218, 13.21579736]), 'dynamicTrap': False, 'previousTarget': array([32.3102453 , 13.11990655]), 'currentState': array([51.68481526,  7.36034902,  1.99096835]), 'targetState': array([30, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6874824529501611
{'scaleFactor': 20, 'currentTarget': array([30., 14.]), 'dynamicTrap': False, 'previousTarget': array([30., 14.]), 'currentState': array([30.94463095, 14.93525225,  3.77704796]), 'targetState': array([30, 14], dtype=int32), 'currentDistance': 1.3292946979042293}
episode index:1807
target Thresh 75.99184910201322
target distance 15.0
model initialize at round 1807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50., 18.]), 'dynamicTrap': False, 'previousTarget': array([50., 18.]), 'currentState': array([45.22884937,  4.49293351,  0.44305992]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 14.324968534462993}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6876024195552819
{'scaleFactor': 20, 'currentTarget': array([50., 18.]), 'dynamicTrap': False, 'previousTarget': array([50., 18.]), 'currentState': array([49.62981569, 18.80176609,  4.84950368]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 0.883099813545036}
episode index:1808
target Thresh 75.99188975478653
target distance 13.0
model initialize at round 1808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.32435734,  15.48483923]), 'dynamicTrap': True, 'previousTarget': array([118.,  14.]), 'currentState': array([105.       ,  15.       ,   3.6394022], dtype=float32), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 13.333175454817393}
done in step count: 26
reward sum = 0.7501431458051551
running average episode reward sum: 0.6876369915432586
{'scaleFactor': 20, 'currentTarget': array([118.,  14.]), 'dynamicTrap': False, 'previousTarget': array([118.,  14.]), 'currentState': array([118.00346711,  13.16125451,   1.10622063]), 'targetState': array([118,  14], dtype=int32), 'currentDistance': 0.8387526541654662}
episode index:1809
target Thresh 75.99193020480328
target distance 15.0
model initialize at round 1809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.43764247, 16.55863726]), 'dynamicTrap': True, 'previousTarget': array([67., 15.]), 'currentState': array([52.      , 14.      ,  4.160057], dtype=float32), 'targetState': array([67, 15], dtype=int32), 'currentDistance': 14.662610434507208}
done in step count: 17
reward sum = 0.7498486283970032
running average episode reward sum: 0.6876713626133436
{'scaleFactor': 20, 'currentTarget': array([67., 15.]), 'dynamicTrap': False, 'previousTarget': array([67., 15.]), 'currentState': array([67.11372073, 15.583993  ,  5.95275019]), 'targetState': array([67, 15], dtype=int32), 'currentDistance': 0.5949623805427867}
episode index:1810
target Thresh 75.99197045307473
target distance 49.0
model initialize at round 1810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.19087416, 13.24345625]), 'dynamicTrap': True, 'previousTarget': array([40.20101013, 13.17157288]), 'currentState': array([60.       , 16.       ,  5.2211485], dtype=float32), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3518878891519325
running average episode reward sum: 0.6874859493204328
{'scaleFactor': 20, 'currentTarget': array([11.,  9.]), 'dynamicTrap': False, 'previousTarget': array([11.,  9.]), 'currentState': array([11.28886433,  9.27899793,  5.23390905]), 'targetState': array([11,  9], dtype=int32), 'currentDistance': 0.4015998611816791}
episode index:1811
target Thresh 75.9920105006071
target distance 39.0
model initialize at round 1811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.83691741,  9.79220983]), 'dynamicTrap': False, 'previousTarget': array([71.00657138, 10.48734798]), 'currentState': array([90.83570069,  9.57160311,  3.58786869]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.620473657069575
running average episode reward sum: 0.6874489668191907
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([51.81639037, 10.90386927,  5.08502719]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 0.9223297424130558}
episode index:1812
target Thresh 75.99205034840155
target distance 14.0
model initialize at round 1812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  16.]), 'dynamicTrap': False, 'previousTarget': array([116.,  16.]), 'currentState': array([103.4603245 ,  15.83404138,   1.51634711]), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 12.540773655873148}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6875838903608827
{'scaleFactor': 20, 'currentTarget': array([116.,  16.]), 'dynamicTrap': False, 'previousTarget': array([116.,  16.]), 'currentState': array([115.81452828,  16.13367642,   6.06791343]), 'targetState': array([116,  16], dtype=int32), 'currentDistance': 0.22862445637410028}
episode index:1813
target Thresh 75.99208999745431
target distance 64.0
model initialize at round 1813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.39713234,  9.21875026]), 'dynamicTrap': False, 'previousTarget': array([94.03894843, 10.24756572]), 'currentState': array([114.32498737,   7.5215191 ,   4.0573889 ]), 'targetState': array([50, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5819439261885881
running average episode reward sum: 0.6875256544379653
{'scaleFactor': 20, 'currentTarget': array([50., 13.]), 'dynamicTrap': False, 'previousTarget': array([50., 13.]), 'currentState': array([49.64179414, 12.68091771,  3.43343773]), 'targetState': array([50, 13], dtype=int32), 'currentDistance': 0.47971340054249073}
episode index:1814
target Thresh 75.9921294487566
target distance 8.0
model initialize at round 1814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.,  5.]), 'dynamicTrap': False, 'previousTarget': array([48.,  5.]), 'currentState': array([46.58619444, 11.83961619,  5.97821796]), 'targetState': array([48,  5], dtype=int32), 'currentDistance': 6.984210461865748}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6876761064245008
{'scaleFactor': 20, 'currentTarget': array([48.,  5.]), 'dynamicTrap': False, 'previousTarget': array([48.,  5.]), 'currentState': array([48.2952986 ,  4.90350613,  5.05620438]), 'targetState': array([48,  5], dtype=int32), 'currentDistance': 0.3106643358897933}
episode index:1815
target Thresh 75.9921687032947
target distance 46.0
model initialize at round 1815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.68431722, 10.50846318]), 'dynamicTrap': False, 'previousTarget': array([29.83200822, 11.41321632]), 'currentState': array([ 9.77456548, 12.40630029,  5.58186769]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6719447218194108
running average episode reward sum: 0.68766744376778
{'scaleFactor': 20, 'currentTarget': array([56.,  8.]), 'dynamicTrap': False, 'previousTarget': array([56.,  8.]), 'currentState': array([56.20167317,  7.71522819,  6.27187603]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 0.34895136421469375}
episode index:1816
target Thresh 75.99220776204996
target distance 62.0
model initialize at round 1816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.88227809, 14.03242433]), 'dynamicTrap': False, 'previousTarget': array([29.57433899, 14.89570311]), 'currentState': array([11.25331508, 17.86697919,  0.26390844]), 'targetState': array([72,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5179924513077853
running average episode reward sum: 0.6875740618236633
{'scaleFactor': 20, 'currentTarget': array([72.,  6.]), 'dynamicTrap': False, 'previousTarget': array([72.,  6.]), 'currentState': array([71.02664054,  5.95856207,  5.10984855]), 'targetState': array([72,  6], dtype=int32), 'currentDistance': 0.9742411065132354}
episode index:1817
target Thresh 75.99224662599887
target distance 61.0
model initialize at round 1817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.91099853,  9.57547443]), 'dynamicTrap': False, 'previousTarget': array([45.34559249, 10.07425377]), 'currentState': array([27.67678849,  4.09412752,  0.04537075]), 'targetState': array([87, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.377975651340148
running average episode reward sum: 0.6874037656682819
{'scaleFactor': 20, 'currentTarget': array([87., 21.]), 'dynamicTrap': False, 'previousTarget': array([87., 21.]), 'currentState': array([86.53201802, 20.36477227,  0.4518653 ]), 'targetState': array([87, 21], dtype=int32), 'currentDistance': 0.789000251244568}
episode index:1818
target Thresh 75.99228529611302
target distance 49.0
model initialize at round 1818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.99408738, 16.51371851]), 'dynamicTrap': True, 'previousTarget': array([36.99583637, 16.59192171]), 'currentState': array([17.       , 17.       ,  1.4866089], dtype=float32), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.2892138551646025
running average episode reward sum: 0.6871848597251793
{'scaleFactor': 20, 'currentTarget': array([64.3594537, 14.8622622]), 'dynamicTrap': True, 'previousTarget': array([66., 16.]), 'currentState': array([64.37955715, 13.89275589,  0.70846018]), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 0.9697147177710403}
episode index:1819
target Thresh 75.99232377335919
target distance 32.0
model initialize at round 1819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.96249361, 13.9671497 ]), 'dynamicTrap': False, 'previousTarget': array([54.91040044, 14.03450014]), 'currentState': array([74.08688934, 19.82013935,  5.0747327 ]), 'targetState': array([42, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.769401792749034
running average episode reward sum: 0.6872300338642033
{'scaleFactor': 20, 'currentTarget': array([43.62872953,  9.78079276]), 'dynamicTrap': True, 'previousTarget': array([42., 10.]), 'currentState': array([43.02477308, 10.32136319,  2.958272  ]), 'targetState': array([42, 10], dtype=int32), 'currentDistance': 0.8105428911199415}
episode index:1820
target Thresh 75.99236205869927
target distance 33.0
model initialize at round 1820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.37921392, 11.35388801]), 'dynamicTrap': False, 'previousTarget': array([27.56299789, 10.71200051]), 'currentState': array([46.999167  ,  7.47349613,  2.58136165]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.5357418098663299
running average episode reward sum: 0.6871468442848525
{'scaleFactor': 20, 'currentTarget': array([14., 14.]), 'dynamicTrap': False, 'previousTarget': array([14., 14.]), 'currentState': array([13.20109612, 13.84853904,  6.21904892]), 'targetState': array([14, 14], dtype=int32), 'currentDistance': 0.8131345728866516}
episode index:1821
target Thresh 75.99240015309044
target distance 63.0
model initialize at round 1821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.00408929, 17.40441925]), 'dynamicTrap': True, 'previousTarget': array([63.00251905, 17.31742033]), 'currentState': array([83.      , 17.      ,  4.258592], dtype=float32), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.550019717971039
running average episode reward sum: 0.687071582415306
{'scaleFactor': 20, 'currentTarget': array([20., 18.]), 'dynamicTrap': False, 'previousTarget': array([20., 18.]), 'currentState': array([19.23463941, 18.10221195,  3.25777663]), 'targetState': array([20, 18], dtype=int32), 'currentDistance': 0.7721554974968408}
episode index:1822
target Thresh 75.99243805748503
target distance 14.0
model initialize at round 1822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.14518059,  11.86290203]), 'dynamicTrap': True, 'previousTarget': array([116.,  12.]), 'currentState': array([102.       ,  17.       ,   2.4814088], dtype=float32), 'targetState': array([116,  12], dtype=int32), 'currentDistance': 15.049116568129273}
done in step count: 19
reward sum = 0.7396858713192276
running average episode reward sum: 0.687100443791556
{'scaleFactor': 20, 'currentTarget': array([116.,  12.]), 'dynamicTrap': False, 'previousTarget': array([116.,  12.]), 'currentState': array([115.77398541,  12.99141603,   6.20504097]), 'targetState': array([116,  12], dtype=int32), 'currentDistance': 1.0168521691984649}
episode index:1823
target Thresh 75.99247577283067
target distance 49.0
model initialize at round 1823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.40696707, 10.07568822]), 'dynamicTrap': False, 'previousTarget': array([91.87600089, 11.14571456]), 'currentState': array([109.64468995,  15.54469348,   3.77612489]), 'targetState': array([62,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6725088221992123
running average episode reward sum: 0.6870924439990164
{'scaleFactor': 20, 'currentTarget': array([61.74975441,  3.90194435]), 'dynamicTrap': True, 'previousTarget': array([62.,  2.]), 'currentState': array([61.6662494 ,  4.69795396,  5.33629968]), 'targetState': array([62,  2], dtype=int32), 'currentDistance': 0.8003776538231374}
episode index:1824
target Thresh 75.99251330007023
target distance 26.0
model initialize at round 1824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.25547185, 11.81352732]), 'dynamicTrap': True, 'previousTarget': array([20.23256605, 11.95885631]), 'currentState': array([40.       , 15.       ,  1.3827987], dtype=float32), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7297256281652563
running average episode reward sum: 0.6871158046478745
{'scaleFactor': 20, 'currentTarget': array([14., 11.]), 'dynamicTrap': False, 'previousTarget': array([14., 11.]), 'currentState': array([14.26251151, 11.58312783,  3.1260088 ]), 'targetState': array([14, 11], dtype=int32), 'currentDistance': 0.6394922666308845}
episode index:1825
target Thresh 75.99255064014191
target distance 27.0
model initialize at round 1825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.01364094, 16.78598718]), 'dynamicTrap': False, 'previousTarget': array([50.27623097, 16.12276932]), 'currentState': array([33.84411592,  8.42725757,  5.93380535]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7190728076977886
running average episode reward sum: 0.687133305744835
{'scaleFactor': 20, 'currentTarget': array([59., 20.]), 'dynamicTrap': False, 'previousTarget': array([59., 20.]), 'currentState': array([59.91598321, 20.1528615 ,  5.71081422]), 'targetState': array([59, 20], dtype=int32), 'currentDistance': 0.9286505681451006}
episode index:1826
target Thresh 75.9925877939792
target distance 69.0
model initialize at round 1826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.3052047 ,  6.61310922]), 'dynamicTrap': False, 'previousTarget': array([22.60061178,  5.97693572]), 'currentState': array([4.68165135, 2.75096554, 5.99862522]), 'targetState': array([72, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.33717911595698136
running average episode reward sum: 0.6869417599376167
{'scaleFactor': 20, 'currentTarget': array([72., 16.]), 'dynamicTrap': False, 'previousTarget': array([72., 16.]), 'currentState': array([72.54493574, 16.11748949,  0.69510664]), 'targetState': array([72, 16], dtype=int32), 'currentDistance': 0.5574573891747373}
episode index:1827
target Thresh 75.99262476251097
target distance 31.0
model initialize at round 1827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.10605425,  8.07267541]), 'dynamicTrap': False, 'previousTarget': array([89.22741199,  8.76826555]), 'currentState': array([70.4945619 , 15.39472108,  5.41735554]), 'targetState': array([102,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.4538466423756515
running average episode reward sum: 0.6868142461971561
{'scaleFactor': 20, 'currentTarget': array([102.,   3.]), 'dynamicTrap': False, 'previousTarget': array([102.,   3.]), 'currentState': array([101.51357822,   2.66581717,   6.27170675]), 'targetState': array([102,   3], dtype=int32), 'currentDistance': 0.5901561799395892}
episode index:1828
target Thresh 75.9926615466614
target distance 28.0
model initialize at round 1828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.95739831,   6.69529587]), 'dynamicTrap': True, 'previousTarget': array([100.949174,   6.575059]), 'currentState': array([81.      ,  8.      ,  4.877983], dtype=float32), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.736682006347845
running average episode reward sum: 0.6868415112382444
{'scaleFactor': 20, 'currentTarget': array([109.,   6.]), 'dynamicTrap': False, 'previousTarget': array([109.,   6.]), 'currentState': array([108.26209202,   5.29515179,   0.94085876]), 'targetState': array([109,   6], dtype=int32), 'currentDistance': 1.0204504859767634}
episode index:1829
target Thresh 75.99269814735011
target distance 68.0
model initialize at round 1829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.5553548 , 17.19822944]), 'dynamicTrap': False, 'previousTarget': array([31.74334797, 16.80622312]), 'currentState': array([13.85983718, 20.67480616,  0.09096467]), 'targetState': array([80,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.45126971129807936
running average episode reward sum: 0.6867127834787143
{'scaleFactor': 20, 'currentTarget': array([80.,  9.]), 'dynamicTrap': False, 'previousTarget': array([80.,  9.]), 'currentState': array([79.23607152,  8.78042918,  2.31260642]), 'targetState': array([80,  9], dtype=int32), 'currentDistance': 0.7948572582542224}
episode index:1830
target Thresh 75.99273456549214
target distance 57.0
model initialize at round 1830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.9937848 , 10.49856743]), 'dynamicTrap': True, 'previousTarget': array([62., 10.]), 'currentState': array([42.       , 10.       ,  3.3827891], dtype=float32), 'targetState': array([99, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5056429531334613
running average episode reward sum: 0.6866138922551506
{'scaleFactor': 20, 'currentTarget': array([99., 10.]), 'dynamicTrap': False, 'previousTarget': array([99., 10.]), 'currentState': array([98.00985062,  9.15157389,  4.70976811]), 'targetState': array([99, 10], dtype=int32), 'currentDistance': 1.3039258648900212}
episode index:1831
target Thresh 75.99277080199792
target distance 32.0
model initialize at round 1831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.87730234,  5.23987135]), 'dynamicTrap': False, 'previousTarget': array([86.96105157,  5.24756572]), 'currentState': array([68.93345233,  3.74225753,  5.76019425]), 'targetState': array([99,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.3437318414349153
running average episode reward sum: 0.6864267295636548
{'scaleFactor': 20, 'currentTarget': array([99.,  6.]), 'dynamicTrap': False, 'previousTarget': array([99.,  6.]), 'currentState': array([99.71065489,  6.56702574,  4.89791601]), 'targetState': array([99,  6], dtype=int32), 'currentDistance': 0.9091471564771905}
episode index:1832
target Thresh 75.99280685777339
target distance 14.0
model initialize at round 1832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,   8.]), 'dynamicTrap': False, 'previousTarget': array([109.,   8.]), 'currentState': array([112.24966456,  20.14671045,   4.28560589]), 'targetState': array([109,   8], dtype=int32), 'currentDistance': 12.573897351478376}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6865658749099927
{'scaleFactor': 20, 'currentTarget': array([109.,   8.]), 'dynamicTrap': False, 'previousTarget': array([109.,   8.]), 'currentState': array([109.25507734,   8.8314403 ,   5.44185277]), 'targetState': array([109,   8], dtype=int32), 'currentDistance': 0.8696881141622096}
episode index:1833
target Thresh 75.99284273371991
target distance 60.0
model initialize at round 1833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.50111297, 13.65846062]), 'dynamicTrap': False, 'previousTarget': array([46.67213115, 14.39344262]), 'currentState': array([25.75368589, 16.82691825,  4.83284056]), 'targetState': array([87,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5086444551182241
running average episode reward sum: 0.6864688621402043
{'scaleFactor': 20, 'currentTarget': array([87.,  7.]), 'dynamicTrap': False, 'previousTarget': array([87.,  7.]), 'currentState': array([86.82466107,  7.13056411,  6.26007173]), 'targetState': array([87,  7], dtype=int32), 'currentDistance': 0.2186109017317037}
episode index:1834
target Thresh 75.9928784307344
target distance 69.0
model initialize at round 1834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.29771785,  6.71733278]), 'dynamicTrap': False, 'previousTarget': array([66.7176794 ,  7.30962451]), 'currentState': array([85.50772504,  1.1517579 ,  5.42554498]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.40886146818046093
running average episode reward sum: 0.6863175774568475
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'dynamicTrap': False, 'previousTarget': array([17., 21.]), 'currentState': array([17.9126123 , 20.43642174,  3.05897082]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 1.072605084501076}
episode index:1835
target Thresh 75.99291394970929
target distance 27.0
model initialize at round 1835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.62383611,   9.19917365]), 'dynamicTrap': False, 'previousTarget': array([108.20583067,   9.80395219]), 'currentState': array([92.63119219, 19.7466877 ,  5.24475724]), 'targetState': array([118,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 93
reward sum = 0.3281058501739341
running average episode reward sum: 0.6861224730302228
{'scaleFactor': 20, 'currentTarget': array([118.,   4.]), 'dynamicTrap': False, 'previousTarget': array([118.,   4.]), 'currentState': array([118.64156106,   3.12051121,   0.56120821]), 'targetState': array([118,   4], dtype=int32), 'currentDistance': 1.0886235027224802}
episode index:1836
target Thresh 75.99294929153255
target distance 64.0
model initialize at round 1836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.0203504 ,  9.01313865]), 'dynamicTrap': False, 'previousTarget': array([72.67029746,  8.13445224]), 'currentState': array([90.39838692,  4.06422962,  2.06665957]), 'targetState': array([28, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6603157394310099
running average episode reward sum: 0.6861084247266849
{'scaleFactor': 20, 'currentTarget': array([28., 20.]), 'dynamicTrap': False, 'previousTarget': array([28., 20.]), 'currentState': array([28.4450328 , 20.53567787,  3.13417349]), 'targetState': array([28, 20], dtype=int32), 'currentDistance': 0.6964229797095631}
episode index:1837
target Thresh 75.99298445708776
target distance 22.0
model initialize at round 1837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.83404576, 19.7003781 ]), 'dynamicTrap': False, 'previousTarget': array([ 9.11145618, 18.94427191]), 'currentState': array([26.01367931, 11.3636567 ,  3.20698345]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7282181741731553
running average episode reward sum: 0.6861313353629452
{'scaleFactor': 20, 'currentTarget': array([ 5., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 21.]), 'currentState': array([ 5.5890343 , 21.63679773,  3.00349594]), 'targetState': array([ 5, 21], dtype=int32), 'currentDistance': 0.8674518730403499}
episode index:1838
target Thresh 75.993019447254
target distance 12.0
model initialize at round 1838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.,  2.]), 'dynamicTrap': False, 'previousTarget': array([55.,  2.]), 'currentState': array([50.58595675, 13.46860985,  4.94988186]), 'targetState': array([55,  2], dtype=int32), 'currentDistance': 12.28872612393939}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.686259999505993
{'scaleFactor': 20, 'currentTarget': array([55.,  2.]), 'dynamicTrap': False, 'previousTarget': array([55.,  2.]), 'currentState': array([54.85381268,  1.16698107,  5.93413417]), 'targetState': array([55,  2], dtype=int32), 'currentDistance': 0.8457489414728036}
episode index:1839
target Thresh 75.99305426290607
target distance 9.0
model initialize at round 1839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 21.]), 'dynamicTrap': False, 'previousTarget': array([86., 21.]), 'currentState': array([89.62463581, 12.88011291,  1.56234145]), 'targetState': array([86, 21], dtype=int32), 'currentDistance': 8.892162343578008}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6863885237967114
{'scaleFactor': 20, 'currentTarget': array([86., 21.]), 'dynamicTrap': False, 'previousTarget': array([86., 21.]), 'currentState': array([86.38924857, 20.98823787,  2.92545827]), 'targetState': array([86, 21], dtype=int32), 'currentDistance': 0.38942623687826566}
episode index:1840
target Thresh 75.99308890491434
target distance 24.0
model initialize at round 1840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.89820811,   6.85502381]), 'dynamicTrap': False, 'previousTarget': array([103.58583933,   7.47433703]), 'currentState': array([85.78311103, 15.33106403,  5.59315681]), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8088631168471427
running average episode reward sum: 0.6864550499200414
{'scaleFactor': 20, 'currentTarget': array([110.,   4.]), 'dynamicTrap': False, 'previousTarget': array([110.,   4.]), 'currentState': array([110.26602559,   3.2023185 ,   1.01107358]), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 0.8408718046841719}
episode index:1841
target Thresh 75.99312337414489
target distance 6.0
model initialize at round 1841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([9.30034368, 6.88335252, 3.11858171]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 4.793031638954297}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6866091454412574
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([4.91585855, 9.69377243, 1.21114235]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.6988561820682427}
episode index:1842
target Thresh 75.99315767145941
target distance 20.0
model initialize at round 1842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.,  8.]), 'dynamicTrap': False, 'previousTarget': array([45.9007438 ,  7.99007438]), 'currentState': array([27.65354646,  6.31332444,  5.46045143]), 'targetState': array([46,  8], dtype=int32), 'currentDistance': 18.423822407541824}
done in step count: 12
reward sum = 0.8581395162440494
running average episode reward sum: 0.686702216722214
{'scaleFactor': 20, 'currentTarget': array([46.,  8.]), 'dynamicTrap': False, 'previousTarget': array([46.,  8.]), 'currentState': array([45.04608957,  7.73581439,  0.27892374]), 'targetState': array([46,  8], dtype=int32), 'currentDistance': 0.9898177338070838}
episode index:1843
target Thresh 75.99319179771538
target distance 19.0
model initialize at round 1843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.34671746, 18.36517151]), 'dynamicTrap': False, 'previousTarget': array([36.48094632, 17.75489296]), 'currentState': array([51.68931081,  5.53533016,  2.15880273]), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8421371176158057
running average episode reward sum: 0.6867865089678178
{'scaleFactor': 20, 'currentTarget': array([32., 22.]), 'dynamicTrap': False, 'previousTarget': array([32., 22.]), 'currentState': array([32.5543909 , 22.65368027,  3.56127701]), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 0.8571156079892145}
episode index:1844
target Thresh 75.99322575376594
target distance 9.0
model initialize at round 1844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,   8.]), 'dynamicTrap': False, 'previousTarget': array([116.,   8.]), 'currentState': array([116.82811689,  18.34001854,   3.12375855]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 10.373126870315224}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.686924554301386
{'scaleFactor': 20, 'currentTarget': array([116.,   8.]), 'dynamicTrap': False, 'previousTarget': array([116.,   8.]), 'currentState': array([115.50903642,   8.81944732,   3.72691023]), 'targetState': array([116,   8], dtype=int32), 'currentDistance': 0.9552691521883561}
episode index:1845
target Thresh 75.99325954045999
target distance 18.0
model initialize at round 1845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.05897325,  6.14576818]), 'dynamicTrap': False, 'previousTarget': array([22.80368799,  5.63557441]), 'currentState': array([ 8.4002613 , 20.75533996,  4.27014625]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7548533268741164
running average episode reward sum: 0.686961352119681
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'dynamicTrap': False, 'previousTarget': array([25.,  3.]), 'currentState': array([24.19229634,  2.61792904,  1.26678014]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.8935118458392605}
episode index:1846
target Thresh 75.9932931586422
target distance 36.0
model initialize at round 1846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.85430671, 13.47770763]), 'dynamicTrap': False, 'previousTarget': array([66.81107799, 13.20711072]), 'currentState': array([49.29248562, 20.92477404,  5.94698996]), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6417092790915957
running average episode reward sum: 0.6869368518094331
{'scaleFactor': 20, 'currentTarget': array([84.,  7.]), 'dynamicTrap': False, 'previousTarget': array([84.,  7.]), 'currentState': array([84.52170713,  6.73534437,  6.15542734]), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 0.58499652685092}
episode index:1847
target Thresh 75.99332660915302
target distance 66.0
model initialize at round 1847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.6445924 , 13.27745741]), 'dynamicTrap': False, 'previousTarget': array([26.96336993, 13.20990121]), 'currentState': array([ 8.68216153, 12.05215995,  5.30418307]), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.5046311391100107
running average episode reward sum: 0.6868382015319983
{'scaleFactor': 20, 'currentTarget': array([73., 16.]), 'dynamicTrap': False, 'previousTarget': array([73., 16.]), 'currentState': array([72.89442163, 16.57952631,  0.54466042]), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 0.5890649645655531}
episode index:1848
target Thresh 75.99335989282872
target distance 44.0
model initialize at round 1848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.59254793, 17.1561136 ]), 'dynamicTrap': False, 'previousTarget': array([65.29527642, 17.73765188]), 'currentState': array([47.25029635, 22.24308752,  0.46562928]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5253653021593695
running average episode reward sum: 0.6867508716783625
{'scaleFactor': 20, 'currentTarget': array([90., 11.]), 'dynamicTrap': False, 'previousTarget': array([90., 11.]), 'currentState': array([90.11635446, 11.9924882 ,  5.29267629]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 0.9992853430836383}
episode index:1849
target Thresh 75.9933930105014
target distance 1.0
model initialize at round 1849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  6.]), 'dynamicTrap': False, 'previousTarget': array([96.,  6.]), 'currentState': array([97.66279058,  6.72723942,  1.54329622]), 'targetState': array([96,  6], dtype=int32), 'currentDistance': 1.814869058471581}
done in step count: 6
reward sum = 0.902470199301
running average episode reward sum: 0.6868674767203207
{'scaleFactor': 20, 'currentTarget': array([96.,  6.]), 'dynamicTrap': False, 'previousTarget': array([96.,  6.]), 'currentState': array([95.2521166 ,  5.7673401 ,  5.43772456]), 'targetState': array([96,  6], dtype=int32), 'currentDistance': 0.7832370071712771}
episode index:1850
target Thresh 75.993425962999
target distance 38.0
model initialize at round 1850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.75691975, 14.6546758 ]), 'dynamicTrap': False, 'previousTarget': array([25.9232797 , 15.52624642]), 'currentState': array([ 6.56824611, 20.29336395,  5.33707488]), 'targetState': array([45,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6415029749099427
running average episode reward sum: 0.6868429686156149
{'scaleFactor': 20, 'currentTarget': array([45.,  9.]), 'dynamicTrap': False, 'previousTarget': array([45.,  9.]), 'currentState': array([44.3909306 ,  8.04460849,  1.24715978]), 'targetState': array([45,  9], dtype=int32), 'currentDistance': 1.1330218307734}
episode index:1851
target Thresh 75.99345875114533
target distance 55.0
model initialize at round 1851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.92735852, 19.38426664]), 'dynamicTrap': False, 'previousTarget': array([71.94731634, 18.45071392]), 'currentState': array([51.94854759, 18.46387885,  0.59592986]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5320063576596246
running average episode reward sum: 0.6867593635341052
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([107.82169079,  20.10056535,   0.88235028]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 1.2182604165842066}
episode index:1852
target Thresh 75.9934913757601
target distance 33.0
model initialize at round 1852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.23595288, 14.02630752]), 'dynamicTrap': False, 'previousTarget': array([30.66755086, 12.9792889 ]), 'currentState': array([46.68525858,  4.2527856 ,  1.82784855]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6106494128617352
running average episode reward sum: 0.6867182896265648
{'scaleFactor': 20, 'currentTarget': array([16.99456936, 21.99989728]), 'dynamicTrap': True, 'previousTarget': array([15., 22.]), 'currentState': array([17.67799162, 22.06445449,  3.26796234]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.6864645788659224}
episode index:1853
target Thresh 75.99352383765893
target distance 6.0
model initialize at round 1853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.,  2.]), 'dynamicTrap': False, 'previousTarget': array([44.,  2.]), 'currentState': array([39.94657654,  2.73194496,  6.1439629 ]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 4.11897865316486}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6868025533286992
{'scaleFactor': 20, 'currentTarget': array([44.,  2.]), 'dynamicTrap': False, 'previousTarget': array([44.,  2.]), 'currentState': array([44.17734635,  1.75123005,  5.26143982]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 0.30551303725668344}
episode index:1854
target Thresh 75.99355613765336
target distance 68.0
model initialize at round 1854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.49912616, 14.06422198]), 'dynamicTrap': False, 'previousTarget': array([23.86301209, 14.33682495]), 'currentState': array([ 5.66010357, 11.53179571,  5.36144442]), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.4308132062014707
running average episode reward sum: 0.6866645536806523
{'scaleFactor': 20, 'currentTarget': array([72., 20.]), 'dynamicTrap': False, 'previousTarget': array([72., 20.]), 'currentState': array([71.12214664, 19.67754298,  0.7135046 ]), 'targetState': array([72, 20], dtype=int32), 'currentDistance': 0.9352032107181164}
episode index:1855
target Thresh 75.99358827655088
target distance 71.0
model initialize at round 1855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.87684355, 14.43797519]), 'dynamicTrap': False, 'previousTarget': array([21.62217106, 15.13083951]), 'currentState': array([ 1.19686138, 18.00144286,  4.76738393]), 'targetState': array([73,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5605801958143284
running average episode reward sum: 0.6865966202981811
{'scaleFactor': 20, 'currentTarget': array([73.,  5.]), 'dynamicTrap': False, 'previousTarget': array([73.,  5.]), 'currentState': array([72.47146397,  4.58961876,  3.48004059]), 'targetState': array([73,  5], dtype=int32), 'currentDistance': 0.6691510270812077}
episode index:1856
target Thresh 75.993620255155
target distance 69.0
model initialize at round 1856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.17407524, 10.05790975]), 'dynamicTrap': False, 'previousTarget': array([29.79321198, 11.13141855]), 'currentState': array([11.33261235, 12.57114482,  5.08575904]), 'targetState': array([79,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5430311257676439
running average episode reward sum: 0.686519309854169
{'scaleFactor': 20, 'currentTarget': array([79.,  4.]), 'dynamicTrap': False, 'previousTarget': array([79.,  4.]), 'currentState': array([79.51124725,  3.03118444,  1.26549634]), 'targetState': array([79,  4], dtype=int32), 'currentDistance': 1.0954347697472258}
episode index:1857
target Thresh 75.99365207426516
target distance 22.0
model initialize at round 1857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.60983243,  7.0436495 ]), 'dynamicTrap': False, 'previousTarget': array([31.9414844 ,  6.93592685]), 'currentState': array([50.18006168, 14.4697242 ,  3.08967376]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 27
reward sum = 0.6291799132712009
running average episode reward sum: 0.686488449037924
{'scaleFactor': 20, 'currentTarget': array([29.,  6.]), 'dynamicTrap': False, 'previousTarget': array([29.,  6.]), 'currentState': array([29.12871246,  5.95324164,  2.39312102]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 0.136942472274063}
episode index:1858
target Thresh 75.99368373467684
target distance 26.0
model initialize at round 1858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.33926592,  4.75069532]), 'dynamicTrap': False, 'previousTarget': array([49.41934502,  5.20720018]), 'currentState': array([30.5159103 , 11.50948836,  5.35867405]), 'targetState': array([57,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7812894232481785
running average episode reward sum: 0.6865394447206622
{'scaleFactor': 20, 'currentTarget': array([57.,  2.]), 'dynamicTrap': False, 'previousTarget': array([57.,  2.]), 'currentState': array([56.48608579,  1.19212997,  6.20473059]), 'targetState': array([57,  2], dtype=int32), 'currentDistance': 0.9574767900313215}
episode index:1859
target Thresh 75.99371523718156
target distance 49.0
model initialize at round 1859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.72088634, 15.87302535]), 'dynamicTrap': False, 'previousTarget': array([98.98789087, 14.79196437]), 'currentState': array([118.57483809,  22.54599347,   2.21515152]), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7166351983433525
running average episode reward sum: 0.6865556252333626
{'scaleFactor': 20, 'currentTarget': array([69.,  5.]), 'dynamicTrap': False, 'previousTarget': array([69.,  5.]), 'currentState': array([68.23816142,  4.42660187,  2.42689653]), 'targetState': array([69,  5], dtype=int32), 'currentDistance': 0.9535111143534085}
episode index:1860
target Thresh 75.9937465825669
target distance 32.0
model initialize at round 1860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.18702331, 20.01569491]), 'dynamicTrap': False, 'previousTarget': array([57., 20.]), 'currentState': array([37.18704097, 20.04226722,  0.19278185]), 'targetState': array([69, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7516085962069136
running average episode reward sum: 0.6865905811554333
{'scaleFactor': 20, 'currentTarget': array([69., 20.]), 'dynamicTrap': False, 'previousTarget': array([69., 20.]), 'currentState': array([69.98896917, 20.30857936,  1.49897228]), 'targetState': array([69, 20], dtype=int32), 'currentDistance': 1.0359928804155267}
episode index:1861
target Thresh 75.99377777161645
target distance 59.0
model initialize at round 1861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.19318944,  9.00426852]), 'dynamicTrap': False, 'previousTarget': array([63.53149897,  8.30355062]), 'currentState': array([4.56260002e+01, 4.86601965e+00, 3.41575742e-02]), 'targetState': array([103,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5348946567352724
running average episode reward sum: 0.6865091118082689
{'scaleFactor': 20, 'currentTarget': array([103.,  17.]), 'dynamicTrap': False, 'previousTarget': array([103.,  17.]), 'currentState': array([102.6953395 ,  17.29523642,   1.30497569]), 'targetState': array([103,  17], dtype=int32), 'currentDistance': 0.42424352114061037}
episode index:1862
target Thresh 75.99380880510999
target distance 50.0
model initialize at round 1862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.927227  , 13.27084582]), 'dynamicTrap': False, 'previousTarget': array([85.44774604, 14.33254095]), 'currentState': array([66.34835791, 17.35347795,  5.93088627]), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6082118649603638
running average episode reward sum: 0.6864670843005675
{'scaleFactor': 20, 'currentTarget': array([116.,   7.]), 'dynamicTrap': False, 'previousTarget': array([116.,   7.]), 'currentState': array([116.11774241,   6.08690993,   5.87918081]), 'targetState': array([116,   7], dtype=int32), 'currentDistance': 0.9206501793615142}
episode index:1863
target Thresh 75.99383968382332
target distance 7.0
model initialize at round 1863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.14214591, 17.85931261]), 'dynamicTrap': True, 'previousTarget': array([53., 18.]), 'currentState': array([60.      , 17.      ,  2.372191], dtype=float32), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 6.91148181295918}
done in step count: 4
reward sum = 0.9505960099999999
running average episode reward sum: 0.6866087843680028
{'scaleFactor': 20, 'currentTarget': array([53., 18.]), 'dynamicTrap': False, 'previousTarget': array([53., 18.]), 'currentState': array([53.94244736, 18.15840874,  3.42980191]), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 0.9556674897231419}
episode index:1864
target Thresh 75.99387040852844
target distance 42.0
model initialize at round 1864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.54722455, 11.95337938]), 'dynamicTrap': False, 'previousTarget': array([91.02633404, 12.67544468]), 'currentState': array([108.5807098 ,  18.09556459,   2.69894886]), 'targetState': array([68,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5889353408557608
running average episode reward sum: 0.6865564125484251
{'scaleFactor': 20, 'currentTarget': array([68.,  5.]), 'dynamicTrap': False, 'previousTarget': array([68.,  5.]), 'currentState': array([68.9169134,  5.748385 ,  4.6337791]), 'targetState': array([68,  5], dtype=int32), 'currentDistance': 1.1835583186302328}
episode index:1865
target Thresh 75.99390097999344
target distance 56.0
model initialize at round 1865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.45251975, 10.81444229]), 'dynamicTrap': False, 'previousTarget': array([68.07924589, 10.22136124]), 'currentState': array([86.33105516, 13.01531254,  2.47927514]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6065743053987771
running average episode reward sum: 0.6865135496828573
{'scaleFactor': 20, 'currentTarget': array([32.,  7.]), 'dynamicTrap': False, 'previousTarget': array([32.,  7.]), 'currentState': array([31.62484783,  6.25444607,  3.72542962]), 'targetState': array([32,  7], dtype=int32), 'currentDistance': 0.834619565671177}
episode index:1866
target Thresh 75.99393139898262
target distance 3.0
model initialize at round 1866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  16.]), 'dynamicTrap': False, 'previousTarget': array([106.,  16.]), 'currentState': array([105.16032763,  12.49685404,   3.9451704 ]), 'targetState': array([106,  16], dtype=int32), 'currentDistance': 3.602371618763204}
done in step count: 7
reward sum = 0.8934454973079899
running average episode reward sum: 0.6866243862911192
{'scaleFactor': 20, 'currentTarget': array([106.,  16.]), 'dynamicTrap': False, 'previousTarget': array([106.,  16.]), 'currentState': array([105.4146738 ,  15.81648177,   2.07511613]), 'targetState': array([106,  16], dtype=int32), 'currentDistance': 0.613421308243072}
episode index:1867
target Thresh 75.99396166625647
target distance 39.0
model initialize at round 1867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.38811796, 14.85785686]), 'dynamicTrap': False, 'previousTarget': array([69.66691212, 14.17958159]), 'currentState': array([52.71574123,  7.69249943,  6.13069886]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.679988766251498
running average episode reward sum: 0.6866208340319974
{'scaleFactor': 20, 'currentTarget': array([90., 22.]), 'dynamicTrap': False, 'previousTarget': array([90., 22.]), 'currentState': array([90.4946817 , 21.59198225,  1.17318503]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 0.6412397932400321}
episode index:1868
target Thresh 75.99399178257167
target distance 21.0
model initialize at round 1868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.58206349, 14.38320068]), 'dynamicTrap': False, 'previousTarget': array([97.35899411, 14.09400392]), 'currentState': array([114.9494119 ,   4.46477294,   1.89590798]), 'targetState': array([93, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.611811382433768
running average episode reward sum: 0.6865808075731433
{'scaleFactor': 20, 'currentTarget': array([93., 17.]), 'dynamicTrap': False, 'previousTarget': array([93., 17.]), 'currentState': array([92.75166528, 16.77043402,  1.44230235]), 'targetState': array([93, 17], dtype=int32), 'currentDistance': 0.3381873299690381}
episode index:1869
target Thresh 75.99402174868109
target distance 16.0
model initialize at round 1869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.,  3.]), 'dynamicTrap': False, 'previousTarget': array([44.,  3.]), 'currentState': array([50.49072721, 20.39934237,  2.88880217]), 'targetState': array([44,  3], dtype=int32), 'currentDistance': 18.570585736183094}
done in step count: 13
reward sum = 0.8503886472314562
running average episode reward sum: 0.6866684053483617
{'scaleFactor': 20, 'currentTarget': array([44.,  3.]), 'dynamicTrap': False, 'previousTarget': array([44.,  3.]), 'currentState': array([43.87059217,  3.34550663,  4.89734637]), 'targetState': array([44,  3], dtype=int32), 'currentDistance': 0.36894608823386693}
episode index:1870
target Thresh 75.99405156533395
target distance 39.0
model initialize at round 1870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.38500139, 15.67780187]), 'dynamicTrap': False, 'previousTarget': array([93.31457586, 15.53328126]), 'currentState': array([111.02953246,  11.92381744,   2.88656002]), 'targetState': array([74, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6187503338605606
running average episode reward sum: 0.6866321049360219
{'scaleFactor': 20, 'currentTarget': array([74., 19.]), 'dynamicTrap': False, 'previousTarget': array([74., 19.]), 'currentState': array([73.87669889, 19.11764675,  1.92229251]), 'targetState': array([74, 19], dtype=int32), 'currentDistance': 0.17042277077524773}
episode index:1871
target Thresh 75.99408123327561
target distance 58.0
model initialize at round 1871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.22562045,  9.97569637]), 'dynamicTrap': False, 'previousTarget': array([46.41479161, 11.05211208]), 'currentState': array([65.69054801,  5.38041671,  3.51359558]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5432043777810164
running average episode reward sum: 0.686555487560405
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 8.93555004, 19.8156985 ,  4.43986933]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 1.2412163128643912}
episode index:1872
target Thresh 75.9941107532478
target distance 41.0
model initialize at round 1872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.31234041,  8.85435785]), 'dynamicTrap': False, 'previousTarget': array([26.85291755,  8.57891249]), 'currentState': array([ 8.5192817 , 11.72399668,  5.71788842]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.738292986933064
running average episode reward sum: 0.6865831103577209
{'scaleFactor': 20, 'currentTarget': array([48.,  6.]), 'dynamicTrap': False, 'previousTarget': array([48.,  6.]), 'currentState': array([48.36545679,  5.78394267,  0.37565097]), 'targetState': array([48,  6], dtype=int32), 'currentDistance': 0.42454615315566047}
episode index:1873
target Thresh 75.99414012598852
target distance 25.0
model initialize at round 1873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.04306833, 15.25370072]), 'dynamicTrap': False, 'previousTarget': array([96.06369443, 15.59490445]), 'currentState': array([115.82760479,  12.32588293,   3.59977388]), 'targetState': array([91, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6866756798584058
{'scaleFactor': 20, 'currentTarget': array([91., 16.]), 'dynamicTrap': False, 'previousTarget': array([91., 16.]), 'currentState': array([91.10870477, 16.27673194,  4.04154392]), 'targetState': array([91, 16], dtype=int32), 'currentDistance': 0.2973168207413343}
episode index:1874
target Thresh 75.99416935223208
target distance 9.0
model initialize at round 1874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  6.]), 'dynamicTrap': False, 'previousTarget': array([19.,  6.]), 'currentState': array([10.52347355,  6.40051148,  6.03867769]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 8.485983158023696}
done in step count: 19
reward sum = 0.6737765614328175
running average episode reward sum: 0.6866688003285788
{'scaleFactor': 20, 'currentTarget': array([14.74129997,  5.59076656]), 'dynamicTrap': True, 'previousTarget': array([14.78409855,  5.75851654]), 'currentState': array([14.01914242,  4.68384909,  6.13061503]), 'targetState': array([19,  6], dtype=int32), 'currentDistance': 1.1593148174192365}
episode index:1875
target Thresh 75.99419843270914
target distance 73.0
model initialize at round 1875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.57502972, 18.34620803]), 'dynamicTrap': False, 'previousTarget': array([57.58160357, 18.21190225]), 'currentState': array([76.9819257 , 23.18070783,  1.74929171]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.6137695685146358
running average episode reward sum: 0.6866299414630064
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'dynamicTrap': False, 'previousTarget': array([4., 5.]), 'currentState': array([3.72965768, 5.22645159, 4.04574131]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.3526546391582904}
episode index:1876
target Thresh 75.99422736814672
target distance 15.0
model initialize at round 1876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.10818231,  6.1550861 ]), 'dynamicTrap': True, 'previousTarget': array([66.,  6.]), 'currentState': array([81.       ,  4.       ,  4.2023005], dtype=float32), 'targetState': array([66,  6], dtype=int32), 'currentDistance': 15.046947542535166}
done in step count: 16
reward sum = 0.8120537810948756
running average episode reward sum: 0.6866967629012759
{'scaleFactor': 20, 'currentTarget': array([66.,  6.]), 'dynamicTrap': False, 'previousTarget': array([66.,  6.]), 'currentState': array([66.71134563,  5.41372395,  3.55804882]), 'targetState': array([66,  6], dtype=int32), 'currentDistance': 0.9218092044305288}
episode index:1877
target Thresh 75.9942561592682
target distance 57.0
model initialize at round 1877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.26212738, 15.16519816]), 'dynamicTrap': False, 'previousTarget': array([49.92349448, 15.74767495]), 'currentState': array([31.37628312, 13.03137579,  0.39673012]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 75
reward sum = 0.19778989984467438
running average episode reward sum: 0.686436429108381
{'scaleFactor': 20, 'currentTarget': array([87., 19.]), 'dynamicTrap': False, 'previousTarget': array([87., 19.]), 'currentState': array([87.84646356, 18.10854497,  1.90102189]), 'targetState': array([87, 19], dtype=int32), 'currentDistance': 1.229305747358233}
episode index:1878
target Thresh 75.99428480679336
target distance 6.0
model initialize at round 1878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([92.68283074, 10.02165174,  5.28605086]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 6.5922583738161595}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6865823362828842
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([89.28755591,  3.81813132,  5.00033337]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 0.735290957682093}
episode index:1879
target Thresh 75.9943133114384
target distance 29.0
model initialize at round 1879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.98117754,  7.9259923 ]), 'dynamicTrap': False, 'previousTarget': array([26.2539594 ,  8.88561001]), 'currentState': array([10.76699743, 18.10793725,  5.56423262]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.5939554225426332
running average episode reward sum: 0.686533066647916
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([3.77065974e+01, 2.14859481e+00, 1.03276597e-02]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 0.32888528200066625}
episode index:1880
target Thresh 75.99434167391593
target distance 34.0
model initialize at round 1880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.92063857, 19.84462195]), 'dynamicTrap': False, 'previousTarget': array([31.69567118, 20.47570668]), 'currentState': array([12.40474619, 15.47084273,  5.98114109]), 'targetState': array([46, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 28
reward sum = 0.7000781082361768
running average episode reward sum: 0.6865402676269634
{'scaleFactor': 20, 'currentTarget': array([46., 23.]), 'dynamicTrap': False, 'previousTarget': array([46., 23.]), 'currentState': array([46.29255   , 23.00626382,  1.00053182]), 'targetState': array([46, 23], dtype=int32), 'currentDistance': 0.2926170546414365}
episode index:1881
target Thresh 75.994369894935
target distance 50.0
model initialize at round 1881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.51689038,  9.52814042]), 'dynamicTrap': False, 'previousTarget': array([47.74071961, 10.60740149]), 'currentState': array([66.93477973, 14.3182942 ,  3.66362619]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.507449314054883
running average episode reward sum: 0.6864451077153948
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'dynamicTrap': False, 'previousTarget': array([17.,  2.]), 'currentState': array([16.41063642,  1.50985008,  0.80071534]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7665483493148356}
episode index:1882
target Thresh 75.99439797520117
target distance 2.0
model initialize at round 1882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([5.08090162, 1.63790738, 5.0752914 ]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1399383190191275}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6866063158366293
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([4.81900217, 1.18761391, 3.43021798]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 1.1535751872348792}
episode index:1883
target Thresh 75.99442591541641
target distance 34.0
model initialize at round 1883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.73565249,  7.22754601]), 'dynamicTrap': False, 'previousTarget': array([46.33410456,  7.88214879]), 'currentState': array([28.3026649 , 11.95607574,  5.1936962 ]), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6959994677022303
running average episode reward sum: 0.6866113015860272
{'scaleFactor': 20, 'currentTarget': array([61.,  4.]), 'dynamicTrap': False, 'previousTarget': array([61.,  4.]), 'currentState': array([61.11463795,  4.52274677,  0.1420396 ]), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 0.5351691785404364}
episode index:1884
target Thresh 75.99445371627925
target distance 31.0
model initialize at round 1884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.495857  ,  8.18996359]), 'dynamicTrap': False, 'previousTarget': array([43.36440299,  8.80043813]), 'currentState': array([62.92386128,  3.44100059,  3.7481879 ]), 'targetState': array([32, 11], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.771188705746422
running average episode reward sum: 0.6866561702354491
{'scaleFactor': 20, 'currentTarget': array([32., 11.]), 'dynamicTrap': False, 'previousTarget': array([32., 11.]), 'currentState': array([32.6462924 , 10.26500903,  2.75116696]), 'targetState': array([32, 11], dtype=int32), 'currentDistance': 0.9787265175836}
episode index:1885
target Thresh 75.9944813784847
target distance 21.0
model initialize at round 1885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.96278609,  9.92575609]), 'dynamicTrap': False, 'previousTarget': array([84.64100589,  9.90599608]), 'currentState': array([66.66838222, 19.97083423,  4.80956984]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8152599204857413
running average episode reward sum: 0.6867243588623051
{'scaleFactor': 20, 'currentTarget': array([89.,  7.]), 'dynamicTrap': False, 'previousTarget': array([89.,  7.]), 'currentState': array([89.57848779,  7.38711457,  0.81386777]), 'targetState': array([89,  7], dtype=int32), 'currentDistance': 0.6960645155305034}
episode index:1886
target Thresh 75.99450890272435
target distance 50.0
model initialize at round 1886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.70234144, 11.69494262]), 'dynamicTrap': False, 'previousTarget': array([68.1565257 , 11.74695771]), 'currentState': array([50.64064684,  5.64086968,  0.79450339]), 'targetState': array([99, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6863604853588534
running average episode reward sum: 0.6867241660305597
{'scaleFactor': 20, 'currentTarget': array([99., 21.]), 'dynamicTrap': False, 'previousTarget': array([99., 21.]), 'currentState': array([98.35345942, 20.75532196,  2.2139954 ]), 'targetState': array([99, 21], dtype=int32), 'currentDistance': 0.6912901405282169}
episode index:1887
target Thresh 75.99453628968627
target distance 46.0
model initialize at round 1887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.83688165, 21.3023826 ]), 'dynamicTrap': False, 'previousTarget': array([92., 22.]), 'currentState': array([110.82899698,  20.74084481,   3.01610327]), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7208295563979308
running average episode reward sum: 0.6867422303263052
{'scaleFactor': 20, 'currentTarget': array([66., 22.]), 'dynamicTrap': False, 'previousTarget': array([66., 22.]), 'currentState': array([65.79125747, 22.2563924 ,  2.42377468]), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 0.3306213987470158}
episode index:1888
target Thresh 75.99456354005514
target distance 4.0
model initialize at round 1888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 23.]), 'dynamicTrap': False, 'previousTarget': array([44., 23.]), 'currentState': array([44.03891651, 20.41643207,  1.20612442]), 'targetState': array([44, 23], dtype=int32), 'currentDistance': 2.583861013838378}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6869027691138508
{'scaleFactor': 20, 'currentTarget': array([44., 23.]), 'dynamicTrap': False, 'previousTarget': array([44., 23.]), 'currentState': array([44.63342796, 22.28481149,  1.32047988]), 'targetState': array([44, 23], dtype=int32), 'currentDistance': 0.9553667289753566}
episode index:1889
target Thresh 75.99459065451224
target distance 12.0
model initialize at round 1889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.05808221, 18.3126339 ]), 'dynamicTrap': True, 'previousTarget': array([72., 17.]), 'currentState': array([84.      , 12.      ,  3.063318], dtype=float32), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 12.632296371754968}
done in step count: 13
reward sum = 0.8290846259546887
running average episode reward sum: 0.6869779976095338
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([72.02110862, 17.05822063,  3.66158232]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 0.06192911492655873}
episode index:1890
target Thresh 75.9946176337354
target distance 16.0
model initialize at round 1890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([104.97786918,  15.49849497,   4.87505019]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 16.326879742071426}
done in step count: 15
reward sum = 0.8029710917431029
running average episode reward sum: 0.6870393371622221
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([89.52228863,  8.02248174,  2.95415537]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 1.088002802744054}
episode index:1891
target Thresh 75.99464447839915
target distance 11.0
model initialize at round 1891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80., 18.]), 'dynamicTrap': False, 'previousTarget': array([80., 18.]), 'currentState': array([90.45365184,  9.75158812,  2.59327161]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 13.315972941297144}
done in step count: 8
reward sum = 0.904196594004571
running average episode reward sum: 0.6871541137250352
{'scaleFactor': 20, 'currentTarget': array([82.1486587 , 18.00534664]), 'dynamicTrap': True, 'previousTarget': array([81.95686458, 18.03786202]), 'currentState': array([83.0475155 , 17.97420432,  3.30951515]), 'targetState': array([80, 18], dtype=int32), 'currentDistance': 0.8993961164661675}
episode index:1892
target Thresh 75.99467118917457
target distance 58.0
model initialize at round 1892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.5443498,  9.1044184]), 'dynamicTrap': False, 'previousTarget': array([47.01188001,  8.31075448]), 'currentState': array([65.51127134, 10.25422014,  2.77485782]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5183306480215734
running average episode reward sum: 0.6870649307003636
{'scaleFactor': 20, 'currentTarget': array([9., 7.]), 'dynamicTrap': False, 'previousTarget': array([9., 7.]), 'currentState': array([8.11305057, 7.11488876, 3.62850114]), 'targetState': array([9, 7], dtype=int32), 'currentDistance': 0.8943593869823481}
episode index:1893
target Thresh 75.99469776672943
target distance 52.0
model initialize at round 1893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.34863335,  9.28196441]), 'dynamicTrap': True, 'previousTarget': array([36.35987106,  9.22305213]), 'currentState': array([56.      , 13.      ,  5.649708], dtype=float32), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6641252272675551
running average episode reward sum: 0.6870528189245279
{'scaleFactor': 20, 'currentTarget': array([4., 3.]), 'dynamicTrap': False, 'previousTarget': array([4., 3.]), 'currentState': array([4.14427638, 2.43428804, 3.46436457]), 'targetState': array([4, 3], dtype=int32), 'currentDistance': 0.5838199148400394}
episode index:1894
target Thresh 75.9947242117282
target distance 70.0
model initialize at round 1894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.54775068, 18.60663209]), 'dynamicTrap': False, 'previousTarget': array([66.87065345, 18.72906818]), 'currentState': array([48.68031951, 20.90558372,  5.21705485]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4253008960505136
running average episode reward sum: 0.686914691260742
{'scaleFactor': 20, 'currentTarget': array([117.,  13.]), 'dynamicTrap': False, 'previousTarget': array([117.,  13.]), 'currentState': array([116.12900213,  12.74685445,   5.63902923]), 'targetState': array([117,  13], dtype=int32), 'currentDistance': 0.9070391124795066}
episode index:1895
target Thresh 75.99475052483197
target distance 67.0
model initialize at round 1895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.18972744, 21.88787584]), 'dynamicTrap': False, 'previousTarget': array([79.00890472, 22.40325089]), 'currentState': array([97.18586822, 22.28075474,  3.13239443]), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6089491027907572
running average episode reward sum: 0.6868735701697769
{'scaleFactor': 20, 'currentTarget': array([32., 21.]), 'dynamicTrap': False, 'previousTarget': array([32., 21.]), 'currentState': array([32.63785654, 20.78488473,  3.84989676]), 'targetState': array([32, 21], dtype=int32), 'currentDistance': 0.6731534311634377}
episode index:1896
target Thresh 75.9947767066986
target distance 43.0
model initialize at round 1896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.17096778,  9.1732942 ]), 'dynamicTrap': False, 'previousTarget': array([26.37605409,  8.956665  ]), 'currentState': array([8.84751044, 4.01538798, 6.02160249]), 'targetState': array([50, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7045949053982046
running average episode reward sum: 0.6868829119384793
{'scaleFactor': 20, 'currentTarget': array([50., 15.]), 'dynamicTrap': False, 'previousTarget': array([50., 15.]), 'currentState': array([49.87209773, 14.45012704,  0.15437142]), 'targetState': array([50, 15], dtype=int32), 'currentDistance': 0.5645522669578071}
episode index:1897
target Thresh 75.99480275798263
target distance 50.0
model initialize at round 1897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.26117659,  7.38582178]), 'dynamicTrap': True, 'previousTarget': array([58.25928039,  7.39259851]), 'currentState': array([39.       ,  2.       ,  0.4295794], dtype=float32), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5563100170490524
running average episode reward sum: 0.6868141169464406
{'scaleFactor': 20, 'currentTarget': array([89., 16.]), 'dynamicTrap': False, 'previousTarget': array([89., 16.]), 'currentState': array([88.17157251, 15.47580055,  2.28409755]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 0.9803454343839689}
episode index:1898
target Thresh 75.99482867933533
target distance 16.0
model initialize at round 1898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.,  3.]), 'dynamicTrap': False, 'previousTarget': array([91.,  3.]), 'currentState': array([85.16083137, 17.46155136,  5.80670071]), 'targetState': array([91,  3], dtype=int32), 'currentDistance': 15.595908380065243}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6869334972152858
{'scaleFactor': 20, 'currentTarget': array([91.,  3.]), 'dynamicTrap': False, 'previousTarget': array([91.,  3.]), 'currentState': array([90.29435148,  3.77183787,  5.54451483]), 'targetState': array([91,  3], dtype=int32), 'currentDistance': 1.0457884728522964}
episode index:1899
target Thresh 75.99485447140475
target distance 37.0
model initialize at round 1899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.23146852, 12.42592975]), 'dynamicTrap': False, 'previousTarget': array([60.97554362, 12.17009396]), 'currentState': array([78.14716904,  5.93006382,  3.47168332]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6015875901816254
running average episode reward sum: 0.6868885783168471
{'scaleFactor': 20, 'currentTarget': array([43., 18.]), 'dynamicTrap': False, 'previousTarget': array([45.92093961, 17.64145322]), 'currentState': array([42.43438843, 18.88863063,  4.20218145]), 'targetState': array([43, 18], dtype=int32), 'currentDistance': 1.0533664351886196}
episode index:1900
target Thresh 75.99488013483568
target distance 33.0
model initialize at round 1900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.78430604,  2.98932585]), 'dynamicTrap': False, 'previousTarget': array([21.03663007,  3.20990121]), 'currentState': array([39.711154  ,  1.28031132,  2.7283988 ]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6869706691190918
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'dynamicTrap': False, 'previousTarget': array([8., 4.]), 'currentState': array([8.75508981, 3.10103306, 3.22098278]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 1.1740111525627293}
episode index:1901
target Thresh 75.99490567026972
target distance 35.0
model initialize at round 1901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.42680396,  9.74620217]), 'dynamicTrap': False, 'previousTarget': array([19.08108108,  9.48648649]), 'currentState': array([36.21945429,  2.9024975 ,  2.36792159]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6870352102332359
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 15.]), 'currentState': array([ 2.94620334, 15.05703029,  3.49601089]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.07839983275726946}
episode index:1902
target Thresh 75.99493107834525
target distance 5.0
model initialize at round 1902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 23.]), 'dynamicTrap': False, 'previousTarget': array([98., 23.]), 'currentState': array([93.3875617 , 22.57852209,  0.36262643]), 'targetState': array([98, 23], dtype=int32), 'currentDistance': 4.631655288262493}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6871892116992195
{'scaleFactor': 20, 'currentTarget': array([98., 23.]), 'dynamicTrap': False, 'previousTarget': array([98., 23.]), 'currentState': array([97.05094539, 23.52518028,  5.86121031]), 'targetState': array([98, 23], dtype=int32), 'currentDistance': 1.0846745972490794}
episode index:1903
target Thresh 75.99495635969747
target distance 61.0
model initialize at round 1903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.43858785, 12.48891776]), 'dynamicTrap': False, 'previousTarget': array([69.21419273, 12.91921747]), 'currentState': array([87.17147499,  9.23113249,  3.31530267]), 'targetState': array([28, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5686411603561352
running average episode reward sum: 0.6871269490672115
{'scaleFactor': 20, 'currentTarget': array([28., 19.]), 'dynamicTrap': False, 'previousTarget': array([28., 19.]), 'currentState': array([28.60851932, 18.59288283,  3.91625704]), 'targetState': array([28, 19], dtype=int32), 'currentDistance': 0.7321476348322437}
episode index:1904
target Thresh 75.99498151495843
target distance 6.0
model initialize at round 1904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  17.]), 'dynamicTrap': False, 'previousTarget': array([109.,  17.]), 'currentState': array([107.29857728,  23.84652607,   0.77072477]), 'targetState': array([109,  17], dtype=int32), 'currentDistance': 7.054768490731272}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6872705023800372
{'scaleFactor': 20, 'currentTarget': array([109.,  17.]), 'dynamicTrap': False, 'previousTarget': array([109.,  17.]), 'currentState': array([109.39057955,  17.19790644,   4.86556272]), 'targetState': array([109,  17], dtype=int32), 'currentDistance': 0.4378576785264311}
episode index:1905
target Thresh 75.99500654475699
target distance 44.0
model initialize at round 1905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.80452329, 20.33442628]), 'dynamicTrap': False, 'previousTarget': array([85., 21.]), 'currentState': array([63.8114979 , 19.80628258,  4.93663263]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.295747884460329
running average episode reward sum: 0.6870650865259345
{'scaleFactor': 20, 'currentTarget': array([109.,  21.]), 'dynamicTrap': False, 'previousTarget': array([109.,  21.]), 'currentState': array([109.40096841,  21.24864354,   3.68222676]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 0.47180427641739314}
episode index:1906
target Thresh 75.99503144971891
target distance 75.0
model initialize at round 1906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.57278486,  9.62314998]), 'dynamicTrap': False, 'previousTarget': array([91.04429684,  8.66961979]), 'currentState': array([110.50140202,  11.31140881,   2.94411832]), 'targetState': array([36,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.42203142421532064
running average episode reward sum: 0.6869261071539834
{'scaleFactor': 20, 'currentTarget': array([36.,  5.]), 'dynamicTrap': False, 'previousTarget': array([36.,  5.]), 'currentState': array([36.11746127,  5.33174194,  0.40463505]), 'targetState': array([36,  5], dtype=int32), 'currentDistance': 0.35192309767969554}
episode index:1907
target Thresh 75.99505623046683
target distance 63.0
model initialize at round 1907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.16308873, 12.79317808]), 'dynamicTrap': False, 'previousTarget': array([50.20101013, 13.82842712]), 'currentState': array([69.89000483,  9.49942929,  3.62921762]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5057588661726319
running average episode reward sum: 0.6868311557698212
{'scaleFactor': 20, 'currentTarget': array([ 7., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 20.]), 'currentState': array([ 6.38448724, 19.59702445,  1.80374574]), 'targetState': array([ 7, 20], dtype=int32), 'currentDistance': 0.7356937206736408}
episode index:1908
target Thresh 75.99508088762025
target distance 21.0
model initialize at round 1908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  8.]), 'dynamicTrap': False, 'previousTarget': array([32.79898987,  7.82842712]), 'currentState': array([14.52118157,  4.50531906,  5.38854635]), 'targetState': array([34,  8], dtype=int32), 'currentDistance': 19.789824719354897}
done in step count: 14
reward sum = 0.8405004572968984
running average episode reward sum: 0.6869116530466819
{'scaleFactor': 20, 'currentTarget': array([34.,  8.]), 'dynamicTrap': False, 'previousTarget': array([34.,  8.]), 'currentState': array([33.45088171,  8.16063387,  0.84802484]), 'targetState': array([34,  8], dtype=int32), 'currentDistance': 0.5721312249985006}
episode index:1909
target Thresh 75.99510542179559
target distance 4.0
model initialize at round 1909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([28.70049754, 11.68385047,  0.78348374]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 4.031285789792009}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6870651547990134
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([32.06097262, 13.56288475,  0.19652715]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 0.441347259505637}
episode index:1910
target Thresh 75.99512983360621
target distance 9.0
model initialize at round 1910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.39443504,  10.94230516]), 'dynamicTrap': True, 'previousTarget': array([114.,  10.]), 'currentState': array([105.      ,  18.      ,   2.745865], dtype=float32), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 12.564049352431741}
done in step count: 10
reward sum = 0.8746810750088044
running average episode reward sum: 0.6871633316280087
{'scaleFactor': 20, 'currentTarget': array([114.,  10.]), 'dynamicTrap': False, 'previousTarget': array([114.,  10.]), 'currentState': array([113.62726026,  10.45552216,   5.32660848]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 0.5885875922271048}
episode index:1911
target Thresh 75.99515412366245
target distance 67.0
model initialize at round 1911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.22404985, 13.72966443]), 'dynamicTrap': False, 'previousTarget': array([64.8219647 , 13.33734803]), 'currentState': array([46.43673693, 16.63865868,  5.69147295]), 'targetState': array([112,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5568837112255617
running average episode reward sum: 0.6870951937512291
{'scaleFactor': 20, 'currentTarget': array([112.,   7.]), 'dynamicTrap': False, 'previousTarget': array([112.,   7.]), 'currentState': array([112.15242612,   7.57061443,   1.15484377]), 'targetState': array([112,   7], dtype=int32), 'currentDistance': 0.5906221709409752}
episode index:1912
target Thresh 75.99517829257151
target distance 39.0
model initialize at round 1912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.33108793, 17.21081168]), 'dynamicTrap': False, 'previousTarget': array([30.51217609, 16.49719013]), 'currentState': array([48.91702747, 13.16223511,  1.98903978]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 27
reward sum = 0.7165625065343539
running average episode reward sum: 0.6871105974693593
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([10.7558201 , 20.5758459 ,  3.20535421]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.489418559788195}
episode index:1913
target Thresh 75.99520234093764
target distance 10.0
model initialize at round 1913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  13.]), 'dynamicTrap': False, 'previousTarget': array([116.,  13.]), 'currentState': array([106.21558029,   7.74636938,   0.57403088]), 'targetState': array([116,  13], dtype=int32), 'currentDistance': 11.105651886187838}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6872434969217792
{'scaleFactor': 20, 'currentTarget': array([116.,  13.]), 'dynamicTrap': False, 'previousTarget': array([116.,  13.]), 'currentState': array([116.02712845,  13.23056973,   1.36472243]), 'targetState': array([116,  13], dtype=int32), 'currentDistance': 0.23216019234699553}
episode index:1914
target Thresh 75.99522626936204
target distance 27.0
model initialize at round 1914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.23410121,  7.51546874]), 'dynamicTrap': False, 'previousTarget': array([67.12232531,  7.20863052]), 'currentState': array([85.14895232,  5.671911  ,  2.87936145]), 'targetState': array([60,  8], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 17
reward sum = 0.8083653637688857
running average episode reward sum: 0.6873067459384095
{'scaleFactor': 20, 'currentTarget': array([60.,  8.]), 'dynamicTrap': False, 'previousTarget': array([62.45570427,  7.5299046 ]), 'currentState': array([60.83896101,  8.13972499,  2.99256972]), 'targetState': array([60,  8], dtype=int32), 'currentDistance': 0.8505166929937853}
episode index:1915
target Thresh 75.99525007844292
target distance 32.0
model initialize at round 1915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.44175952, 12.6197701 ]), 'dynamicTrap': False, 'previousTarget': array([74.10917872, 13.51132248]), 'currentState': array([57.18626695, 20.78904124,  0.1825183 ]), 'targetState': array([88,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 24
reward sum = 0.7399978027490338
running average episode reward sum: 0.6873342464899809
{'scaleFactor': 20, 'currentTarget': array([88.,  7.]), 'dynamicTrap': False, 'previousTarget': array([88.,  7.]), 'currentState': array([87.31100255,  7.31861067,  4.34064358]), 'targetState': array([88,  7], dtype=int32), 'currentDistance': 0.7590983101612729}
episode index:1916
target Thresh 75.99527376877552
target distance 42.0
model initialize at round 1916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.25569983, 17.33103825]), 'dynamicTrap': False, 'previousTarget': array([63.14023452, 17.6357422 ]), 'currentState': array([81.12456545, 19.61755938,  2.82644439]), 'targetState': array([41, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7354816623372671
running average episode reward sum: 0.6873593625128537
{'scaleFactor': 20, 'currentTarget': array([42.42650067, 15.98812388]), 'dynamicTrap': True, 'previousTarget': array([41., 15.]), 'currentState': array([42.75076878, 15.73327024,  3.88431918]), 'targetState': array([41, 15], dtype=int32), 'currentDistance': 0.412432041113454}
episode index:1917
target Thresh 75.99529734095209
target distance 21.0
model initialize at round 1917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.87806482,  9.44321115]), 'dynamicTrap': False, 'previousTarget': array([61.9086344 ,  9.87913569]), 'currentState': array([45.27993999, 20.60126887,  5.24697542]), 'targetState': array([67,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8251151628932347
running average episode reward sum: 0.6874311851407893
{'scaleFactor': 20, 'currentTarget': array([67.,  6.]), 'dynamicTrap': False, 'previousTarget': array([67.,  6.]), 'currentState': array([66.29001933,  6.92335244,  5.27448897]), 'targetState': array([67,  6], dtype=int32), 'currentDistance': 1.1647541717356915}
episode index:1918
target Thresh 75.99532079556195
target distance 63.0
model initialize at round 1918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.84495113, 19.94153539]), 'dynamicTrap': False, 'previousTarget': array([79.02263725, 18.95130299]), 'currentState': array([99.83912576, 19.45885436,  2.0588155 ]), 'targetState': array([36, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6704415188971546
running average episode reward sum: 0.6874223317451438
{'scaleFactor': 20, 'currentTarget': array([36., 21.]), 'dynamicTrap': False, 'previousTarget': array([36., 21.]), 'currentState': array([36.61295259, 21.76349982,  3.84787542]), 'targetState': array([36, 21], dtype=int32), 'currentDistance': 0.9791030846968719}
episode index:1919
target Thresh 75.99534413319144
target distance 22.0
model initialize at round 1919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.08709031, 18.11525093]), 'dynamicTrap': True, 'previousTarget': array([19.17429997, 18.22895002]), 'currentState': array([35.       ,  6.       ,  0.7524705], dtype=float32), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8230431933839267
running average episode reward sum: 0.6874929676105807
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'dynamicTrap': False, 'previousTarget': array([13., 23.]), 'currentState': array([12.68833389, 23.41167156,  2.5335629 ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.5163421670610229}
episode index:1920
target Thresh 75.99536735442403
target distance 15.0
model initialize at round 1920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'dynamicTrap': False, 'previousTarget': array([23., 15.]), 'currentState': array([36.01569327,  3.95186485,  3.38113511]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 17.072479658842802}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6876058718830419
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'dynamicTrap': False, 'previousTarget': array([23., 15.]), 'currentState': array([22.456152  , 15.62579578,  1.85349299]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.8290904710344362}
episode index:1921
target Thresh 75.99539045984022
target distance 48.0
model initialize at round 1921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.90851224, 15.51348449]), 'dynamicTrap': False, 'previousTarget': array([30.00433887, 14.58342373]), 'currentState': array([50.88115875, 16.55913812,  1.86736095]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.3653711439602134
running average episode reward sum: 0.6874382159371923
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 14.]), 'currentState': array([ 1.7334669 , 14.39474951,  2.077483  ]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 0.4763056478204693}
episode index:1922
target Thresh 75.9954134500177
target distance 11.0
model initialize at round 1922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([12.31764944, 22.95434187,  4.17872548]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 13.754304703854269}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6875510312565223
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([ 4.18811802, 12.06872635,  0.01460935]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.20027905695867076}
episode index:1923
target Thresh 75.99543632553116
target distance 9.0
model initialize at round 1923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  9.]), 'dynamicTrap': False, 'previousTarget': array([32.,  9.]), 'currentState': array([26.58147869, 16.56266726,  5.43904424]), 'targetState': array([32,  9], dtype=int32), 'currentDistance': 9.303456846713434}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6876879538233848
{'scaleFactor': 20, 'currentTarget': array([32.,  9.]), 'dynamicTrap': False, 'previousTarget': array([32.,  9.]), 'currentState': array([32.20305588,  9.22998678,  5.70493403]), 'targetState': array([32,  9], dtype=int32), 'currentDistance': 0.3067989771843608}
episode index:1924
target Thresh 75.99545908695254
target distance 63.0
model initialize at round 1924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.845302  , 12.57665414]), 'dynamicTrap': False, 'previousTarget': array([98.15932542, 11.48054926]), 'currentState': array([118.6240384 ,  15.54340092,   2.17066826]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4703289968086911
running average episode reward sum: 0.6875750400794811
{'scaleFactor': 20, 'currentTarget': array([55.,  6.]), 'dynamicTrap': False, 'previousTarget': array([55.,  6.]), 'currentState': array([54.79409107,  6.11104222,  2.59544512]), 'targetState': array([55,  6], dtype=int32), 'currentDistance': 0.23394200208061783}
episode index:1925
target Thresh 75.99548173485084
target distance 37.0
model initialize at round 1925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.69212997, 12.87230333]), 'dynamicTrap': False, 'previousTarget': array([26.97084547, 13.07950516]), 'currentState': array([ 8.74617948, 11.40292961,  0.36654192]), 'targetState': array([44, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6531021089882798
running average episode reward sum: 0.6875571413613653
{'scaleFactor': 20, 'currentTarget': array([44., 14.]), 'dynamicTrap': False, 'previousTarget': array([44., 14.]), 'currentState': array([44.07256914, 13.92783057,  4.94743935]), 'targetState': array([44, 14], dtype=int32), 'currentDistance': 0.10234601538165276}
episode index:1926
target Thresh 75.99550426979228
target distance 41.0
model initialize at round 1926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.25490432, 10.9657422 ]), 'dynamicTrap': False, 'previousTarget': array([31.93538725, 10.04487721]), 'currentState': array([50.49457898,  5.5036072 ,  2.74781132]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5408694173755013
running average episode reward sum: 0.6874810190344395
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([10.39055705, 17.54196443,  2.61327487]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.6680271353467184}
episode index:1927
target Thresh 75.99552669234023
target distance 33.0
model initialize at round 1927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.90900705,  9.72387807]), 'dynamicTrap': False, 'previousTarget': array([52.85467564, 10.40662735]), 'currentState': array([32.16492993,  6.53461191,  5.19827986]), 'targetState': array([66, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7653689281715755
running average episode reward sum: 0.6875214173275603
{'scaleFactor': 20, 'currentTarget': array([65.01500395, 10.60943309]), 'dynamicTrap': True, 'previousTarget': array([66., 12.]), 'currentState': array([64.40981951, 10.36498465,  6.28288199]), 'targetState': array([66, 12], dtype=int32), 'currentDistance': 0.6526892427579876}
episode index:1928
target Thresh 75.99554900305527
target distance 75.0
model initialize at round 1928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.06421975,  7.39854265]), 'dynamicTrap': True, 'previousTarget': array([62.06369443,  7.40509555]), 'currentState': array([82.        ,  9.        ,  0.10533885], dtype=float32), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4795687145641837
running average episode reward sum: 0.6874136139565064
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([7.7958066 , 2.98591014, 3.66425933]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.7959313199232948}
episode index:1929
target Thresh 75.99557120249514
target distance 9.0
model initialize at round 1929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 17.]), 'dynamicTrap': False, 'previousTarget': array([88., 17.]), 'currentState': array([82.63926747,  9.78173956,  0.64922488]), 'targetState': array([88, 17], dtype=int32), 'currentDistance': 8.991147707862142}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6875501820580314
{'scaleFactor': 20, 'currentTarget': array([88., 17.]), 'dynamicTrap': False, 'previousTarget': array([88., 17.]), 'currentState': array([87.14374939, 17.45731991,  0.23311232]), 'targetState': array([88, 17], dtype=int32), 'currentDistance': 0.9707247846500725}
episode index:1930
target Thresh 75.99559329121485
target distance 10.0
model initialize at round 1930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61., 15.]), 'dynamicTrap': False, 'previousTarget': array([61., 15.]), 'currentState': array([51.9977592 , 18.59245552,  6.26589346]), 'targetState': array([61, 15], dtype=int32), 'currentDistance': 9.69257840531149}
done in step count: 11
reward sum = 0.8482594917423573
running average episode reward sum: 0.6876334080081528
{'scaleFactor': 20, 'currentTarget': array([61., 15.]), 'dynamicTrap': False, 'previousTarget': array([61., 15.]), 'currentState': array([61.23207561, 14.79635813,  4.43276155]), 'targetState': array([61, 15], dtype=int32), 'currentDistance': 0.3087541068005811}
episode index:1931
target Thresh 75.9956152697666
target distance 7.0
model initialize at round 1931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([19.27795482,  4.42338679,  5.89689445]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 7.198946242942943}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6877697209697944
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([26.32213199,  6.99787494,  5.81905115]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.32213900373429916}
episode index:1932
target Thresh 75.99563713869989
target distance 18.0
model initialize at round 1932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.82774868,  4.04732396]), 'dynamicTrap': True, 'previousTarget': array([29.,  4.]), 'currentState': array([35.      , 22.      ,  6.114063], dtype=float32), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 18.98407920187104}
done in step count: 15
reward sum = 0.8303573546412885
running average episode reward sum: 0.6878434859122008
{'scaleFactor': 20, 'currentTarget': array([29.,  4.]), 'dynamicTrap': False, 'previousTarget': array([29.,  4.]), 'currentState': array([29.39930196,  4.09524329,  2.26272446]), 'targetState': array([29,  4], dtype=int32), 'currentDistance': 0.41050376454486837}
episode index:1933
target Thresh 75.99565889856139
target distance 72.0
model initialize at round 1933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.40210508,  7.24353532]), 'dynamicTrap': False, 'previousTarget': array([49.93091516,  8.3390904 ]), 'currentState': array([29.44002471,  8.4745297 ,  5.37057662]), 'targetState': array([102,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5254693303630431
running average episode reward sum: 0.6877595282309448
{'scaleFactor': 20, 'currentTarget': array([101.74071805,   5.6743239 ]), 'dynamicTrap': True, 'previousTarget': array([101.60034729,   5.55038071]), 'currentState': array([102.58150649,   6.60371978,   5.95219568]), 'targetState': array([102,   4], dtype=int32), 'currentDistance': 1.2532764659639766}
episode index:1934
target Thresh 75.99568054989516
target distance 13.0
model initialize at round 1934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  15.]), 'dynamicTrap': False, 'previousTarget': array([101.,  15.]), 'currentState': array([87.55146332, 20.37790169,  5.45261359]), 'targetState': array([101,  15], dtype=int32), 'currentDistance': 14.483955447579124}
done in step count: 12
reward sum = 0.8492756675268881
running average episode reward sum: 0.687842999103966
{'scaleFactor': 20, 'currentTarget': array([101.02063218,  16.80388653]), 'dynamicTrap': True, 'previousTarget': array([101.,  15.]), 'currentState': array([101.3214766 ,  16.30248598,   5.47172205]), 'targetState': array([101,  15], dtype=int32), 'currentDistance': 0.584730594132684}
episode index:1935
target Thresh 75.99570209324243
target distance 17.0
model initialize at round 1935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56., 20.]), 'dynamicTrap': False, 'previousTarget': array([56., 20.]), 'currentState': array([71.59992629, 14.93390668,  3.56333089]), 'targetState': array([56, 20], dtype=int32), 'currentDistance': 16.401920670738285}
done in step count: 12
reward sum = 0.8580434566430494
running average episode reward sum: 0.6879309125634386
{'scaleFactor': 20, 'currentTarget': array([56., 20.]), 'dynamicTrap': False, 'previousTarget': array([56., 20.]), 'currentState': array([55.57065944, 19.31489615,  3.5898197 ]), 'targetState': array([56, 20], dtype=int32), 'currentDistance': 0.8085175336474515}
episode index:1936
target Thresh 75.9957235291418
target distance 61.0
model initialize at round 1936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.20373957, 13.34447306]), 'dynamicTrap': False, 'previousTarget': array([71.98925886, 12.65538554]), 'currentState': array([53.20645231, 13.01507627,  5.97264952]), 'targetState': array([113,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.553334378516792
running average episode reward sum: 0.6878614254524182
{'scaleFactor': 20, 'currentTarget': array([113.,  14.]), 'dynamicTrap': False, 'previousTarget': array([113.,  14.]), 'currentState': array([1.13815568e+02, 1.39107269e+01, 8.59579550e-02]), 'targetState': array([113,  14], dtype=int32), 'currentDistance': 0.8204392961406124}
episode index:1937
target Thresh 75.99574485812919
target distance 62.0
model initialize at round 1937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.50438435,  8.53168651]), 'dynamicTrap': False, 'previousTarget': array([79.12626552,  8.24380873]), 'currentState': array([97.38378605,  6.33865482,  1.92504501]), 'targetState': array([37, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5684311841896945
running average episode reward sum: 0.6877997999409307
{'scaleFactor': 20, 'currentTarget': array([37., 13.]), 'dynamicTrap': False, 'previousTarget': array([37., 13.]), 'currentState': array([37.38092513, 13.63482982,  5.36740385]), 'targetState': array([37, 13], dtype=int32), 'currentDistance': 0.7403464401358921}
episode index:1938
target Thresh 75.99576608073781
target distance 11.0
model initialize at round 1938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 10.]), 'dynamicTrap': False, 'previousTarget': array([46., 10.]), 'currentState': array([36.57364411, 21.73416295,  5.80292505]), 'targetState': array([46, 10], dtype=int32), 'currentDistance': 15.05147053778102}
done in step count: 35
reward sum = 0.5325189891978961
running average episode reward sum: 0.6877197170060451
{'scaleFactor': 20, 'currentTarget': array([46., 10.]), 'dynamicTrap': False, 'previousTarget': array([46., 10.]), 'currentState': array([46.33670758, 10.40066029,  4.13481568]), 'targetState': array([46, 10], dtype=int32), 'currentDistance': 0.5233552032060779}
episode index:1939
target Thresh 75.99578719749822
target distance 6.0
model initialize at round 1939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.82828921,   6.10196731]), 'dynamicTrap': True, 'previousTarget': array([108.,   6.]), 'currentState': array([103.       ,  12.       ,   5.8405356], dtype=float32), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 7.62228091143868}
done in step count: 6
reward sum = 0.931480149401
running average episode reward sum: 0.6878453667134652
{'scaleFactor': 20, 'currentTarget': array([108.,   6.]), 'dynamicTrap': False, 'previousTarget': array([108.,   6.]), 'currentState': array([107.69391372,   6.16061404,   2.91845753]), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 0.34566700977753756}
episode index:1940
target Thresh 75.99580820893836
target distance 2.0
model initialize at round 1940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 12.]), 'dynamicTrap': False, 'previousTarget': array([32., 12.]), 'currentState': array([31.83812462, 11.47150233,  2.0453545 ]), 'targetState': array([32, 12], dtype=int32), 'currentDistance': 0.5527326929907332}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6880061882659055
{'scaleFactor': 20, 'currentTarget': array([32., 12.]), 'dynamicTrap': False, 'previousTarget': array([32., 12.]), 'currentState': array([31.83812462, 11.47150233,  2.0453545 ]), 'targetState': array([32, 12], dtype=int32), 'currentDistance': 0.5527326929907332}
episode index:1941
target Thresh 75.9958291155835
target distance 36.0
model initialize at round 1941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.08104637,  7.447837  ]), 'dynamicTrap': False, 'previousTarget': array([69.87767469,  8.20863052]), 'currentState': array([50.33322759,  4.2818216 ,  5.79555786]), 'targetState': array([86, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6880688667828753
{'scaleFactor': 20, 'currentTarget': array([86., 10.]), 'dynamicTrap': False, 'previousTarget': array([86., 10.]), 'currentState': array([86.31710996,  9.9491936 ,  5.36331637]), 'targetState': array([86, 10], dtype=int32), 'currentDistance': 0.3211541903284337}
episode index:1942
target Thresh 75.99584991795632
target distance 74.0
model initialize at round 1942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.27472264, 17.46957174]), 'dynamicTrap': False, 'previousTarget': array([91.02915453, 17.92049484]), 'currentState': array([109.25244169,  18.41336269,   2.76475823]), 'targetState': array([37, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5593473903571053
running average episode reward sum: 0.6880026179530112
{'scaleFactor': 20, 'currentTarget': array([37., 15.]), 'dynamicTrap': False, 'previousTarget': array([37., 15.]), 'currentState': array([37.52854802, 15.67486997,  3.78774923]), 'targetState': array([37, 15], dtype=int32), 'currentDistance': 0.8572120418169801}
episode index:1943
target Thresh 75.99587061657687
target distance 27.0
model initialize at round 1943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.15844853, 16.27558486]), 'dynamicTrap': True, 'previousTarget': array([98.20583067, 16.19604781]), 'currentState': array([81.      ,  6.      ,  4.467844], dtype=float32), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.3899865675907161
running average episode reward sum: 0.6878493175155821
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([107.48967429,  22.08769802,   5.99062671]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 0.5178062097585235}
episode index:1944
target Thresh 75.9958912119626
target distance 13.0
model initialize at round 1944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([ 8.45324324, 22.189923  ,  4.02941632]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 13.634476805223146}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6879700863468995
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([5.37478804, 8.83939122, 3.36649338]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.4077514627718456}
episode index:1945
target Thresh 75.99591170462843
target distance 39.0
model initialize at round 1945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.43894044, 10.07921476]), 'dynamicTrap': False, 'previousTarget': array([31.31457586, 10.46671874]), 'currentState': array([49.13427237, 13.55684329,  2.86956537]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5385093966211112
running average episode reward sum: 0.6878932822925696
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'dynamicTrap': False, 'previousTarget': array([12.,  7.]), 'currentState': array([11.03697124,  6.37341514,  1.78732072]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 1.148926881750285}
episode index:1946
target Thresh 75.99593209508666
target distance 42.0
model initialize at round 1946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.65500595, 16.82890459]), 'dynamicTrap': False, 'previousTarget': array([70.45612429, 17.36758945]), 'currentState': array([50.06961546, 20.88013638,  5.02914667]), 'targetState': array([93, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6429735427374773
running average episode reward sum: 0.687870211034452
{'scaleFactor': 20, 'currentTarget': array([93., 12.]), 'dynamicTrap': False, 'previousTarget': array([93., 12.]), 'currentState': array([92.46654654, 12.21530512,  0.54431089]), 'targetState': array([93, 12], dtype=int32), 'currentDistance': 0.5752641860491224}
episode index:1947
target Thresh 75.99595238384705
target distance 42.0
model initialize at round 1947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.7054717 , 14.40053693]), 'dynamicTrap': False, 'previousTarget': array([64.53893626, 15.49614485]), 'currentState': array([46.98997546, 21.45250123,  5.7451942 ]), 'targetState': array([88,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6265924963537015
running average episode reward sum: 0.6878387543020696
{'scaleFactor': 20, 'currentTarget': array([88.,  6.]), 'dynamicTrap': False, 'previousTarget': array([88.,  6.]), 'currentState': array([87.54430942,  6.51019791,  0.6565221 ]), 'targetState': array([88,  6], dtype=int32), 'currentDistance': 0.684072959669018}
episode index:1948
target Thresh 75.99597257141684
target distance 62.0
model initialize at round 1948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.15277508,  5.30829531]), 'dynamicTrap': False, 'previousTarget': array([69.00260095,  5.32253869]), 'currentState': array([87.1498081 ,  4.96380999,  2.91587526]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6292154653525575
running average episode reward sum: 0.6878086756520185
{'scaleFactor': 20, 'currentTarget': array([27.,  6.]), 'dynamicTrap': False, 'previousTarget': array([27.,  6.]), 'currentState': array([27.28367411,  5.63032465,  2.82120623]), 'targetState': array([27,  6], dtype=int32), 'currentDistance': 0.46597302773156496}
episode index:1949
target Thresh 75.99599265830071
target distance 21.0
model initialize at round 1949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.,  5.]), 'dynamicTrap': False, 'previousTarget': array([61.09009055,  4.89618185]), 'currentState': array([79.40641457,  4.02906905,  3.13417673]), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 19.430687923908035}
done in step count: 25
reward sum = 0.7406391558051552
running average episode reward sum: 0.6878357682059432
{'scaleFactor': 20, 'currentTarget': array([60.18336796,  3.06887148]), 'dynamicTrap': True, 'previousTarget': array([60.,  5.]), 'currentState': array([59.23612059,  3.50202041,  2.08807759]), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 1.0415832052231553}
episode index:1950
target Thresh 75.9960126450008
target distance 21.0
model initialize at round 1950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.21993824,  14.00111155]), 'dynamicTrap': False, 'previousTarget': array([109.00530293,  13.52709229]), 'currentState': array([91.53969343,  4.65183652,  0.88708007]), 'targetState': array([113,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5694074405103662
running average episode reward sum: 0.6877750668590977
{'scaleFactor': 20, 'currentTarget': array([113.,  16.]), 'dynamicTrap': False, 'previousTarget': array([113.,  16.]), 'currentState': array([112.77204921,  15.66892125,   0.44457514]), 'targetState': array([113,  16], dtype=int32), 'currentDistance': 0.40196355433128955}
episode index:1951
target Thresh 75.99603253201684
target distance 7.0
model initialize at round 1951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([118.26663263,   4.982257  ,   1.58384138]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 9.434891664171708}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6878860335640924
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([111.50082399,  11.82008393,   0.74847007]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 0.9609174401844769}
episode index:1952
target Thresh 75.99605231984594
target distance 62.0
model initialize at round 1952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.67315539, 11.8594348 ]), 'dynamicTrap': False, 'previousTarget': array([70.12626552, 10.75619127]), 'currentState': array([90.4872409 , 14.58009937,  2.28168672]), 'targetState': array([28,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5006875173971325
running average episode reward sum: 0.6877901817893013
{'scaleFactor': 20, 'currentTarget': array([28.,  6.]), 'dynamicTrap': False, 'previousTarget': array([28.,  6.]), 'currentState': array([27.51341594,  5.17281091,  3.43588621]), 'targetState': array([28,  6], dtype=int32), 'currentDistance': 0.9596904920644099}
episode index:1953
target Thresh 75.99607200898286
target distance 20.0
model initialize at round 1953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'dynamicTrap': False, 'previousTarget': array([21.40285  , 12.8507125]), 'currentState': array([3.10675569, 9.63451231, 0.69275957]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 19.190653663716443}
done in step count: 30
reward sum = 0.6069060537338805
running average episode reward sum: 0.6877487876603067
{'scaleFactor': 20, 'currentTarget': array([22., 13.]), 'dynamicTrap': False, 'previousTarget': array([22., 13.]), 'currentState': array([22.14467298, 13.11758043,  6.18926882]), 'targetState': array([22, 13], dtype=int32), 'currentDistance': 0.18642807541767092}
episode index:1954
target Thresh 75.9960915999198
target distance 67.0
model initialize at round 1954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.27096406,  8.60570984]), 'dynamicTrap': False, 'previousTarget': array([74.10827014,  8.07824043]), 'currentState': array([92.17740578,  6.67346291,  3.20698214]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5586575195251344
running average episode reward sum: 0.6876827563211072
{'scaleFactor': 20, 'currentTarget': array([27., 13.]), 'dynamicTrap': False, 'previousTarget': array([27., 13.]), 'currentState': array([26.99262238, 12.13972816,  1.47559195]), 'targetState': array([27, 13], dtype=int32), 'currentDistance': 0.8603034730821177}
episode index:1955
target Thresh 75.99611109314651
target distance 21.0
model initialize at round 1955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.53738258, 15.65617162]), 'dynamicTrap': False, 'previousTarget': array([71.6897547 , 15.88009345]), 'currentState': array([54.28959769, 23.84264502,  0.81361958]), 'targetState': array([74, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8232124156227957
running average episode reward sum: 0.6877520455129792
{'scaleFactor': 20, 'currentTarget': array([74., 15.]), 'dynamicTrap': False, 'previousTarget': array([74., 15.]), 'currentState': array([74.62703343, 14.17655296,  3.5805949 ]), 'targetState': array([74, 15], dtype=int32), 'currentDistance': 1.0350052902053366}
episode index:1956
target Thresh 75.99613048915036
target distance 51.0
model initialize at round 1956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.69425493, 16.70181262]), 'dynamicTrap': False, 'previousTarget': array([38.14019333, 16.34359765]), 'currentState': array([55.38223411, 23.82637815,  2.90537184]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6038020058563656
running average episode reward sum: 0.6877091482009421
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'dynamicTrap': False, 'previousTarget': array([6., 5.]), 'currentState': array([6.65031413, 4.67395922, 4.96614957]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.7274689377609552}
episode index:1957
target Thresh 75.99614978841623
target distance 67.0
model initialize at round 1957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.74143763, 13.57829214]), 'dynamicTrap': False, 'previousTarget': array([71.31326714, 14.52598201]), 'currentState': array([90.34710646,  9.62636225,  3.26109028]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5855737528766528
running average episode reward sum: 0.6876569850776916
{'scaleFactor': 20, 'currentTarget': array([24., 23.]), 'dynamicTrap': False, 'previousTarget': array([24., 23.]), 'currentState': array([23.88501011, 22.34300772,  2.02516944]), 'targetState': array([24, 23], dtype=int32), 'currentDistance': 0.6669794076320957}
episode index:1958
target Thresh 75.99616899142663
target distance 28.0
model initialize at round 1958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.596761  ,  7.37813919]), 'dynamicTrap': False, 'previousTarget': array([25.88618308,  7.86933753]), 'currentState': array([6.62279921, 8.39835989, 6.03223598]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7731355028825682
running average episode reward sum: 0.6877006188284853
{'scaleFactor': 20, 'currentTarget': array([34.,  7.]), 'dynamicTrap': False, 'previousTarget': array([34.,  7.]), 'currentState': array([34.3773978 ,  6.67589533,  1.03075784]), 'targetState': array([34,  7], dtype=int32), 'currentDistance': 0.4974665196846292}
episode index:1959
target Thresh 75.99618809866159
target distance 33.0
model initialize at round 1959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.28331938, 13.05524279]), 'dynamicTrap': False, 'previousTarget': array([52.33244914, 12.9792889 ]), 'currentState': array([34.90123504,  3.16266254,  2.02486603]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6783193626232386
running average episode reward sum: 0.6876958324732785
{'scaleFactor': 20, 'currentTarget': array([68., 22.]), 'dynamicTrap': False, 'previousTarget': array([68., 22.]), 'currentState': array([67.44482726, 22.40465895,  0.31271386]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 0.6869975544624523}
episode index:1960
target Thresh 75.99620711059883
target distance 21.0
model initialize at round 1960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.5980114 ,  8.57610579]), 'dynamicTrap': False, 'previousTarget': array([25.48275862,  9.20689655]), 'currentState': array([12.6819262 , 22.94073501,  5.2379635 ]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6973039113303531
running average episode reward sum: 0.6877007320545417
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([31.1214518 ,  3.39590761,  1.50703123]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.9636336343312109}
episode index:1961
target Thresh 75.99622602771365
target distance 35.0
model initialize at round 1961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.22831316, 10.50918356]), 'dynamicTrap': False, 'previousTarget': array([18.08108108, 10.48648649]), 'currentState': array([37.17328596,  4.0991909 ,  0.6208869 ]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.535892852032171
running average episode reward sum: 0.6876233580076394
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.5869907 , 15.46534205,  1.97362894]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7939881613292442}
episode index:1962
target Thresh 75.99624485047894
target distance 37.0
model initialize at round 1962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.94644884, 12.99169508]), 'dynamicTrap': False, 'previousTarget': array([31.69273214, 12.2181805 ]), 'currentState': array([49.34307444,  8.11615254,  2.46668702]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6114597532988061
running average episode reward sum: 0.6875845584127801
{'scaleFactor': 20, 'currentTarget': array([14., 17.]), 'dynamicTrap': False, 'previousTarget': array([14., 17.]), 'currentState': array([13.55173157, 17.33971263,  1.8104666 ]), 'targetState': array([14, 17], dtype=int32), 'currentDistance': 0.5624493388234179}
episode index:1963
target Thresh 75.99626357936532
target distance 16.0
model initialize at round 1963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61., 17.]), 'dynamicTrap': False, 'previousTarget': array([61., 17.]), 'currentState': array([75.52127122, 18.5613295 ,  2.41997671]), 'targetState': array([61, 17], dtype=int32), 'currentDistance': 14.604967222852602}
done in step count: 9
reward sum = 0.9036172474836408
running average episode reward sum: 0.687694554690311
{'scaleFactor': 20, 'currentTarget': array([61., 17.]), 'dynamicTrap': False, 'previousTarget': array([61., 17.]), 'currentState': array([61.39502256, 17.27721877,  3.55601499]), 'targetState': array([61, 17], dtype=int32), 'currentDistance': 0.48258996328077597}
episode index:1964
target Thresh 75.99628221484097
target distance 15.0
model initialize at round 1964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.,   3.]), 'dynamicTrap': False, 'previousTarget': array([103.,   3.]), 'currentState': array([89.49489953, 14.86264592,  5.81681088]), 'targetState': array([103,   3], dtype=int32), 'currentDistance': 17.97526374681354}
done in step count: 13
reward sum = 0.8407826871314562
running average episode reward sum: 0.687772462136846
{'scaleFactor': 20, 'currentTarget': array([103.,   3.]), 'dynamicTrap': False, 'previousTarget': array([103.,   3.]), 'currentState': array([102.41276197,   3.90401713,   4.7896812 ]), 'targetState': array([103,   3], dtype=int32), 'currentDistance': 1.0780053201157795}
episode index:1965
target Thresh 75.99630075737183
target distance 41.0
model initialize at round 1965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.32888418, 16.9927904 ]), 'dynamicTrap': False, 'previousTarget': array([75.68314326, 16.18257132]), 'currentState': array([93.74847727, 12.20954837,  1.60791087]), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6712797909209396
running average episode reward sum: 0.6877640731891267
{'scaleFactor': 20, 'currentTarget': array([54., 22.]), 'dynamicTrap': False, 'previousTarget': array([54., 22.]), 'currentState': array([53.91471775, 22.85047518,  4.50510225]), 'targetState': array([54, 22], dtype=int32), 'currentDistance': 0.8547403627784254}
episode index:1966
target Thresh 75.9963192074214
target distance 43.0
model initialize at round 1966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.75003213, 14.2361962 ]), 'dynamicTrap': False, 'previousTarget': array([91.04849798, 13.60803474]), 'currentState': array([109.64515676,  16.28168293,   2.39612848]), 'targetState': array([68, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7132823635258944
running average episode reward sum: 0.6877770463921449
{'scaleFactor': 20, 'currentTarget': array([68., 12.]), 'dynamicTrap': False, 'previousTarget': array([68., 12.]), 'currentState': array([68.62775565, 11.84174534,  5.92801491]), 'targetState': array([68, 12], dtype=int32), 'currentDistance': 0.6473960844980431}
episode index:1967
target Thresh 75.99633756545097
target distance 14.0
model initialize at round 1967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63., 22.]), 'dynamicTrap': False, 'previousTarget': array([63., 22.]), 'currentState': array([69.43062046,  7.35931523,  2.53972292]), 'targetState': array([63, 22], dtype=int32), 'currentDistance': 15.990701360562062}
done in step count: 9
reward sum = 0.9043820750088044
running average episode reward sum: 0.687887109922946
{'scaleFactor': 20, 'currentTarget': array([64.57118806, 21.396878  ]), 'dynamicTrap': True, 'previousTarget': array([63., 22.]), 'currentState': array([64.17511034, 20.61896679,  3.76507384]), 'targetState': array([63, 22], dtype=int32), 'currentDistance': 0.8729395230792548}
episode index:1968
target Thresh 75.9963558319195
target distance 72.0
model initialize at round 1968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.80253742,  7.36125807]), 'dynamicTrap': False, 'previousTarget': array([42.99807127,  7.27775099]), 'currentState': array([24.80415637,  7.10678695,  5.56283731]), 'targetState': array([95,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.654345898147547
running average episode reward sum: 0.6878700752800941
{'scaleFactor': 20, 'currentTarget': array([95.,  8.]), 'dynamicTrap': False, 'previousTarget': array([95.,  8.]), 'currentState': array([94.5551392 ,  8.26591893,  0.93876812]), 'targetState': array([95,  8], dtype=int32), 'currentDistance': 0.5182798548919975}
episode index:1969
target Thresh 75.99637400728362
target distance 64.0
model initialize at round 1969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.4128408, 16.0892339]), 'dynamicTrap': False, 'previousTarget': array([43.80513158, 17.21490337]), 'currentState': array([24.54779136, 18.4086758 ,  6.05391979]), 'targetState': array([88, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5725505915621396
running average episode reward sum: 0.6878115374711001
{'scaleFactor': 20, 'currentTarget': array([88., 11.]), 'dynamicTrap': False, 'previousTarget': array([88., 11.]), 'currentState': array([87.01384612, 11.60137748,  0.61059954]), 'targetState': array([88, 11], dtype=int32), 'currentDistance': 1.1550559952271664}
episode index:1970
target Thresh 75.99639209199773
target distance 36.0
model initialize at round 1970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.36440298, 11.04600423]), 'dynamicTrap': False, 'previousTarget': array([93.5237412 , 11.66139084]), 'currentState': array([75.78397263, 15.12114196,  6.01152429]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6878692843265379
{'scaleFactor': 20, 'currentTarget': array([110.,   8.]), 'dynamicTrap': False, 'previousTarget': array([110.,   8.]), 'currentState': array([109.23170255,   7.09686337,   2.17005357]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 1.185722035637438}
episode index:1971
target Thresh 75.99641008651398
target distance 8.0
model initialize at round 1971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.80737052, 17.97948051]), 'dynamicTrap': True, 'previousTarget': array([52., 18.]), 'currentState': array([6.0000000e+01, 1.5000000e+01, 1.6084444e-02], dtype=float32), 'targetState': array([52, 18], dtype=int32), 'currentDistance': 8.717596106078616}
done in step count: 12
reward sum = 0.8283502196231193
running average episode reward sum: 0.6879405221233416
{'scaleFactor': 20, 'currentTarget': array([52., 18.]), 'dynamicTrap': False, 'previousTarget': array([52., 18.]), 'currentState': array([51.46967441, 17.21218123,  2.26547414]), 'targetState': array([52, 18], dtype=int32), 'currentDistance': 0.9496860743760436}
episode index:1972
target Thresh 75.99642799128218
target distance 13.0
model initialize at round 1972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.80796343,   6.05574021]), 'dynamicTrap': True, 'previousTarget': array([114.,   6.]), 'currentState': array([101.      ,   4.      ,   5.967498], dtype=float32), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 12.971892499185588}
done in step count: 16
reward sum = 0.7929379204958755
running average episode reward sum: 0.6879937392537888
{'scaleFactor': 20, 'currentTarget': array([114.,   6.]), 'dynamicTrap': False, 'previousTarget': array([114.,   6.]), 'currentState': array([113.13857135,   6.18836098,   5.12417384]), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 0.881781820683005}
episode index:1973
target Thresh 75.99644580674999
target distance 15.0
model initialize at round 1973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.8501732 ,  18.13248306]), 'dynamicTrap': True, 'previousTarget': array([112.,  18.]), 'currentState': array([97.      ,  8.      ,  5.555636], dtype=float32), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 17.977621007311036}
done in step count: 16
reward sum = 0.7929379204958755
running average episode reward sum: 0.6880469024661708
{'scaleFactor': 20, 'currentTarget': array([112.,  18.]), 'dynamicTrap': False, 'previousTarget': array([112.,  18.]), 'currentState': array([111.48873111,  18.39439174,   1.62402201]), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 0.6457094750370213}
episode index:1974
target Thresh 75.99646353336277
target distance 2.0
model initialize at round 1974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([ 6.8670491 , 17.7531366 ,  2.96774077]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 3.360639235294841}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6881947774522639
{'scaleFactor': 20, 'currentTarget': array([ 6., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 21.]), 'currentState': array([ 5.62335128, 20.90614516,  2.88287735]), 'targetState': array([ 6, 21], dtype=int32), 'currentDistance': 0.38816619064674296}
episode index:1975
target Thresh 75.9964811715637
target distance 1.0
model initialize at round 1975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([6.90283335, 8.02918522, 4.95005703]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.3257410739531699}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6883525736175209
{'scaleFactor': 20, 'currentTarget': array([6., 9.]), 'dynamicTrap': False, 'previousTarget': array([6., 9.]), 'currentState': array([6.90283335, 8.02918522, 4.95005703]), 'targetState': array([6, 9], dtype=int32), 'currentDistance': 1.3257410739531699}
episode index:1976
target Thresh 75.99649872179376
target distance 15.0
model initialize at round 1976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([109.78254756,   5.05794177,   0.86032468]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 17.052613543974015}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6884664657135584
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([117.88644724,  19.87905697,   2.18249803]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 0.16589588988961568}
episode index:1977
target Thresh 75.99651618449167
target distance 74.0
model initialize at round 1977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.46352927, 15.11579289]), 'dynamicTrap': False, 'previousTarget': array([69.02915453, 14.07950516]), 'currentState': array([88.45112025, 14.41137323,  2.86249244]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 53
reward sum = 0.535771975663649
running average episode reward sum: 0.6883892693080731
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'dynamicTrap': False, 'previousTarget': array([15., 17.]), 'currentState': array([14.7750674 , 16.89504623,  3.67919006]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.24821355131026085}
episode index:1978
target Thresh 75.996533560094
target distance 1.0
model initialize at round 1978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90., 10.]), 'dynamicTrap': False, 'previousTarget': array([90., 10.]), 'currentState': array([90.7703555 ,  7.18588707,  4.88396603]), 'targetState': array([90, 10], dtype=int32), 'currentDistance': 2.9176496005577834}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6885219629819447
{'scaleFactor': 20, 'currentTarget': array([90., 10.]), 'dynamicTrap': False, 'previousTarget': array([90., 10.]), 'currentState': array([89.81096646, 10.56714786,  1.40919232]), 'targetState': array([90, 10], dtype=int32), 'currentDistance': 0.5978213541835229}
episode index:1979
target Thresh 75.99655084903516
target distance 12.0
model initialize at round 1979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([11.36581254, 10.40226785,  1.89023352]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 12.778862830217033}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6886402572907557
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.08395875, 22.53608351,  0.30730602]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.5426182826252471}
episode index:1980
target Thresh 75.99656805174736
target distance 9.0
model initialize at round 1980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.95508121,  5.80685251]), 'dynamicTrap': True, 'previousTarget': array([53.,  6.]), 'currentState': array([62.       , 12.       ,  1.2471709], dtype=float32), 'targetState': array([53,  6], dtype=int32), 'currentDistance': 10.962008558226668}
done in step count: 12
reward sum = 0.8290179206937801
running average episode reward sum: 0.6887111193116559
{'scaleFactor': 20, 'currentTarget': array([53.,  6.]), 'dynamicTrap': False, 'previousTarget': array([53.,  6.]), 'currentState': array([53.45408086,  6.71684847,  3.31130211]), 'targetState': array([53,  6], dtype=int32), 'currentDistance': 0.8485641749626448}
episode index:1981
target Thresh 75.99658516866069
target distance 60.0
model initialize at round 1981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.21488579, 12.62766441]), 'dynamicTrap': False, 'previousTarget': array([71.9007438 , 12.00992562]), 'currentState': array([53.35574541, 14.99716722,  5.90734953]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5371818652124829
running average episode reward sum: 0.6886346666102939
{'scaleFactor': 20, 'currentTarget': array([112.,   8.]), 'dynamicTrap': False, 'previousTarget': array([112.,   8.]), 'currentState': array([111.47030831,   7.84620396,   6.14538308]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 0.5515673221412725}
episode index:1982
target Thresh 75.99660220020303
target distance 17.0
model initialize at round 1982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([28.34484922,  5.35273952,  3.90875578]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 17.422894754845203}
done in step count: 14
reward sum = 0.8221094470014666
running average episode reward sum: 0.688701976131419
{'scaleFactor': 20, 'currentTarget': array([11.,  7.]), 'dynamicTrap': False, 'previousTarget': array([11.,  7.]), 'currentState': array([11.27253185,  6.53272336,  2.05562935]), 'targetState': array([11,  7], dtype=int32), 'currentDistance': 0.5409446110810496}
episode index:1983
target Thresh 75.99661914680021
target distance 57.0
model initialize at round 1983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.72318004,  9.98148152]), 'dynamicTrap': False, 'previousTarget': array([65.42272845, 10.77049471]), 'currentState': array([47.44484334,  4.65741032,  5.24555558]), 'targetState': array([103,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5218981139290141
running average episode reward sum: 0.6886179016040993
{'scaleFactor': 20, 'currentTarget': array([103.,  20.]), 'dynamicTrap': False, 'previousTarget': array([103.,  20.]), 'currentState': array([102.92141928,  20.99188579,   4.84370532]), 'targetState': array([103,  20], dtype=int32), 'currentDistance': 0.9949936460257075}
episode index:1984
target Thresh 75.99663600887591
target distance 62.0
model initialize at round 1984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.67382731,  9.45312366]), 'dynamicTrap': False, 'previousTarget': array([50.0647209,  8.3923162]), 'currentState': array([69.56061259, 11.57815799,  2.84374368]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5486792944279391
running average episode reward sum: 0.6885474035652196
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'dynamicTrap': False, 'previousTarget': array([8., 5.]), 'currentState': array([8.82449035, 4.73036485, 3.56883925]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.8674603467223332}
episode index:1985
target Thresh 75.99665278685163
target distance 17.0
model initialize at round 1985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,  21.]), 'dynamicTrap': False, 'previousTarget': array([110.,  21.]), 'currentState': array([108.69944685,   4.61845777,   1.68770838]), 'targetState': array([110,  21], dtype=int32), 'currentDistance': 16.433087483247213}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.688660681432248
{'scaleFactor': 20, 'currentTarget': array([110.,  21.]), 'dynamicTrap': False, 'previousTarget': array([110.,  21.]), 'currentState': array([109.9407557 ,  20.3366216 ,   2.21842057]), 'targetState': array([110,  21], dtype=int32), 'currentDistance': 0.6660186093401407}
episode index:1986
target Thresh 75.99666948114685
target distance 19.0
model initialize at round 1986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.87831985, 14.83602253]), 'dynamicTrap': True, 'previousTarget': array([40.30234469, 15.39288577]), 'currentState': array([56.       ,  3.       ,  2.2781024], dtype=float32), 'targetState': array([37, 18], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 20
reward sum = 0.7406516320251508
running average episode reward sum: 0.6886868469836285
{'scaleFactor': 20, 'currentTarget': array([37., 18.]), 'dynamicTrap': False, 'previousTarget': array([37., 18.]), 'currentState': array([37.86738063, 17.24053843,  1.86511305]), 'targetState': array([37, 18], dtype=int32), 'currentDistance': 1.1528794556113862}
episode index:1987
target Thresh 75.99668609217893
target distance 60.0
model initialize at round 1987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.98166533, 15.1881613 ]), 'dynamicTrap': False, 'previousTarget': array([58.8434743 , 15.25304229]), 'currentState': array([78.15182722, 20.88946756,  5.55290394]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6265565472015259
running average episode reward sum: 0.6886555943177421
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([18.04856547,  3.42340673,  3.24721156]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.4261829038687977}
episode index:1988
target Thresh 75.99670262036315
target distance 38.0
model initialize at round 1988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.05901569,  8.21428554]), 'dynamicTrap': False, 'previousTarget': array([47.34149075,  9.08986599]), 'currentState': array([27.93147395,  2.37158523,  5.59744382]), 'targetState': array([66, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7669426858341388
running average episode reward sum: 0.6886949543436428
{'scaleFactor': 20, 'currentTarget': array([66., 14.]), 'dynamicTrap': False, 'previousTarget': array([66., 14.]), 'currentState': array([6.58574974e+01, 1.34654551e+01, 2.68115365e-02]), 'targetState': array([66, 14], dtype=int32), 'currentDistance': 0.5532135488878651}
episode index:1989
target Thresh 75.99671906611269
target distance 18.0
model initialize at round 1989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([72.0329065 ,  4.48910674,  0.53870177]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 17.97374958342541}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6888033398314142
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([90.32636323,  3.93592245,  4.90433941]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 0.33259418522341067}
episode index:1990
target Thresh 75.99673542983872
target distance 27.0
model initialize at round 1990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.17889559,  13.6182685 ]), 'dynamicTrap': False, 'previousTarget': array([105.75497521,  12.94628712]), 'currentState': array([87.04636905,  7.79191216,  0.7351141 ]), 'targetState': array([114,  16], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6888893544043976
{'scaleFactor': 20, 'currentTarget': array([114.,  16.]), 'dynamicTrap': False, 'previousTarget': array([114.,  16.]), 'currentState': array([113.44814861,  15.24216622,   1.54608758]), 'targetState': array([114,  16], dtype=int32), 'currentDistance': 0.9374710674411246}
episode index:1991
target Thresh 75.99675171195032
target distance 12.0
model initialize at round 1991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([7.63724651, 4.98756824, 1.50449038]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 11.948962657667819}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6890161570123277
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.34501253, 15.66916353,  1.82592094]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.4780025229999987}
episode index:1992
target Thresh 75.99676791285457
target distance 56.0
model initialize at round 1992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.03474521, 16.82161079]), 'dynamicTrap': True, 'previousTarget': array([52.01274291, 17.28616939]), 'currentState': array([72.       , 18.       ,  6.2375975], dtype=float32), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5625980451842431
running average episode reward sum: 0.6889527259476873
{'scaleFactor': 20, 'currentTarget': array([17.51284546, 17.19390425]), 'dynamicTrap': True, 'previousTarget': array([16., 16.]), 'currentState': array([17.61190015, 17.16274292,  3.86813817]), 'targetState': array([16, 16], dtype=int32), 'currentDistance': 0.103840550590707}
episode index:1993
target Thresh 75.99678403295646
target distance 61.0
model initialize at round 1993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.42165609,  8.90477446]), 'dynamicTrap': False, 'previousTarget': array([93.06684958,  9.36613521]), 'currentState': array([111.36758177,  10.3744848 ,   2.69868457]), 'targetState': array([52,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.24580167149542353
running average episode reward sum: 0.6887304836936992
{'scaleFactor': 20, 'currentTarget': array([52.,  6.]), 'dynamicTrap': False, 'previousTarget': array([52.,  6.]), 'currentState': array([51.31592841,  6.31153778,  1.80078072]), 'targetState': array([52,  6], dtype=int32), 'currentDistance': 0.751671287081251}
episode index:1994
target Thresh 75.996800072659
target distance 18.0
model initialize at round 1994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.95868277,  4.26957048]), 'dynamicTrap': True, 'previousTarget': array([31.09400392,  4.35899411]), 'currentState': array([20.        , 21.        ,  0.17946371], dtype=float32), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8500583546412884
running average episode reward sum: 0.6888113497944248
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([31.68280436,  2.11663468,  5.25338819]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.9385879565496167}
episode index:1995
target Thresh 75.99681603236318
target distance 54.0
model initialize at round 1995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.45635107, 13.69417839]), 'dynamicTrap': False, 'previousTarget': array([49.03079294, 13.10940039]), 'currentState': array([67.44018362, 12.89016563,  2.45595521]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5492272883728985
running average episode reward sum: 0.6887414178999252
{'scaleFactor': 20, 'currentTarget': array([16.33127161, 16.29458767]), 'dynamicTrap': True, 'previousTarget': array([15., 15.]), 'currentState': array([16.02882171, 16.17632307,  4.5687669 ]), 'targetState': array([15, 15], dtype=int32), 'currentDistance': 0.32474984735431706}
episode index:1996
target Thresh 75.99683191246804
target distance 5.0
model initialize at round 1996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.,  4.]), 'dynamicTrap': False, 'previousTarget': array([62.,  4.]), 'currentState': array([65.19035519,  3.85121188,  2.72377633]), 'targetState': array([62,  4], dtype=int32), 'currentDistance': 3.1938228147855905}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6888873160381825
{'scaleFactor': 20, 'currentTarget': array([62.,  4.]), 'dynamicTrap': False, 'previousTarget': array([62.,  4.]), 'currentState': array([61.68008294,  4.43448312,  4.10584679]), 'targetState': array([62,  4], dtype=int32), 'currentDistance': 0.5395577006849205}
episode index:1997
target Thresh 75.99684771337051
target distance 75.0
model initialize at round 1997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.10908024, 16.91402037]), 'dynamicTrap': True, 'previousTarget': array([67.11281599, 16.87870037]), 'currentState': array([87.       , 19.       ,  3.8451405], dtype=float32), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.23862829789512915
running average episode reward sum: 0.688661961174247
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([12.27736054, 10.81121997,  3.63903365]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.33550971874519037}
episode index:1998
target Thresh 75.99686343546567
target distance 32.0
model initialize at round 1998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.50008278, 13.68858107]), 'dynamicTrap': False, 'previousTarget': array([46.96105157, 14.24756572]), 'currentState': array([27.62887294, 11.42252295,  6.07591844]), 'targetState': array([59, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8261686238355866
running average episode reward sum: 0.6887307488994403
{'scaleFactor': 20, 'currentTarget': array([57.72380201, 13.65675794]), 'dynamicTrap': True, 'previousTarget': array([59., 15.]), 'currentState': array([58.06411134, 12.91898406,  0.86851655]), 'targetState': array([59, 15], dtype=int32), 'currentDistance': 0.8124781510040123}
episode index:1999
target Thresh 75.99687907914655
target distance 50.0
model initialize at round 1999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.23368307, 11.04839552]), 'dynamicTrap': True, 'previousTarget': array([36.31633352, 11.54305997]), 'currentState': array([56.       ,  8.       ,  1.1943338], dtype=float32), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5529312026127315
running average episode reward sum: 0.6886628491262969
{'scaleFactor': 20, 'currentTarget': array([ 6., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 17.]), 'currentState': array([ 6.04239049, 16.62301478,  3.50639536]), 'targetState': array([ 6, 17], dtype=int32), 'currentDistance': 0.3793610579473281}
episode index:2000
target Thresh 75.99689464480424
target distance 53.0
model initialize at round 2000
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.3059772 , 13.51496541]), 'dynamicTrap': True, 'previousTarget': array([85.22401827, 14.01494615]), 'currentState': array([105.       ,  17.       ,   1.3881109], dtype=float32), 'targetState': array([52,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5350656662896902
running average episode reward sum: 0.6885860889149843
{'scaleFactor': 20, 'currentTarget': array([52.,  9.]), 'dynamicTrap': False, 'previousTarget': array([52.,  9.]), 'currentState': array([51.41978529,  9.06715917,  3.7435681 ]), 'targetState': array([52,  9], dtype=int32), 'currentDistance': 0.5840885771081354}
episode index:2001
target Thresh 75.99691013282789
target distance 26.0
model initialize at round 2001
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.89471474,  2.48000664]), 'dynamicTrap': False, 'previousTarget': array([65.05891029,  2.46607002]), 'currentState': array([84.82873424,  4.10323391,  2.62788612]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7475927819434768
running average episode reward sum: 0.6886155627876259
{'scaleFactor': 20, 'currentTarget': array([59.,  2.]), 'dynamicTrap': False, 'previousTarget': array([59.,  2.]), 'currentState': array([58.49153082,  1.93709473,  2.66418825]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 0.5123455618195292}
episode index:2002
target Thresh 75.9969255436047
target distance 67.0
model initialize at round 2002
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.63025738, 17.21452864]), 'dynamicTrap': False, 'previousTarget': array([26.85893586, 17.62878378]), 'currentState': array([ 8.76106125, 19.49817599,  0.45065504]), 'targetState': array([74, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.581984999604128
running average episode reward sum: 0.6885623273591769
{'scaleFactor': 20, 'currentTarget': array([74., 12.]), 'dynamicTrap': False, 'previousTarget': array([74., 12.]), 'currentState': array([73.88071781, 11.37207665,  5.66609069]), 'targetState': array([74, 12], dtype=int32), 'currentDistance': 0.6391525435732147}
episode index:2003
target Thresh 75.99694087751996
target distance 3.0
model initialize at round 2003
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  23.]), 'dynamicTrap': False, 'previousTarget': array([107.,  23.]), 'currentState': array([108.16960098,  21.6932398 ,   2.44913447]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 1.7537356348807416}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6887127453594966
{'scaleFactor': 20, 'currentTarget': array([107.,  23.]), 'dynamicTrap': False, 'previousTarget': array([107.,  23.]), 'currentState': array([107.22892376,  23.40404563,   1.69031769]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 0.4643909547758366}
episode index:2004
target Thresh 75.99695613495697
target distance 7.0
model initialize at round 2004
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 19.]), 'dynamicTrap': False, 'previousTarget': array([88., 19.]), 'currentState': array([81.3417712 , 20.64790186,  0.35629773]), 'targetState': array([88, 19], dtype=int32), 'currentDistance': 6.859124668890137}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6888531873817613
{'scaleFactor': 20, 'currentTarget': array([88., 19.]), 'dynamicTrap': False, 'previousTarget': array([88., 19.]), 'currentState': array([87.04415837, 19.88162142,  0.51840615]), 'targetState': array([88, 19], dtype=int32), 'currentDistance': 1.300342087912614}
episode index:2005
target Thresh 75.9969713162972
target distance 1.0
model initialize at round 2005
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,   5.]), 'dynamicTrap': False, 'previousTarget': array([114.,   5.]), 'currentState': array([114.72242717,   6.28338999,   5.54612393]), 'targetState': array([114,   5], dtype=int32), 'currentDistance': 1.4727494319694587}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6890033104189588
{'scaleFactor': 20, 'currentTarget': array([114.,   5.]), 'dynamicTrap': False, 'previousTarget': array([114.,   5.]), 'currentState': array([114.64846928,   4.53590624,   3.77636189]), 'targetState': array([114,   5], dtype=int32), 'currentDistance': 0.7974305136980713}
episode index:2006
target Thresh 75.9969864219202
target distance 75.0
model initialize at round 2006
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.06426859, 12.21163644]), 'dynamicTrap': False, 'previousTarget': array([29.78829829, 12.90228375]), 'currentState': array([11.32459518,  8.99522642,  0.3610484 ]), 'targetState': array([85, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4459835196089492
running average episode reward sum: 0.688882224324883
{'scaleFactor': 20, 'currentTarget': array([85., 21.]), 'dynamicTrap': False, 'previousTarget': array([85., 21.]), 'currentState': array([84.04750348, 20.84334225,  6.24567655]), 'targetState': array([85, 21], dtype=int32), 'currentDistance': 0.9652933648590913}
episode index:2007
target Thresh 75.99700145220358
target distance 63.0
model initialize at round 2007
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.87778932,  9.58412361]), 'dynamicTrap': False, 'previousTarget': array([70.04019095, 10.73271054]), 'currentState': array([89.86415388, 10.32252156,  3.62158298]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3855345290640287
running average episode reward sum: 0.68873115475553
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'dynamicTrap': False, 'previousTarget': array([27.,  8.]), 'currentState': array([27.48994699,  8.46582356,  0.42254359]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.6760470682771693}
episode index:2008
target Thresh 75.9970164075231
target distance 59.0
model initialize at round 2008
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.64790594, 13.95059849]), 'dynamicTrap': True, 'previousTarget': array([43.54034249, 14.38245415]), 'currentState': array([63.       , 19.       ,  1.5449264], dtype=float32), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.617016303831579
running average episode reward sum: 0.6886954579656226
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'dynamicTrap': False, 'previousTarget': array([4., 5.]), 'currentState': array([4.28572635, 5.71243216, 5.16142931]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.76759307709309}
episode index:2009
target Thresh 75.99703128825266
target distance 17.0
model initialize at round 2009
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.64115973,  8.26413395]), 'dynamicTrap': False, 'previousTarget': array([66.43860471,  9.28585494]), 'currentState': array([51.53415715, 20.12012265,  5.36454874]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8323750817431029
running average episode reward sum: 0.6887669403655119
{'scaleFactor': 20, 'currentTarget': array([68.,  8.]), 'dynamicTrap': False, 'previousTarget': array([68.,  8.]), 'currentState': array([68.54958603,  8.43767361,  2.36656761]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 0.7025688540748322}
episode index:2010
target Thresh 75.99704609476427
target distance 51.0
model initialize at round 2010
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.12185885, 10.85939931]), 'dynamicTrap': False, 'previousTarget': array([85.75839053, 11.90064462]), 'currentState': array([66.27627218, 13.33986092,  5.88729286]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6887959832068674
{'scaleFactor': 20, 'currentTarget': array([117.,   7.]), 'dynamicTrap': False, 'previousTarget': array([117.,   7.]), 'currentState': array([116.54415582,   6.66664191,   1.90459897]), 'targetState': array([117,   7], dtype=int32), 'currentDistance': 0.564731382660506}
episode index:2011
target Thresh 75.9970608274281
target distance 10.0
model initialize at round 2011
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  14.]), 'dynamicTrap': False, 'previousTarget': array([111.,  14.]), 'currentState': array([112.40277718,   3.65724373,   2.34298182]), 'targetState': array([111,  14], dtype=int32), 'currentDistance': 10.437451364067583}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6889215717586539
{'scaleFactor': 20, 'currentTarget': array([111.,  14.]), 'dynamicTrap': False, 'previousTarget': array([111.,  14.]), 'currentState': array([110.4989078 ,  13.91856469,   3.31024657]), 'targetState': array([111,  14], dtype=int32), 'currentDistance': 0.507666333892096}
episode index:2012
target Thresh 75.99707548661246
target distance 59.0
model initialize at round 2012
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.59334955,  6.95156664]), 'dynamicTrap': False, 'previousTarget': array([67.95419404,  7.64717329]), 'currentState': array([49.62051292,  7.99358246,  5.19189203]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6412884856714948
running average episode reward sum: 0.6888979090233895
{'scaleFactor': 20, 'currentTarget': array([107.,   5.]), 'dynamicTrap': False, 'previousTarget': array([107.,   5.]), 'currentState': array([106.81714699,   4.17781969,   0.96284421]), 'targetState': array([107,   5], dtype=int32), 'currentDistance': 0.8422681758433185}
episode index:2013
target Thresh 75.99709007268383
target distance 3.0
model initialize at round 2013
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.09292273,  20.17675581]), 'dynamicTrap': True, 'previousTarget': array([100.,  20.]), 'currentState': array([103.      ,  19.      ,   4.323437], dtype=float32), 'targetState': array([100,  20], dtype=int32), 'currentDistance': 3.136216272603201}
done in step count: 10
reward sum = 0.8364474229157944
running average episode reward sum: 0.6889711709468713
{'scaleFactor': 20, 'currentTarget': array([100.,  20.]), 'dynamicTrap': False, 'previousTarget': array([100.,  20.]), 'currentState': array([99.14796968, 19.77220171,  5.00913637]), 'targetState': array([100,  20], dtype=int32), 'currentDistance': 0.881956762781235}
episode index:2014
target Thresh 75.99710458600687
target distance 24.0
model initialize at round 2014
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.77339437,  9.55003916]), 'dynamicTrap': False, 'previousTarget': array([48.84555753,  9.51930531]), 'currentState': array([30.05781022, 12.91095295,  1.59459215]), 'targetState': array([53,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8240165798048569
running average episode reward sum: 0.6890381910008953
{'scaleFactor': 20, 'currentTarget': array([53.,  9.]), 'dynamicTrap': False, 'previousTarget': array([53.,  9.]), 'currentState': array([52.43430047,  8.68058508,  1.03387721]), 'targetState': array([53,  9], dtype=int32), 'currentDistance': 0.6496474790028345}
episode index:2015
target Thresh 75.9971190269444
target distance 56.0
model initialize at round 2015
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.47985165,  9.7637141 ]), 'dynamicTrap': False, 'previousTarget': array([41.83483823,  8.72672794]), 'currentState': array([23.51600759,  3.409769  ,  0.20991921]), 'targetState': array([79, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6122145889427386
running average episode reward sum: 0.6890000840554299
{'scaleFactor': 20, 'currentTarget': array([79.10441238, 20.24138233]), 'dynamicTrap': True, 'previousTarget': array([79., 22.]), 'currentState': array([79.32595088, 20.26319423,  2.39483489]), 'targetState': array([79, 22], dtype=int32), 'currentDistance': 0.22260967144833646}
episode index:2016
target Thresh 75.99713339585746
target distance 43.0
model initialize at round 2016
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.96662864, 22.96830298]), 'dynamicTrap': False, 'previousTarget': array([87.00540614, 22.53500945]), 'currentState': array([105.94722589,  23.84906044,   2.20793423]), 'targetState': array([64, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6604540368248715
running average episode reward sum: 0.6889859313299809
{'scaleFactor': 20, 'currentTarget': array([64., 22.]), 'dynamicTrap': False, 'previousTarget': array([64., 22.]), 'currentState': array([63.51534056, 22.10557566,  4.61057444]), 'targetState': array([64, 22], dtype=int32), 'currentDistance': 0.49602519892033947}
episode index:2017
target Thresh 75.99714769310528
target distance 53.0
model initialize at round 2017
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.35710901, 16.99218119]), 'dynamicTrap': False, 'previousTarget': array([29.96803691, 15.86973376]), 'currentState': array([ 9.4357451 , 18.76397691,  1.19972789]), 'targetState': array([63, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6451313197375823
running average episode reward sum: 0.6889641996096676
{'scaleFactor': 20, 'currentTarget': array([63., 14.]), 'dynamicTrap': False, 'previousTarget': array([63., 14.]), 'currentState': array([62.99556013, 14.17189552,  1.74909649]), 'targetState': array([63, 14], dtype=int32), 'currentDistance': 0.17195285141293337}
episode index:2018
target Thresh 75.99716191904525
target distance 18.0
model initialize at round 2018
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([31.25965541,  8.12187149,  6.00724811]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 17.824602308790038}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6890754195442262
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([47.21439849,  1.45572731,  4.81905801]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 0.9557209299805949}
episode index:2019
target Thresh 75.99717607403308
target distance 43.0
model initialize at round 2019
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.95654622,  3.68232674]), 'dynamicTrap': True, 'previousTarget': array([37.95150202,  3.60803474]), 'currentState': array([18.       ,  5.       ,  3.9102511], dtype=float32), 'targetState': array([61,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.671149237272292
running average episode reward sum: 0.689066545196567
{'scaleFactor': 20, 'currentTarget': array([61.,  2.]), 'dynamicTrap': False, 'previousTarget': array([61.,  2.]), 'currentState': array([60.14345355,  1.53652495,  5.79161348]), 'targetState': array([61,  2], dtype=int32), 'currentDistance': 0.973899869407959}
episode index:2020
target Thresh 75.9971901584226
target distance 4.0
model initialize at round 2020
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'dynamicTrap': False, 'previousTarget': array([21., 17.]), 'currentState': array([23.33070819, 20.59835791,  4.10812032]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 4.287234579511588}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6892105498748468
{'scaleFactor': 20, 'currentTarget': array([21., 17.]), 'dynamicTrap': False, 'previousTarget': array([21., 17.]), 'currentState': array([20.84955386, 17.84609144,  4.5768261 ]), 'targetState': array([21, 17], dtype=int32), 'currentDistance': 0.859362996158695}
episode index:2021
target Thresh 75.99720417256592
target distance 16.0
model initialize at round 2021
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,   4.]), 'dynamicTrap': False, 'previousTarget': array([102.,   4.]), 'currentState': array([89.82356313, 18.49890587,  5.60538697]), 'targetState': array([102,   4], dtype=int32), 'currentDistance': 18.93367070331052}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6893169650702641
{'scaleFactor': 20, 'currentTarget': array([102.,   4.]), 'dynamicTrap': False, 'previousTarget': array([102.,   4.]), 'currentState': array([102.04301711,   3.90418887,   5.56613742]), 'targetState': array([102,   4], dtype=int32), 'currentDistance': 0.10502497030350812}
episode index:2022
target Thresh 75.99721811681343
target distance 49.0
model initialize at round 2022
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.60855976,  5.9375607 ]), 'dynamicTrap': True, 'previousTarget': array([60.59608118,  5.99920024]), 'currentState': array([41.       ,  2.       ,  1.2013457], dtype=float32), 'targetState': array([90, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6830738271357416
running average episode reward sum: 0.689313878991206
{'scaleFactor': 20, 'currentTarget': array([90., 12.]), 'dynamicTrap': False, 'previousTarget': array([90., 12.]), 'currentState': array([89.43920836, 11.41670014,  1.42473855]), 'targetState': array([90, 12], dtype=int32), 'currentDistance': 0.8091514046895265}
episode index:2023
target Thresh 75.9972319915137
target distance 16.0
model initialize at round 2023
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.54959448,  5.45911338]), 'dynamicTrap': False, 'previousTarget': array([25.14213562,  6.85786438]), 'currentState': array([12.54349958, 19.73599349,  5.72052053]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7803236646990452
running average episode reward sum: 0.6893588443003502
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([27.34597147,  4.57392368,  5.25569169]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.5488508780811235}
episode index:2024
target Thresh 75.99724579701362
target distance 12.0
model initialize at round 2024
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  20.]), 'dynamicTrap': False, 'previousTarget': array([101.,  20.]), 'currentState': array([93.25727182,  8.99650265,  0.56937745]), 'targetState': array([101,  20], dtype=int32), 'currentDistance': 13.454619786198696}
done in step count: 12
reward sum = 0.8481357712927802
running average episode reward sum: 0.6894372526593588
{'scaleFactor': 20, 'currentTarget': array([101.,  20.]), 'dynamicTrap': False, 'previousTarget': array([101.,  20.]), 'currentState': array([100.33916718,  19.69557645,   1.63603761]), 'targetState': array([101,  20], dtype=int32), 'currentDistance': 0.7275807247791729}
episode index:2025
target Thresh 75.99725953365832
target distance 17.0
model initialize at round 2025
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.60114429, 18.31164282]), 'dynamicTrap': False, 'previousTarget': array([14.28585494, 17.43860471]), 'currentState': array([26.75675042,  3.24744595,  2.48692608]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6644192062249001
running average episode reward sum: 0.6894249041665482
{'scaleFactor': 20, 'currentTarget': array([13., 19.]), 'dynamicTrap': False, 'previousTarget': array([13., 19.]), 'currentState': array([12.61271666, 19.55557863,  2.12044773]), 'targetState': array([13, 19], dtype=int32), 'currentDistance': 0.6772414602077469}
episode index:2026
target Thresh 75.99727320179122
target distance 58.0
model initialize at round 2026
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.27961735, 15.33264279]), 'dynamicTrap': True, 'previousTarget': array([84.29079769, 15.39813833]), 'currentState': array([104.       ,  12.       ,   1.4070739], dtype=float32), 'targetState': array([46, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6261132691428754
running average episode reward sum: 0.6893936700101478
{'scaleFactor': 20, 'currentTarget': array([46., 22.]), 'dynamicTrap': False, 'previousTarget': array([46., 22.]), 'currentState': array([46.61879154, 22.25064102,  3.59592224]), 'targetState': array([46, 22], dtype=int32), 'currentDistance': 0.6676255615113147}
episode index:2027
target Thresh 75.99728680175403
target distance 16.0
model initialize at round 2027
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  17.]), 'dynamicTrap': False, 'previousTarget': array([117.,  17.]), 'currentState': array([102.959255  ,  19.61465129,   6.15194023]), 'targetState': array([117,  17], dtype=int32), 'currentDistance': 14.282118943766962}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6895087346178488
{'scaleFactor': 20, 'currentTarget': array([117.,  17.]), 'dynamicTrap': False, 'previousTarget': array([117.,  17.]), 'currentState': array([117.38605791,  17.10426013,   5.85032649]), 'targetState': array([117,  17], dtype=int32), 'currentDistance': 0.3998885878048244}
episode index:2028
target Thresh 75.99730033388673
target distance 18.0
model initialize at round 2028
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.70809779, 17.42030937]), 'dynamicTrap': False, 'previousTarget': array([32.21295565, 16.27881227]), 'currentState': array([46.18362386,  4.75113051,  2.42395794]), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7322890126060353
running average episode reward sum: 0.6895298190328257
{'scaleFactor': 20, 'currentTarget': array([30., 18.]), 'dynamicTrap': False, 'previousTarget': array([30., 18.]), 'currentState': array([30.19399485, 18.73432561,  1.19723284]), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 0.7595183364159881}
episode index:2029
target Thresh 75.99731379852764
target distance 5.0
model initialize at round 2029
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'dynamicTrap': False, 'previousTarget': array([6., 8.]), 'currentState': array([5.18036759, 4.28121845, 0.34322989]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 3.808035389938641}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6896729570530067
{'scaleFactor': 20, 'currentTarget': array([6., 8.]), 'dynamicTrap': False, 'previousTarget': array([6., 8.]), 'currentState': array([6.74163128, 7.31948165, 0.00891674]), 'targetState': array([6, 8], dtype=int32), 'currentDistance': 1.0065397020294955}
episode index:2030
target Thresh 75.99732719601339
target distance 9.0
model initialize at round 2030
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47., 13.]), 'dynamicTrap': False, 'previousTarget': array([47., 13.]), 'currentState': array([52.59807249, 22.39182243,  5.51362652]), 'targetState': array([47, 13], dtype=int32), 'currentDistance': 10.933651908159574}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.689796938930086
{'scaleFactor': 20, 'currentTarget': array([47., 13.]), 'dynamicTrap': False, 'previousTarget': array([47., 13.]), 'currentState': array([46.75967701, 13.44595261,  4.96734965]), 'targetState': array([47, 13], dtype=int32), 'currentDistance': 0.5065854990770561}
episode index:2031
target Thresh 75.99734052667888
target distance 38.0
model initialize at round 2031
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.02630228,  4.0253776 ]), 'dynamicTrap': True, 'previousTarget': array([46.00692161,  3.52613364]), 'currentState': array([66.       ,  3.       ,  4.6185412], dtype=float32), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 50
reward sum = 0.3448066814852783
running average episode reward sum: 0.6896271602600836
{'scaleFactor': 20, 'currentTarget': array([28.,  4.]), 'dynamicTrap': False, 'previousTarget': array([28.,  4.]), 'currentState': array([28.7348564 ,  3.03434224,  2.64350435]), 'targetState': array([28,  4], dtype=int32), 'currentDistance': 1.213469746605012}
episode index:2032
target Thresh 75.9973537908574
target distance 25.0
model initialize at round 2032
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.78708691, 12.14103758]), 'dynamicTrap': True, 'previousTarget': array([76.81774824, 12.22561063]), 'currentState': array([58.      , 19.      ,  0.537681], dtype=float32), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7629117069638968
running average episode reward sum: 0.6896632077498543
{'scaleFactor': 20, 'currentTarget': array([81.30163128,  9.99703827]), 'dynamicTrap': True, 'previousTarget': array([83., 10.]), 'currentState': array([81.22691377, 10.42901045,  0.96616435]), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 0.4383864451871434}
episode index:2033
target Thresh 75.99736698888056
target distance 16.0
model initialize at round 2033
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 17.]), 'dynamicTrap': False, 'previousTarget': array([46., 17.]), 'currentState': array([31.88138497, 21.28096822,  5.88037455]), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 14.753371792023158}
done in step count: 10
reward sum = 0.8761367195367246
running average episode reward sum: 0.689754885975905
{'scaleFactor': 20, 'currentTarget': array([46., 17.]), 'dynamicTrap': False, 'previousTarget': array([46., 17.]), 'currentState': array([45.01989686, 17.39165494,  4.43691502]), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 1.0554599750780163}
episode index:2034
target Thresh 75.99738012107831
target distance 7.0
model initialize at round 2034
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 11.]), 'currentState': array([3.32712651, 4.18407164, 2.02200031]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 8.26393526348995}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6898832570638282
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.52563022, 11.14136022,  0.48413205]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.5443067478138539}
episode index:2035
target Thresh 75.99739318777894
target distance 8.0
model initialize at round 2035
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.01391459,  4.19932695]), 'dynamicTrap': True, 'previousTarget': array([47.,  4.]), 'currentState': array([55.       , 11.       ,  4.5683374], dtype=float32), 'targetState': array([47,  4], dtype=int32), 'currentDistance': 10.489361947282815}
done in step count: 7
reward sum = 0.9220653479069899
running average episode reward sum: 0.6899972954188592
{'scaleFactor': 20, 'currentTarget': array([47.,  4.]), 'dynamicTrap': False, 'previousTarget': array([47.,  4.]), 'currentState': array([47.09380338,  4.28371304,  4.78450084]), 'targetState': array([47,  4], dtype=int32), 'currentDistance': 0.2988179387612204}
episode index:2036
target Thresh 75.99740618930913
target distance 11.0
model initialize at round 2036
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60., 20.]), 'dynamicTrap': False, 'previousTarget': array([60., 20.]), 'currentState': array([50.76929565, 21.11847318,  5.8930057 ]), 'targetState': array([60, 20], dtype=int32), 'currentDistance': 9.298219455673578}
done in step count: 8
reward sum = 0.875665931911561
running average episode reward sum: 0.690088443497648
{'scaleFactor': 20, 'currentTarget': array([57.3209972 , 19.57757967]), 'dynamicTrap': True, 'previousTarget': array([57.49503335, 19.54992516]), 'currentState': array([56.36991282, 20.35229086,  6.28136732]), 'targetState': array([60, 20], dtype=int32), 'currentDistance': 1.2266780111093691}
episode index:2037
target Thresh 75.99741912599393
target distance 12.0
model initialize at round 2037
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([38.12764552, 21.48166047,  2.78811415]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 10.139092687404217}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6902164619502497
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.6150817 , 21.6278968 ,  3.74796893]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.878965238009964}
episode index:2038
target Thresh 75.99743199815673
target distance 59.0
model initialize at round 2038
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.40150932, 17.07744007]), 'dynamicTrap': False, 'previousTarget': array([82.96278884, 15.86937268]), 'currentState': array([102.28785376,  23.65816902,   2.40891206]), 'targetState': array([43,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5725257910656799
running average episode reward sum: 0.6901587421508948
{'scaleFactor': 20, 'currentTarget': array([43.,  3.]), 'dynamicTrap': False, 'previousTarget': array([43.,  3.]), 'currentState': array([42.12163859,  2.75672622,  4.19154566]), 'targetState': array([43,  3], dtype=int32), 'currentDistance': 0.9114279458350747}
episode index:2039
target Thresh 75.99744480611936
target distance 29.0
model initialize at round 2039
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.62520732,  4.41151648]), 'dynamicTrap': False, 'previousTarget': array([62.10616412,  4.94201698]), 'currentState': array([81.60248281,  5.36465045,  3.46393585]), 'targetState': array([53,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7716834631481785
running average episode reward sum: 0.6901987052494228
{'scaleFactor': 20, 'currentTarget': array([53.,  4.]), 'dynamicTrap': False, 'previousTarget': array([53.,  4.]), 'currentState': array([52.2143302 ,  3.17166768,  1.74037317]), 'targetState': array([53,  4], dtype=int32), 'currentDistance': 1.1416704737655103}
episode index:2040
target Thresh 75.997457550202
target distance 28.0
model initialize at round 2040
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.15031202,   4.77723234]), 'dynamicTrap': False, 'previousTarget': array([105.40285  ,   4.1492875]), 'currentState': array([86.06791176, 10.76572134,  0.67248774]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7635859730789749
running average episode reward sum: 0.690234661774572
{'scaleFactor': 20, 'currentTarget': array([114.,   2.]), 'dynamicTrap': False, 'previousTarget': array([114.,   2.]), 'currentState': array([113.57194803,   1.72742987,   5.41988822]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 0.5074672070190449}
episode index:2041
target Thresh 75.99747023072328
target distance 3.0
model initialize at round 2041
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'dynamicTrap': False, 'previousTarget': array([6., 3.]), 'currentState': array([4.67444125, 4.66032813, 5.1166907 ]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 2.1245694886032376}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6903814616463768
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'dynamicTrap': False, 'previousTarget': array([6., 3.]), 'currentState': array([5.60663634, 2.89339307, 5.28065773]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.40755368511161716}
episode index:2042
target Thresh 75.99748284800017
target distance 14.0
model initialize at round 2042
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 19.]), 'dynamicTrap': False, 'previousTarget': array([73., 19.]), 'currentState': array([69.14432629,  6.28501606,  1.78832179]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 13.286724063307007}
done in step count: 15
reward sum = 0.850923182166452
running average episode reward sum: 0.6904600430073755
{'scaleFactor': 20, 'currentTarget': array([73., 19.]), 'dynamicTrap': False, 'previousTarget': array([73., 19.]), 'currentState': array([72.43712856, 18.72704498,  4.62237404]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 0.6255627099891783}
episode index:2043
target Thresh 75.99749540234815
target distance 15.0
model initialize at round 2043
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([14.15930261,  5.61438188,  1.14368993]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 13.753398325596878}
done in step count: 13
reward sum = 0.8199629724756188
running average episode reward sum: 0.6905234006049626
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([10.94802773, 19.68816988,  1.22807024]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.6901296246085055}
episode index:2044
target Thresh 75.99750789408105
target distance 14.0
model initialize at round 2044
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.,  5.]), 'dynamicTrap': False, 'previousTarget': array([49.,  5.]), 'currentState': array([41.46259493, 18.12056959,  4.77214384]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 15.131484448298465}
done in step count: 29
reward sum = 0.5668999625528547
running average episode reward sum: 0.690462949046013
{'scaleFactor': 20, 'currentTarget': array([49.,  5.]), 'dynamicTrap': False, 'previousTarget': array([49.,  5.]), 'currentState': array([48.74830126,  4.96371771,  2.14793127]), 'targetState': array([49,  5], dtype=int32), 'currentDistance': 0.25430032948835163}
episode index:2045
target Thresh 75.99752032351118
target distance 10.0
model initialize at round 2045
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66., 16.]), 'dynamicTrap': False, 'previousTarget': array([66., 16.]), 'currentState': array([74.15660662, 22.35937805,  3.86260444]), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 10.342723079604973}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6905902838949153
{'scaleFactor': 20, 'currentTarget': array([66., 16.]), 'dynamicTrap': False, 'previousTarget': array([66., 16.]), 'currentState': array([66.56690525, 16.15514875,  4.49424419]), 'targetState': array([66, 16], dtype=int32), 'currentDistance': 0.58775224406217}
episode index:2046
target Thresh 75.99753269094927
target distance 73.0
model initialize at round 2046
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.00004901,  7.95572274]), 'dynamicTrap': True, 'previousTarget': array([84.,  8.]), 'currentState': array([104.       ,   8.       ,   2.1571507], dtype=float32), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = 0.09529852639177055
running average episode reward sum: 0.6902994720934971
{'scaleFactor': 20, 'currentTarget': array([31.,  8.]), 'dynamicTrap': False, 'previousTarget': array([31.,  8.]), 'currentState': array([30.4936693 ,  8.42952096,  3.30386679]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 0.6639721636535794}
episode index:2047
target Thresh 75.99754499670449
target distance 16.0
model initialize at round 2047
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  18.]), 'dynamicTrap': False, 'previousTarget': array([108.,  18.]), 'currentState': array([98.2048824 ,  1.2248268 ,  0.21286934]), 'targetState': array([108,  18], dtype=int32), 'currentDistance': 19.425518390687117}
done in step count: 28
reward sum = 0.5860508767426785
running average episode reward sum: 0.6902485694590484
{'scaleFactor': 20, 'currentTarget': array([108.,  18.]), 'dynamicTrap': False, 'previousTarget': array([108.,  18.]), 'currentState': array([108.81935158,  17.0955571 ,   2.30879685]), 'targetState': array([108,  18], dtype=int32), 'currentDistance': 1.2203909082068851}
episode index:2048
target Thresh 75.99755724108451
target distance 17.0
model initialize at round 2048
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([3.76345081, 5.66830908, 0.7050488 ]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 16.95371318109808}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6903575341628184
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([10.20850637, 20.76515175,  2.03152239]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 0.825600308712871}
episode index:2049
target Thresh 75.99756942439542
target distance 42.0
model initialize at round 2049
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.81482748,  3.99884384]), 'dynamicTrap': False, 'previousTarget': array([26.09009055,  4.89618185]), 'currentState': array([45.62820598,  1.27303531,  3.6396327 ]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6904079033089066
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.69721829, 6.50208126, 4.49166429]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.5827519533363079}
episode index:2050
target Thresh 75.99758154694183
target distance 37.0
model initialize at round 2050
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.00077945, 14.17657175]), 'dynamicTrap': True, 'previousTarget': array([24.02915453, 15.07950516]), 'currentState': array([44.        , 14.        ,  0.46451938], dtype=float32), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.656115294681099
running average episode reward sum: 0.6903911833632079
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.89114724, 16.93022896,  2.30577745]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.936576129125989}
episode index:2051
target Thresh 75.99759360902678
target distance 55.0
model initialize at round 2051
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.66803455, 15.56342775]), 'dynamicTrap': False, 'previousTarget': array([72.6773982 , 14.57770876]), 'currentState': array([5.39141720e+01, 1.24353429e+01, 2.55632401e-02]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5067475986511004
running average episode reward sum: 0.6903016884388844
{'scaleFactor': 20, 'currentTarget': array([108.,  21.]), 'dynamicTrap': False, 'previousTarget': array([108.,  21.]), 'currentState': array([108.30854745,  20.72656923,   5.83940164]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 0.4122692213472511}
episode index:2052
target Thresh 75.99760561095184
target distance 18.0
model initialize at round 2052
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.,  3.]), 'dynamicTrap': False, 'previousTarget': array([72.,  3.]), 'currentState': array([90.3201464 ,  6.30118188,  3.96789336]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 18.615197173002898}
done in step count: 16
reward sum = 0.8146224053273639
running average episode reward sum: 0.6903622440730239
{'scaleFactor': 20, 'currentTarget': array([72.,  3.]), 'dynamicTrap': False, 'previousTarget': array([72.,  3.]), 'currentState': array([71.58022264,  2.65164502,  5.15979861]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 0.5454944770636536}
episode index:2053
target Thresh 75.99761755301702
target distance 26.0
model initialize at round 2053
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.81800222,  21.50107126]), 'dynamicTrap': False, 'previousTarget': array([107.98522349,  21.76866244]), 'currentState': array([8.89100633e+01, 1.95843118e+01, 1.05301698e-02]), 'targetState': array([114,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7976783956825759
running average episode reward sum: 0.6904144914691337
{'scaleFactor': 20, 'currentTarget': array([114.,  22.]), 'dynamicTrap': False, 'previousTarget': array([114.,  22.]), 'currentState': array([114.55346772,  21.37809218,   5.81071408]), 'targetState': array([114,  22], dtype=int32), 'currentDistance': 0.8325237872535277}
episode index:2054
target Thresh 75.99762943552092
target distance 33.0
model initialize at round 2054
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.53709149, 19.24992068]), 'dynamicTrap': False, 'previousTarget': array([85.6773982 , 19.42229124]), 'currentState': array([67.91155767, 23.10198791,  5.82092339]), 'targetState': array([99, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7577288357112169
running average episode reward sum: 0.6904472478410277
{'scaleFactor': 20, 'currentTarget': array([99., 17.]), 'dynamicTrap': False, 'previousTarget': array([99., 17.]), 'currentState': array([99.16830483, 17.31565316,  1.06690214]), 'targetState': array([99, 17], dtype=int32), 'currentDistance': 0.35771976731447896}
episode index:2055
target Thresh 75.9976412587606
target distance 50.0
model initialize at round 2055
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.18835065,  7.22221572]), 'dynamicTrap': False, 'previousTarget': array([85.93630557,  7.59490445]), 'currentState': array([64.26416471,  5.4824414 ,  3.96097159]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.45208430761758955
running average episode reward sum: 0.6903313125588179
{'scaleFactor': 20, 'currentTarget': array([116.,  10.]), 'dynamicTrap': False, 'previousTarget': array([116.,  10.]), 'currentState': array([115.28038131,  10.24084824,   4.72553922]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 0.7588536933419918}
episode index:2056
target Thresh 75.9976530230316
target distance 47.0
model initialize at round 2056
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.19100023, 10.07251597]), 'dynamicTrap': False, 'previousTarget': array([32.2835766 ,  9.35598696]), 'currentState': array([52.99968856,  7.31283003,  1.92999094]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6311176967328417
running average episode reward sum: 0.6903025261631806
{'scaleFactor': 20, 'currentTarget': array([ 5., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 14.]), 'currentState': array([ 4.55878514, 14.64951728,  3.2040781 ]), 'targetState': array([ 5, 14], dtype=int32), 'currentDistance': 0.7852026782647918}
episode index:2057
target Thresh 75.99766472862807
target distance 15.0
model initialize at round 2057
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([13.15223927,  4.55217156,  0.47743231]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 17.346976573357924}
done in step count: 15
reward sum = 0.8125040490692086
running average episode reward sum: 0.6903619049401029
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([27.03095681, 15.21883053,  0.33119242]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.2210093329179098}
episode index:2058
target Thresh 75.99767637584263
target distance 8.0
model initialize at round 2058
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 14.]), 'dynamicTrap': False, 'previousTarget': array([49., 14.]), 'currentState': array([55.91921682,  4.40121355,  3.65190411]), 'targetState': array([49, 14], dtype=int32), 'currentDistance': 11.832677752972552}
done in step count: 16
reward sum = 0.8417547810948756
running average episode reward sum: 0.6904354323204597
{'scaleFactor': 20, 'currentTarget': array([49., 14.]), 'dynamicTrap': False, 'previousTarget': array([49., 14.]), 'currentState': array([48.35493636, 14.26312031,  4.47333587]), 'targetState': array([49, 14], dtype=int32), 'currentDistance': 0.6966630461644338}
episode index:2059
target Thresh 75.99768796496646
target distance 68.0
model initialize at round 2059
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.24171156, 17.78674723]), 'dynamicTrap': False, 'previousTarget': array([61.33410456, 16.88214879]), 'currentState': array([43.02634563, 23.33379261,  6.18812055]), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.46681228850206935
running average episode reward sum: 0.690326877396276
{'scaleFactor': 20, 'currentTarget': array([110.,   4.]), 'dynamicTrap': False, 'previousTarget': array([110.,   4.]), 'currentState': array([110.86069503,   3.20361242,   1.60054639]), 'targetState': array([110,   4], dtype=int32), 'currentDistance': 1.1726163540153258}
episode index:2060
target Thresh 75.9976994962893
target distance 52.0
model initialize at round 2060
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.41414686,  9.09392236]), 'dynamicTrap': False, 'previousTarget': array([42.09181942,  8.91424813]), 'currentState': array([60.3234689 ,  7.19158277,  2.02533436]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5343745257024334
running average episode reward sum: 0.6902512091033628
{'scaleFactor': 20, 'currentTarget': array([10., 12.]), 'dynamicTrap': False, 'previousTarget': array([10., 12.]), 'currentState': array([10.90037428, 12.78660712,  3.72535705]), 'targetState': array([10, 12], dtype=int32), 'currentDistance': 1.1955854662381953}
episode index:2061
target Thresh 75.99771097009942
target distance 21.0
model initialize at round 2061
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.06044981,  9.52333412]), 'dynamicTrap': False, 'previousTarget': array([97.02633404,  9.67544468]), 'currentState': array([116.44496357,  14.44681119,   3.98140001]), 'targetState': array([95,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7563515756250013
running average episode reward sum: 0.6902832655371754
{'scaleFactor': 20, 'currentTarget': array([95.,  9.]), 'dynamicTrap': False, 'previousTarget': array([95.,  9.]), 'currentState': array([95.38096884,  9.19342583,  0.12291666]), 'targetState': array([95,  9], dtype=int32), 'currentDistance': 0.4272596545347314}
episode index:2062
target Thresh 75.99772238668368
target distance 9.0
model initialize at round 2062
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.,  4.]), 'dynamicTrap': False, 'previousTarget': array([53.,  4.]), 'currentState': array([61.80160798,  2.02484875,  2.97490191]), 'targetState': array([53,  4], dtype=int32), 'currentDistance': 9.020505834367464}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6904096381907687
{'scaleFactor': 20, 'currentTarget': array([53.,  4.]), 'dynamicTrap': False, 'previousTarget': array([53.,  4.]), 'currentState': array([53.35574282,  3.4140251 ,  2.08152914]), 'targetState': array([53,  4], dtype=int32), 'currentDistance': 0.6855067801356199}
episode index:2063
target Thresh 75.99773374632748
target distance 40.0
model initialize at round 2063
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.58216261,  7.69573098]), 'dynamicTrap': False, 'previousTarget': array([65.84555753,  7.48069469]), 'currentState': array([47.73687564,  5.21287271,  5.84594787]), 'targetState': array([86, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.730669559789593
running average episode reward sum: 0.6904291439667372
{'scaleFactor': 20, 'currentTarget': array([86., 10.]), 'dynamicTrap': False, 'previousTarget': array([86., 10.]), 'currentState': array([86.28010755,  9.46271376,  2.42222066]), 'targetState': array([86, 10], dtype=int32), 'currentDistance': 0.6059180990498241}
episode index:2064
target Thresh 75.99774504931483
target distance 69.0
model initialize at round 2064
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.58469154, 10.39515326]), 'dynamicTrap': False, 'previousTarget': array([52.2494043 , 10.14864569]), 'currentState': array([70.33406743,  7.23886555,  1.9891808 ]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 83
reward sum = 0.1900513053355342
running average episode reward sum: 0.6901868302434291
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 18.]), 'currentState': array([ 2.93960334, 18.50065798,  4.80523578]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.5042877814164433}
episode index:2065
target Thresh 75.9977562959283
target distance 56.0
model initialize at round 2065
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.95995386,  9.3247872 ]), 'dynamicTrap': False, 'previousTarget': array([22.481944  ,  9.52259414]), 'currentState': array([4.54609952, 4.51830616, 0.70797699]), 'targetState': array([59, 18], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 35
reward sum = 0.6711616358532946
running average episode reward sum: 0.6901776215336565
{'scaleFactor': 20, 'currentTarget': array([59., 18.]), 'dynamicTrap': False, 'previousTarget': array([59., 18.]), 'currentState': array([58.01258831, 17.18769274,  2.67901762]), 'targetState': array([59, 18], dtype=int32), 'currentDistance': 1.2786027263042716}
episode index:2066
target Thresh 75.99776748644904
target distance 9.0
model initialize at round 2066
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.15784846, 20.1223257 ]), 'dynamicTrap': True, 'previousTarget': array([86., 20.]), 'currentState': array([77.      , 18.      ,  3.706782], dtype=float32), 'targetState': array([86, 20], dtype=int32), 'currentDistance': 9.400556085966358}
done in step count: 9
reward sum = 0.8838162474836408
running average episode reward sum: 0.6902713025331485
{'scaleFactor': 20, 'currentTarget': array([86., 20.]), 'dynamicTrap': False, 'previousTarget': array([86., 20.]), 'currentState': array([85.35886441, 20.8611833 ,  6.1448515 ]), 'targetState': array([86, 20], dtype=int32), 'currentDistance': 1.073634726238082}
episode index:2067
target Thresh 75.99777862115683
target distance 21.0
model initialize at round 2067
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.85521177,  9.67804132]), 'dynamicTrap': False, 'previousTarget': array([21.3102453 , 10.11990655]), 'currentState': array([40.00430424,  1.27503933,  3.83806384]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.649031984991791
running average episode reward sum: 0.6902513608902368
{'scaleFactor': 20, 'currentTarget': array([19., 11.]), 'dynamicTrap': False, 'previousTarget': array([19., 11.]), 'currentState': array([18.9071692 , 10.70758167,  3.24595374]), 'targetState': array([19, 11], dtype=int32), 'currentDistance': 0.3067996685431286}
episode index:2068
target Thresh 75.99778970033003
target distance 27.0
model initialize at round 2068
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.1378392 ,  20.55342326]), 'dynamicTrap': False, 'previousTarget': array([100.5237412 ,  20.33860916]), 'currentState': array([82.72029958, 15.761852  ,  5.22925967]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.690317053139123
{'scaleFactor': 20, 'currentTarget': array([108.,  22.]), 'dynamicTrap': False, 'previousTarget': array([108.,  22.]), 'currentState': array([107.76471813,  21.81639373,   3.58419229]), 'targetState': array([108,  22], dtype=int32), 'currentDistance': 0.2984440000594083}
episode index:2069
target Thresh 75.99780072424562
target distance 46.0
model initialize at round 2069
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.23890351, 10.67100544]), 'dynamicTrap': False, 'previousTarget': array([28.54842217,  9.65146426]), 'currentState': array([46.80746671,  6.53925832,  2.52629304]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.06494322233982619
running average episode reward sum: 0.6899521931026595
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.26000599, 17.61584491,  3.22293645]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 1.6366300430139562}
episode index:2070
target Thresh 75.99781169317922
target distance 31.0
model initialize at round 2070
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.8014843 , 10.76221169]), 'dynamicTrap': False, 'previousTarget': array([65.22741199, 10.23173445]), 'currentState': array([48.68515244,  2.28881095,  5.48861343]), 'targetState': array([78, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7443519962529602
running average episode reward sum: 0.6899784605112305
{'scaleFactor': 20, 'currentTarget': array([78., 16.]), 'dynamicTrap': False, 'previousTarget': array([78., 16.]), 'currentState': array([78.41881754, 16.24580389,  0.95474451]), 'targetState': array([78, 16], dtype=int32), 'currentDistance': 0.48562092419584346}
episode index:2071
target Thresh 75.99782260740501
target distance 40.0
model initialize at round 2071
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.15082735, 17.85625058]), 'dynamicTrap': False, 'previousTarget': array([83.00624707, 17.49984382]), 'currentState': array([101.15020016,  17.69786119,   2.75497687]), 'targetState': array([63, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.58871740995082
running average episode reward sum: 0.6899295893478326
{'scaleFactor': 20, 'currentTarget': array([63., 18.]), 'dynamicTrap': False, 'previousTarget': array([63., 18.]), 'currentState': array([62.14310653, 17.70831732,  3.83746782]), 'targetState': array([63, 18], dtype=int32), 'currentDistance': 0.9051768858281611}
episode index:2072
target Thresh 75.99783346719587
target distance 34.0
model initialize at round 2072
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.42593042, 16.24256083]), 'dynamicTrap': True, 'previousTarget': array([71.58914087, 16.96694159]), 'currentState': array([52.       , 21.       ,  1.6269163], dtype=float32), 'targetState': array([86, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.43618655726207456
running average episode reward sum: 0.6898071855696919
{'scaleFactor': 20, 'currentTarget': array([86., 14.]), 'dynamicTrap': False, 'previousTarget': array([86., 14.]), 'currentState': array([86.18592153, 14.14748408,  3.25802916]), 'targetState': array([86, 14], dtype=int32), 'currentDistance': 0.23731491167304408}
episode index:2073
target Thresh 75.99784427282331
target distance 20.0
model initialize at round 2073
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.46645156, 22.80299477]), 'dynamicTrap': False, 'previousTarget': array([95.12283287, 22.60700849]), 'currentState': array([112.89060733,  15.02157559,   2.89100277]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8507377011622186
running average episode reward sum: 0.689884779839505
{'scaleFactor': 20, 'currentTarget': array([95.37998755, 22.22840366]), 'dynamicTrap': True, 'previousTarget': array([94., 23.]), 'currentState': array([96.12434195, 23.02656435,  1.68276774]), 'targetState': array([94, 23], dtype=int32), 'currentDistance': 1.0913862582020253}
episode index:2074
target Thresh 75.99785502455747
target distance 59.0
model initialize at round 2074
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.99795638,  2.71409654]), 'dynamicTrap': True, 'previousTarget': array([75.99712788,  2.66106563]), 'currentState': array([56.       ,  3.       ,  3.9325688], dtype=float32), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.594163504049697
running average episode reward sum: 0.6898386491041846
{'scaleFactor': 20, 'currentTarget': array([115.,   2.]), 'dynamicTrap': False, 'previousTarget': array([115.,   2.]), 'currentState': array([114.18896985,   1.80118409,   5.72456645]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 0.8350435171539192}
episode index:2075
target Thresh 75.99786572266713
target distance 32.0
model initialize at round 2075
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.03365159,  8.15971162]), 'dynamicTrap': True, 'previousTarget': array([22.03894843,  8.24756572]), 'currentState': array([42.      ,  7.      ,  2.026282], dtype=float32), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6978991762794998
running average episode reward sum: 0.6898425318244039
{'scaleFactor': 20, 'currentTarget': array([10.,  9.]), 'dynamicTrap': False, 'previousTarget': array([10.,  9.]), 'currentState': array([10.52620302,  9.56312666,  3.90897344]), 'targetState': array([10,  9], dtype=int32), 'currentDistance': 0.7707147681365577}
episode index:2076
target Thresh 75.99787636741972
target distance 45.0
model initialize at round 2076
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.42463433, 18.252511  ]), 'dynamicTrap': False, 'previousTarget': array([85.42798013, 18.25093819]), 'currentState': array([65.99678649, 23.00211396,  2.46823559]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5528034036421728
running average episode reward sum: 0.6897765524656257
{'scaleFactor': 20, 'currentTarget': array([111.,  12.]), 'dynamicTrap': False, 'previousTarget': array([111.,  12.]), 'currentState': array([111.72701157,  11.88338869,   1.48158669]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 0.7363042989340076}
episode index:2077
target Thresh 75.9978869590814
target distance 15.0
model initialize at round 2077
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.,  4.]), 'dynamicTrap': False, 'previousTarget': array([43.,  4.]), 'currentState': array([53.19657199, 18.88263604,  3.9861958 ]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 18.0405913428913}
done in step count: 15
reward sum = 0.8240621301566156
running average episode reward sum: 0.6898411749765454
{'scaleFactor': 20, 'currentTarget': array([43.,  4.]), 'dynamicTrap': False, 'previousTarget': array([43.,  4.]), 'currentState': array([42.67220723,  4.99701566,  1.09969672]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 1.0495181453593079}
episode index:2078
target Thresh 75.99789749791695
target distance 37.0
model initialize at round 2078
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.36218564, 21.66078   ]), 'dynamicTrap': False, 'previousTarget': array([89., 21.]), 'currentState': array([108.34589642,  22.46781377,   3.00001121]), 'targetState': array([72, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7683251363843665
running average episode reward sum: 0.6898789257997334
{'scaleFactor': 20, 'currentTarget': array([72., 21.]), 'dynamicTrap': False, 'previousTarget': array([72., 21.]), 'currentState': array([72.2966552 , 21.59354638,  4.7824924 ]), 'targetState': array([72, 21], dtype=int32), 'currentDistance': 0.6635522698375108}
episode index:2079
target Thresh 75.99790798418984
target distance 72.0
model initialize at round 2079
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.10542657, 18.05084079]), 'dynamicTrap': True, 'previousTarget': array([57.06908484, 17.6609096 ]), 'currentState': array([77.      , 16.      ,  5.163829], dtype=float32), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6031393764643098
running average episode reward sum: 0.689837224093322
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.85876953, 21.54079234,  3.10362107]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.9738361174977367}
episode index:2080
target Thresh 75.99791841816223
target distance 32.0
model initialize at round 2080
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.03158623, 12.14806682]), 'dynamicTrap': True, 'previousTarget': array([33.72658355, 13.02246883]), 'currentState': array([15.       ,  6.       ,  1.0268015], dtype=float32), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6272218204123616
running average episode reward sum: 0.6898071349997704
{'scaleFactor': 20, 'currentTarget': array([47., 18.]), 'dynamicTrap': False, 'previousTarget': array([47., 18.]), 'currentState': array([46.10459026, 18.16550392,  1.20720829]), 'targetState': array([47, 18], dtype=int32), 'currentDistance': 0.9105768180190036}
episode index:2081
target Thresh 75.99792880009495
target distance 5.0
model initialize at round 2081
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.,  4.]), 'dynamicTrap': False, 'previousTarget': array([82.,  4.]), 'currentState': array([82.98196047,  7.91881529,  4.967062  ]), 'targetState': array([82,  4], dtype=int32), 'currentDistance': 4.039970254408022}
done in step count: 5
reward sum = 0.9215860598999999
running average episode reward sum: 0.689918460131807
{'scaleFactor': 20, 'currentTarget': array([82.,  4.]), 'dynamicTrap': False, 'previousTarget': array([82.,  4.]), 'currentState': array([82.55171671,  4.30739495,  5.70640951]), 'targetState': array([82,  4], dtype=int32), 'currentDistance': 0.6315718396250601}
episode index:2082
target Thresh 75.99793913024757
target distance 6.0
model initialize at round 2082
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.84005928,  3.96938945]), 'dynamicTrap': True, 'previousTarget': array([55.,  3.]), 'currentState': array([59.        ,  9.        ,  0.29385293], dtype=float32), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 7.20638818692614}
done in step count: 9
reward sum = 0.8645072973836408
running average episode reward sum: 0.6900022761842562
{'scaleFactor': 20, 'currentTarget': array([55.,  3.]), 'dynamicTrap': False, 'previousTarget': array([55.,  3.]), 'currentState': array([54.49773966,  2.76071684,  3.29390172]), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 0.5563469025189245}
episode index:2083
target Thresh 75.99794940887836
target distance 48.0
model initialize at round 2083
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.51267993,  9.76085427]), 'dynamicTrap': False, 'previousTarget': array([21.08959956,  8.96549986]), 'currentState': array([3.35480232, 4.01840484, 5.93791563]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6626652618479489
running average episode reward sum: 0.689989158614997
{'scaleFactor': 20, 'currentTarget': array([50., 18.]), 'dynamicTrap': False, 'previousTarget': array([50., 18.]), 'currentState': array([50.595715  , 18.05917502,  0.19558957]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 0.598646845150957}
episode index:2084
target Thresh 75.99795963624423
target distance 72.0
model initialize at round 2084
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.59402759, 13.63023199]), 'dynamicTrap': False, 'previousTarget': array([65.95194842, 14.38555197]), 'currentState': array([47.66876073, 11.90288125,  5.79572998]), 'targetState': array([118,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.5094270999474475
running average episode reward sum: 0.6899025581072428
{'scaleFactor': 20, 'currentTarget': array([118.,  18.]), 'dynamicTrap': False, 'previousTarget': array([118.,  18.]), 'currentState': array([118.13121787,  18.51854986,   4.15909801]), 'targetState': array([118,  18], dtype=int32), 'currentDistance': 0.5348944619765288}
episode index:2085
target Thresh 75.99796981260093
target distance 55.0
model initialize at round 2085
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.36205832,  7.8350061 ]), 'dynamicTrap': False, 'previousTarget': array([34.10805282,  8.90612542]), 'currentState': array([15.43440199,  1.37405989,  5.9834609 ]), 'targetState': array([70, 20], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.5557748708376492
running average episode reward sum: 0.6898382591200569
{'scaleFactor': 20, 'currentTarget': array([70., 20.]), 'dynamicTrap': False, 'previousTarget': array([70., 20.]), 'currentState': array([69.45731886, 20.34343383,  6.23236637]), 'targetState': array([70, 20], dtype=int32), 'currentDistance': 0.6422224010069921}
episode index:2086
target Thresh 75.99797993820282
target distance 23.0
model initialize at round 2086
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.50410231,  7.26997819]), 'dynamicTrap': False, 'previousTarget': array([39.41810404,  7.57871024]), 'currentState': array([24.71064162, 19.54053644,  5.7824573 ]), 'targetState': array([46,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7563082576937094
running average episode reward sum: 0.6898701086641746
{'scaleFactor': 20, 'currentTarget': array([46.,  3.]), 'dynamicTrap': False, 'previousTarget': array([46.,  3.]), 'currentState': array([46.33519178,  2.37738475,  1.04034274]), 'targetState': array([46,  3], dtype=int32), 'currentDistance': 0.707109104769261}
episode index:2087
target Thresh 75.99799001330307
target distance 10.0
model initialize at round 2087
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  5.]), 'dynamicTrap': False, 'previousTarget': array([29.,  5.]), 'currentState': array([25.3976765 , 15.45414593,  3.68523192]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 11.05739127169432}
done in step count: 9
reward sum = 0.8938162474836409
running average episode reward sum: 0.6899677840180153
{'scaleFactor': 20, 'currentTarget': array([29.,  5.]), 'dynamicTrap': False, 'previousTarget': array([29.,  5.]), 'currentState': array([28.7773565 ,  4.04550712,  0.06555269]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 0.9801157011504288}
episode index:2088
target Thresh 75.99800003815353
target distance 11.0
model initialize at round 2088
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,  23.]), 'dynamicTrap': False, 'previousTarget': array([115.,  23.]), 'currentState': array([114.15086962,  13.22163526,   1.11153078]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 9.815163748437868}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6900927348394045
{'scaleFactor': 20, 'currentTarget': array([115.,  23.]), 'dynamicTrap': False, 'previousTarget': array([115.,  23.]), 'currentState': array([115.27342496,  22.27569645,   1.62405205]), 'targetState': array([115,  23], dtype=int32), 'currentDistance': 0.7741943216414736}
episode index:2089
target Thresh 75.99801001300486
target distance 58.0
model initialize at round 2089
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.36668242,  9.99315652]), 'dynamicTrap': True, 'previousTarget': array([79.36293823, 10.00765644]), 'currentState': array([60.       ,  5.       ,  0.5452103], dtype=float32), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.41307072748716234
running average episode reward sum: 0.6899601884244034
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([118.19721355,  20.54269597,   1.72242961]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 0.5774184806762572}
episode index:2090
target Thresh 75.9980199381064
target distance 12.0
model initialize at round 2090
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([34.27448392,  1.16808531,  3.45962667]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 13.031934306268925}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6900759728143998
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([32.27559574, 13.08208432,  1.23043638]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 0.9583956401417041}
episode index:2091
target Thresh 75.9980298137063
target distance 18.0
model initialize at round 2091
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.22350593,  20.59240397]), 'dynamicTrap': False, 'previousTarget': array([109.05572809,  20.88854382]), 'currentState': array([118.83967935,   3.05589324,   1.02998739]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6901572263429977
{'scaleFactor': 20, 'currentTarget': array([109.,  21.]), 'dynamicTrap': False, 'previousTarget': array([109.,  21.]), 'currentState': array([109.21191093,  20.72146181,   5.26401124]), 'targetState': array([109,  21], dtype=int32), 'currentDistance': 0.3499853754669696}
episode index:2092
target Thresh 75.99803964005143
target distance 27.0
model initialize at round 2092
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.52997175,  11.79581692]), 'dynamicTrap': False, 'previousTarget': array([110.87767469,  12.20863052]), 'currentState': array([89.72908125,  8.98072627,  4.73328638]), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.69023429301512
{'scaleFactor': 20, 'currentTarget': array([118.,  13.]), 'dynamicTrap': False, 'previousTarget': array([118.,  13.]), 'currentState': array([117.49045716,  13.13239905,   0.46847194]), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 0.526463115403863}
episode index:2093
target Thresh 75.99804941738746
target distance 62.0
model initialize at round 2093
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.44056384, 20.37738154]), 'dynamicTrap': False, 'previousTarget': array([70.09299973, 20.0735161 ]), 'currentState': array([88.32441939, 22.52965673,  2.61798283]), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.7019526437258792
running average episode reward sum: 0.6902398891711423
{'scaleFactor': 20, 'currentTarget': array([28., 16.]), 'dynamicTrap': False, 'previousTarget': array([28., 16.]), 'currentState': array([28.16707458, 16.87013807,  4.26154497]), 'targetState': array([28, 16], dtype=int32), 'currentDistance': 0.8860328263319933}
episode index:2094
target Thresh 75.99805914595883
target distance 4.0
model initialize at round 2094
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([69.80087066, 14.38388464,  0.70370698]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 3.4176350675485225}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6903782472192707
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([71.88721156, 16.58533847,  5.77335531]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 0.429727140419331}
episode index:2095
target Thresh 75.99806882600873
target distance 16.0
model initialize at round 2095
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94., 16.]), 'dynamicTrap': False, 'previousTarget': array([94., 16.]), 'currentState': array([108.48194931,   4.00687443,   2.12706423]), 'targetState': array([94, 16], dtype=int32), 'currentDistance': 18.803242186571207}
done in step count: 23
reward sum = 0.6782373598512027
running average episode reward sum: 0.6903724548111753
{'scaleFactor': 20, 'currentTarget': array([94., 16.]), 'dynamicTrap': False, 'previousTarget': array([94., 16.]), 'currentState': array([94.14354112, 16.82551333,  3.8075884 ]), 'targetState': array([94, 16], dtype=int32), 'currentDistance': 0.8378999445698322}
episode index:2096
target Thresh 75.99807845777922
target distance 19.0
model initialize at round 2096
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'dynamicTrap': False, 'previousTarget': array([27., 23.]), 'currentState': array([21.38134556,  5.70859367,  0.55346346]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 18.18136437887127}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6904701972048077
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'dynamicTrap': False, 'previousTarget': array([27., 23.]), 'currentState': array([26.07076536, 23.05278261,  1.25299031]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.9307325226129209}
episode index:2097
target Thresh 75.99808804151101
target distance 61.0
model initialize at round 2097
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.17728114, 14.51668076]), 'dynamicTrap': False, 'previousTarget': array([72.73417352, 15.63083524]), 'currentState': array([92.58311968, 19.3554234 ,  4.21922731]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5514320765005216
running average episode reward sum: 0.6904039254599534
{'scaleFactor': 20, 'currentTarget': array([31.,  4.]), 'dynamicTrap': False, 'previousTarget': array([31.,  4.]), 'currentState': array([31.06434719,  3.98213911,  4.18422195]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 0.06678003268359949}
episode index:2098
target Thresh 75.99809757744376
target distance 33.0
model initialize at round 2098
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.81698241, 14.22348371]), 'dynamicTrap': True, 'previousTarget': array([96.79586847, 14.16513874]), 'currentState': array([78.       , 21.       ,  3.3162453], dtype=float32), 'targetState': array([111,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6712721408862283
running average episode reward sum: 0.690394810746007
{'scaleFactor': 20, 'currentTarget': array([111.,   9.]), 'dynamicTrap': False, 'previousTarget': array([111.,   9.]), 'currentState': array([111.32639542,   8.69071322,   1.26850707]), 'targetState': array([111,   9], dtype=int32), 'currentDistance': 0.44965796582617684}
episode index:2099
target Thresh 75.99810706581584
target distance 2.0
model initialize at round 2099
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.88533081, 13.83654409]), 'dynamicTrap': True, 'previousTarget': array([95., 14.]), 'currentState': array([95.       , 12.       ,  0.8600444], dtype=float32), 'targetState': array([95, 14], dtype=int32), 'currentDistance': 1.8401204327975142}
done in step count: 2
reward sum = 0.9701
running average episode reward sum: 0.6905280036932707
{'scaleFactor': 20, 'currentTarget': array([95., 14.]), 'dynamicTrap': False, 'previousTarget': array([95., 14.]), 'currentState': array([94.52543844, 14.06650914,  2.63732859]), 'targetState': array([95, 14], dtype=int32), 'currentDistance': 0.4791994770616721}
episode index:2100
target Thresh 75.99811650686448
target distance 21.0
model initialize at round 2100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.41397154, 20.55741303]), 'dynamicTrap': False, 'previousTarget': array([30.99469707, 19.52709229]), 'currentState': array([46.58198877, 10.29782316,  1.9097656 ]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7953888878118469
running average episode reward sum: 0.6905779136809521
{'scaleFactor': 20, 'currentTarget': array([27., 22.]), 'dynamicTrap': False, 'previousTarget': array([27., 22.]), 'currentState': array([27.16190204, 21.84404029,  2.63671487]), 'targetState': array([27, 22], dtype=int32), 'currentDistance': 0.22480147032773484}
episode index:2101
target Thresh 75.99812590082568
target distance 23.0
model initialize at round 2101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.9197807 , 16.46820585]), 'dynamicTrap': False, 'previousTarget': array([55.07518824, 16.26740767]), 'currentState': array([74.66749381, 19.63488026,  2.77516037]), 'targetState': array([52, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8077231661796749
running average episode reward sum: 0.6906336440579735
{'scaleFactor': 20, 'currentTarget': array([52., 16.]), 'dynamicTrap': False, 'previousTarget': array([52., 16.]), 'currentState': array([51.58123187, 15.39901166,  1.402165  ]), 'targetState': array([52, 16], dtype=int32), 'currentDistance': 0.7324982826916034}
episode index:2102
target Thresh 75.99813524793431
target distance 13.0
model initialize at round 2102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.,  5.]), 'dynamicTrap': False, 'previousTarget': array([86.,  5.]), 'currentState': array([71.85099863, 11.11207863,  4.80951214]), 'targetState': array([86,  5], dtype=int32), 'currentDistance': 15.412713749322998}
done in step count: 12
reward sum = 0.8682058784912048
running average episode reward sum: 0.6907180816397296
{'scaleFactor': 20, 'currentTarget': array([86.,  5.]), 'dynamicTrap': False, 'previousTarget': array([86.,  5.]), 'currentState': array([85.55371155,  5.8687442 ,  5.22117373]), 'targetState': array([86,  5], dtype=int32), 'currentDistance': 0.976672854440089}
episode index:2103
target Thresh 75.99814454842404
target distance 17.0
model initialize at round 2103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  12.]), 'dynamicTrap': False, 'previousTarget': array([112.,  12.]), 'currentState': array([95.54543662, 20.28493089,  0.15936017]), 'targetState': array([112,  12], dtype=int32), 'currentDistance': 18.422614797235312}
done in step count: 15
reward sum = 0.8134503797500929
running average episode reward sum: 0.6907764144810368
{'scaleFactor': 20, 'currentTarget': array([112.,  12.]), 'dynamicTrap': False, 'previousTarget': array([112.,  12.]), 'currentState': array([111.60929614,  12.55622585,   5.20028618]), 'targetState': array([112,  12], dtype=int32), 'currentDistance': 0.6797328153637742}
episode index:2104
target Thresh 75.99815380252738
target distance 12.0
model initialize at round 2104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([42.52810304, 20.8431563 ,  3.67552373]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 15.113404389286869}
done in step count: 12
reward sum = 0.8485335561440494
running average episode reward sum: 0.690851358491328
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([31.2682472 , 10.49480421,  3.16947353]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 0.8833421577379738}
episode index:2105
target Thresh 75.9981630104757
target distance 22.0
model initialize at round 2105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.99434924,  7.8992619 ]), 'dynamicTrap': False, 'previousTarget': array([66.51093912,  8.42734309]), 'currentState': array([49.74476424, 16.08172163,  5.4318741 ]), 'targetState': array([70,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.5563092387644488
running average episode reward sum: 0.6907874733442592
{'scaleFactor': 20, 'currentTarget': array([70.,  7.]), 'dynamicTrap': False, 'previousTarget': array([70.,  7.]), 'currentState': array([69.11231282,  6.75498008,  3.12105292]), 'targetState': array([70,  7], dtype=int32), 'currentDistance': 0.9208818062594523}
episode index:2106
target Thresh 75.99817217249915
target distance 6.0
model initialize at round 2106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 13.]), 'dynamicTrap': False, 'previousTarget': array([46., 13.]), 'currentState': array([49.20405702,  8.03296863,  1.68555075]), 'targetState': array([46, 13], dtype=int32), 'currentDistance': 5.91078523450353}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6909201318761319
{'scaleFactor': 20, 'currentTarget': array([46., 13.]), 'dynamicTrap': False, 'previousTarget': array([46., 13.]), 'currentState': array([46.43426444, 13.20363785,  1.63517047]), 'targetState': array([46, 13], dtype=int32), 'currentDistance': 0.47963942705976287}
episode index:2107
target Thresh 75.99818128882686
target distance 26.0
model initialize at round 2107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.31551693, 21.70011015]), 'dynamicTrap': False, 'previousTarget': array([ 9.35987106, 21.77694787]), 'currentState': array([27.74304066, 16.94918166,  2.82790279]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 26
reward sum = 0.6910396597716018
running average episode reward sum: 0.6909201885781696
{'scaleFactor': 20, 'currentTarget': array([ 3., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 23.]), 'currentState': array([ 3.50731936, 22.86434594,  3.59079448]), 'targetState': array([ 3, 23], dtype=int32), 'currentDistance': 0.5251427955098846}
episode index:2108
target Thresh 75.99819035968667
target distance 21.0
model initialize at round 2108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.87462329,  9.38292221]), 'dynamicTrap': False, 'previousTarget': array([87.6897547 ,  9.88009345]), 'currentState': array([69.9406771 , 15.82541253,  5.74562275]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8232124156227957
running average episode reward sum: 0.6909829160447626
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([89.08630952,  9.35878826,  0.10108984]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 0.9816105665880526}
episode index:2109
target Thresh 75.99819938530538
target distance 49.0
model initialize at round 2109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.17379542, 14.63089565]), 'dynamicTrap': True, 'previousTarget': array([53.26134238, 15.22263798]), 'currentState': array([73.       , 12.       ,  6.2418623], dtype=float32), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6390943393724118
running average episode reward sum: 0.6909583243022638
{'scaleFactor': 20, 'currentTarget': array([24., 20.]), 'dynamicTrap': False, 'previousTarget': array([24., 20.]), 'currentState': array([24.14044306, 20.30423125,  4.12180677]), 'targetState': array([24, 20], dtype=int32), 'currentDistance': 0.3350834311376741}
episode index:2110
target Thresh 75.99820836590865
target distance 51.0
model initialize at round 2110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.89989685, 14.4349952 ]), 'dynamicTrap': False, 'previousTarget': array([72.0954295 , 14.04857152]), 'currentState': array([90.76920763, 16.71764479,  3.40301275]), 'targetState': array([41, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.36675046883790596
running average episode reward sum: 0.6904572779767593
{'scaleFactor': 20, 'currentTarget': array([41.61060862, 10.88589309]), 'dynamicTrap': True, 'previousTarget': array([41.60891055, 10.685941  ]), 'currentState': array([45.95491366,  9.53396332,  3.78905695]), 'targetState': array([41, 11], dtype=int32), 'currentDistance': 4.54980223858397}
episode index:2111
target Thresh 75.99821730172096
target distance 62.0
model initialize at round 2111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.00080963,  6.21412782]), 'dynamicTrap': False, 'previousTarget': array([69.99739905,  7.32253869]), 'currentState': array([51.01975629,  5.34377875,  5.61167759]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6814327799736872
running average episode reward sum: 0.6904530050136896
{'scaleFactor': 20, 'currentTarget': array([112.,   8.]), 'dynamicTrap': False, 'previousTarget': array([112.,   8.]), 'currentState': array([111.12772656,   7.63592536,   0.27116096]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 0.9452043695177118}
episode index:2112
target Thresh 75.99822619296572
target distance 68.0
model initialize at round 2112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.69461613, 20.02919238]), 'dynamicTrap': False, 'previousTarget': array([64.94615251, 20.53337114]), 'currentState': array([46.73727404, 21.33475608,  5.69471109]), 'targetState': array([113,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3752734858106801
running average episode reward sum: 0.6903038429127889
{'scaleFactor': 20, 'currentTarget': array([113.,  17.]), 'dynamicTrap': False, 'previousTarget': array([113.,  17.]), 'currentState': array([112.62245925,  17.3292444 ,   5.89240722]), 'targetState': array([113,  17], dtype=int32), 'currentDistance': 0.5009380159933314}
episode index:2113
target Thresh 75.99823503986521
target distance 30.0
model initialize at round 2113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.81132845, 13.22494843]), 'dynamicTrap': True, 'previousTarget': array([45.85014149, 13.28991511]), 'currentState': array([63.       ,  3.       ,  1.6533937], dtype=float32), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7811797677979093
running average episode reward sum: 0.6903468305782975
{'scaleFactor': 20, 'currentTarget': array([33., 21.]), 'dynamicTrap': False, 'previousTarget': array([33., 21.]), 'currentState': array([33.95339453, 20.60186686,  3.17735202]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 1.0331849443952996}
episode index:2114
target Thresh 75.9982438426406
target distance 9.0
model initialize at round 2114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'dynamicTrap': False, 'previousTarget': array([11., 13.]), 'currentState': array([7.98086123, 5.77380669, 2.42818391]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 7.831543181243274}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6904523485059124
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'dynamicTrap': False, 'previousTarget': array([11., 13.]), 'currentState': array([11.62037329, 13.84986231,  5.7028063 ]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 1.0522019646480654}
episode index:2115
target Thresh 75.99825260151198
target distance 3.0
model initialize at round 2115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80., 10.]), 'dynamicTrap': False, 'previousTarget': array([80., 10.]), 'currentState': array([76.86175648,  9.14426274,  2.25810761]), 'targetState': array([80, 10], dtype=int32), 'currentDistance': 3.25282317821345}
done in step count: 7
reward sum = 0.8934454973079899
running average episode reward sum: 0.6905482809958945
{'scaleFactor': 20, 'currentTarget': array([80., 10.]), 'dynamicTrap': False, 'previousTarget': array([80., 10.]), 'currentState': array([80.76815195, 10.03710913,  4.33084085]), 'targetState': array([80, 10], dtype=int32), 'currentDistance': 0.7690477869682342}
episode index:2116
target Thresh 75.99826131669829
target distance 37.0
model initialize at round 2116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.15632243, 12.70110423]), 'dynamicTrap': False, 'previousTarget': array([68.54828331, 12.22665585]), 'currentState': array([50.57625264,  8.6242345 ,  5.97674472]), 'targetState': array([86, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5110095606721002
running average episode reward sum: 0.690463472908826
{'scaleFactor': 20, 'currentTarget': array([86., 16.]), 'dynamicTrap': False, 'previousTarget': array([86., 16.]), 'currentState': array([85.89085027, 15.31357848,  0.40959108]), 'targetState': array([86, 16], dtype=int32), 'currentDistance': 0.6950454437162561}
episode index:2117
target Thresh 75.99826998841743
target distance 18.0
model initialize at round 2117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   4.]), 'dynamicTrap': False, 'previousTarget': array([106.,   4.]), 'currentState': array([100.55705487,  22.86619711,   3.61095405]), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 19.635657491966768}
done in step count: 41
reward sum = 0.4316834143722639
running average episode reward sum: 0.6903412915780722
{'scaleFactor': 20, 'currentTarget': array([106.,   4.]), 'dynamicTrap': False, 'previousTarget': array([106.,   4.]), 'currentState': array([106.74873933,   4.94388788,   3.54113299]), 'targetState': array([106,   4], dtype=int32), 'currentDistance': 1.2047966234006726}
episode index:2118
target Thresh 75.9982786168862
target distance 57.0
model initialize at round 2118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.27836214, 18.8572964 ]), 'dynamicTrap': False, 'previousTarget': array([74.95093522, 18.40006563]), 'currentState': array([56.31424536, 17.65978175,  5.79867644]), 'targetState': array([112,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6053921681117039
running average episode reward sum: 0.6903012023267903
{'scaleFactor': 20, 'currentTarget': array([112.,  21.]), 'dynamicTrap': False, 'previousTarget': array([112.,  21.]), 'currentState': array([111.517718  ,  20.78073407,   6.03021477]), 'targetState': array([112,  21], dtype=int32), 'currentDistance': 0.5297862554467789}
episode index:2119
target Thresh 75.9982872023203
target distance 38.0
model initialize at round 2119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.39170336, 13.14218557]), 'dynamicTrap': True, 'previousTarget': array([91.25636504, 12.8326788 ]), 'currentState': array([73.       , 21.       ,  4.4096665], dtype=float32), 'targetState': array([111,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6773220311785899
running average episode reward sum: 0.6902950800762487
{'scaleFactor': 20, 'currentTarget': array([111.,   4.]), 'dynamicTrap': False, 'previousTarget': array([111.,   4.]), 'currentState': array([110.87964317,   4.56202268,   5.9092951 ]), 'targetState': array([111,   4], dtype=int32), 'currentDistance': 0.5747653985189443}
episode index:2120
target Thresh 75.99829574493437
target distance 24.0
model initialize at round 2120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.99763251, 13.97841957]), 'dynamicTrap': False, 'previousTarget': array([54.8, 14.4]), 'currentState': array([72.01048864, 20.18416706,  3.1271982 ]), 'targetState': array([50, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8315276925482784
running average episode reward sum: 0.6903616678237602
{'scaleFactor': 20, 'currentTarget': array([50., 13.]), 'dynamicTrap': False, 'previousTarget': array([50., 13.]), 'currentState': array([50.23838019, 13.27232231,  3.26577813]), 'targetState': array([50, 13], dtype=int32), 'currentDistance': 0.36191788762461097}
episode index:2121
target Thresh 75.99830424494196
target distance 54.0
model initialize at round 2121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.16238674,  7.32127593]), 'dynamicTrap': False, 'previousTarget': array([53.05464491,  7.52256629]), 'currentState': array([71.11049905,  8.76100344,  3.81183004]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5457284264641613
running average episode reward sum: 0.6902935088975776
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'dynamicTrap': False, 'previousTarget': array([19.,  5.]), 'currentState': array([19.85260971,  5.00827733,  4.40837354]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 0.8526498847389283}
episode index:2122
target Thresh 75.99831270255561
target distance 21.0
model initialize at round 2122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.56417726,  2.10825035]), 'dynamicTrap': False, 'previousTarget': array([29.23047895,  2.50557744]), 'currentState': array([11.15395832,  6.92939136,  0.19647878]), 'targetState': array([31,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8405004572968984
running average episode reward sum: 0.690364261110672
{'scaleFactor': 20, 'currentTarget': array([31.,  2.]), 'dynamicTrap': False, 'previousTarget': array([31.,  2.]), 'currentState': array([30.31461322,  2.20693896,  0.38983459]), 'targetState': array([31,  2], dtype=int32), 'currentDistance': 0.7159460626388747}
episode index:2123
target Thresh 75.9983211179867
target distance 69.0
model initialize at round 2123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.9232071 , 11.51255435]), 'dynamicTrap': False, 'previousTarget': array([41.99160369, 12.42053323]), 'currentState': array([20.92425465, 11.71725097,  5.02453589]), 'targetState': array([91, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5993097751205549
running average episode reward sum: 0.6903213917669854
{'scaleFactor': 20, 'currentTarget': array([91., 11.]), 'dynamicTrap': False, 'previousTarget': array([91., 11.]), 'currentState': array([90.94605507, 10.63124264,  0.20643628]), 'targetState': array([91, 11], dtype=int32), 'currentDistance': 0.3726822325167987}
episode index:2124
target Thresh 75.99832949144569
target distance 16.0
model initialize at round 2124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.57889734, 11.36429623]), 'dynamicTrap': True, 'previousTarget': array([88., 13.]), 'currentState': array([104.        ,   3.        ,   0.41878882], dtype=float32), 'targetState': array([88, 13], dtype=int32), 'currentDistance': 18.428620781274198}
done in step count: 19
reward sum = 0.7305506988443912
running average episode reward sum: 0.69034032320561
{'scaleFactor': 20, 'currentTarget': array([88., 13.]), 'dynamicTrap': False, 'previousTarget': array([88., 13.]), 'currentState': array([87.67989423, 12.73723102,  2.58106948]), 'targetState': array([88, 13], dtype=int32), 'currentDistance': 0.4141439890195722}
episode index:2125
target Thresh 75.99833782314187
target distance 9.0
model initialize at round 2125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,  10.]), 'dynamicTrap': False, 'previousTarget': array([116.,  10.]), 'currentState': array([108.25669447,  14.62737707,   5.83307117]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 9.020609686783967}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6904629242059366
{'scaleFactor': 20, 'currentTarget': array([116.,  10.]), 'dynamicTrap': False, 'previousTarget': array([116.,  10.]), 'currentState': array([116.34527201,  10.06463578,   6.00896095]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 0.3512699038261895}
episode index:2126
target Thresh 75.99834611328353
target distance 40.0
model initialize at round 2126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.95030926, 16.5910428 ]), 'dynamicTrap': True, 'previousTarget': array([42.9439862 , 16.50420104]), 'currentState': array([23.      , 18.      ,  4.718024], dtype=float32), 'targetState': array([63, 15], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 58
reward sum = 0.26856017691878364
running average episode reward sum: 0.6902645684244194
{'scaleFactor': 20, 'currentTarget': array([55.84710756, 16.21880666]), 'dynamicTrap': True, 'previousTarget': array([56.04630191, 16.23253502]), 'currentState': array([54.98752486, 16.56695278,  5.99288436]), 'targetState': array([63, 15], dtype=int32), 'currentDistance': 0.9274093638288551}
episode index:2127
target Thresh 75.99835436207793
target distance 38.0
model initialize at round 2127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.10220582, 14.83367827]), 'dynamicTrap': False, 'previousTarget': array([57.00692161, 15.52613364]), 'currentState': array([76.05585891, 13.47289597,  3.35467768]), 'targetState': array([39, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7194174143350832
running average episode reward sum: 0.6902782680700541
{'scaleFactor': 20, 'currentTarget': array([39., 16.]), 'dynamicTrap': False, 'previousTarget': array([39., 16.]), 'currentState': array([38.65329458, 15.94686986,  1.45381122]), 'targetState': array([39, 16], dtype=int32), 'currentDistance': 0.35075270383773377}
episode index:2128
target Thresh 75.99836256973131
target distance 34.0
model initialize at round 2128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.55252805, 10.66860096]), 'dynamicTrap': True, 'previousTarget': array([64.53165663, 10.58078667]), 'currentState': array([84.       ,  6.       ,  3.7538939], dtype=float32), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6754316029219434
running average episode reward sum: 0.6902712945307642
{'scaleFactor': 20, 'currentTarget': array([50., 14.]), 'dynamicTrap': False, 'previousTarget': array([50., 14.]), 'currentState': array([49.61157022, 14.13732168,  2.32859439]), 'targetState': array([50, 14], dtype=int32), 'currentDistance': 0.4119889988288683}
episode index:2129
target Thresh 75.99837073644886
target distance 46.0
model initialize at round 2129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.07742277, 19.61895778]), 'dynamicTrap': False, 'previousTarget': array([94.04239788, 19.69841725]), 'currentState': array([112.03236378,  20.96072129,   2.84798312]), 'targetState': array([68, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7370535670878147
running average episode reward sum: 0.6902932580390069
{'scaleFactor': 20, 'currentTarget': array([68., 18.]), 'dynamicTrap': False, 'previousTarget': array([68., 18.]), 'currentState': array([67.81971667, 17.25352651,  2.240831  ]), 'targetState': array([68, 18], dtype=int32), 'currentDistance': 0.7679353835754292}
episode index:2130
target Thresh 75.99837886243472
target distance 24.0
model initialize at round 2130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.0243864 ,  7.80754918]), 'dynamicTrap': False, 'previousTarget': array([33.81870354,  8.66690579]), 'currentState': array([50.26862193, 16.00192957,  3.59570721]), 'targetState': array([28,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7636092941813178
running average episode reward sum: 0.6903276625608945
{'scaleFactor': 20, 'currentTarget': array([28.,  6.]), 'dynamicTrap': False, 'previousTarget': array([28.,  6.]), 'currentState': array([28.41119831,  5.07598952,  2.46293422]), 'targetState': array([28,  6], dtype=int32), 'currentDistance': 1.0113750112658257}
episode index:2131
target Thresh 75.99838694789206
target distance 11.0
model initialize at round 2131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  22.]), 'dynamicTrap': False, 'previousTarget': array([101.,  22.]), 'currentState': array([110.33463672,  21.44472707,   3.51053941]), 'targetState': array([101,  22], dtype=int32), 'currentDistance': 9.351137399512876}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6904499244686519
{'scaleFactor': 20, 'currentTarget': array([101.,  22.]), 'dynamicTrap': False, 'previousTarget': array([101.,  22.]), 'currentState': array([101.76636085,  21.95270389,   2.33901978]), 'targetState': array([101,  22], dtype=int32), 'currentDistance': 0.7678189094616419}
episode index:2132
target Thresh 75.998394993023
target distance 34.0
model initialize at round 2132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.77442189,  2.9171545 ]), 'dynamicTrap': False, 'previousTarget': array([32.99135509,  2.41201897]), 'currentState': array([13.82233923,  4.30077208,  0.17795026]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8174811657078969
running average episode reward sum: 0.6905094796684829
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([46.37442621,  1.17157209,  5.8211092 ]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 1.038092173327423}
episode index:2133
target Thresh 75.9984029980287
target distance 17.0
model initialize at round 2133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([84.39099664, 21.89315757,  4.20569658]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 17.51061633706393}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6906054631617303
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([88.94798473,  4.38607486,  3.23933546]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 0.6161247167378932}
episode index:2134
target Thresh 75.99841096310925
target distance 21.0
model initialize at round 2134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.40826003,  9.9899639 ]), 'dynamicTrap': False, 'previousTarget': array([12.02263725, 10.04869701]), 'currentState': array([31.40221971,  9.49845997,  3.55267191]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7892102361398824
running average episode reward sum: 0.6906516480671065
{'scaleFactor': 20, 'currentTarget': array([11., 10.]), 'dynamicTrap': False, 'previousTarget': array([11., 10.]), 'currentState': array([11.7910558 ,  9.84873467,  3.49780251]), 'targetState': array([11, 10], dtype=int32), 'currentDistance': 0.805388405804602}
episode index:2135
target Thresh 75.99841888846382
target distance 22.0
model initialize at round 2135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.8047398 , 19.51788039]), 'dynamicTrap': False, 'previousTarget': array([30.21853056, 20.17458624]), 'currentState': array([13.19125584,  8.38270411,  5.84009886]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 24
reward sum = 0.7111426204321802
running average episode reward sum: 0.6906612412189629
{'scaleFactor': 20, 'currentTarget': array([35., 23.]), 'dynamicTrap': False, 'previousTarget': array([35., 23.]), 'currentState': array([35.56284546, 22.68141482,  1.52123736]), 'targetState': array([35, 23], dtype=int32), 'currentDistance': 0.6467546093797283}
episode index:2136
target Thresh 75.9984267742905
target distance 6.0
model initialize at round 2136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 11.]), 'dynamicTrap': False, 'previousTarget': array([16., 11.]), 'currentState': array([12.78945007,  5.16224968,  0.84587329]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 6.662353914346163}
done in step count: 3
reward sum = 0.9605960100000001
running average episode reward sum: 0.6907875560382334
{'scaleFactor': 20, 'currentTarget': array([15.70774775,  9.31989488]), 'dynamicTrap': True, 'previousTarget': array([16., 11.]), 'currentState': array([14.72135806,  8.64565369,  1.10513077]), 'targetState': array([16, 11], dtype=int32), 'currentDistance': 1.1948078590067495}
episode index:2137
target Thresh 75.99843462078647
target distance 61.0
model initialize at round 2137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.38108321,  8.62854843]), 'dynamicTrap': False, 'previousTarget': array([49.00459082,  9.23101338]), 'currentState': array([31.526434  ,  1.95755127,  5.99043894]), 'targetState': array([91, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5538688478823202
running average episode reward sum: 0.690723515482501
{'scaleFactor': 20, 'currentTarget': array([91., 23.]), 'dynamicTrap': False, 'previousTarget': array([91., 23.]), 'currentState': array([90.93654962, 23.76132427,  2.92357635]), 'targetState': array([91, 23], dtype=int32), 'currentDistance': 0.7639637379317279}
episode index:2138
target Thresh 75.99844242814785
target distance 32.0
model initialize at round 2138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.02628251,   4.37552004]), 'dynamicTrap': False, 'previousTarget': array([103.,   5.]), 'currentState': array([83.05342778,  3.33385128,  5.58811903]), 'targetState': array([115,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7693919274653074
running average episode reward sum: 0.69076029360872
{'scaleFactor': 20, 'currentTarget': array([115.,   5.]), 'dynamicTrap': False, 'previousTarget': array([115.,   5.]), 'currentState': array([115.28938876,   4.76446314,   6.27579856]), 'targetState': array([115,   5], dtype=int32), 'currentDistance': 0.3731266114860198}
episode index:2139
target Thresh 75.99845019656988
target distance 72.0
model initialize at round 2139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.6463771 , 16.96172742]), 'dynamicTrap': False, 'previousTarget': array([86.09385676, 15.93531948]), 'currentState': array([106.58779796,  15.43210852,   0.48220503]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5604289682835558
running average episode reward sum: 0.6906993911202504
{'scaleFactor': 20, 'currentTarget': array([34., 21.]), 'dynamicTrap': False, 'previousTarget': array([34., 21.]), 'currentState': array([34.76858856, 20.54341458,  3.10568646]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 0.8939790909767518}
episode index:2140
target Thresh 75.99845792624674
target distance 64.0
model initialize at round 2140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.34969476,  5.62915578]), 'dynamicTrap': False, 'previousTarget': array([88.00244096,  6.68753814]), 'currentState': array([107.34896297,   5.45806777,   3.41258764]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2721825581000179
running average episode reward sum: 0.6905039138512078
{'scaleFactor': 20, 'currentTarget': array([45.840982  ,  6.32910177]), 'dynamicTrap': True, 'previousTarget': array([44.,  6.]), 'currentState': array([46.0783773 ,  6.19227764,  3.69065905]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 0.2740025024644981}
episode index:2141
target Thresh 75.99846561737166
target distance 17.0
model initialize at round 2141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.47068371,  7.54257559]), 'dynamicTrap': False, 'previousTarget': array([74.85786438,  8.85786438]), 'currentState': array([87.27148168, 22.01800219,  3.85015211]), 'targetState': array([72,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7451079356610051
running average episode reward sum: 0.6905294059248818
{'scaleFactor': 20, 'currentTarget': array([72.,  6.]), 'dynamicTrap': False, 'previousTarget': array([72.,  6.]), 'currentState': array([71.01059676,  5.72219344,  4.38768672]), 'targetState': array([72,  6], dtype=int32), 'currentDistance': 1.0276649559370836}
episode index:2142
target Thresh 75.99847327013696
target distance 6.0
model initialize at round 2142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([39.38731769, 21.38970667,  3.40769863]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 5.421775528676326}
done in step count: 6
reward sum = 0.912076159401
running average episode reward sum: 0.6906327875177312
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([33.52501599, 22.33554157,  3.38042106]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 0.5815479002977249}
episode index:2143
target Thresh 75.99848088473391
target distance 56.0
model initialize at round 2143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.19563557, 15.03888354]), 'dynamicTrap': False, 'previousTarget': array([90.44395172, 15.80941823]), 'currentState': array([108.80727518,  18.96107574,   2.84818172]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5996867344430877
running average episode reward sum: 0.6905903686496927
{'scaleFactor': 20, 'currentTarget': array([54.,  8.]), 'dynamicTrap': False, 'previousTarget': array([54.,  8.]), 'currentState': array([54.77330471,  8.70614328,  4.79375629]), 'targetState': array([54,  8], dtype=int32), 'currentDistance': 1.04720508760398}
episode index:2144
target Thresh 75.9984884613529
target distance 22.0
model initialize at round 2144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.70721157, 18.59935538]), 'dynamicTrap': True, 'previousTarget': array([36.12677025, 17.73750984]), 'currentState': array([53.       ,  7.       ,  5.5522165], dtype=float32), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6915652116862567
running average episode reward sum: 0.6905908231219708
{'scaleFactor': 20, 'currentTarget': array([31., 21.]), 'dynamicTrap': False, 'previousTarget': array([31., 21.]), 'currentState': array([31.84546363, 21.27818046,  3.23006693]), 'targetState': array([31, 21], dtype=int32), 'currentDistance': 0.8900523128684694}
episode index:2145
target Thresh 75.99849600018337
target distance 44.0
model initialize at round 2145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.36021476, 15.77873466]), 'dynamicTrap': True, 'previousTarget': array([76.49734288, 16.43242207]), 'currentState': array([96.       , 12.       ,  0.8525248], dtype=float32), 'targetState': array([52, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7114791700870874
running average episode reward sum: 0.6906005567412463
{'scaleFactor': 20, 'currentTarget': array([52., 22.]), 'dynamicTrap': False, 'previousTarget': array([52., 22.]), 'currentState': array([52.41096756, 22.20960322,  4.058164  ]), 'targetState': array([52, 22], dtype=int32), 'currentDistance': 0.461332685541442}
episode index:2146
target Thresh 75.99850350141374
target distance 75.0
model initialize at round 2146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.0209659 , 14.55319459]), 'dynamicTrap': False, 'previousTarget': array([65.17544199, 13.6432744 ]), 'currentState': array([85.88983641, 12.26671595,  1.88810536]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.48144748538095455
running average episode reward sum: 0.6905031403130394
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'dynamicTrap': False, 'previousTarget': array([10., 21.]), 'currentState': array([ 9.58152115, 20.48072294,  3.54115409]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.6669131974043362}
episode index:2147
target Thresh 75.99851096523157
target distance 64.0
model initialize at round 2147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.37414661, 12.55841716]), 'dynamicTrap': False, 'previousTarget': array([55., 13.]), 'currentState': array([73.37306072, 12.3500076 ,  2.63339531]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.3716058146814785
running average episode reward sum: 0.6903546778709391
{'scaleFactor': 20, 'currentTarget': array([11., 13.]), 'dynamicTrap': False, 'previousTarget': array([11., 13.]), 'currentState': array([10.65284496, 12.17241309,  1.56657874]), 'targetState': array([11, 13], dtype=int32), 'currentDistance': 0.897450119052263}
episode index:2148
target Thresh 75.99851839182347
target distance 9.0
model initialize at round 2148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 3.73270582, 11.66103036,  1.84772116]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 8.489421353026822}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6904804300031536
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.56028726, 18.15194143,  0.94920726]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.9552751587760906}
episode index:2149
target Thresh 75.99852578137508
target distance 70.0
model initialize at round 2149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.92958648,  6.67677748]), 'dynamicTrap': True, 'previousTarget': array([62.92693298,  6.70802283]), 'currentState': array([43.        ,  5.        ,  0.75869733], dtype=float32), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5526393048189943
running average episode reward sum: 0.6904163178519052
{'scaleFactor': 20, 'currentTarget': array([113.,  11.]), 'dynamicTrap': False, 'previousTarget': array([113.,  11.]), 'currentState': array([113.10583644,  10.34477233,   0.35240985]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 0.6637203112782134}
episode index:2150
target Thresh 75.99853313407115
target distance 36.0
model initialize at round 2150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.02415818, 18.62752832]), 'dynamicTrap': False, 'previousTarget': array([50.72787848, 18.71202025]), 'currentState': array([29.23445095, 21.5201893 ,  4.13868999]), 'targetState': array([67, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.2801474566548876
running average episode reward sum: 0.6902255838392614
{'scaleFactor': 20, 'currentTarget': array([67., 16.]), 'dynamicTrap': False, 'previousTarget': array([67., 16.]), 'currentState': array([67.22060855, 16.3087358 ,  6.16687761]), 'targetState': array([67, 16], dtype=int32), 'currentDistance': 0.37945477295421126}
episode index:2151
target Thresh 75.99854045009549
target distance 42.0
model initialize at round 2151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.41357189, 10.30913773]), 'dynamicTrap': False, 'previousTarget': array([40.83483823,  9.72672794]), 'currentState': array([23.57360385,  3.59679058,  5.63567752]), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6551606583593436
running average episode reward sum: 0.6902092897289082
{'scaleFactor': 20, 'currentTarget': array([64., 18.]), 'dynamicTrap': False, 'previousTarget': array([64., 18.]), 'currentState': array([64.08734337, 18.05446854,  5.53741038]), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 0.10293534544776273}
episode index:2152
target Thresh 75.998547729631
target distance 36.0
model initialize at round 2152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.45023856,  5.00527246]), 'dynamicTrap': False, 'previousTarget': array([31.12232531,  4.79136948]), 'currentState': array([49.26040241,  7.75434652,  3.30079651]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7085824651704069
running average episode reward sum: 0.6902178234843385
{'scaleFactor': 20, 'currentTarget': array([15.,  3.]), 'dynamicTrap': False, 'previousTarget': array([15.,  3.]), 'currentState': array([14.50231651,  3.43201204,  3.05421545]), 'targetState': array([15,  3], dtype=int32), 'currentDistance': 0.6590320649369052}
episode index:2153
target Thresh 75.9985549728597
target distance 41.0
model initialize at round 2153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.72182451, 17.1624284 ]), 'dynamicTrap': False, 'previousTarget': array([27.19474814, 17.38202493]), 'currentState': array([ 9.58593284, 22.97772289,  5.25913954]), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7606193235605081
running average episode reward sum: 0.6902505075605113
{'scaleFactor': 20, 'currentTarget': array([49., 11.]), 'dynamicTrap': False, 'previousTarget': array([49., 11.]), 'currentState': array([49.09758828, 10.35015124,  5.05572439]), 'targetState': array([49, 11], dtype=int32), 'currentDistance': 0.6571353622206215}
episode index:2154
target Thresh 75.99856217996262
target distance 69.0
model initialize at round 2154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.50688333, 15.52580707]), 'dynamicTrap': True, 'previousTarget': array([76.51694593, 15.48219036]), 'currentState': array([96.      , 20.      ,  4.561214], dtype=float32), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 77
reward sum = 0.21454815820367823
running average episode reward sum: 0.6900297640109258
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'dynamicTrap': False, 'previousTarget': array([27.,  4.]), 'currentState': array([26.52559108,  3.39760052,  4.96217643]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 0.7667783011977978}
episode index:2155
target Thresh 75.99856935111997
target distance 15.0
model initialize at round 2155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.52347244,  5.26515378]), 'dynamicTrap': True, 'previousTarget': array([69.64636501,  5.37889464]), 'currentState': array([56.       , 20.       ,  1.2281237], dtype=float32), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8046805739648112
running average episode reward sum: 0.690082941566563
{'scaleFactor': 20, 'currentTarget': array([70.,  5.]), 'dynamicTrap': False, 'previousTarget': array([70.,  5.]), 'currentState': array([69.33774999,  5.53476164,  6.14971253]), 'targetState': array([70,  5], dtype=int32), 'currentDistance': 0.8512021444177745}
episode index:2156
target Thresh 75.99857648651103
target distance 14.0
model initialize at round 2156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64., 16.]), 'dynamicTrap': False, 'previousTarget': array([64., 16.]), 'currentState': array([49.7262657 ,  6.36024261,  5.52805841]), 'targetState': array([64, 16], dtype=int32), 'currentDistance': 17.223948839114705}
done in step count: 23
reward sum = 0.6708346744198214
running average episode reward sum: 0.6900740179378441
{'scaleFactor': 20, 'currentTarget': array([62.2089251, 16.7725515]), 'dynamicTrap': True, 'previousTarget': array([64., 16.]), 'currentState': array([62.9468601 , 16.40053981,  6.25857953]), 'targetState': array([64, 16], dtype=int32), 'currentDistance': 0.8264023042979641}
episode index:2157
target Thresh 75.99858358631417
target distance 30.0
model initialize at round 2157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.13235626,  9.68097054]), 'dynamicTrap': True, 'previousTarget': array([99.14985851,  9.71008489]), 'currentState': array([82.       , 20.       ,  2.3897517], dtype=float32), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5331321948242732
running average episode reward sum: 0.6900012923478934
{'scaleFactor': 20, 'currentTarget': array([112.,   2.]), 'dynamicTrap': False, 'previousTarget': array([112.,   2.]), 'currentState': array([112.22753488,   2.93458614,   4.94060192]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 0.9618853276139675}
episode index:2158
target Thresh 75.9985906507069
target distance 19.0
model initialize at round 2158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87.23313766, 16.91410718]), 'currentState': array([104.70621809,   9.01379348,   2.92926025]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 19.42394536678344}
done in step count: 17
reward sum = 0.7858703413665676
running average episode reward sum: 0.6900456967244653
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87., 17.]), 'currentState': array([86.4787828 , 17.51506625,  1.14027623]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 0.7327759589842037}
episode index:2159
target Thresh 75.99859767986582
target distance 4.0
model initialize at round 2159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 11.]), 'dynamicTrap': False, 'previousTarget': array([98., 11.]), 'currentState': array([101.55231724,  14.62233419,   2.85004532]), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 5.073486255387599}
done in step count: 6
reward sum = 0.912076159401
running average episode reward sum: 0.6901484886053341
{'scaleFactor': 20, 'currentTarget': array([98., 11.]), 'dynamicTrap': False, 'previousTarget': array([98., 11.]), 'currentState': array([98.4733491 , 10.28377145,  5.07890639]), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 0.8585119109268561}
episode index:2160
target Thresh 75.99860467396668
target distance 60.0
model initialize at round 2160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.31376119, 17.63311751]), 'dynamicTrap': False, 'previousTarget': array([29.67213115, 18.39344262]), 'currentState': array([11.60141716, 21.01298328,  6.01942301]), 'targetState': array([70, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6094612171594582
running average episode reward sum: 0.6901111506731518
{'scaleFactor': 20, 'currentTarget': array([70., 11.]), 'dynamicTrap': False, 'previousTarget': array([70., 11.]), 'currentState': array([70.33192741, 10.78264725,  0.29615842]), 'targetState': array([70, 11], dtype=int32), 'currentDistance': 0.3967593998544132}
episode index:2161
target Thresh 75.9986116331843
target distance 28.0
model initialize at round 2161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.43317977, 17.07377092]), 'dynamicTrap': False, 'previousTarget': array([64.23047895, 17.49442256]), 'currentState': array([45.50684038, 10.60896797,  6.03384686]), 'targetState': array([73, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6527599636738974
running average episode reward sum: 0.690093874453448
{'scaleFactor': 20, 'currentTarget': array([71.29087691, 20.89194216]), 'dynamicTrap': True, 'previousTarget': array([73., 20.]), 'currentState': array([71.2985564 , 21.05258265,  5.58134857]), 'targetState': array([73, 20], dtype=int32), 'currentDistance': 0.1608239542336941}
episode index:2162
target Thresh 75.99861855769268
target distance 39.0
model initialize at round 2162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.6037527 ,  8.97536053]), 'dynamicTrap': False, 'previousTarget': array([49.82389342,  9.24270493]), 'currentState': array([29.58871678, 15.17442568,  4.68089581]), 'targetState': array([70,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.704759747498441
running average episode reward sum: 0.69010065479235
{'scaleFactor': 20, 'currentTarget': array([70.,  2.]), 'dynamicTrap': False, 'previousTarget': array([70.,  2.]), 'currentState': array([69.78783845,  2.42620873,  4.74270539]), 'targetState': array([70,  2], dtype=int32), 'currentDistance': 0.47609495167583793}
episode index:2163
target Thresh 75.99862544766493
target distance 4.0
model initialize at round 2163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.,   9.]), 'dynamicTrap': False, 'previousTarget': array([100.,   9.]), 'currentState': array([101.86891726,   6.39993235,   2.02530582]), 'targetState': array([100,   9], dtype=int32), 'currentDistance': 3.2020623833164397}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6902168190689714
{'scaleFactor': 20, 'currentTarget': array([100.,   9.]), 'dynamicTrap': False, 'previousTarget': array([100.,   9.]), 'currentState': array([100.30404299,   8.39420467,   5.92340695]), 'targetState': array([100,   9], dtype=int32), 'currentDistance': 0.6778127509667738}
episode index:2164
target Thresh 75.99863230327331
target distance 61.0
model initialize at round 2164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.57277659,  7.11174129]), 'dynamicTrap': True, 'previousTarget': array([33.62388913,  6.86043721]), 'currentState': array([14.       ,  3.       ,  5.6267214], dtype=float32), 'targetState': array([75, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5290416683662701
running average episode reward sum: 0.69014237327188
{'scaleFactor': 20, 'currentTarget': array([75., 15.]), 'dynamicTrap': False, 'previousTarget': array([75., 15.]), 'currentState': array([75.25613224, 14.25746597,  4.99206561]), 'targetState': array([75, 15], dtype=int32), 'currentDistance': 0.7854683396206183}
episode index:2165
target Thresh 75.99863912468919
target distance 73.0
model initialize at round 2165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.11276585,  3.29439802]), 'dynamicTrap': False, 'previousTarget': array([96.09132042,  3.90905147]), 'currentState': array([115.99835531,   1.15820238,   3.70141172]), 'targetState': array([43,  9], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 44
reward sum = 0.6252746427102578
running average episode reward sum: 0.6901124251044924
{'scaleFactor': 20, 'currentTarget': array([43.,  9.]), 'dynamicTrap': False, 'previousTarget': array([43.,  9.]), 'currentState': array([43.65230185,  9.96148748,  2.45185309]), 'targetState': array([43,  9], dtype=int32), 'currentDistance': 1.1618760164187216}
episode index:2166
target Thresh 75.99864591208312
target distance 6.0
model initialize at round 2166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 21.]), 'dynamicTrap': False, 'previousTarget': array([46., 21.]), 'currentState': array([50.30961443, 18.62644313,  2.11588955]), 'targetState': array([46, 21], dtype=int32), 'currentDistance': 4.920015115077815}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6902197773284533
{'scaleFactor': 20, 'currentTarget': array([46., 21.]), 'dynamicTrap': False, 'previousTarget': array([46., 21.]), 'currentState': array([45.99955147, 20.77138233,  4.57809219]), 'targetState': array([46, 21], dtype=int32), 'currentDistance': 0.22861810819455572}
episode index:2167
target Thresh 75.99865266562477
target distance 35.0
model initialize at round 2167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.932711 , 15.1790846]), 'dynamicTrap': False, 'previousTarget': array([48.56953382, 15.57218647]), 'currentState': array([31.42831788, 22.76772317,  5.95283664]), 'targetState': array([65,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5937184526017067
running average episode reward sum: 0.6901752656473064
{'scaleFactor': 20, 'currentTarget': array([65.,  9.]), 'dynamicTrap': False, 'previousTarget': array([65.,  9.]), 'currentState': array([64.44048271,  8.89392604,  2.86566582]), 'targetState': array([65,  9], dtype=int32), 'currentDistance': 0.5694833503327634}
episode index:2168
target Thresh 75.99865938548301
target distance 38.0
model initialize at round 2168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.399802  , 20.77290391]), 'dynamicTrap': False, 'previousTarget': array([54.02764341, 20.94882334]), 'currentState': array([72.37762765, 21.71443597,  2.30582428]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7501380865009611
running average episode reward sum: 0.6902029110234491
{'scaleFactor': 20, 'currentTarget': array([36., 20.]), 'dynamicTrap': False, 'previousTarget': array([36., 20.]), 'currentState': array([36.80381524, 20.52161837,  4.11818342]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 0.9582299650541007}
episode index:2169
target Thresh 75.99866607182581
target distance 64.0
model initialize at round 2169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.11385036, 14.33666069]), 'dynamicTrap': False, 'previousTarget': array([71.88143384, 13.17453183]), 'currentState': array([52.18316691, 12.67297279,  0.45174479]), 'targetState': array([116,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.412358414449955
running average episode reward sum: 0.690074872084936
{'scaleFactor': 20, 'currentTarget': array([116.,  18.]), 'dynamicTrap': False, 'previousTarget': array([116.,  18.]), 'currentState': array([115.49721334,  17.08858658,   0.73358914]), 'targetState': array([116,  18], dtype=int32), 'currentDistance': 1.0408981000667428}
episode index:2170
target Thresh 75.99867272482032
target distance 73.0
model initialize at round 2170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.23548439,  1.38947769]), 'dynamicTrap': False, 'previousTarget': array([54.,  2.]), 'currentState': array([35.23687529,  1.1536096 ,  5.50508918]), 'targetState': array([107,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5410322248424974
running average episode reward sum: 0.6900062204740458
{'scaleFactor': 20, 'currentTarget': array([107.,   2.]), 'dynamicTrap': False, 'previousTarget': array([107.,   2.]), 'currentState': array([107.18794926,   1.63046872,   0.5171298 ]), 'targetState': array([107,   2], dtype=int32), 'currentDistance': 0.4145820678642231}
episode index:2171
target Thresh 75.9986793446329
target distance 44.0
model initialize at round 2171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.12249602, 16.94152252]), 'dynamicTrap': False, 'previousTarget': array([51.18035416, 17.33307718]), 'currentState': array([33.98408616, 22.74852415,  6.16824491]), 'targetState': array([76, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.674141460786106
running average episode reward sum: 0.6899989162568783
{'scaleFactor': 20, 'currentTarget': array([74.35991464,  9.61566274]), 'dynamicTrap': True, 'previousTarget': array([76., 10.]), 'currentState': array([73.9924154 , 10.03818808,  5.73251251]), 'targetState': array([76, 10], dtype=int32), 'currentDistance': 0.5599851417971813}
episode index:2172
target Thresh 75.99868593142904
target distance 19.0
model initialize at round 2172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.,  4.]), 'dynamicTrap': False, 'previousTarget': array([93.92524322,  4.43827311]), 'currentState': array([110.25691803,  13.33210947,   3.59815335]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 19.618600538975986}
done in step count: 22
reward sum = 0.7116082112329215
running average episode reward sum: 0.6900088607092373
{'scaleFactor': 20, 'currentTarget': array([93.,  4.]), 'dynamicTrap': False, 'previousTarget': array([93.,  4.]), 'currentState': array([93.02443193,  3.38440859,  2.4101601 ]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 0.616076054536371}
episode index:2173
target Thresh 75.99869248537337
target distance 30.0
model initialize at round 2173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.75257532,  9.14033682]), 'dynamicTrap': False, 'previousTarget': array([83.72787848,  9.71202025]), 'currentState': array([63.87527009, 11.35228996,  5.64683414]), 'targetState': array([94,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7564069181100938
running average episode reward sum: 0.6900394025939662
{'scaleFactor': 20, 'currentTarget': array([94.,  8.]), 'dynamicTrap': False, 'previousTarget': array([94.,  8.]), 'currentState': array([94.31319706,  7.52767169,  5.81636014]), 'targetState': array([94,  8], dtype=int32), 'currentDistance': 0.5667331166389608}
episode index:2174
target Thresh 75.99869900662978
target distance 21.0
model initialize at round 2174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92., 14.]), 'dynamicTrap': False, 'previousTarget': array([90.79898987, 13.82842712]), 'currentState': array([72.40881238, 11.39014528,  5.54334683]), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 19.76426001357583}
done in step count: 17
reward sum = 0.7964153836444163
running average episode reward sum: 0.690088311091001
{'scaleFactor': 20, 'currentTarget': array([92., 14.]), 'dynamicTrap': False, 'previousTarget': array([92., 14.]), 'currentState': array([91.86861536, 14.2577787 ,  5.95242788]), 'targetState': array([92, 14], dtype=int32), 'currentDistance': 0.28932988325164616}
episode index:2175
target Thresh 75.99870549536128
target distance 66.0
model initialize at round 2175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.24019697,  7.64363871]), 'dynamicTrap': False, 'previousTarget': array([67.99082358,  8.60578253]), 'currentState': array([49.26785402,  6.59220298,  5.79646957]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 43
reward sum = 0.6083973326053169
running average episode reward sum: 0.6900507692810351
{'scaleFactor': 20, 'currentTarget': array([114.,  10.]), 'dynamicTrap': False, 'previousTarget': array([114.,  10.]), 'currentState': array([114.15168828,   9.91946152,   0.1593002 ]), 'targetState': array([114,  10], dtype=int32), 'currentDistance': 0.17174335448427894}
episode index:2176
target Thresh 75.9987119517301
target distance 10.0
model initialize at round 2176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.14200828,  7.1404585 ]), 'dynamicTrap': True, 'previousTarget': array([93.,  7.]), 'currentState': array([103.       ,  12.       ,   3.8335755], dtype=float32), 'targetState': array([93,  7], dtype=int32), 'currentDistance': 10.99068443549489}
done in step count: 13
reward sum = 0.8113070880077723
running average episode reward sum: 0.6901064680953332
{'scaleFactor': 20, 'currentTarget': array([93.,  7.]), 'dynamicTrap': False, 'previousTarget': array([93.,  7.]), 'currentState': array([93.51030167,  6.70875569,  3.23814163]), 'targetState': array([93,  7], dtype=int32), 'currentDistance': 0.587563653446799}
episode index:2177
target Thresh 75.99871837589765
target distance 8.0
model initialize at round 2177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99., 11.]), 'dynamicTrap': False, 'previousTarget': array([99., 11.]), 'currentState': array([108.50214971,  16.49068886,   4.9833352 ]), 'targetState': array([99, 11], dtype=int32), 'currentDistance': 10.974448196153256}
done in step count: 14
reward sum = 0.8216670502526191
running average episode reward sum: 0.6901668724030271
{'scaleFactor': 20, 'currentTarget': array([99., 11.]), 'dynamicTrap': False, 'previousTarget': array([99., 11.]), 'currentState': array([98.4652905 , 11.14675131,  3.03700703]), 'targetState': array([99, 11], dtype=int32), 'currentDistance': 0.5544819185275495}
episode index:2178
target Thresh 75.99872476802452
target distance 21.0
model initialize at round 2178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.83911048,  11.57332781]), 'dynamicTrap': False, 'previousTarget': array([103.05721038,  12.40132839]), 'currentState': array([86.90680857, 20.42954303,  5.82016712]), 'targetState': array([106,  11], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 72
reward sum = 0.07228926197013474
running average episode reward sum: 0.6898833122330258
{'scaleFactor': 20, 'currentTarget': array([106.,  11.]), 'dynamicTrap': False, 'previousTarget': array([106.,  11.]), 'currentState': array([105.81318252,  10.95261636,   4.02480072]), 'targetState': array([106,  11], dtype=int32), 'currentDistance': 0.19273292361154007}
episode index:2179
target Thresh 75.99873112827053
target distance 55.0
model initialize at round 2179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.5569627 ,  9.78420242]), 'dynamicTrap': False, 'previousTarget': array([49.08213587,  8.81071492]), 'currentState': array([68.51597511,  8.50442758,  2.79093069]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.2630603703136978
running average episode reward sum: 0.6896875218926958
{'scaleFactor': 20, 'currentTarget': array([14., 12.]), 'dynamicTrap': False, 'previousTarget': array([14., 12.]), 'currentState': array([14.20513371, 11.6176933 ,  2.30103147]), 'targetState': array([14, 12], dtype=int32), 'currentDistance': 0.43386432285867543}
episode index:2180
target Thresh 75.99873745679469
target distance 17.0
model initialize at round 2180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 20.]), 'dynamicTrap': False, 'previousTarget': array([73., 20.]), 'currentState': array([89.53063491, 17.49356644,  2.88528192]), 'targetState': array([73, 20], dtype=int32), 'currentDistance': 16.719572352325113}
done in step count: 13
reward sum = 0.8394766179258879
running average episode reward sum: 0.6897562009830366
{'scaleFactor': 20, 'currentTarget': array([73., 20.]), 'dynamicTrap': False, 'previousTarget': array([73., 20.]), 'currentState': array([73.52419869, 20.05219366,  3.57348766]), 'targetState': array([73, 20], dtype=int32), 'currentDistance': 0.5267907021919876}
episode index:2181
target Thresh 75.99874375375519
target distance 27.0
model initialize at round 2181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.00011318,  1.93271703]), 'dynamicTrap': True, 'previousTarget': array([44.01370332,  2.74023321]), 'currentState': array([64.       ,  2.       ,  6.2575817], dtype=float32), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7297781724949882
running average episode reward sum: 0.6897745428581566
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([37.31146996,  3.02457968,  3.91625244]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 0.3124383084956159}
episode index:2182
target Thresh 75.99875001930948
target distance 21.0
model initialize at round 2182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.74170408, 11.13559325]), 'dynamicTrap': False, 'previousTarget': array([25.64677133, 11.25775784]), 'currentState': array([ 5.85682342, 13.27837672,  5.56224644]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6898687085527972
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'dynamicTrap': False, 'previousTarget': array([27., 11.]), 'currentState': array([26.02948297, 10.71726695,  0.38497953]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 1.0108616542042776}
episode index:2183
target Thresh 75.99875625361418
target distance 46.0
model initialize at round 2183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.29222951,  9.25853635]), 'dynamicTrap': False, 'previousTarget': array([87.11006407,  8.5704125 ]), 'currentState': array([104.19507664,   2.72536177,   2.88958946]), 'targetState': array([60, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5651949918715525
running average episode reward sum: 0.6898116235176868
{'scaleFactor': 20, 'currentTarget': array([61.58162027, 17.25386465]), 'dynamicTrap': True, 'previousTarget': array([60., 18.]), 'currentState': array([61.86792397, 17.70727212,  3.45739986]), 'targetState': array([60, 18], dtype=int32), 'currentDistance': 0.5362351492760012}
episode index:2184
target Thresh 75.99876245682516
target distance 64.0
model initialize at round 2184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.43789381, 15.8623986 ]), 'dynamicTrap': False, 'previousTarget': array([54., 17.]), 'currentState': array([73.43103862, 15.33879519,  3.52892613]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6558915415393506
running average episode reward sum: 0.6897960994527081
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([ 9.99847217, 16.02553662,  3.02856503]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.9744645815878347}
episode index:2185
target Thresh 75.9987686290975
target distance 19.0
model initialize at round 2185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.,  4.]), 'dynamicTrap': False, 'previousTarget': array([81.,  4.]), 'currentState': array([99.83249221,  5.10918613,  2.60259324]), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 18.8651280594388}
done in step count: 12
reward sum = 0.8680222522970136
running average episode reward sum: 0.6898776301722161
{'scaleFactor': 20, 'currentTarget': array([81.,  4.]), 'dynamicTrap': False, 'previousTarget': array([81.,  4.]), 'currentState': array([80.60279926,  3.08657889,  2.12846952]), 'targetState': array([81,  4], dtype=int32), 'currentDistance': 0.9960454612455498}
episode index:2186
target Thresh 75.9987747705855
target distance 23.0
model initialize at round 2186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.52183524,  2.19834186]), 'dynamicTrap': False, 'previousTarget': array([23.07518824,  2.26740767]), 'currentState': array([41.35410809,  4.78309593,  3.44430888]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7890135129164108
running average episode reward sum: 0.6899229597939556
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'dynamicTrap': False, 'previousTarget': array([20.,  2.]), 'currentState': array([20.02995727,  2.32961246,  3.51988433]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.3309710101627377}
episode index:2187
target Thresh 75.99878088144268
target distance 30.0
model initialize at round 2187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.63506907,  9.13773271]), 'dynamicTrap': False, 'previousTarget': array([88.88854382, 10.05572809]), 'currentState': array([71.41077615, 17.37637018,  5.9701879 ]), 'targetState': array([101,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6662667029207643
running average episode reward sum: 0.6899121479763718
{'scaleFactor': 20, 'currentTarget': array([101.,   4.]), 'dynamicTrap': False, 'previousTarget': array([101.,   4.]), 'currentState': array([101.69952495,   4.33044109,   6.09027179]), 'targetState': array([101,   4], dtype=int32), 'currentDistance': 0.7736449281897876}
episode index:2188
target Thresh 75.99878696182186
target distance 40.0
model initialize at round 2188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.07898668, 16.77190185]), 'dynamicTrap': False, 'previousTarget': array([68.0992562 , 16.00992562]), 'currentState': array([86.87119152, 19.64742403,  2.26653504]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6968058252241526
running average episode reward sum: 0.6899152972122091
{'scaleFactor': 20, 'currentTarget': array([48., 14.]), 'dynamicTrap': False, 'previousTarget': array([48., 14.]), 'currentState': array([47.65590248, 14.08765225,  4.72395583]), 'targetState': array([48, 14], dtype=int32), 'currentDistance': 0.35508593296416985}
episode index:2189
target Thresh 75.99879301187502
target distance 68.0
model initialize at round 2189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.06763494, 12.92041355]), 'dynamicTrap': False, 'previousTarget': array([67.89486598, 13.95199909]), 'currentState': array([47.13151814, 14.51767586,  5.09174693]), 'targetState': array([116,   9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 45
reward sum = 0.6361854860638709
running average episode reward sum: 0.6898907630518674
{'scaleFactor': 20, 'currentTarget': array([116.,   9.]), 'dynamicTrap': False, 'previousTarget': array([116.,   9.]), 'currentState': array([116.70348129,   8.35273725,   5.83500164]), 'targetState': array([116,   9], dtype=int32), 'currentDistance': 0.9559471704340075}
episode index:2190
target Thresh 75.9987990317534
target distance 46.0
model initialize at round 2190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.64745807, 14.34886514]), 'dynamicTrap': False, 'previousTarget': array([70.51477147, 15.36479691]), 'currentState': array([89.378255  , 21.36008804,  3.92908001]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.677974622540954
running average episode reward sum: 0.6898853243752306
{'scaleFactor': 20, 'currentTarget': array([44.84406359,  4.37216014]), 'dynamicTrap': True, 'previousTarget': array([43.,  4.]), 'currentState': array([44.0398821 ,  4.18665376,  2.7444011 ]), 'targetState': array([43,  4], dtype=int32), 'currentDistance': 0.8253002360647045}
episode index:2191
target Thresh 75.99880502160752
target distance 49.0
model initialize at round 2191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.76459961, 10.98310694]), 'dynamicTrap': False, 'previousTarget': array([83.99583637, 11.40807829]), 'currentState': array([65.7785257 , 10.2368833 ,  5.43079166]), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6952828039523279
running average episode reward sum: 0.6898877867290523
{'scaleFactor': 20, 'currentTarget': array([113.,  12.]), 'dynamicTrap': False, 'previousTarget': array([113.,  12.]), 'currentState': array([112.68245942,  11.72124754,   5.7484151 ]), 'targetState': array([113,  12], dtype=int32), 'currentDistance': 0.4225339690906599}
episode index:2192
target Thresh 75.99881098158711
target distance 16.0
model initialize at round 2192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.27185356,  5.39181446]), 'dynamicTrap': False, 'previousTarget': array([31.59074408,  5.32117742]), 'currentState': array([15.69543904, 17.93674613,  4.83543015]), 'targetState': array([33,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6851753549786952
running average episode reward sum: 0.6898856378773649
{'scaleFactor': 20, 'currentTarget': array([33.,  4.]), 'dynamicTrap': False, 'previousTarget': array([33.,  4.]), 'currentState': array([32.54339689,  4.0901477 ,  0.47243579]), 'targetState': array([33,  4], dtype=int32), 'currentDistance': 0.4654170233030904}
episode index:2193
target Thresh 75.9988169118412
target distance 16.0
model initialize at round 2193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30., 18.]), 'dynamicTrap': False, 'previousTarget': array([30., 18.]), 'currentState': array([44.91077915, 23.79998334,  3.21295059]), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 15.999098150363789}
done in step count: 12
reward sum = 0.8578542096231192
running average episode reward sum: 0.6899621960231014
{'scaleFactor': 20, 'currentTarget': array([30., 18.]), 'dynamicTrap': False, 'previousTarget': array([30., 18.]), 'currentState': array([30.10032491, 17.88619968,  3.05347869]), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 0.15170893589014792}
episode index:2194
target Thresh 75.998822812518
target distance 20.0
model initialize at round 2194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.2520651, 19.6668704]), 'dynamicTrap': False, 'previousTarget': array([99.11145618, 19.94427191]), 'currentState': array([116.46267839,   9.47889757,   3.36282635]), 'targetState': array([97, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6900476442358466
{'scaleFactor': 20, 'currentTarget': array([97., 21.]), 'dynamicTrap': False, 'previousTarget': array([97., 21.]), 'currentState': array([97.90827991, 21.35406678,  3.26740321]), 'targetState': array([97, 21], dtype=int32), 'currentDistance': 0.9748516175174246}
episode index:2195
target Thresh 75.99882868376507
target distance 64.0
model initialize at round 2195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.33660668, 14.0297064 ]), 'dynamicTrap': False, 'previousTarget': array([53.06075717, 13.44224665]), 'currentState': array([71.24662046, 15.92479238,  2.88918731]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.6900537462626061
{'scaleFactor': 20, 'currentTarget': array([ 9., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 10.]), 'currentState': array([9.68617676, 9.91934632, 4.60276071]), 'targetState': array([ 9, 10], dtype=int32), 'currentDistance': 0.6909005468320311}
episode index:2196
target Thresh 75.99883452572917
target distance 35.0
model initialize at round 2196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.3218065 , 20.97393229]), 'dynamicTrap': False, 'previousTarget': array([77.92693298, 20.29197717]), 'currentState': array([58.5002402 , 23.63954846,  0.31810951]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.690111940705635
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([93.39563705, 19.54399745,  6.24989637]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 0.6726528837907849}
episode index:2197
target Thresh 75.99884033855633
target distance 15.0
model initialize at round 2197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.,  5.]), 'dynamicTrap': False, 'previousTarget': array([74.,  5.]), 'currentState': array([71.38355879, 19.8912477 ,  4.21877027]), 'targetState': array([74,  5], dtype=int32), 'currentDistance': 15.119359198149315}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.690213580972595
{'scaleFactor': 20, 'currentTarget': array([74.,  5.]), 'dynamicTrap': False, 'previousTarget': array([74.,  5.]), 'currentState': array([73.7354566 ,  4.62701448,  4.21978654]), 'targetState': array([74,  5], dtype=int32), 'currentDistance': 0.4572760700117943}
episode index:2198
target Thresh 75.99884612239191
target distance 66.0
model initialize at round 2198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.57179006, 10.74812157]), 'dynamicTrap': True, 'previousTarget': array([60.56299789, 10.71200051]), 'currentState': array([80.       ,  6.       ,  5.4580765], dtype=float32), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.15223368901603518
running average episode reward sum: 0.689968933454652
{'scaleFactor': 20, 'currentTarget': array([14., 22.]), 'dynamicTrap': False, 'previousTarget': array([14., 22.]), 'currentState': array([14.43581541, 21.00396846,  2.74761319]), 'targetState': array([14, 22], dtype=int32), 'currentDistance': 1.087204629717535}
episode index:2199
target Thresh 75.99885187738049
target distance 61.0
model initialize at round 2199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.44177638, 13.97572867]), 'dynamicTrap': False, 'previousTarget': array([48.04286102, 14.30866485]), 'currentState': array([66.38324097, 12.4466799 ,  3.1737448 ]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6135478584388203
running average episode reward sum: 0.6899341966023721
{'scaleFactor': 20, 'currentTarget': array([ 7., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 17.]), 'currentState': array([ 7.57573896, 16.42887009,  3.94106313]), 'targetState': array([ 7, 17], dtype=int32), 'currentDistance': 0.8109653063680087}
episode index:2200
target Thresh 75.99885760366594
target distance 15.0
model initialize at round 2200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'dynamicTrap': False, 'previousTarget': array([3., 7.]), 'currentState': array([ 8.95863095, 23.32209628,  3.24797058]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 17.375733360582267}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6900275196635516
{'scaleFactor': 20, 'currentTarget': array([3., 7.]), 'dynamicTrap': False, 'previousTarget': array([3., 7.]), 'currentState': array([3.57003812, 7.71601389, 5.12178044]), 'targetState': array([3, 7], dtype=int32), 'currentDistance': 0.9152154658191461}
episode index:2201
target Thresh 75.99886330139144
target distance 12.0
model initialize at round 2201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  2.]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([91.55073512, 14.67348246,  5.69588095]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 12.681443004187475}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6901374369334169
{'scaleFactor': 20, 'currentTarget': array([92.,  2.]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([91.85347756,  2.3615465 ,  2.86391558]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 0.3901085738758352}
episode index:2202
target Thresh 75.99886897069939
target distance 26.0
model initialize at round 2202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.94168187,  9.41910484]), 'dynamicTrap': False, 'previousTarget': array([27.64012894,  9.77694787]), 'currentState': array([8.58969875, 4.36927817, 5.98051524]), 'targetState': array([34, 11], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 28
reward sum = 0.6236814993902107
running average episode reward sum: 0.6901072708246819
{'scaleFactor': 20, 'currentTarget': array([33.10961582,  9.5707199 ]), 'dynamicTrap': True, 'previousTarget': array([34., 11.]), 'currentState': array([32.40179445,  9.64954658,  2.00208385]), 'targetState': array([34, 11], dtype=int32), 'currentDistance': 0.7121971212158518}
episode index:2203
target Thresh 75.99887461173157
target distance 32.0
model initialize at round 2203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.35253552,  5.65658831]), 'dynamicTrap': False, 'previousTarget': array([75.34255626,  5.6857707 ]), 'currentState': array([95.0020652 ,  1.92885666,  3.73140955]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6078927610640201
running average episode reward sum: 0.6900699684155346
{'scaleFactor': 20, 'currentTarget': array([63.,  8.]), 'dynamicTrap': False, 'previousTarget': array([65.25177188,  7.57073473]), 'currentState': array([63.8848962 ,  8.36831942,  3.60298975]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 0.9584886405929867}
episode index:2204
target Thresh 75.99888022462898
target distance 41.0
model initialize at round 2204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.62072371, 12.70146257]), 'dynamicTrap': True, 'previousTarget': array([32.78245704, 13.12836937]), 'currentState': array([14.        , 20.        ,  0.22882745], dtype=float32), 'targetState': array([55,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6656169714855962
running average episode reward sum: 0.6900588786210086
{'scaleFactor': 20, 'currentTarget': array([53.55397577,  5.31231055]), 'dynamicTrap': True, 'previousTarget': array([55.,  5.]), 'currentState': array([52.66183113,  6.27302036,  0.6928342 ]), 'targetState': array([55,  5], dtype=int32), 'currentDistance': 1.311062701183724}
episode index:2205
target Thresh 75.99888580953194
target distance 1.0
model initialize at round 2205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67., 16.]), 'dynamicTrap': False, 'previousTarget': array([67., 16.]), 'currentState': array([66.16644134, 16.3385647 ,  1.7448824 ]), 'targetState': array([67, 16], dtype=int32), 'currentDistance': 0.899692220215969}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6901993777694124
{'scaleFactor': 20, 'currentTarget': array([67., 16.]), 'dynamicTrap': False, 'previousTarget': array([67., 16.]), 'currentState': array([66.16644134, 16.3385647 ,  1.7448824 ]), 'targetState': array([67, 16], dtype=int32), 'currentDistance': 0.899692220215969}
episode index:2206
target Thresh 75.99889136658007
target distance 60.0
model initialize at round 2206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.15879163, 19.88868814]), 'dynamicTrap': False, 'previousTarget': array([31.97504678, 19.00124766]), 'currentState': array([12.21115506, 21.3349922 ,  0.40392637]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4615944522193245
running average episode reward sum: 0.6900957960179172
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([72.16488324, 17.58300483,  5.36112655]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 0.6058721909197449}
episode index:2207
target Thresh 75.99889689591232
target distance 16.0
model initialize at round 2207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.06742002,   6.87609148]), 'dynamicTrap': False, 'previousTarget': array([107.52228  ,   6.3881475]), 'currentState': array([92.49070431, 20.56986229,  0.31747448]), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7879859517105771
running average episode reward sum: 0.6901401303275606
{'scaleFactor': 20, 'currentTarget': array([108.,   6.]), 'dynamicTrap': False, 'previousTarget': array([108.,   6.]), 'currentState': array([107.62735902,   5.37435392,   2.7012886 ]), 'targetState': array([108,   6], dtype=int32), 'currentDistance': 0.7282131004631961}
episode index:2208
target Thresh 75.99890239766691
target distance 29.0
model initialize at round 2208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.48074881,  15.05261477]), 'dynamicTrap': False, 'previousTarget': array([105.81242258,  15.26725206]), 'currentState': array([87.6738848 , 17.82536395,  5.1692316 ]), 'targetState': array([115,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7883173082635068
running average episode reward sum: 0.6901845745004606
{'scaleFactor': 20, 'currentTarget': array([115.,  14.]), 'dynamicTrap': False, 'previousTarget': array([115.,  14.]), 'currentState': array([115.7533831 ,  14.43106036,   6.02221436]), 'targetState': array([115,  14], dtype=int32), 'currentDistance': 0.8679856735384572}
episode index:2209
target Thresh 75.99890787198139
target distance 5.0
model initialize at round 2209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 23.]), 'dynamicTrap': False, 'previousTarget': array([93., 23.]), 'currentState': array([92.03933703, 19.99306206,  1.69271648]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 3.1566674050799826}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6903157579509128
{'scaleFactor': 20, 'currentTarget': array([93., 23.]), 'dynamicTrap': False, 'previousTarget': array([93., 23.]), 'currentState': array([93.03726801, 23.80665913,  1.15311017]), 'targetState': array([93, 23], dtype=int32), 'currentDistance': 0.80751956660718}
episode index:2210
target Thresh 75.9989133189926
target distance 55.0
model initialize at round 2210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.06724661,  9.97080538]), 'dynamicTrap': False, 'previousTarget': array([67.45968477, 10.7366585 ]), 'currentState': array([85.66126234, 13.98011296,  2.78262043]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5892912621251917
running average episode reward sum: 0.6902700661843701
{'scaleFactor': 20, 'currentTarget': array([32.,  3.]), 'dynamicTrap': False, 'previousTarget': array([32.,  3.]), 'currentState': array([31.80223952,  2.88443761,  2.7682976 ]), 'targetState': array([32,  3], dtype=int32), 'currentDistance': 0.22904993753847042}
episode index:2211
target Thresh 75.99891873883674
target distance 12.0
model initialize at round 2211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([13.21075434, 21.51356824,  5.23427606]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 15.363777161245006}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6903668618483956
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([26.86118531, 13.67159249,  4.66000591]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.0920973505137845}
episode index:2212
target Thresh 75.99892413164929
target distance 73.0
model initialize at round 2212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.58156701, 13.9303276 ]), 'dynamicTrap': True, 'previousTarget': array([38.59069837, 13.97451403]), 'currentState': array([19.       , 18.       ,  1.3203154], dtype=float32), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.4677545397587246
running average episode reward sum: 0.6902662688424807
{'scaleFactor': 20, 'currentTarget': array([92.,  3.]), 'dynamicTrap': False, 'previousTarget': array([92.,  3.]), 'currentState': array([91.79145336,  3.43371589,  5.93776784]), 'targetState': array([92,  3], dtype=int32), 'currentDistance': 0.4812495912615408}
episode index:2213
target Thresh 75.99892949756507
target distance 25.0
model initialize at round 2213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.24686539, 15.44731219]), 'dynamicTrap': False, 'previousTarget': array([21.74433602, 16.22705473]), 'currentState': array([4.00042896, 5.32009981, 5.72264433]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.7650334398314456
running average episode reward sum: 0.6903000390190792
{'scaleFactor': 20, 'currentTarget': array([29., 20.]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([29.3659173 , 19.20152539,  2.01540303]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 0.8783263473057425}
episode index:2214
target Thresh 75.99893483671825
target distance 12.0
model initialize at round 2214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 10.]), 'dynamicTrap': False, 'previousTarget': array([72., 10.]), 'currentState': array([79.19996294, 20.64668387,  3.16848898]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 12.852678465067273}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6904049801727626
{'scaleFactor': 20, 'currentTarget': array([72., 10.]), 'dynamicTrap': False, 'previousTarget': array([72., 10.]), 'currentState': array([71.45752431,  9.69039022,  4.0327336 ]), 'targetState': array([72, 10], dtype=int32), 'currentDistance': 0.6246103475664873}
episode index:2215
target Thresh 75.99894014924227
target distance 28.0
model initialize at round 2215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.53664925, 11.53175239]), 'dynamicTrap': False, 'previousTarget': array([88.44395172, 11.80941823]), 'currentState': array([106.00915731,  16.0948029 ,   3.23941964]), 'targetState': array([80, 10], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 25
reward sum = 0.7153715051479685
running average episode reward sum: 0.6904162466551522
{'scaleFactor': 20, 'currentTarget': array([80., 10.]), 'dynamicTrap': False, 'previousTarget': array([80., 10.]), 'currentState': array([79.78304173,  9.33100381,  1.03838113]), 'targetState': array([80, 10], dtype=int32), 'currentDistance': 0.7032970919628474}
episode index:2216
target Thresh 75.99894543526999
target distance 74.0
model initialize at round 2216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.95834452,  8.29014879]), 'dynamicTrap': True, 'previousTarget': array([33.98358488,  7.81014533]), 'currentState': array([14.      ,  7.      ,  5.138483], dtype=float32), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.527031280710959
running average episode reward sum: 0.6903425502338874
{'scaleFactor': 20, 'currentTarget': array([88., 10.]), 'dynamicTrap': False, 'previousTarget': array([88., 10.]), 'currentState': array([87.43226121,  9.12625616,  0.55717972]), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 1.0419959874023474}
episode index:2217
target Thresh 75.99895069493353
target distance 4.0
model initialize at round 2217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.19013302,  4.94385241]), 'dynamicTrap': True, 'previousTarget': array([95.,  5.]), 'currentState': array([97.       ,  9.       ,  2.7597256], dtype=float32), 'targetState': array([95,  5], dtype=int32), 'currentDistance': 4.441615892323082}
done in step count: 10
reward sum = 0.8369277694367245
running average episode reward sum: 0.690408639151472
{'scaleFactor': 20, 'currentTarget': array([95.,  5.]), 'dynamicTrap': False, 'previousTarget': array([95.,  5.]), 'currentState': array([94.37828557,  4.7224072 ,  3.17070244]), 'targetState': array([95,  5], dtype=int32), 'currentDistance': 0.6808719334190723}
episode index:2218
target Thresh 75.99895592836437
target distance 69.0
model initialize at round 2218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.8778787 , 17.66398381]), 'dynamicTrap': False, 'previousTarget': array([23.99160369, 17.57946677]), 'currentState': array([ 5.88591233, 17.09716742,  5.88043082]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5516317670348063
running average episode reward sum: 0.6903460988756196
{'scaleFactor': 20, 'currentTarget': array([73., 19.]), 'dynamicTrap': False, 'previousTarget': array([73., 19.]), 'currentState': array([7.33128577e+01, 1.90118723e+01, 2.58345574e-02]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 0.31308291051429044}
episode index:2219
target Thresh 75.99896113569338
target distance 16.0
model initialize at round 2219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.,  4.]), 'dynamicTrap': False, 'previousTarget': array([68.,  4.]), 'currentState': array([76.44123172, 19.92411668,  4.20023584]), 'targetState': array([68,  4], dtype=int32), 'currentDistance': 18.023093101340752}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6904384376843506
{'scaleFactor': 20, 'currentTarget': array([68.,  4.]), 'dynamicTrap': False, 'previousTarget': array([68.,  4.]), 'currentState': array([67.12427739,  3.5433226 ,  4.14702864]), 'targetState': array([68,  4], dtype=int32), 'currentDistance': 0.9876458560255844}
episode index:2220
target Thresh 75.99896631705072
target distance 32.0
model initialize at round 2220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.12538792,  7.73534052]), 'dynamicTrap': False, 'previousTarget': array([52.00975848,  7.62469505]), 'currentState': array([70.11855937,  7.21275489,  3.28210729]), 'targetState': array([40,  8], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 42
reward sum = 0.4579271851203539
running average episode reward sum: 0.6903337500424939
{'scaleFactor': 20, 'currentTarget': array([40.,  8.]), 'dynamicTrap': False, 'previousTarget': array([40.,  8.]), 'currentState': array([39.88152083,  7.90777304,  5.35336824]), 'targetState': array([40,  8], dtype=int32), 'currentDistance': 0.15014368489828867}
episode index:2221
target Thresh 75.99897147256594
target distance 17.0
model initialize at round 2221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'dynamicTrap': False, 'previousTarget': array([23.,  3.]), 'currentState': array([7.03611325, 9.52051021, 5.98442996]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 17.24420870180161}
done in step count: 15
reward sum = 0.8510145338912004
running average episode reward sum: 0.6904060636265842
{'scaleFactor': 20, 'currentTarget': array([23.,  3.]), 'dynamicTrap': False, 'previousTarget': array([23.,  3.]), 'currentState': array([23.00633251,  3.90489366,  2.75412533]), 'targetState': array([23,  3], dtype=int32), 'currentDistance': 0.9049158203279455}
episode index:2222
target Thresh 75.99897660236792
target distance 28.0
model initialize at round 2222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.77756281,  22.91024532]), 'dynamicTrap': False, 'previousTarget': array([108.98725709,  22.71383061]), 'currentState': array([90.77964311, 22.62178812,  6.02118254]), 'targetState': array([117,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6904746813187826
{'scaleFactor': 20, 'currentTarget': array([117.,  23.]), 'dynamicTrap': False, 'previousTarget': array([117.,  23.]), 'currentState': array([117.56651758,  23.08670953,   6.1040985 ]), 'targetState': array([117,  23], dtype=int32), 'currentDistance': 0.5731149225435194}
episode index:2223
target Thresh 75.9989817065849
target distance 7.0
model initialize at round 2223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.71719235, 22.42605469]), 'dynamicTrap': True, 'previousTarget': array([15., 22.]), 'currentState': array([22.       , 15.       ,  3.0123856], dtype=float32), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 9.11341565684415}
done in step count: 9
reward sum = 0.8743082973836408
running average episode reward sum: 0.6905573403188118
{'scaleFactor': 20, 'currentTarget': array([15., 22.]), 'dynamicTrap': False, 'previousTarget': array([15., 22.]), 'currentState': array([14.4164837 , 21.8650253 ,  1.25128521]), 'targetState': array([15, 22], dtype=int32), 'currentDistance': 0.5989235673316033}
episode index:2224
target Thresh 75.9989867853445
target distance 48.0
model initialize at round 2224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.37334476,  2.20344548]), 'dynamicTrap': False, 'previousTarget': array([28.,  3.]), 'currentState': array([9.38228824, 1.60539942, 5.82815617]), 'targetState': array([56,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6200940815311379
running average episode reward sum: 0.6905256714384578
{'scaleFactor': 20, 'currentTarget': array([56.,  3.]), 'dynamicTrap': False, 'previousTarget': array([56.,  3.]), 'currentState': array([55.93345935,  2.40176162,  5.70594029]), 'targetState': array([56,  3], dtype=int32), 'currentDistance': 0.6019275805358341}
episode index:2225
target Thresh 75.99899183877368
target distance 72.0
model initialize at round 2225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.60864711, 14.69553419]), 'dynamicTrap': False, 'previousTarget': array([62.4762588 , 14.66139084]), 'currentState': array([80.09506833, 19.19879833,  2.5251013 ]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.554371160250575
running average episode reward sum: 0.6904645058898559
{'scaleFactor': 20, 'currentTarget': array([10.,  3.]), 'dynamicTrap': False, 'previousTarget': array([10.,  3.]), 'currentState': array([10.20711089,  2.32146717,  2.96031562]), 'targetState': array([10,  3], dtype=int32), 'currentDistance': 0.7094376106412257}
episode index:2226
target Thresh 75.99899686699877
target distance 19.0
model initialize at round 2226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89., 10.]), 'dynamicTrap': False, 'previousTarget': array([89.23313766, 10.08589282]), 'currentState': array([106.45862149,  17.67567771,   3.73846114]), 'targetState': array([89, 10], dtype=int32), 'currentDistance': 19.071431316257616}
done in step count: 16
reward sum = 0.7950223528585987
running average episode reward sum: 0.6905114559783015
{'scaleFactor': 20, 'currentTarget': array([89., 10.]), 'dynamicTrap': False, 'previousTarget': array([89., 10.]), 'currentState': array([89.30795508,  9.7197319 ,  2.29426275]), 'targetState': array([89, 10], dtype=int32), 'currentDistance': 0.4163970926770023}
episode index:2227
target Thresh 75.99900187014548
target distance 39.0
model initialize at round 2227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.57659004, 13.99784351]), 'dynamicTrap': False, 'previousTarget': array([26.62672268, 12.96750701]), 'currentState': array([45.12799929,  9.78567113,  2.22310185]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6766457349104844
running average episode reward sum: 0.6905052325846446
{'scaleFactor': 20, 'currentTarget': array([ 7., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 18.]), 'currentState': array([ 7.63669107, 18.71983628,  3.83498884]), 'targetState': array([ 7, 18], dtype=int32), 'currentDistance': 0.9610097818635609}
episode index:2228
target Thresh 75.99900684833891
target distance 70.0
model initialize at round 2228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.14471108, 13.79276125]), 'dynamicTrap': False, 'previousTarget': array([29.66377472, 12.65184388]), 'currentState': array([ 9.40018005, 10.60630644,  0.97246623]), 'targetState': array([80, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4180616325674369
running average episode reward sum: 0.6903830057564628
{'scaleFactor': 20, 'currentTarget': array([78.1093546 , 21.35590282]), 'dynamicTrap': True, 'previousTarget': array([80., 22.]), 'currentState': array([77.90048829, 21.18745897,  0.41666157]), 'targetState': array([80, 22], dtype=int32), 'currentDistance': 0.26832529859703547}
episode index:2229
target Thresh 75.99901180170347
target distance 53.0
model initialize at round 2229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.72140884, 16.67343518]), 'dynamicTrap': True, 'previousTarget': array([51.71773129, 16.65170601]), 'currentState': array([32.      , 20.      ,  5.767728], dtype=float32), 'targetState': array([85, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.43406982866940397
running average episode reward sum: 0.6898787668172583
{'scaleFactor': 20, 'currentTarget': array([73.37681952,  8.60552851]), 'dynamicTrap': True, 'previousTarget': array([73.41965122,  8.4102    ]), 'currentState': array([72.60643136, 18.32405791,  5.71654886]), 'targetState': array([85, 11], dtype=int32), 'currentDistance': 9.749015931259308}
episode index:2230
target Thresh 75.99901673036304
target distance 10.0
model initialize at round 2230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  13.]), 'dynamicTrap': False, 'previousTarget': array([107.,  13.]), 'currentState': array([116.91655852,   9.66723411,   2.60980439]), 'targetState': array([107,  13], dtype=int32), 'currentDistance': 10.461618482825136}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6899749135264432
{'scaleFactor': 20, 'currentTarget': array([107.,  13.]), 'dynamicTrap': False, 'previousTarget': array([107.,  13.]), 'currentState': array([107.00340354,  12.26388773,   2.43715071]), 'targetState': array([107,  13], dtype=int32), 'currentDistance': 0.7361201431669171}
episode index:2231
target Thresh 75.99902163444081
target distance 26.0
model initialize at round 2231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.35882538, 11.02343229]), 'dynamicTrap': False, 'previousTarget': array([ 9.51217609, 10.49719013]), 'currentState': array([28.03477714,  7.43777724,  3.08739555]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7790898613192275
running average episode reward sum: 0.6900148395783218
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.10407563, 11.60658572,  2.56165762]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.4069478229943616}
episode index:2232
target Thresh 75.9990265140594
target distance 38.0
model initialize at round 2232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.19466926, 10.52711196]), 'dynamicTrap': False, 'previousTarget': array([39.5672925, 10.23886  ]), 'currentState': array([56.30447497, 19.01445178,  2.87487507]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.635728047313582
running average episode reward sum: 0.6899905284308677
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'dynamicTrap': False, 'previousTarget': array([20.,  2.]), 'currentState': array([20.77908248,  1.49395018,  2.35239013]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.9290080394047996}
episode index:2233
target Thresh 75.99903136934078
target distance 24.0
model initialize at round 2233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.83056499, 17.98049036]), 'dynamicTrap': False, 'previousTarget': array([24.18129646, 17.33309421]), 'currentState': array([ 6.20166117, 10.70285733,  0.49740648]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6900705442251104
{'scaleFactor': 20, 'currentTarget': array([30., 20.]), 'dynamicTrap': False, 'previousTarget': array([30., 20.]), 'currentState': array([29.48240585, 20.1046331 ,  0.17984353]), 'targetState': array([30, 20], dtype=int32), 'currentDistance': 0.5280641930127568}
episode index:2234
target Thresh 75.99903620040635
target distance 46.0
model initialize at round 2234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.42996744, 10.26419902]), 'dynamicTrap': False, 'previousTarget': array([30.45647272,  9.24859289]), 'currentState': array([50.11643197,  6.73671915,  2.4130969 ]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.4968300423401984
running average episode reward sum: 0.6899840831504416
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 15.]), 'currentState': array([ 4.80472116, 15.24440125,  2.68844796]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.8410161222088113}
episode index:2235
target Thresh 75.99904100737687
target distance 18.0
model initialize at round 2235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.,  5.]), 'dynamicTrap': False, 'previousTarget': array([66.,  5.]), 'currentState': array([47.23159815,  4.4662265 ,  5.14504623]), 'targetState': array([66,  5], dtype=int32), 'currentDistance': 18.775990573363977}
done in step count: 15
reward sum = 0.8329259788737768
running average episode reward sum: 0.6900480106530013
{'scaleFactor': 20, 'currentTarget': array([66.,  5.]), 'dynamicTrap': False, 'previousTarget': array([66.,  5.]), 'currentState': array([66.24989854,  4.87345932,  0.92672681]), 'targetState': array([66,  5], dtype=int32), 'currentDistance': 0.2801103778569829}
episode index:2236
target Thresh 75.99904579037253
target distance 68.0
model initialize at round 2236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.64246386, 16.4979806 ]), 'dynamicTrap': False, 'previousTarget': array([81.25665203, 15.80622312]), 'currentState': array([99.31855904, 20.08284858,  2.55722398]), 'targetState': array([33,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.556170245703671
running average episode reward sum: 0.6899881636414011
{'scaleFactor': 20, 'currentTarget': array([33.,  8.]), 'dynamicTrap': False, 'previousTarget': array([33.,  8.]), 'currentState': array([33.39495169,  7.58590292,  4.00519239]), 'targetState': array([33,  8], dtype=int32), 'currentDistance': 0.5722440298141785}
episode index:2237
target Thresh 75.99905054951292
target distance 18.0
model initialize at round 2237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 20.]), 'currentState': array([12.07714268,  3.98909408,  1.36220127]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 16.303923310421123}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6900719584847245
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 20.]), 'currentState': array([ 9.68547986, 20.47700081,  3.37939571]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.8351122103673759}
episode index:2238
target Thresh 75.99905528491698
target distance 27.0
model initialize at round 2238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.70500333, 14.38069617]), 'dynamicTrap': False, 'previousTarget': array([11.4781528 , 13.54593775]), 'currentState': array([28.33399786,  7.10329536,  2.63078728]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 22
reward sum = 0.7370142247821119
running average episode reward sum: 0.6900929242133075
{'scaleFactor': 20, 'currentTarget': array([ 3., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 17.]), 'currentState': array([ 2.61563664, 17.62761136,  3.16649852]), 'targetState': array([ 3, 17], dtype=int32), 'currentDistance': 0.7359559858349268}
episode index:2239
target Thresh 75.9990599967031
target distance 48.0
model initialize at round 2239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.40013298, 22.86350203]), 'dynamicTrap': False, 'previousTarget': array([61.00433887, 22.41657627]), 'currentState': array([79.39986566, 22.76009638,  1.96741509]), 'targetState': array([33, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6659106250490399
running average episode reward sum: 0.6900821285440377
{'scaleFactor': 20, 'currentTarget': array([33., 23.]), 'dynamicTrap': False, 'previousTarget': array([33., 23.]), 'currentState': array([33.62714871, 23.76621598,  3.11366329]), 'targetState': array([33, 23], dtype=int32), 'currentDistance': 0.9901527326495574}
episode index:2240
target Thresh 75.99906468498911
target distance 22.0
model initialize at round 2240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.50680565, 15.47021642]), 'dynamicTrap': False, 'previousTarget': array([10.9414844 , 15.06407315]), 'currentState': array([28.37457717,  8.83642429,  2.26223588]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8137938100725264
running average episode reward sum: 0.6901373323287449
{'scaleFactor': 20, 'currentTarget': array([ 8., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 16.]), 'currentState': array([ 7.41015159, 15.0785104 ,  0.89949999]), 'targetState': array([ 8, 16], dtype=int32), 'currentDistance': 1.0941043033663074}
episode index:2241
target Thresh 75.9990693498922
target distance 47.0
model initialize at round 2241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.68769308, 12.44302242]), 'dynamicTrap': False, 'previousTarget': array([84.43788142, 11.83784711]), 'currentState': array([103.12914358,  17.13663536,   3.23456645]), 'targetState': array([57,  6], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 48
reward sum = 0.49239841409110785
running average episode reward sum: 0.6900491347737772
{'scaleFactor': 20, 'currentTarget': array([57.,  6.]), 'dynamicTrap': False, 'previousTarget': array([57.,  6.]), 'currentState': array([57.28204762,  6.23163724,  0.69583595]), 'targetState': array([57,  6], dtype=int32), 'currentDistance': 0.3649748900945863}
episode index:2242
target Thresh 75.99907399152896
target distance 39.0
model initialize at round 2242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.60928395,  8.27610463]), 'dynamicTrap': False, 'previousTarget': array([76.97366596,  8.32455532]), 'currentState': array([59.82537218,  1.40845157,  0.18810671]), 'targetState': array([97, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7775808621250063
running average episode reward sum: 0.6900881591729529
{'scaleFactor': 20, 'currentTarget': array([95.27706346, 14.33745721]), 'dynamicTrap': True, 'previousTarget': array([97., 15.]), 'currentState': array([95.04082637, 14.9346155 ,  0.60100213]), 'targetState': array([97, 15], dtype=int32), 'currentDistance': 0.6421884399765397}
episode index:2243
target Thresh 75.99907861001549
target distance 74.0
model initialize at round 2243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.92321842, 11.62182275]), 'dynamicTrap': False, 'previousTarget': array([88.14629717, 10.41463953]), 'currentState': array([107.82447842,   9.63691704,   2.63174719]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.41085854053436804
running average episode reward sum: 0.6899637252965541
{'scaleFactor': 20, 'currentTarget': array([34., 17.]), 'dynamicTrap': False, 'previousTarget': array([34., 17.]), 'currentState': array([34.50902675, 17.04110175,  2.72925026]), 'targetState': array([34, 17], dtype=int32), 'currentDistance': 0.5106834532896154}
episode index:2244
target Thresh 75.9990832054672
target distance 68.0
model initialize at round 2244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.73231774,  6.34664214]), 'dynamicTrap': False, 'previousTarget': array([51.89486598,  6.95199909]), 'currentState': array([33.81999543,  8.2173159 ,  5.63655037]), 'targetState': array([100,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6103801634545039
running average episode reward sum: 0.6899282760485176
{'scaleFactor': 20, 'currentTarget': array([100.,   2.]), 'dynamicTrap': False, 'previousTarget': array([100.,   2.]), 'currentState': array([99.72347874,  1.14794678,  5.07280783]), 'targetState': array([100,   2], dtype=int32), 'currentDistance': 0.8958005908175238}
episode index:2245
target Thresh 75.99908777799901
target distance 35.0
model initialize at round 2245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.48348098,  7.2196763 ]), 'dynamicTrap': False, 'previousTarget': array([96.49717013,  6.45649603]), 'currentState': array([76.79702244,  3.692163  ,  0.71645701]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8010404100935686
running average episode reward sum: 0.6899777471678608
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([112.54423191,   9.79628777,   1.779013  ]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 0.5811084568573401}
episode index:2246
target Thresh 75.99909232772522
target distance 8.0
model initialize at round 2246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7.82605956, 2.09803868]), 'dynamicTrap': True, 'previousTarget': array([8., 2.]), 'currentState': array([16.      ,  5.      ,  5.868935], dtype=float32), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 8.673792812083436}
done in step count: 16
reward sum = 0.75583984610368
running average episode reward sum: 0.690007058293333
{'scaleFactor': 20, 'currentTarget': array([8., 2.]), 'dynamicTrap': False, 'previousTarget': array([8., 2.]), 'currentState': array([7.33389901, 2.12965635, 3.85324337]), 'targetState': array([8, 2], dtype=int32), 'currentDistance': 0.6786024597766199}
episode index:2247
target Thresh 75.99909685475957
target distance 19.0
model initialize at round 2247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.73761541, 10.60604009]), 'dynamicTrap': False, 'previousTarget': array([30.49385478, 10.70632169]), 'currentState': array([47.98818948, 20.72620284,  4.3621877 ]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6900904719787002
{'scaleFactor': 20, 'currentTarget': array([28.,  9.]), 'dynamicTrap': False, 'previousTarget': array([28.,  9.]), 'currentState': array([28.79573373,  9.76940636,  3.31027362]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 1.1068777302267732}
episode index:2248
target Thresh 75.99910135921526
target distance 18.0
model initialize at round 2248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.,  4.]), 'dynamicTrap': False, 'previousTarget': array([30.,  4.]), 'currentState': array([31.34852381, 22.02776339,  4.08526683]), 'targetState': array([30,  4], dtype=int32), 'currentDistance': 18.078129585092114}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6901898169211212
{'scaleFactor': 20, 'currentTarget': array([30.,  4.]), 'dynamicTrap': False, 'previousTarget': array([30.,  4.]), 'currentState': array([30.44198596,  4.68323991,  5.77348071]), 'targetState': array([30,  4], dtype=int32), 'currentDistance': 0.8137372830270211}
episode index:2249
target Thresh 75.99910584120487
target distance 14.0
model initialize at round 2249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  21.]), 'dynamicTrap': False, 'previousTarget': array([118.,  21.]), 'currentState': array([113.5495668 ,   8.62493737,   0.29020226]), 'targetState': array([118,  21], dtype=int32), 'currentDistance': 13.15098972062627}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.690297317157115
{'scaleFactor': 20, 'currentTarget': array([118.,  21.]), 'dynamicTrap': False, 'previousTarget': array([118.,  21.]), 'currentState': array([118.84053767,  20.71105843,   1.0115693 ]), 'targetState': array([118,  21], dtype=int32), 'currentDistance': 0.88881426674931}
episode index:2250
target Thresh 75.99911030084047
target distance 28.0
model initialize at round 2250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.21679862,  16.41636817]), 'dynamicTrap': False, 'previousTarget': array([107.0957503 ,  15.37956268]), 'currentState': array([90.84676104,  6.50265068,  0.11988604]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5726528056071578
running average episode reward sum: 0.6902450539356356
{'scaleFactor': 20, 'currentTarget': array([118.,  22.]), 'dynamicTrap': False, 'previousTarget': array([118.,  22.]), 'currentState': array([1.17261401e+02, 2.16877825e+01, 9.79859908e-02]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 0.8018780325830782}
episode index:2251
target Thresh 75.99911473823353
target distance 27.0
model initialize at round 2251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.67846811,  6.83488574]), 'dynamicTrap': True, 'previousTarget': array([17.82403775,  6.31823341]), 'currentState': array([37.       , 12.       ,  2.9695876], dtype=float32), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7698341387800695
running average episode reward sum: 0.6902803954475558
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'dynamicTrap': False, 'previousTarget': array([10.,  4.]), 'currentState': array([10.17444152,  3.94402154,  2.23575842]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.1832032507330962}
episode index:2252
target Thresh 75.99911915349502
target distance 45.0
model initialize at round 2252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.58205694, 16.59711201]), 'dynamicTrap': False, 'previousTarget': array([94.30874984, 16.49933331]), 'currentState': array([112.21253581,  20.42389906,   3.3072511 ]), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.727586911436121
running average episode reward sum: 0.690296954043201
{'scaleFactor': 20, 'currentTarget': array([69., 12.]), 'dynamicTrap': False, 'previousTarget': array([69., 12.]), 'currentState': array([68.17200856, 12.04699142,  3.79481526]), 'targetState': array([69, 12], dtype=int32), 'currentDistance': 0.8293238301542158}
episode index:2253
target Thresh 75.99912354673529
target distance 14.0
model initialize at round 2253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.,  6.]), 'dynamicTrap': False, 'previousTarget': array([65.,  6.]), 'currentState': array([52.47080439, 19.17816454,  4.76677323]), 'targetState': array([65,  6], dtype=int32), 'currentDistance': 18.1836399889536}
done in step count: 13
reward sum = 0.8593310112749681
running average episode reward sum: 0.6903719469701006
{'scaleFactor': 20, 'currentTarget': array([65.55328526,  7.42219781]), 'dynamicTrap': True, 'previousTarget': array([65.,  6.]), 'currentState': array([65.35918994,  7.69427134,  4.25537611]), 'targetState': array([65,  6], dtype=int32), 'currentDistance': 0.33421100912398516}
episode index:2254
target Thresh 75.99912791806418
target distance 36.0
model initialize at round 2254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.13687277,  9.50502469]), 'dynamicTrap': False, 'previousTarget': array([90.12703142,  9.84437071]), 'currentState': array([72.23858532,  2.95867218,  6.11433947]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7027286289453665
running average episode reward sum: 0.6903774266516861
{'scaleFactor': 20, 'currentTarget': array([107.,  15.]), 'dynamicTrap': False, 'previousTarget': array([107.,  15.]), 'currentState': array([106.65446876,  14.38475107,   0.92231942]), 'targetState': array([107,  15], dtype=int32), 'currentDistance': 0.7056366543167804}
episode index:2255
target Thresh 75.99913226759098
target distance 52.0
model initialize at round 2255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.57678812, 11.76852601]), 'dynamicTrap': True, 'previousTarget': array([77.59715  , 11.8507125]), 'currentState': array([97.       ,  7.       ,  2.0875502], dtype=float32), 'targetState': array([45, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.594822766319405
running average episode reward sum: 0.6903350708625317
{'scaleFactor': 20, 'currentTarget': array([45., 20.]), 'dynamicTrap': False, 'previousTarget': array([45., 20.]), 'currentState': array([44.61049345, 20.83153653,  3.859849  ]), 'targetState': array([45, 20], dtype=int32), 'currentDistance': 0.918241992189094}
episode index:2256
target Thresh 75.99913659542442
target distance 52.0
model initialize at round 2256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.39165163, 14.62960914]), 'dynamicTrap': False, 'previousTarget': array([75.59715  , 13.8507125]), 'currentState': array([95.9215628 , 10.31885865,  1.97090453]), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6587453648704877
running average episode reward sum: 0.6903210745373248
{'scaleFactor': 20, 'currentTarget': array([41.98684247, 20.55883071]), 'dynamicTrap': True, 'previousTarget': array([43., 22.]), 'currentState': array([41.45023753, 20.40470276,  0.10519629]), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 0.5583012562343437}
episode index:2257
target Thresh 75.9991409016727
target distance 34.0
model initialize at round 2257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.11898745, 13.96109351]), 'dynamicTrap': False, 'previousTarget': array([27.90362596, 14.48405927]), 'currentState': array([44.05725895, 22.80521109,  2.85879385]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.734483164988378
running average episode reward sum: 0.6903406325933262
{'scaleFactor': 20, 'currentTarget': array([12.,  7.]), 'dynamicTrap': False, 'previousTarget': array([12.,  7.]), 'currentState': array([12.01235078,  7.10950866,  3.18307001]), 'targetState': array([12,  7], dtype=int32), 'currentDistance': 0.1102029465094955}
episode index:2258
target Thresh 75.99914518644349
target distance 4.0
model initialize at round 2258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.,  7.]), 'dynamicTrap': False, 'previousTarget': array([31.,  7.]), 'currentState': array([29.45375913,  2.24467047,  2.71654332]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 5.000401960612127}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6904645628135151
{'scaleFactor': 20, 'currentTarget': array([31.,  7.]), 'dynamicTrap': False, 'previousTarget': array([31.,  7.]), 'currentState': array([30.49766327,  7.08488779,  6.21288282]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 0.5094586611219455}
episode index:2259
target Thresh 75.99914944984388
target distance 62.0
model initialize at round 2259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.95464586, 19.47347446]), 'dynamicTrap': False, 'previousTarget': array([64.9352791, 19.6076838]), 'currentState': array([45.02462587, 17.80185757,  4.93703861]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5935120781746048
running average episode reward sum: 0.6904216634840289
{'scaleFactor': 20, 'currentTarget': array([107.,  23.]), 'dynamicTrap': False, 'previousTarget': array([107.,  23.]), 'currentState': array([107.66354948,  22.839645  ,   6.27845527]), 'targetState': array([107,  23], dtype=int32), 'currentDistance': 0.6826504528355682}
episode index:2260
target Thresh 75.9991536919805
target distance 44.0
model initialize at round 2260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.45542706, 19.1410669 ]), 'dynamicTrap': False, 'previousTarget': array([51.91786413, 18.81071492]), 'currentState': array([33.52307207, 17.49752658,  5.60244424]), 'targetState': array([76, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6874511903294608
running average episode reward sum: 0.6904203496966983
{'scaleFactor': 20, 'currentTarget': array([76., 21.]), 'dynamicTrap': False, 'previousTarget': array([76., 21.]), 'currentState': array([76.55200091, 20.7812026 ,  1.50417503]), 'targetState': array([76, 21], dtype=int32), 'currentDistance': 0.5937822067035836}
episode index:2261
target Thresh 75.99915791295935
target distance 42.0
model initialize at round 2261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.79339341, 11.64184708]), 'dynamicTrap': False, 'previousTarget': array([31.16516177, 10.72672794]), 'currentState': array([49.8262368 ,  5.49767321,  2.50476074]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.506470821782803
running average episode reward sum: 0.6903390280663207
{'scaleFactor': 20, 'currentTarget': array([ 8., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 19.]), 'currentState': array([ 7.98670955, 19.09628765,  0.89653598]), 'targetState': array([ 8, 19], dtype=int32), 'currentDistance': 0.0972005562251755}
episode index:2262
target Thresh 75.99916211288598
target distance 18.0
model initialize at round 2262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.89402755,  3.84050863]), 'dynamicTrap': True, 'previousTarget': array([40.,  4.]), 'currentState': array([22.       , 10.       ,  1.0833368], dtype=float32), 'targetState': array([40,  4], dtype=int32), 'currentDistance': 18.92446976536177}
done in step count: 19
reward sum = 0.7893075718257502
running average episode reward sum: 0.6903827614042612
{'scaleFactor': 20, 'currentTarget': array([40.,  4.]), 'dynamicTrap': False, 'previousTarget': array([40.,  4.]), 'currentState': array([40.74259282,  3.312025  ,  2.92462098]), 'targetState': array([40,  4], dtype=int32), 'currentDistance': 1.0123011862504474}
episode index:2263
target Thresh 75.99916629186541
target distance 22.0
model initialize at round 2263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.28131504, 10.12558531]), 'dynamicTrap': False, 'previousTarget': array([34.47545314, 10.73326351]), 'currentState': array([49.43464997, 21.91836999,  3.65081227]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7546325905363878
running average episode reward sum: 0.6904111403040546
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([28.34630952,  6.81325247,  2.35546111]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 0.6798425449528559}
episode index:2264
target Thresh 75.99917045000207
target distance 46.0
model initialize at round 2264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.77229555,  8.19209257]), 'dynamicTrap': False, 'previousTarget': array([53.07518824,  7.26740767]), 'currentState': array([73.63163278, 10.55994491,  2.19606686]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.539944886276924
running average episode reward sum: 0.6903447092868241
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([26.57667926,  4.89387346,  2.68918278]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.4364209984302315}
episode index:2265
target Thresh 75.99917458739995
target distance 62.0
model initialize at round 2265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.72535419, 15.6681782 ]), 'dynamicTrap': False, 'previousTarget': array([66.98960229, 14.64482588]), 'currentState': array([46.72597026, 15.5111992 ,  0.76743817]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5401341652630829
running average episode reward sum: 0.6902784204324447
{'scaleFactor': 20, 'currentTarget': array([109.,  16.]), 'dynamicTrap': False, 'previousTarget': array([109.,  16.]), 'currentState': array([108.91355167,  15.74053093,   5.34936208]), 'targetState': array([109,  16], dtype=int32), 'currentDistance': 0.27349133912670553}
episode index:2266
target Thresh 75.99917870416246
target distance 37.0
model initialize at round 2266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.75622235,  9.7041784 ]), 'dynamicTrap': False, 'previousTarget': array([66.81984861,  9.67835792]), 'currentState': array([48.97926023,  6.7256269 ,  6.07094003]), 'targetState': array([84, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8173934136055969
running average episode reward sum: 0.6903344923306243
{'scaleFactor': 20, 'currentTarget': array([84., 12.]), 'dynamicTrap': False, 'previousTarget': array([84., 12.]), 'currentState': array([83.26668583, 11.46629843,  0.13762788]), 'targetState': array([84, 12], dtype=int32), 'currentDistance': 0.906965839140721}
episode index:2267
target Thresh 75.99918280039255
target distance 63.0
model initialize at round 2267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.99676808, 18.13779188]), 'dynamicTrap': False, 'previousTarget': array([86., 17.]), 'currentState': array([104.98943215,  18.67944049,   2.22354892]), 'targetState': array([43, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6458696863741662
running average episode reward sum: 0.6903148870369927
{'scaleFactor': 20, 'currentTarget': array([44.23649354, 16.01300503]), 'dynamicTrap': True, 'previousTarget': array([43., 17.]), 'currentState': array([44.31257545, 16.82502851,  2.98677199]), 'targetState': array([43, 17], dtype=int32), 'currentDistance': 0.8155799115517667}
episode index:2268
target Thresh 75.99918687619258
target distance 33.0
model initialize at round 2268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.90160712,  4.39249876]), 'dynamicTrap': False, 'previousTarget': array([22.6773982 ,  4.42229124]), 'currentState': array([4.28156292, 8.27242992, 5.55453688]), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.776990682679529
running average episode reward sum: 0.6903530870350724
{'scaleFactor': 20, 'currentTarget': array([36.,  2.]), 'dynamicTrap': False, 'previousTarget': array([36.,  2.]), 'currentState': array([35.16650827,  1.63304596,  0.29874412]), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 0.9106940953366645}
episode index:2269
target Thresh 75.99919093166449
target distance 11.0
model initialize at round 2269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.,  8.]), 'dynamicTrap': False, 'previousTarget': array([50.,  8.]), 'currentState': array([61.07970594, 11.82214119,  5.04392548]), 'targetState': array([50,  8], dtype=int32), 'currentDistance': 11.720437147196801}
done in step count: 14
reward sum = 0.7922630602526192
running average episode reward sum: 0.6903979812964017
{'scaleFactor': 20, 'currentTarget': array([50.,  8.]), 'dynamicTrap': False, 'previousTarget': array([50.,  8.]), 'currentState': array([49.42700817,  8.60115048,  3.18675621]), 'targetState': array([50,  8], dtype=int32), 'currentDistance': 0.8304827125734853}
episode index:2270
target Thresh 75.99919496690966
target distance 8.0
model initialize at round 2270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.,  8.]), 'dynamicTrap': False, 'previousTarget': array([56.,  8.]), 'currentState': array([49.31894122, 15.47983104,  4.31548357]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 10.029178369950438}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6905127290148533
{'scaleFactor': 20, 'currentTarget': array([56.,  8.]), 'dynamicTrap': False, 'previousTarget': array([56.,  8.]), 'currentState': array([55.24955744,  8.54742865,  5.42928492]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 0.9288929761621894}
episode index:2271
target Thresh 75.99919898202894
target distance 52.0
model initialize at round 2271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.12348155, 15.78098796]), 'dynamicTrap': True, 'previousTarget': array([76.13182128, 15.70751784]), 'currentState': array([96.      , 18.      ,  5.171259], dtype=float32), 'targetState': array([44, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.514879363258516
running average episode reward sum: 0.6904354255968268
{'scaleFactor': 20, 'currentTarget': array([44., 12.]), 'dynamicTrap': False, 'previousTarget': array([44., 12.]), 'currentState': array([4.42095138e+01, 1.24761136e+01, 2.18640007e-03]), 'targetState': array([44, 12], dtype=int32), 'currentDistance': 0.5201731987752939}
episode index:2272
target Thresh 75.99920297712275
target distance 54.0
model initialize at round 2272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.00487727, 16.5583362 ]), 'dynamicTrap': True, 'previousTarget': array([56.00342847, 16.62969312]), 'currentState': array([76.       , 17.       ,  1.5742081], dtype=float32), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.43213651336754294
running average episode reward sum: 0.690321787711992
{'scaleFactor': 20, 'currentTarget': array([22., 16.]), 'dynamicTrap': False, 'previousTarget': array([22., 16.]), 'currentState': array([22.24633171, 16.78934106,  3.35642137]), 'targetState': array([22, 16], dtype=int32), 'currentDistance': 0.8268848897111025}
episode index:2273
target Thresh 75.99920695229092
target distance 65.0
model initialize at round 2273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.23840036, 19.17230946]), 'dynamicTrap': False, 'previousTarget': array([96.11497719, 18.85853601]), 'currentState': array([114.09682128,  21.5478344 ,   3.10624212]), 'targetState': array([51, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.669552166039737
running average episode reward sum: 0.6903126541932267
{'scaleFactor': 20, 'currentTarget': array([51., 14.]), 'dynamicTrap': False, 'previousTarget': array([51., 14.]), 'currentState': array([51.03026955, 14.22355128,  3.65165704]), 'targetState': array([51, 14], dtype=int32), 'currentDistance': 0.22559126800106064}
episode index:2274
target Thresh 75.99921090763287
target distance 54.0
model initialize at round 2274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.0346917,  3.8225178]), 'dynamicTrap': True, 'previousTarget': array([45.03079294,  3.89059961]), 'currentState': array([65.       ,  5.       ,  1.1436962], dtype=float32), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5443515848293047
running average episode reward sum: 0.6902484954814184
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'dynamicTrap': False, 'previousTarget': array([11.,  2.]), 'currentState': array([11.65784213,  1.80737914,  3.85648078]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.6854626693511555}
episode index:2275
target Thresh 75.99921484324747
target distance 31.0
model initialize at round 2275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.48855215, 20.46068418]), 'dynamicTrap': False, 'previousTarget': array([48.09299973, 19.9264839 ]), 'currentState': array([68.46655134, 19.52284179,  2.28350681]), 'targetState': array([37, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8258263033223977
running average episode reward sum: 0.6903080639382906
{'scaleFactor': 20, 'currentTarget': array([37., 21.]), 'dynamicTrap': False, 'previousTarget': array([37., 21.]), 'currentState': array([37.53465286, 20.4506643 ,  3.83520074]), 'targetState': array([37, 21], dtype=int32), 'currentDistance': 0.7665659744575801}
episode index:2276
target Thresh 75.99921875923312
target distance 32.0
model initialize at round 2276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.11688207, 21.10278322]), 'dynamicTrap': False, 'previousTarget': array([66.00975848, 20.62469505]), 'currentState': array([87.11626808, 21.25949739,  1.85561293]), 'targetState': array([54, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.615842097052729
running average episode reward sum: 0.6902753603955213
{'scaleFactor': 20, 'currentTarget': array([54., 21.]), 'dynamicTrap': False, 'previousTarget': array([54., 21.]), 'currentState': array([54.6524905 , 20.50603212,  4.59320657]), 'targetState': array([54, 21], dtype=int32), 'currentDistance': 0.8183813978619646}
episode index:2277
target Thresh 75.99922265568769
target distance 34.0
model initialize at round 2277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.91335058, 20.85969589]), 'dynamicTrap': True, 'previousTarget': array([43.922597  , 20.75787621]), 'currentState': array([24.     , 19.     ,  4.19904], dtype=float32), 'targetState': array([58, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6375748802680329
running average episode reward sum: 0.690252225856396
{'scaleFactor': 20, 'currentTarget': array([58., 22.]), 'dynamicTrap': False, 'previousTarget': array([58., 22.]), 'currentState': array([58.51503813, 21.77925793,  0.88147714]), 'targetState': array([58, 22], dtype=int32), 'currentDistance': 0.5603493005506944}
episode index:2278
target Thresh 75.99922653270863
target distance 10.0
model initialize at round 2278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  10.]), 'dynamicTrap': False, 'previousTarget': array([117.,  10.]), 'currentState': array([108.9471704 ,  14.85058513,   5.82554895]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 9.400863821963638}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6903666347304827
{'scaleFactor': 20, 'currentTarget': array([117.,  10.]), 'dynamicTrap': False, 'previousTarget': array([117.,  10.]), 'currentState': array([117.13981066,  10.62632294,   5.16703103]), 'targetState': array([117,  10], dtype=int32), 'currentDistance': 0.6417378321925736}
episode index:2279
target Thresh 75.99923039039284
target distance 28.0
model initialize at round 2279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.5786138 ,  8.39464639]), 'dynamicTrap': False, 'previousTarget': array([72.85982065,  8.57777387]), 'currentState': array([89.37887299, 17.51335096,  3.70499158]), 'targetState': array([63,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.773385234066966
running average episode reward sum: 0.6904030463968583
{'scaleFactor': 20, 'currentTarget': array([64.83386093,  4.72573226]), 'dynamicTrap': True, 'previousTarget': array([63.,  4.]), 'currentState': array([65.41265892,  5.14536283,  3.23211559]), 'targetState': array([63,  4], dtype=int32), 'currentDistance': 0.7149104271039862}
episode index:2280
target Thresh 75.99923422883677
target distance 48.0
model initialize at round 2280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.6053669 ,  8.98151036]), 'dynamicTrap': False, 'previousTarget': array([37.79065962,  8.88613786]), 'currentState': array([19.83320356,  5.97126834,  5.54428947]), 'targetState': array([66, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6681458855717091
running average episode reward sum: 0.6903932887638793
{'scaleFactor': 20, 'currentTarget': array([66., 13.]), 'dynamicTrap': False, 'previousTarget': array([66., 13.]), 'currentState': array([66.86094413, 12.89800094,  1.60039487]), 'targetState': array([66, 13], dtype=int32), 'currentDistance': 0.8669651643598514}
episode index:2281
target Thresh 75.99923804813638
target distance 4.0
model initialize at round 2281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  14.]), 'dynamicTrap': False, 'previousTarget': array([106.,  14.]), 'currentState': array([108.7889338 ,  11.28002871,   2.02844817]), 'targetState': array([106,  14], dtype=int32), 'currentDistance': 3.8956893570534357}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6905202417486453
{'scaleFactor': 20, 'currentTarget': array([106.,  14.]), 'dynamicTrap': False, 'previousTarget': array([106.,  14.]), 'currentState': array([106.873153  ,  14.7093991 ,   1.81939918]), 'targetState': array([106,  14], dtype=int32), 'currentDistance': 1.125008109400824}
episode index:2282
target Thresh 75.99924184838716
target distance 25.0
model initialize at round 2282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.64233966,   2.28278757]), 'dynamicTrap': False, 'previousTarget': array([109.93630557,   2.40509555]), 'currentState': array([91.71289732,  3.96127732,  5.32392121]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8420429696008656
running average episode reward sum: 0.690586611756465
{'scaleFactor': 20, 'currentTarget': array([115.,   2.]), 'dynamicTrap': False, 'previousTarget': array([115.,   2.]), 'currentState': array([115.74479455,   2.51379314,   5.56863283]), 'targetState': array([115,   2], dtype=int32), 'currentDistance': 0.9048216992029091}
episode index:2283
target Thresh 75.9992456296841
target distance 4.0
model initialize at round 2283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'dynamicTrap': False, 'previousTarget': array([11.,  8.]), 'currentState': array([10.8813563 ,  5.60888913,  1.82548183]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 2.394052528667271}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6907177034325787
{'scaleFactor': 20, 'currentTarget': array([11.,  8.]), 'dynamicTrap': False, 'previousTarget': array([11.,  8.]), 'currentState': array([10.88862938,  7.58625281,  1.30353504]), 'targetState': array([11,  8], dtype=int32), 'currentDistance': 0.42847420938612396}
episode index:2284
target Thresh 75.99924939212174
target distance 75.0
model initialize at round 2284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.08694281, 17.86283474]), 'dynamicTrap': True, 'previousTarget': array([57.08654609, 17.85858903]), 'currentState': array([77.      , 16.      ,  3.194353], dtype=float32), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 69
reward sum = 0.29515077267034306
running average episode reward sum: 0.690544588802048
{'scaleFactor': 20, 'currentTarget': array([ 2., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 23.]), 'currentState': array([ 2.12387311, 22.52268696,  2.34562912]), 'targetState': array([ 2, 23], dtype=int32), 'currentDistance': 0.49312502195969304}
episode index:2285
target Thresh 75.99925313579415
target distance 19.0
model initialize at round 2285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.03418911, 14.9945178 ]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([107.78192467,  11.82798339,   0.97636825]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7792107414805874
running average episode reward sum: 0.6905833753955207
{'scaleFactor': 20, 'currentTarget': array([88., 15.]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([87.39805009, 14.52679356,  1.81792977]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 0.7656814106549944}
episode index:2286
target Thresh 75.99925686079492
target distance 17.0
model initialize at round 2286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55., 14.]), 'dynamicTrap': False, 'previousTarget': array([55., 14.]), 'currentState': array([71.27939272,  5.75817209,  2.51690173]), 'targetState': array([55, 14], dtype=int32), 'currentDistance': 18.24681765615685}
done in step count: 24
reward sum = 0.707464689634001
running average episode reward sum: 0.6905907568184496
{'scaleFactor': 20, 'currentTarget': array([55., 14.]), 'dynamicTrap': False, 'previousTarget': array([55., 14.]), 'currentState': array([55.24704855, 14.89433712,  1.04192344]), 'targetState': array([55, 14], dtype=int32), 'currentDistance': 0.9278318113511532}
episode index:2287
target Thresh 75.99926056721716
target distance 33.0
model initialize at round 2287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.60817842, 16.57398142]), 'dynamicTrap': False, 'previousTarget': array([66.4353175 , 15.84991583]), 'currentState': array([87.03520928, 21.32692496,  1.91827267]), 'targetState': array([53, 13], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 52
reward sum = 0.3198292529884285
running average episode reward sum: 0.6904287107066358
{'scaleFactor': 20, 'currentTarget': array([53., 13.]), 'dynamicTrap': False, 'previousTarget': array([53., 13.]), 'currentState': array([53.11870481, 13.3762816 ,  3.85858571]), 'targetState': array([53, 13], dtype=int32), 'currentDistance': 0.3945613720234596}
episode index:2288
target Thresh 75.99926425515355
target distance 8.0
model initialize at round 2288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.,  2.]), 'dynamicTrap': False, 'previousTarget': array([40.,  2.]), 'currentState': array([45.30606179,  8.33497121,  3.87392753]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 8.263543542930027}
done in step count: 10
reward sum = 0.8749780850088044
running average episode reward sum: 0.6905093351602409
{'scaleFactor': 20, 'currentTarget': array([40.,  2.]), 'dynamicTrap': False, 'previousTarget': array([40.,  2.]), 'currentState': array([39.57137426,  2.34993925,  2.11338706]), 'targetState': array([40,  2], dtype=int32), 'currentDistance': 0.5533330889532792}
episode index:2289
target Thresh 75.99926792469628
target distance 62.0
model initialize at round 2289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.28004944, 10.94148556]), 'dynamicTrap': False, 'previousTarget': array([49.50882004, 11.59478257]), 'currentState': array([28.68564243, 14.94888048,  4.82616353]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.48897941587479754
running average episode reward sum: 0.6904213308286753
{'scaleFactor': 20, 'currentTarget': array([92.,  2.]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([91.41960095,  1.16553328,  5.88019795]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 1.0164633569637684}
episode index:2290
target Thresh 75.9992715759371
target distance 27.0
model initialize at round 2290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.261326  , 14.48826636]), 'dynamicTrap': False, 'previousTarget': array([65.35993796, 15.01924318]), 'currentState': array([46.23823595,  8.31396113,  5.82907772]), 'targetState': array([73, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7652728685705755
running average episode reward sum: 0.6904540028224517
{'scaleFactor': 20, 'currentTarget': array([73., 17.]), 'dynamicTrap': False, 'previousTarget': array([73., 17.]), 'currentState': array([72.54841504, 16.09131309,  1.62710207]), 'targetState': array([73, 17], dtype=int32), 'currentDistance': 1.01471220858149}
episode index:2291
target Thresh 75.99927520896728
target distance 30.0
model initialize at round 2291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.8233925 , 16.67958845]), 'dynamicTrap': True, 'previousTarget': array([42.8434743 , 16.74695771]), 'currentState': array([62.        , 11.        ,  0.19706593], dtype=float32), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7129506015977958
running average episode reward sum: 0.6904638180924235
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([32.70370081, 19.80079495,  3.801614  ]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 0.7313531886118193}
episode index:2292
target Thresh 75.99927882387763
target distance 38.0
model initialize at round 2292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.82435587,  9.8702822 ]), 'dynamicTrap': False, 'previousTarget': array([55.60310443,  9.65666931]), 'currentState': array([38.47109622, 17.81747416,  1.5172016 ]), 'targetState': array([75,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6671098120295043
running average episode reward sum: 0.690453633179182
{'scaleFactor': 20, 'currentTarget': array([75.,  2.]), 'dynamicTrap': False, 'previousTarget': array([75.,  2.]), 'currentState': array([74.06125728,  1.36602928,  6.21603336]), 'targetState': array([75,  2], dtype=int32), 'currentDistance': 1.1327650976275405}
episode index:2293
target Thresh 75.99928242075853
target distance 7.0
model initialize at round 2293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 8.90757888, 19.29249905,  2.97896278]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 5.914815624376481}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6905127940295117
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.51169155, 18.7321988 ,  4.30085346]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.5775341759258398}
episode index:2294
target Thresh 75.99928599969994
target distance 54.0
model initialize at round 2294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.842564  , 14.50452665]), 'dynamicTrap': True, 'previousTarget': array([35.83405013, 14.57108057]), 'currentState': array([16.       , 12.       ,  2.2108855], dtype=float32), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5745253277005266
running average episode reward sum: 0.6904622548284969
{'scaleFactor': 20, 'currentTarget': array([70., 19.]), 'dynamicTrap': False, 'previousTarget': array([70., 19.]), 'currentState': array([69.5980243 , 18.30836856,  0.85717172]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 0.7999615659616423}
episode index:2295
target Thresh 75.9992895607913
target distance 16.0
model initialize at round 2295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([41.08533247, 17.80149187,  0.17170614]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 15.253778607367039}
done in step count: 16
reward sum = 0.788771431308244
running average episode reward sum: 0.6905050724140718
{'scaleFactor': 20, 'currentTarget': array([56., 21.]), 'dynamicTrap': False, 'previousTarget': array([56., 21.]), 'currentState': array([56.63754557, 20.78940635,  2.13870417]), 'targetState': array([56, 21], dtype=int32), 'currentDistance': 0.6714268674518088}
episode index:2296
target Thresh 75.99929310412163
target distance 4.0
model initialize at round 2296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 18.]), 'dynamicTrap': False, 'previousTarget': array([33., 18.]), 'currentState': array([31.19035193, 14.880467  ,  1.46252125]), 'targetState': array([33, 18], dtype=int32), 'currentDistance': 3.606426525561412}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6905903487742381
{'scaleFactor': 20, 'currentTarget': array([33., 18.]), 'dynamicTrap': False, 'previousTarget': array([33., 18.]), 'currentState': array([32.34079041, 17.97412771,  4.24324611]), 'targetState': array([33, 18], dtype=int32), 'currentDistance': 0.6597171053754367}
episode index:2297
target Thresh 75.99929662977954
target distance 20.0
model initialize at round 2297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.29945381, 19.11054251]), 'dynamicTrap': False, 'previousTarget': array([89.28991511, 19.14985851]), 'currentState': array([79.15521666,  1.87411445,  5.66987885]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6906716937151539
{'scaleFactor': 20, 'currentTarget': array([91., 22.]), 'dynamicTrap': False, 'previousTarget': array([91., 22.]), 'currentState': array([91.13566477, 21.63572161,  1.67135519]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 0.3887205625779624}
episode index:2298
target Thresh 75.99930013785314
target distance 40.0
model initialize at round 2298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.6733592 , 10.11630287]), 'dynamicTrap': False, 'previousTarget': array([57.2384301 , 11.20729355]), 'currentState': array([38.69565976,  1.35260942,  5.48051012]), 'targetState': array([79, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5807972459173026
running average episode reward sum: 0.6906239014368599
{'scaleFactor': 20, 'currentTarget': array([79., 21.]), 'dynamicTrap': False, 'previousTarget': array([79., 21.]), 'currentState': array([79.83092167, 21.71737621,  1.7838747 ]), 'targetState': array([79, 21], dtype=int32), 'currentDistance': 1.0977520039633277}
episode index:2299
target Thresh 75.99930362843016
target distance 74.0
model initialize at round 2299
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.85155163,  6.43226192]), 'dynamicTrap': True, 'previousTarget': array([58.85370283,  6.41463953]), 'currentState': array([39.       ,  4.       ,  6.0761557], dtype=float32), 'targetState': array([113,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.492778859295275
running average episode reward sum: 0.6905378818533201
{'scaleFactor': 20, 'currentTarget': array([113.,  13.]), 'dynamicTrap': False, 'previousTarget': array([113.,  13.]), 'currentState': array([113.33125536,  13.10954352,   0.68519644]), 'targetState': array([113,  13], dtype=int32), 'currentDistance': 0.3488981231195361}
episode index:2300
target Thresh 75.99930710159785
target distance 47.0
model initialize at round 2300
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.32270159, 16.63950331]), 'dynamicTrap': False, 'previousTarget': array([21.16771367, 16.29046827]), 'currentState': array([ 3.2956111 , 22.80146952,  5.88327164]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6934083732143747
running average episode reward sum: 0.6905391293506521
{'scaleFactor': 20, 'currentTarget': array([49.,  8.]), 'dynamicTrap': False, 'previousTarget': array([49.,  8.]), 'currentState': array([48.29508091,  7.68005635,  5.47139052]), 'targetState': array([49,  8], dtype=int32), 'currentDistance': 0.7741284515107595}
episode index:2301
target Thresh 75.99931055744305
target distance 24.0
model initialize at round 2301
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.65521215,  7.49736707]), 'dynamicTrap': False, 'previousTarget': array([21.93091516,  7.3390904 ]), 'currentState': array([ 2.87272591, 10.43900576,  0.01562691]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7897423267354249
running average episode reward sum: 0.6905822237022529
{'scaleFactor': 20, 'currentTarget': array([26.,  7.]), 'dynamicTrap': False, 'previousTarget': array([26.,  7.]), 'currentState': array([25.64535092,  6.74009455,  6.2210339 ]), 'targetState': array([26,  7], dtype=int32), 'currentDistance': 0.4396894483074222}
episode index:2302
target Thresh 75.99931399605215
target distance 14.0
model initialize at round 2302
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.,  9.]), 'dynamicTrap': False, 'previousTarget': array([44.,  9.]), 'currentState': array([56.8142932 ,  3.90133356,  2.87890673]), 'targetState': array([44,  9], dtype=int32), 'currentDistance': 13.7913925924846}
done in step count: 9
reward sum = 0.8847790850088044
running average episode reward sum: 0.6906665471331285
{'scaleFactor': 20, 'currentTarget': array([45.99793863,  8.98789779]), 'dynamicTrap': True, 'previousTarget': array([44.,  9.]), 'currentState': array([46.50093817,  9.03372741,  3.2127084 ]), 'targetState': array([44,  9], dtype=int32), 'currentDistance': 0.5050830558308228}
episode index:2303
target Thresh 75.99931741751111
target distance 14.0
model initialize at round 2303
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,   8.]), 'dynamicTrap': False, 'previousTarget': array([110.,   8.]), 'currentState': array([102.80202753,  21.9738434 ,   3.36992019]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 15.718750171605944}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6907632705273778
{'scaleFactor': 20, 'currentTarget': array([110.,   8.]), 'dynamicTrap': False, 'previousTarget': array([110.,   8.]), 'currentState': array([109.30717847,   8.36843155,   4.87946931]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 0.7846932345829178}
episode index:2304
target Thresh 75.99932082190548
target distance 23.0
model initialize at round 2304
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.65056391, 20.64366468]), 'dynamicTrap': False, 'previousTarget': array([93.58874323, 19.84114513]), 'currentState': array([110.4912027 ,  11.60421738,   2.88792306]), 'targetState': array([88, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7731355028825682
running average episode reward sum: 0.6907990068537792
{'scaleFactor': 20, 'currentTarget': array([88., 23.]), 'dynamicTrap': False, 'previousTarget': array([88., 23.]), 'currentState': array([87.52180394, 23.8317239 ,  2.69557859]), 'targetState': array([88, 23], dtype=int32), 'currentDistance': 0.959393615626659}
episode index:2305
target Thresh 75.99932420932036
target distance 75.0
model initialize at round 2305
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.47654847, 18.50986765]), 'dynamicTrap': False, 'previousTarget': array([64.04429684, 19.66961979]), 'currentState': array([84.4561115 , 19.4137822 ,  3.98238182]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.454923005117217
running average episode reward sum: 0.6906967189085335
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.47842624, 16.66512851,  3.79807817]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.8452426418383572}
episode index:2306
target Thresh 75.99932757984044
target distance 72.0
model initialize at round 2306
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.24119483, 10.3178318 ]), 'dynamicTrap': False, 'previousTarget': array([51.57960839,  9.07908508]), 'currentState': array([32.58219749,  6.64035666,  0.32048869]), 'targetState': array([104,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.43918115552996934
running average episode reward sum: 0.6905876961242341
{'scaleFactor': 20, 'currentTarget': array([104.,  20.]), 'dynamicTrap': False, 'previousTarget': array([104.,  20.]), 'currentState': array([104.42011728,  20.40647385,   5.62264312]), 'targetState': array([104,  20], dtype=int32), 'currentDistance': 0.5845678082665225}
episode index:2307
target Thresh 75.99933093354997
target distance 49.0
model initialize at round 2307
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.02075215,  8.38057725]), 'dynamicTrap': False, 'previousTarget': array([31.48567496,  7.38076685]), 'currentState': array([50.65603315,  4.57850677,  2.79534888]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5339053986283636
running average episode reward sum: 0.690519809513534
{'scaleFactor': 20, 'currentTarget': array([ 2., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 14.]), 'currentState': array([ 2.59678196, 14.82862353,  3.58325669]), 'targetState': array([ 2, 14], dtype=int32), 'currentDistance': 1.0211589785885622}
episode index:2308
target Thresh 75.99933427053281
target distance 25.0
model initialize at round 2308
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.72662902, 12.65490134]), 'dynamicTrap': False, 'previousTarget': array([34.45012032, 12.22793262]), 'currentState': array([1.78811420e+01, 2.34358831e+01, 1.32774115e-02]), 'targetState': array([42,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6508887443646771
running average episode reward sum: 0.6905026457780862
{'scaleFactor': 20, 'currentTarget': array([42.,  8.]), 'dynamicTrap': False, 'previousTarget': array([42.,  8.]), 'currentState': array([41.71947704,  8.77133235,  3.98936703]), 'targetState': array([42,  8], dtype=int32), 'currentDistance': 0.8207598427241076}
episode index:2309
target Thresh 75.99933759087237
target distance 6.0
model initialize at round 2309
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.,  9.]), 'dynamicTrap': False, 'previousTarget': array([97.,  9.]), 'currentState': array([97.06257993,  3.18994054,  1.27456051]), 'targetState': array([97,  9], dtype=int32), 'currentDistance': 5.8103964759106566}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6906237697409529
{'scaleFactor': 20, 'currentTarget': array([97.,  9.]), 'dynamicTrap': False, 'previousTarget': array([97.,  9.]), 'currentState': array([96.71345074,  8.77284551,  1.25831872]), 'targetState': array([97,  9], dtype=int32), 'currentDistance': 0.36566328873481335}
episode index:2310
target Thresh 75.9993408946517
target distance 25.0
model initialize at round 2310
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.50098051, 11.94384698]), 'dynamicTrap': False, 'previousTarget': array([ 8.06369443, 11.59490445]), 'currentState': array([27.49942425, 11.6943518 ,  2.48800552]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5877740598714489
running average episode reward sum: 0.6905792653230085
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.07753491, 12.33345898,  4.14755587]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.34235443103622876}
episode index:2311
target Thresh 75.99934418195333
target distance 14.0
model initialize at round 2311
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.,  4.]), 'dynamicTrap': False, 'previousTarget': array([77.,  4.]), 'currentState': array([72.82659842, 16.20438094,  5.29302073]), 'targetState': array([77,  4], dtype=int32), 'currentDistance': 12.89822449058692}
done in step count: 56
reward sum = 0.2627188215123399
running average episode reward sum: 0.6903942045774156
{'scaleFactor': 20, 'currentTarget': array([77.,  4.]), 'dynamicTrap': False, 'previousTarget': array([77.,  4.]), 'currentState': array([76.43371046,  3.4371478 ,  0.24671948]), 'targetState': array([77,  4], dtype=int32), 'currentDistance': 0.7984274781297707}
episode index:2312
target Thresh 75.99934745285948
target distance 41.0
model initialize at round 2312
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.44603745,  4.75156387]), 'dynamicTrap': False, 'previousTarget': array([51.05332553,  4.45951277]), 'currentState': array([69.40494799,  3.47020127,  3.28698653]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 44
reward sum = 0.48593300341415324
running average episode reward sum: 0.6903058080356244
{'scaleFactor': 20, 'currentTarget': array([30.,  6.]), 'dynamicTrap': False, 'previousTarget': array([30.,  6.]), 'currentState': array([30.99335438,  5.64080686,  2.81165106]), 'targetState': array([30,  6], dtype=int32), 'currentDistance': 1.0563013937363126}
episode index:2313
target Thresh 75.99935070745192
target distance 36.0
model initialize at round 2313
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.29883265, 14.24929121]), 'dynamicTrap': True, 'previousTarget': array([53.5237412 , 13.33860916]), 'currentState': array([34.      ,  9.      ,  5.356239], dtype=float32), 'targetState': array([70, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7648504349264941
running average episode reward sum: 0.6903380226539869
{'scaleFactor': 20, 'currentTarget': array([70., 17.]), 'dynamicTrap': False, 'previousTarget': array([70., 17.]), 'currentState': array([69.61390507, 16.79755155,  1.17828461]), 'targetState': array([70, 17], dtype=int32), 'currentDistance': 0.4359525961808478}
episode index:2314
target Thresh 75.99935394581202
target distance 63.0
model initialize at round 2314
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.00459077,  7.16656865]), 'dynamicTrap': False, 'previousTarget': array([90.20101013,  8.17157288]), 'currentState': array([110.86814927,   9.49874438,   4.31578374]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5176428500612311
running average episode reward sum: 0.6902634243072946
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([46.36609468,  2.23987612,  5.50628074]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 0.677773201035893}
episode index:2315
target Thresh 75.99935716802072
target distance 39.0
model initialize at round 2315
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.51976767, 21.25981407]), 'dynamicTrap': False, 'previousTarget': array([22., 22.]), 'currentState': array([ 1.53418961, 20.50042664,  5.30105412]), 'targetState': array([41, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7459934183911155
running average episode reward sum: 0.6902874873444638
{'scaleFactor': 20, 'currentTarget': array([41., 22.]), 'dynamicTrap': False, 'previousTarget': array([41., 22.]), 'currentState': array([41.46520407, 22.795238  ,  0.78860942]), 'targetState': array([41, 22], dtype=int32), 'currentDistance': 0.9213133555328982}
episode index:2316
target Thresh 75.9993603741586
target distance 55.0
model initialize at round 2316
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.36916817, 16.07444751]), 'dynamicTrap': False, 'previousTarget': array([64.94731634, 15.45071392]), 'currentState': array([46.4018698 , 14.93120844,  5.85952109]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.4664651514733627
running average episode reward sum: 0.6901908872858229
{'scaleFactor': 20, 'currentTarget': array([98.32328444, 16.60644531]), 'dynamicTrap': True, 'previousTarget': array([98.49611464, 16.70705866]), 'currentState': array([97.49460812, 16.41652342,  0.8677059 ]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 0.8501616155841366}
episode index:2317
target Thresh 75.99936356430578
target distance 71.0
model initialize at round 2317
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.97024987, 17.90953223]), 'dynamicTrap': True, 'previousTarget': array([54.96833562, 17.87502335]), 'currentState': array([35.      , 19.      ,  3.651027], dtype=float32), 'targetState': array([106,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.48621391016077464
running average episode reward sum: 0.690102890315536
{'scaleFactor': 20, 'currentTarget': array([106.,  15.]), 'dynamicTrap': False, 'previousTarget': array([106.,  15.]), 'currentState': array([106.99746831,  14.19452338,   0.31487808]), 'targetState': array([106,  15], dtype=int32), 'currentDistance': 1.2820825307813302}
episode index:2318
target Thresh 75.99936673854205
target distance 63.0
model initialize at round 2318
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.2765222 ,  7.18020398]), 'dynamicTrap': False, 'previousTarget': array([31.90990945,  8.10381815]), 'currentState': array([11.32921624,  8.6310607 ,  5.13986135]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5617677426575708
running average episode reward sum: 0.6900475495877835
{'scaleFactor': 20, 'currentTarget': array([75.,  4.]), 'dynamicTrap': False, 'previousTarget': array([75.,  4.]), 'currentState': array([75.03529479,  3.88441652,  2.94216661]), 'targetState': array([75,  4], dtype=int32), 'currentDistance': 0.12085223335493331}
episode index:2319
target Thresh 75.99936989694675
target distance 18.0
model initialize at round 2319
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.85610896, 18.8762172 ]), 'dynamicTrap': False, 'previousTarget': array([85.54026305, 17.73247066]), 'currentState': array([72.64749677,  4.8008721 ,  6.08469338]), 'targetState': array([89, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8307513945412884
running average episode reward sum: 0.6901081977968152
{'scaleFactor': 20, 'currentTarget': array([89., 21.]), 'dynamicTrap': False, 'previousTarget': array([89., 21.]), 'currentState': array([88.81510599, 20.13211674,  0.94027543]), 'targetState': array([89, 21], dtype=int32), 'currentDistance': 0.8873596478412447}
episode index:2320
target Thresh 75.99937303959884
target distance 19.0
model initialize at round 2320
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.11891127, 10.07019467]), 'dynamicTrap': False, 'previousTarget': array([74.30163555, 10.68507134]), 'currentState': array([90.34194926, 20.2371491 ,  3.56002724]), 'targetState': array([73, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7143732175439041
running average episode reward sum: 0.6901186523507777
{'scaleFactor': 20, 'currentTarget': array([73., 10.]), 'dynamicTrap': False, 'previousTarget': array([73., 10.]), 'currentState': array([72.94512208, 10.06315581,  3.82547113]), 'targetState': array([73, 10], dtype=int32), 'currentDistance': 0.0836674479199173}
episode index:2321
target Thresh 75.99937616657688
target distance 25.0
model initialize at round 2321
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.18999291, 15.67189136]), 'dynamicTrap': False, 'previousTarget': array([90.45012032, 15.77206738]), 'currentState': array([74.31055509,  4.94414327,  0.33181971]), 'targetState': array([98, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.36391601866214607
running average episode reward sum: 0.689978168873737
{'scaleFactor': 20, 'currentTarget': array([98., 20.]), 'dynamicTrap': False, 'previousTarget': array([98., 20.]), 'currentState': array([98.37454727, 19.29602707,  5.41804982]), 'targetState': array([98, 20], dtype=int32), 'currentDistance': 0.7974105213668559}
episode index:2322
target Thresh 75.99937927795906
target distance 3.0
model initialize at round 2322
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'dynamicTrap': False, 'previousTarget': array([17.,  8.]), 'currentState': array([14.43238867,  6.16243455,  5.0295006 ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 3.15741583185194}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6900988407769338
{'scaleFactor': 20, 'currentTarget': array([17.,  8.]), 'dynamicTrap': False, 'previousTarget': array([17.,  8.]), 'currentState': array([17.79414009,  8.14581168,  0.5756371 ]), 'targetState': array([17,  8], dtype=int32), 'currentDistance': 0.807415339661047}
episode index:2323
target Thresh 75.99938237382317
target distance 17.0
model initialize at round 2323
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.22783823,  3.74918435]), 'dynamicTrap': True, 'previousTarget': array([89.,  2.]), 'currentState': array([79.      , 19.      ,  4.652592], dtype=float32), 'targetState': array([89,  2], dtype=int32), 'currentDistance': 17.825273530864237}
done in step count: 15
reward sum = 0.8023013041179393
running average episode reward sum: 0.6901471206664954
{'scaleFactor': 20, 'currentTarget': array([89.,  2.]), 'dynamicTrap': False, 'previousTarget': array([89.,  2.]), 'currentState': array([88.62276993,  2.08439424,  5.91853238]), 'targetState': array([89,  2], dtype=int32), 'currentDistance': 0.3865551916465998}
episode index:2324
target Thresh 75.99938545424658
target distance 69.0
model initialize at round 2324
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.27748757, 11.1578511 ]), 'dynamicTrap': False, 'previousTarget': array([28.48305407, 11.51780964]), 'currentState': array([10.86619909,  6.34101819,  6.19686973]), 'targetState': array([78, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4780497800111047
running average episode reward sum: 0.6900558960038479
{'scaleFactor': 20, 'currentTarget': array([76.34665823, 23.51540988]), 'dynamicTrap': True, 'previousTarget': array([78., 23.]), 'currentState': array([76.60309567, 23.84442154,  0.72264769]), 'targetState': array([78, 23], dtype=int32), 'currentDistance': 0.41714366687263843}
episode index:2325
target Thresh 75.9993885193063
target distance 45.0
model initialize at round 2325
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.12558878, 17.21937169]), 'dynamicTrap': False, 'previousTarget': array([22.61161351, 17.9223227 ]), 'currentState': array([ 3.64471686, 12.69216182,  6.18039179]), 'targetState': array([48, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5854921581453938
running average episode reward sum: 0.6900109416883455
{'scaleFactor': 20, 'currentTarget': array([48., 23.]), 'dynamicTrap': False, 'previousTarget': array([48., 23.]), 'currentState': array([48.80362119, 23.80257543,  2.86834991]), 'targetState': array([48, 23], dtype=int32), 'currentDistance': 1.1357527593355183}
episode index:2326
target Thresh 75.99939156907898
target distance 13.0
model initialize at round 2326
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([26.9294486 , 19.12759818,  6.15461439]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 15.009980739006467}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6901069908098735
{'scaleFactor': 20, 'currentTarget': array([32.,  5.]), 'dynamicTrap': False, 'previousTarget': array([32.,  5.]), 'currentState': array([32.33154507,  5.81945494,  5.85532362]), 'targetState': array([32,  5], dtype=int32), 'currentDistance': 0.8839844594069071}
episode index:2327
target Thresh 75.99939460364087
target distance 38.0
model initialize at round 2327
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.04287893, 10.19912028]), 'dynamicTrap': False, 'previousTarget': array([69.78871471,  9.56116153]), 'currentState': array([87.2936675 ,  4.77628527,  2.42869401]), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7345365941400305
running average episode reward sum: 0.6901260756910291
{'scaleFactor': 20, 'currentTarget': array([51., 15.]), 'dynamicTrap': False, 'previousTarget': array([51., 15.]), 'currentState': array([51.07332745, 14.22049321,  3.67530611]), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 0.7829481196847379}
episode index:2328
target Thresh 75.9993976230678
target distance 7.0
model initialize at round 2328
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44., 20.]), 'dynamicTrap': False, 'previousTarget': array([44., 20.]), 'currentState': array([51.80170046, 17.47975148,  2.08428559]), 'targetState': array([44, 20], dtype=int32), 'currentDistance': 8.19866956614327}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6902422070496848
{'scaleFactor': 20, 'currentTarget': array([44., 20.]), 'dynamicTrap': False, 'previousTarget': array([44., 20.]), 'currentState': array([44.28362176, 19.02052113,  2.81367745]), 'targetState': array([44, 20], dtype=int32), 'currentDistance': 1.0197157215689914}
episode index:2329
target Thresh 75.99940062743528
target distance 12.0
model initialize at round 2329
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,  21.]), 'dynamicTrap': False, 'previousTarget': array([101.,  21.]), 'currentState': array([89.23261353, 10.81055546,  0.7021811 ]), 'targetState': array([101,  21], dtype=int32), 'currentDistance': 15.565865356634022}
done in step count: 14
reward sum = 0.8311769507516191
running average episode reward sum: 0.6903026940641491
{'scaleFactor': 20, 'currentTarget': array([101.,  21.]), 'dynamicTrap': False, 'previousTarget': array([101.,  21.]), 'currentState': array([101.11665385,  20.70589321,   4.36454948]), 'targetState': array([101,  21], dtype=int32), 'currentDistance': 0.31639678447101766}
episode index:2330
target Thresh 75.99940361681841
target distance 53.0
model initialize at round 2330
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.4958311 ,  4.00740625]), 'dynamicTrap': False, 'previousTarget': array([80.87305937,  4.24978031]), 'currentState': array([62.65453254,  1.49287414,  5.45320258]), 'targetState': array([114,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7225023696543975
running average episode reward sum: 0.6903165077387909
{'scaleFactor': 20, 'currentTarget': array([114.,   8.]), 'dynamicTrap': False, 'previousTarget': array([114.,   8.]), 'currentState': array([114.45509102,   7.81521131,   5.80021615]), 'targetState': array([114,   8], dtype=int32), 'currentDistance': 0.4911768483785237}
episode index:2331
target Thresh 75.99940659129194
target distance 13.0
model initialize at round 2331
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.,  4.]), 'dynamicTrap': False, 'previousTarget': array([68.,  4.]), 'currentState': array([80.96880442, 14.4048769 ,  3.68283463]), 'targetState': array([68,  4], dtype=int32), 'currentDistance': 16.62682625668179}
done in step count: 18
reward sum = 0.7496305267279023
running average episode reward sum: 0.6903419425668309
{'scaleFactor': 20, 'currentTarget': array([68.,  4.]), 'dynamicTrap': False, 'previousTarget': array([68.,  4.]), 'currentState': array([68.1319656 ,  4.85880896,  3.85365021]), 'targetState': array([68,  4], dtype=int32), 'currentDistance': 0.8688887976477806}
episode index:2332
target Thresh 75.99940955093022
target distance 43.0
model initialize at round 2332
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.17501234, 22.48976439]), 'dynamicTrap': False, 'previousTarget': array([28.00540614, 22.53500945]), 'currentState': array([46.16966482, 22.95222781,  2.45009768]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7039198093972004
running average episode reward sum: 0.6903477624840321
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 5.85307799, 21.21727223,  3.21980924]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 1.1577585328536826}
episode index:2333
target Thresh 75.99941249580725
target distance 26.0
model initialize at round 2333
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.94474163, 13.04024152]), 'dynamicTrap': False, 'previousTarget': array([97.51217609, 12.50280987]), 'currentState': array([117.13379319,  18.67764343,   2.50121409]), 'targetState': array([91, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.699408353948977
running average episode reward sum: 0.6903516444855167
{'scaleFactor': 20, 'currentTarget': array([91., 11.]), 'dynamicTrap': False, 'previousTarget': array([91., 11.]), 'currentState': array([91.74542077, 11.20374148,  3.85836183]), 'targetState': array([91, 11], dtype=int32), 'currentDistance': 0.7727630430919695}
episode index:2334
target Thresh 75.99941542599663
target distance 26.0
model initialize at round 2334
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.8796994 , 15.59976897]), 'dynamicTrap': True, 'previousTarget': array([80.89972147, 15.54221128]), 'currentState': array([62.       ,  9.       ,  3.8121905], dtype=float32), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.35860990288123157
running average episode reward sum: 0.6902095709345084
{'scaleFactor': 20, 'currentTarget': array([88., 18.]), 'dynamicTrap': False, 'previousTarget': array([88., 18.]), 'currentState': array([88.94652068, 17.94560975,  3.0096033 ]), 'targetState': array([88, 18], dtype=int32), 'currentDistance': 0.9480821130483515}
episode index:2335
target Thresh 75.99941834157164
target distance 7.0
model initialize at round 2335
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.93503201,  7.18913472]), 'dynamicTrap': True, 'previousTarget': array([48.,  7.]), 'currentState': array([41.      ,  9.      ,  5.020228], dtype=float32), 'targetState': array([48,  7], dtype=int32), 'currentDistance': 7.167558999910411}
done in step count: 5
reward sum = 0.9409900498999999
running average episode reward sum: 0.6903169255915999
{'scaleFactor': 20, 'currentTarget': array([48.,  7.]), 'dynamicTrap': False, 'previousTarget': array([48.,  7.]), 'currentState': array([48.57017371,  7.32085759,  0.31127427]), 'targetState': array([48,  7], dtype=int32), 'currentDistance': 0.6542535093834031}
episode index:2336
target Thresh 75.99942124260515
target distance 47.0
model initialize at round 2336
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.82418603,  6.60841334]), 'dynamicTrap': False, 'previousTarget': array([36.0045254 ,  6.57456437]), 'currentState': array([55.8190435 ,  7.06192713,  2.8660938 ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.534421299751258
running average episode reward sum: 0.6902502180067301
{'scaleFactor': 20, 'currentTarget': array([9., 6.]), 'dynamicTrap': False, 'previousTarget': array([9., 6.]), 'currentState': array([8.32225729, 5.2161544 , 6.1571181 ]), 'targetState': array([9, 6], dtype=int32), 'currentDistance': 1.0362186603643098}
episode index:2337
target Thresh 75.9994241291697
target distance 13.0
model initialize at round 2337
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([25.98212853, 11.03145116,  4.21912003]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 14.314819741883863}
done in step count: 29
reward sum = 0.552170888617885
running average episode reward sum: 0.6901911592687536
{'scaleFactor': 20, 'currentTarget': array([13.,  5.]), 'dynamicTrap': False, 'previousTarget': array([13.,  5.]), 'currentState': array([12.84518416,  5.57623026,  4.12625398]), 'targetState': array([13,  5], dtype=int32), 'currentDistance': 0.596665114661449}
episode index:2338
target Thresh 75.99942700133745
target distance 65.0
model initialize at round 2338
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.45558303, 13.94545181]), 'dynamicTrap': False, 'previousTarget': array([26.91533358, 12.83833848]), 'currentState': array([ 6.50041229, 12.60710927,  0.86218739]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 84
reward sum = 0.10382254009966208
running average episode reward sum: 0.6899404672554278
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([71.19533811, 16.35363919,  2.06293747]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 1.0321158140527917}
episode index:2339
target Thresh 75.9994298591802
target distance 49.0
model initialize at round 2339
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.66873441, 20.01142247]), 'dynamicTrap': False, 'previousTarget': array([88.89668285, 19.03027376]), 'currentState': array([68.71454116, 18.65858414,  0.73123646]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6940697175613875
running average episode reward sum: 0.6899422318923107
{'scaleFactor': 20, 'currentTarget': array([118.,  22.]), 'dynamicTrap': False, 'previousTarget': array([118.,  22.]), 'currentState': array([117.77736506,  21.87041641,   6.27342041]), 'targetState': array([118,  22], dtype=int32), 'currentDistance': 0.25760090309589795}
episode index:2340
target Thresh 75.99943270276941
target distance 31.0
model initialize at round 2340
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98.41915231, 19.67429151]), 'dynamicTrap': False, 'previousTarget': array([98.63445353, 18.99756038]), 'currentState': array([118.01682262,  15.68288594,   2.57080036]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6900075889882064
{'scaleFactor': 20, 'currentTarget': array([87., 22.]), 'dynamicTrap': False, 'previousTarget': array([87., 22.]), 'currentState': array([86.67227713, 22.26437074,  3.20078306]), 'targetState': array([87, 22], dtype=int32), 'currentDistance': 0.42106313838695153}
episode index:2341
target Thresh 75.99943553217615
target distance 67.0
model initialize at round 2341
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.07044841, 22.53264811]), 'dynamicTrap': False, 'previousTarget': array([87.00890472, 22.40325089]), 'currentState': array([105.05889462,  23.21236742,   2.65072739]), 'targetState': array([40, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.43054047669865636
running average episode reward sum: 0.6898968002980742
{'scaleFactor': 20, 'currentTarget': array([40., 21.]), 'dynamicTrap': False, 'previousTarget': array([40., 21.]), 'currentState': array([39.52419673, 20.49566753,  4.42143865]), 'targetState': array([40, 21], dtype=int32), 'currentDistance': 0.6933541587889075}
episode index:2342
target Thresh 75.99943834747116
target distance 64.0
model initialize at round 2342
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.273124  , 12.46961408]), 'dynamicTrap': False, 'previousTarget': array([49.06075717, 12.44224665]), 'currentState': array([67.20609756, 14.10563524,  3.45983255]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6226946568123476
running average episode reward sum: 0.6898681182052506
{'scaleFactor': 20, 'currentTarget': array([5., 9.]), 'dynamicTrap': False, 'previousTarget': array([5., 9.]), 'currentState': array([5.05261207, 8.89237774, 3.57872388]), 'targetState': array([5, 9], dtype=int32), 'currentDistance': 0.11979390678038299}
episode index:2343
target Thresh 75.99944114872484
target distance 14.0
model initialize at round 2343
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   3.]), 'dynamicTrap': False, 'previousTarget': array([118.,   3.]), 'currentState': array([106.45545031,  16.33160316,   4.56000543]), 'targetState': array([118,   3], dtype=int32), 'currentDistance': 17.635426572437417}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6899444312148768
{'scaleFactor': 20, 'currentTarget': array([118.,   3.]), 'dynamicTrap': False, 'previousTarget': array([118.,   3.]), 'currentState': array([117.40628048,   2.39673598,   5.656637  ]), 'targetState': array([118,   3], dtype=int32), 'currentDistance': 0.8464220883215825}
episode index:2344
target Thresh 75.9994439360072
target distance 64.0
model initialize at round 2344
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.44242825,  8.69252025]), 'dynamicTrap': False, 'previousTarget': array([51.34255626,  7.6857707 ]), 'currentState': array([70.16534629,  5.37491509,  3.06637919]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5335507449669548
running average episode reward sum: 0.6898777388113596
{'scaleFactor': 20, 'currentTarget': array([ 7., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 16.]), 'currentState': array([ 6.77644817, 15.56745431,  2.356243  ]), 'targetState': array([ 7, 16], dtype=int32), 'currentDistance': 0.4868995738782911}
episode index:2345
target Thresh 75.99944670938794
target distance 22.0
model initialize at round 2345
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.20152597, 22.73156271]), 'dynamicTrap': False, 'previousTarget': array([ 7.18339664, 22.70226409]), 'currentState': array([28.13159186, 21.06049304,  1.72703117]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6899539826621515
{'scaleFactor': 20, 'currentTarget': array([ 5., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 23.]), 'currentState': array([ 5.14913969, 23.84426593,  2.44651848]), 'targetState': array([ 5, 23], dtype=int32), 'currentDistance': 0.8573375130237187}
episode index:2346
target Thresh 75.99944946893638
target distance 18.0
model initialize at round 2346
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'dynamicTrap': False, 'previousTarget': array([25., 22.]), 'currentState': array([ 8.84931936, 16.68379941,  0.64914829]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 17.00313129715965}
done in step count: 13
reward sum = 0.8406856572314563
running average episode reward sum: 0.6900182057872343
{'scaleFactor': 20, 'currentTarget': array([25., 22.]), 'dynamicTrap': False, 'previousTarget': array([25., 22.]), 'currentState': array([24.73419807, 22.17654689,  5.74114768]), 'targetState': array([25, 22], dtype=int32), 'currentDistance': 0.31909163089681963}
episode index:2347
target Thresh 75.9994522147215
target distance 12.0
model initialize at round 2347
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.,  3.]), 'dynamicTrap': False, 'previousTarget': array([53.,  3.]), 'currentState': array([52.46879151, 13.53206244,  4.21542144]), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 10.545450286215935}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6901095021540236
{'scaleFactor': 20, 'currentTarget': array([53.,  3.]), 'dynamicTrap': False, 'previousTarget': array([53.,  3.]), 'currentState': array([52.05320819,  3.28986013,  5.86985982]), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 0.9901684815555353}
episode index:2348
target Thresh 75.99945494681198
target distance 8.0
model initialize at round 2348
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61., 23.]), 'dynamicTrap': False, 'previousTarget': array([61., 23.]), 'currentState': array([55.7519286 , 16.41158825,  2.09134571]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 8.423148033507232}
done in step count: 8
reward sum = 0.89334070442792
running average episode reward sum: 0.6901960203329397
{'scaleFactor': 20, 'currentTarget': array([61., 23.]), 'dynamicTrap': False, 'previousTarget': array([61., 23.]), 'currentState': array([61.50087945, 22.76968366,  1.66098476]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 0.5512946962749095}
episode index:2349
target Thresh 75.9994576652761
target distance 52.0
model initialize at round 2349
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.04111703, 14.28179193]), 'dynamicTrap': True, 'previousTarget': array([42.05891029, 14.53392998]), 'currentState': array([62.       , 13.       ,  2.6881988], dtype=float32), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6390074492128731
running average episode reward sum: 0.6901742379622503
{'scaleFactor': 20, 'currentTarget': array([10., 17.]), 'dynamicTrap': False, 'previousTarget': array([10., 17.]), 'currentState': array([10.45590311, 17.43202843,  4.81216437]), 'targetState': array([10, 17], dtype=int32), 'currentDistance': 0.6280893316231266}
episode index:2350
target Thresh 75.99946037018182
target distance 43.0
model initialize at round 2350
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.2773763 , 10.73755262]), 'dynamicTrap': False, 'previousTarget': array([82.5992144 , 10.35317779]), 'currentState': array([65.75697681,  3.18806239,  5.52992398]), 'targetState': array([107,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6135483637078746
running average episode reward sum: 0.6901416450765615
{'scaleFactor': 20, 'currentTarget': array([107.,  20.]), 'dynamicTrap': False, 'previousTarget': array([107.,  20.]), 'currentState': array([107.9769602 ,  20.64529411,   1.7832287 ]), 'targetState': array([107,  20], dtype=int32), 'currentDistance': 1.1708354802023069}
episode index:2351
target Thresh 75.99946306159677
target distance 11.0
model initialize at round 2351
at step 0:
{'scaleFactor': 20, 'currentTarget': array([116.,   3.]), 'dynamicTrap': False, 'previousTarget': array([116.,   3.]), 'currentState': array([1.06875457e+02, 5.02129823e+00, 2.09333531e-02]), 'targetState': array([116,   3], dtype=int32), 'currentDistance': 9.3457435443347}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.690252550010585
{'scaleFactor': 20, 'currentTarget': array([116.,   3.]), 'dynamicTrap': False, 'previousTarget': array([116.,   3.]), 'currentState': array([115.58316383,   3.31982608,   4.99047623]), 'targetState': array([116,   3], dtype=int32), 'currentDistance': 0.5253961507695128}
episode index:2352
target Thresh 75.99946573958823
target distance 39.0
model initialize at round 2352
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.15090538, 15.13709764]), 'dynamicTrap': False, 'previousTarget': array([88.33308788, 14.17958159]), 'currentState': array([107.08303783,   8.68927934,   2.51760125]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6186200920851711
running average episode reward sum: 0.6902221069770427
{'scaleFactor': 20, 'currentTarget': array([68., 22.]), 'dynamicTrap': False, 'previousTarget': array([68., 22.]), 'currentState': array([68.02672497, 21.17249068,  4.02152517]), 'targetState': array([68, 22], dtype=int32), 'currentDistance': 0.8279407545936693}
episode index:2353
target Thresh 75.99946840422315
target distance 66.0
model initialize at round 2353
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.26065058, 10.32695022]), 'dynamicTrap': False, 'previousTarget': array([72.03663007, 10.20990121]), 'currentState': array([90.22427646,  9.1212796 ,  2.39346123]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.44840070855660996
running average episode reward sum: 0.690119379110254
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([26.93546045, 12.55868123,  3.53894991]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 1.0343348190662396}
episode index:2354
target Thresh 75.99947105556815
target distance 20.0
model initialize at round 2354
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.99689258,  2.11892988]), 'dynamicTrap': False, 'previousTarget': array([33.00124766,  2.02495322]), 'currentState': array([32.47450803, 22.11210658,  4.02893424]), 'targetState': array([33,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6902027190221885
{'scaleFactor': 20, 'currentTarget': array([33.,  2.]), 'dynamicTrap': False, 'previousTarget': array([33.,  2.]), 'currentState': array([32.50560489,  1.60082551,  4.13682473]), 'targetState': array([33,  2], dtype=int32), 'currentDistance': 0.6354264741646364}
episode index:2355
target Thresh 75.9994736936895
target distance 63.0
model initialize at round 2355
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.50942603,  9.82522957]), 'dynamicTrap': False, 'previousTarget': array([52.15932542,  9.48054926]), 'currentState': array([70.31534857, 12.60469575,  2.07942033]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.47487916471481734
running average episode reward sum: 0.6901113253234162
{'scaleFactor': 20, 'currentTarget': array([9., 4.]), 'dynamicTrap': False, 'previousTarget': array([9., 4.]), 'currentState': array([8.64684324, 4.99306974, 2.79794237]), 'targetState': array([9, 4], dtype=int32), 'currentDistance': 1.0539958238486091}
episode index:2356
target Thresh 75.99947631865318
target distance 38.0
model initialize at round 2356
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.55975271, 20.09800682]), 'dynamicTrap': False, 'previousTarget': array([53., 20.]), 'currentState': array([34.56010808, 20.21723262,  1.12371581]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7574327853351389
running average episode reward sum: 0.6901398876738667
{'scaleFactor': 20, 'currentTarget': array([71., 20.]), 'dynamicTrap': False, 'previousTarget': array([71., 20.]), 'currentState': array([71.33425925, 19.27740787,  5.68390146]), 'targetState': array([71, 20], dtype=int32), 'currentDistance': 0.7961586723451521}
episode index:2357
target Thresh 75.99947893052479
target distance 10.0
model initialize at round 2357
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 16.]), 'dynamicTrap': False, 'previousTarget': array([53., 16.]), 'currentState': array([46.08588617,  4.84628099,  0.19432324]), 'targetState': array([53, 16], dtype=int32), 'currentDistance': 13.122896697649297}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6902385326300813
{'scaleFactor': 20, 'currentTarget': array([53., 16.]), 'dynamicTrap': False, 'previousTarget': array([53., 16.]), 'currentState': array([53.24302781, 16.63883414,  0.79154476]), 'targetState': array([53, 16], dtype=int32), 'currentDistance': 0.6834995083935562}
episode index:2358
target Thresh 75.99948152936963
target distance 13.0
model initialize at round 2358
at step 0:
{'scaleFactor': 20, 'currentTarget': array([98., 12.]), 'dynamicTrap': False, 'previousTarget': array([98., 12.]), 'currentState': array([86.99748721,  8.02327492,  0.09696422]), 'targetState': array([98, 12], dtype=int32), 'currentDistance': 11.69912945736058}
done in step count: 35
reward sum = 0.5618896363735439
running average episode reward sum: 0.6901841244502354
{'scaleFactor': 20, 'currentTarget': array([98., 12.]), 'dynamicTrap': False, 'previousTarget': array([98., 12.]), 'currentState': array([97.6870252 , 11.50761383,  1.28208082]), 'targetState': array([98, 12], dtype=int32), 'currentDistance': 0.5834358259652755}
episode index:2359
target Thresh 75.9994841152527
target distance 45.0
model initialize at round 2359
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.89027103,  7.15072012]), 'dynamicTrap': False, 'previousTarget': array([29.00493644,  6.55566525]), 'currentState': array([47.86711083,  8.11294242,  3.37507594]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4636558044552358
running average episode reward sum: 0.6900881378739663
{'scaleFactor': 20, 'currentTarget': array([4., 6.]), 'dynamicTrap': False, 'previousTarget': array([4., 6.]), 'currentState': array([4.49068779, 5.16182283, 4.46199862]), 'targetState': array([4, 6], dtype=int32), 'currentDistance': 0.9712442925360263}
episode index:2360
target Thresh 75.9994866882386
target distance 4.0
model initialize at round 2360
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([114.82748583,  13.23940504,   5.63759601]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 2.5011882537017343}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6902151653462771
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([116.19283632,  11.89482837,   5.37064007]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.8139866525687438}
episode index:2361
target Thresh 75.9994892483917
target distance 46.0
model initialize at round 2361
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.04889505, 15.10991834]), 'dynamicTrap': False, 'previousTarget': array([24.24618806, 15.5608599 ]), 'currentState': array([ 3.66613815, 20.04030728,  4.66334319]), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6295419348318501
running average episode reward sum: 0.690189478119133
{'scaleFactor': 20, 'currentTarget': array([51.,  8.]), 'dynamicTrap': False, 'previousTarget': array([51.,  8.]), 'currentState': array([51.47058654,  8.88605962,  0.19860993]), 'targetState': array([51,  8], dtype=int32), 'currentDistance': 1.0032713215757147}
episode index:2362
target Thresh 75.99949179577597
target distance 40.0
model initialize at round 2362
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.65022135, 11.64593464]), 'dynamicTrap': False, 'previousTarget': array([88.97504678, 11.00124766]), 'currentState': array([69.72218682, 13.34105698,  0.16157186]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.41043158675605995
running average episode reward sum: 0.6900710871367534
{'scaleFactor': 20, 'currentTarget': array([109.,  10.]), 'dynamicTrap': False, 'previousTarget': array([109.,  10.]), 'currentState': array([108.71367479,  10.95459676,   5.03570447]), 'targetState': array([109,  10], dtype=int32), 'currentDistance': 0.9966128130047572}
episode index:2363
target Thresh 75.99949433045512
target distance 19.0
model initialize at round 2363
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.,  14.]), 'dynamicTrap': False, 'previousTarget': array([109.,  14.]), 'currentState': array([91.68220765, 20.53628611,  5.73932666]), 'targetState': array([109,  14], dtype=int32), 'currentDistance': 18.510239544148902}
done in step count: 15
reward sum = 0.8125040490692086
running average episode reward sum: 0.6901228777297874
{'scaleFactor': 20, 'currentTarget': array([109.,  14.]), 'dynamicTrap': False, 'previousTarget': array([109.,  14.]), 'currentState': array([108.54126509,  13.88953102,   5.05658869]), 'targetState': array([109,  14], dtype=int32), 'currentDistance': 0.47184861191835414}
episode index:2364
target Thresh 75.9994968524925
target distance 58.0
model initialize at round 2364
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.70124837, 14.43573558]), 'dynamicTrap': False, 'previousTarget': array([93.04739343, 13.37604183]), 'currentState': array([112.68405549,  13.60662792,   2.67911726]), 'targetState': array([55, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.2646569898334606
running average episode reward sum: 0.6897191653122131
{'scaleFactor': 20, 'currentTarget': array([54.26127626, 15.26223299]), 'dynamicTrap': True, 'previousTarget': array([54.444985  , 15.34045036]), 'currentState': array([57.55558311, 12.58078996,  6.1913298 ]), 'targetState': array([55, 16], dtype=int32), 'currentDistance': 4.247657509799734}
episode index:2365
target Thresh 75.99949936195115
target distance 16.0
model initialize at round 2365
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.,  18.]), 'dynamicTrap': False, 'previousTarget': array([100.,  18.]), 'currentState': array([108.04607639,   3.67153582,   1.60617232]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 16.433022576842447}
done in step count: 17
reward sum = 0.7892102361398824
running average episode reward sum: 0.6897612156380069
{'scaleFactor': 20, 'currentTarget': array([100.,  18.]), 'dynamicTrap': False, 'previousTarget': array([100.,  18.]), 'currentState': array([99.42806574, 17.9858234 ,  4.25482644]), 'targetState': array([100,  18], dtype=int32), 'currentDistance': 0.572109928244681}
episode index:2366
target Thresh 75.99950185889384
target distance 2.0
model initialize at round 2366
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([32.26617989, 18.29540545,  1.07620335]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 1.0173226823078203}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6898922839879698
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([32.26617989, 18.29540545,  1.07620335]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 1.0173226823078203}
episode index:2367
target Thresh 75.99950434338297
target distance 62.0
model initialize at round 2367
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.8445082 ,  8.28268048]), 'dynamicTrap': False, 'previousTarget': array([73.1222365 ,  8.86004022]), 'currentState': array([54.82605398,  2.09411044,  0.1785385 ]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5411398383603797
running average episode reward sum: 0.6898294662322147
{'scaleFactor': 20, 'currentTarget': array([116.,  22.]), 'dynamicTrap': False, 'previousTarget': array([116.,  22.]), 'currentState': array([115.64416588,  21.0629329 ,   1.71915935]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 1.0023535678471949}
episode index:2368
target Thresh 75.99950681548067
target distance 58.0
model initialize at round 2368
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.50144745, 15.62606997]), 'dynamicTrap': False, 'previousTarget': array([90.14408774, 14.6035968 ]), 'currentState': array([109.28010852,  18.59331894,   1.99308427]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.485609673639752
running average episode reward sum: 0.6897432611699132
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([52.20166175, 10.17216336,  2.26289368]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 0.2651559583279573}
episode index:2369
target Thresh 75.99950927524871
target distance 75.0
model initialize at round 2369
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.00430554, 16.17959704]), 'dynamicTrap': False, 'previousTarget': array([57.02838391, 14.93484714]), 'currentState': array([76.94681457, 17.69496298,  2.59046823]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6249458109911473
running average episode reward sum: 0.6897159204736353
{'scaleFactor': 20, 'currentTarget': array([ 2., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 12.]), 'currentState': array([ 1.61252841, 11.23808331,  2.13820031]), 'targetState': array([ 2, 12], dtype=int32), 'currentDistance': 0.8547814215535312}
episode index:2370
target Thresh 75.99951172274862
target distance 61.0
model initialize at round 2370
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.22318536, 19.49934056]), 'dynamicTrap': False, 'previousTarget': array([72.06684958, 20.36613521]), 'currentState': array([93.1882384 , 20.68114308,  4.60162044]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5895102933723649
running average episode reward sum: 0.6896736574508173
{'scaleFactor': 20, 'currentTarget': array([31., 17.]), 'dynamicTrap': False, 'previousTarget': array([31., 17.]), 'currentState': array([30.28051036, 17.65849289,  3.8113565 ]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 0.9753349249497851}
episode index:2371
target Thresh 75.99951415804158
target distance 18.0
model initialize at round 2371
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.,  5.]), 'dynamicTrap': False, 'previousTarget': array([52.,  5.]), 'currentState': array([35.33704593,  2.39380939,  5.57568318]), 'targetState': array([52,  5], dtype=int32), 'currentDistance': 16.865534915214255}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6897641753334303
{'scaleFactor': 20, 'currentTarget': array([52.,  5.]), 'dynamicTrap': False, 'previousTarget': array([52.,  5.]), 'currentState': array([51.14312272,  4.99798832,  0.43677204]), 'targetState': array([52,  5], dtype=int32), 'currentDistance': 0.8568796443118546}
episode index:2372
target Thresh 75.99951658118844
target distance 9.0
model initialize at round 2372
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([15.79320767,  6.76660648,  1.58972561]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 11.959674092363347}
done in step count: 17
reward sum = 0.7515084046480061
running average episode reward sum: 0.6897901948148102
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([25.86143194, 13.65627393,  4.56893579]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.6707433037586564}
episode index:2373
target Thresh 75.99951899224983
target distance 29.0
model initialize at round 2373
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.77233334, 16.78128555]), 'dynamicTrap': False, 'previousTarget': array([38.98896727, 16.69498132]), 'currentState': array([55.41428976,  7.35996009,  2.56141341]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6686873194007033
running average episode reward sum: 0.6897813056507773
{'scaleFactor': 20, 'currentTarget': array([28., 22.]), 'dynamicTrap': False, 'previousTarget': array([28., 22.]), 'currentState': array([27.40367217, 21.99939012,  3.36774123]), 'targetState': array([28, 22], dtype=int32), 'currentDistance': 0.5963281399016006}
episode index:2374
target Thresh 75.999521391286
target distance 15.0
model initialize at round 2374
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 13.]), 'dynamicTrap': False, 'previousTarget': array([48., 13.]), 'currentState': array([61.09753481,  5.59305173,  2.98837179]), 'targetState': array([48, 13], dtype=int32), 'currentDistance': 15.046870132871861}
done in step count: 15
reward sum = 0.8306543646412884
running average episode reward sum: 0.6898406206229839
{'scaleFactor': 20, 'currentTarget': array([48., 13.]), 'dynamicTrap': False, 'previousTarget': array([48., 13.]), 'currentState': array([47.1321745 , 12.36357169,  0.20691404]), 'targetState': array([48, 13], dtype=int32), 'currentDistance': 1.0761794018353854}
episode index:2375
target Thresh 75.99952377835692
target distance 70.0
model initialize at round 2375
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.30393672, 20.89098997]), 'dynamicTrap': False, 'previousTarget': array([52.12934655, 20.72906818]), 'currentState': array([70.15684113, 23.31218519,  3.24527949]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5920656565918768
running average episode reward sum: 0.6897994695438462
{'scaleFactor': 20, 'currentTarget': array([ 2., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 15.]), 'currentState': array([ 2.5589337 , 14.94843888,  4.05274766]), 'targetState': array([ 2, 15], dtype=int32), 'currentDistance': 0.5613068900778694}
episode index:2376
target Thresh 75.99952615352227
target distance 66.0
model initialize at round 2376
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.05512782, 17.51606136]), 'dynamicTrap': True, 'previousTarget': array([57.05714624, 17.48917775]), 'currentState': array([77.       , 19.       ,  5.9932127], dtype=float32), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 49
reward sum = 0.5491284068059021
running average episode reward sum: 0.6897402894585546
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'dynamicTrap': False, 'previousTarget': array([11., 14.]), 'currentState': array([10.41192784, 14.02312081,  4.6330756 ]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.5885264950652132}
episode index:2377
target Thresh 75.99952851684144
target distance 64.0
model initialize at round 2377
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.63378318,  1.54450387]), 'dynamicTrap': False, 'previousTarget': array([62.,  2.]), 'currentState': array([43.63493901,  1.32948831,  5.60170603]), 'targetState': array([106,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6175106062693209
running average episode reward sum: 0.6897099153276929
{'scaleFactor': 20, 'currentTarget': array([106.,   2.]), 'dynamicTrap': False, 'previousTarget': array([106.,   2.]), 'currentState': array([106.37242157,   2.54100344,   5.72710723]), 'targetState': array([106,   2], dtype=int32), 'currentDistance': 0.6567971860891955}
episode index:2378
target Thresh 75.9995308683735
target distance 11.0
model initialize at round 2378
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'dynamicTrap': False, 'previousTarget': array([27., 12.]), 'currentState': array([37.46118716, 17.47916798,  3.36707234]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 11.809221759097719}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6898157456068326
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'dynamicTrap': False, 'previousTarget': array([27., 12.]), 'currentState': array([27.69985909, 12.86441594,  3.22231607]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 1.1122129557408034}
episode index:2379
target Thresh 75.99953320817724
target distance 48.0
model initialize at round 2379
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.02557424, 15.88701501]), 'dynamicTrap': False, 'previousTarget': array([68.2, 16.4]), 'currentState': array([47.683926  , 20.97628248,  4.81268525]), 'targetState': array([97,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.31642799003719563
running average episode reward sum: 0.6896588599952487
{'scaleFactor': 20, 'currentTarget': array([97.,  8.]), 'dynamicTrap': False, 'previousTarget': array([97.,  8.]), 'currentState': array([96.64142813,  8.4159399 ,  3.91710975]), 'targetState': array([97,  8], dtype=int32), 'currentDistance': 0.5491628046487801}
episode index:2380
target Thresh 75.99953553631119
target distance 8.0
model initialize at round 2380
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62., 17.]), 'dynamicTrap': False, 'previousTarget': array([62., 17.]), 'currentState': array([69.49529459, 20.54124773,  3.36929607]), 'targetState': array([62, 17], dtype=int32), 'currentDistance': 8.289745256446132}
done in step count: 9
reward sum = 0.8662453954662818
running average episode reward sum: 0.6897330248568493
{'scaleFactor': 20, 'currentTarget': array([62., 17.]), 'dynamicTrap': False, 'previousTarget': array([63.83927551, 18.3451095 ]), 'currentState': array([62.82458402, 16.06234913,  3.5201091 ]), 'targetState': array([62, 17], dtype=int32), 'currentDistance': 1.2486504565565684}
episode index:2381
target Thresh 75.9995378528335
target distance 17.0
model initialize at round 2381
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.01241989,   9.99233557]), 'dynamicTrap': True, 'previousTarget': array([112.,  10.]), 'currentState': array([95.       , 18.       ,  2.6835315], dtype=float32), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 18.80279553457422}
done in step count: 49
reward sum = 0.3179178750214853
running average episode reward sum: 0.6895769311751384
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([112.0215784 ,   9.80331253,   5.14028056]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 0.1978675989530032}
episode index:2382
target Thresh 75.99954015780209
target distance 16.0
model initialize at round 2382
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  19.]), 'dynamicTrap': False, 'previousTarget': array([106.,  19.]), 'currentState': array([105.06761213,   4.69208542,   1.30284587]), 'targetState': array([106,  19], dtype=int32), 'currentDistance': 14.338262338084428}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.689670905290249
{'scaleFactor': 20, 'currentTarget': array([106.,  19.]), 'dynamicTrap': False, 'previousTarget': array([106.,  19.]), 'currentState': array([106.99011138,  19.23887097,   3.4183    ]), 'targetState': array([106,  19], dtype=int32), 'currentDistance': 1.0185184739016075}
episode index:2383
target Thresh 75.99954245127462
target distance 19.0
model initialize at round 2383
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([8.51316571e+00, 1.99040414e+01, 1.43164953e-02]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 17.521144332683114}
done in step count: 11
reward sum = 0.8762223936597164
running average episode reward sum: 0.6897491567534911
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([25.20995011, 21.06975286,  6.07538296]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.7931231253450809}
episode index:2384
target Thresh 75.99954473330841
target distance 74.0
model initialize at round 2384
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.8405791 , 13.30433749]), 'dynamicTrap': False, 'previousTarget': array([32.97084547, 14.07950516]), 'currentState': array([11.88531808, 11.96734176,  2.93044817]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.4420178384097183
running average episode reward sum: 0.6896452861797622
{'scaleFactor': 20, 'currentTarget': array([87., 17.]), 'dynamicTrap': False, 'previousTarget': array([87., 17.]), 'currentState': array([86.52409481, 16.87353069,  0.24541051]), 'targetState': array([87, 17], dtype=int32), 'currentDistance': 0.4924228229823953}
episode index:2385
target Thresh 75.99954700396052
target distance 2.0
model initialize at round 2385
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([12.53322281, 11.557452  ,  2.3937183 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7714138356349932}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6897753594043305
{'scaleFactor': 20, 'currentTarget': array([12., 11.]), 'dynamicTrap': False, 'previousTarget': array([12., 11.]), 'currentState': array([12.53322281, 11.557452  ,  2.3937183 ]), 'targetState': array([12, 11], dtype=int32), 'currentDistance': 0.7714138356349932}
episode index:2386
target Thresh 75.99954926328769
target distance 40.0
model initialize at round 2386
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.65088104, 11.06200794]), 'dynamicTrap': False, 'previousTarget': array([89.2384301 , 10.20729355]), 'currentState': array([72.33940263,  3.01901359,  5.9235608 ]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7037304159993938
running average episode reward sum: 0.6897812056785639
{'scaleFactor': 20, 'currentTarget': array([111.,  20.]), 'dynamicTrap': False, 'previousTarget': array([111.,  20.]), 'currentState': array([110.27432385,  19.32965017,   2.79356152]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 0.9879143553051658}
episode index:2387
target Thresh 75.99955151134641
target distance 38.0
model initialize at round 2387
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.93224274, 15.35509895]), 'dynamicTrap': True, 'previousTarget': array([74.93796297, 15.42595029]), 'currentState': array([55.       , 17.       ,  0.9840726], dtype=float32), 'targetState': array([93, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.25009290469200735
running average episode reward sum: 0.6895970815994239
{'scaleFactor': 20, 'currentTarget': array([93., 14.]), 'dynamicTrap': False, 'previousTarget': array([93., 14.]), 'currentState': array([93.63109386, 13.41253805,  4.1492459 ]), 'targetState': array([93, 14], dtype=int32), 'currentDistance': 0.8622012551816578}
episode index:2388
target Thresh 75.99955374819291
target distance 37.0
model initialize at round 2388
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.22278065, 11.83645962]), 'dynamicTrap': True, 'previousTarget': array([36.20865964, 11.8639063 ]), 'currentState': array([54.       , 21.       ,  0.7042481], dtype=float32), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5757409616903622
running average episode reward sum: 0.6895494231147403
{'scaleFactor': 20, 'currentTarget': array([17.,  2.]), 'dynamicTrap': False, 'previousTarget': array([17.,  2.]), 'currentState': array([16.37384702,  1.60587309,  3.17539956]), 'targetState': array([17,  2], dtype=int32), 'currentDistance': 0.7398672639922117}
episode index:2389
target Thresh 75.99955597388309
target distance 59.0
model initialize at round 2389
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.68038837, 11.17229218]), 'dynamicTrap': True, 'previousTarget': array([82.69719344, 11.2346594 ]), 'currentState': array([102.       ,   6.       ,   1.0633812], dtype=float32), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 43
reward sum = 0.5663619446705244
running average episode reward sum: 0.6894978802367302
{'scaleFactor': 20, 'currentTarget': array([43., 22.]), 'dynamicTrap': False, 'previousTarget': array([43., 22.]), 'currentState': array([43.83215749, 22.32360944,  2.88879021]), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 0.8928656979520344}
episode index:2390
target Thresh 75.99955818847258
target distance 23.0
model initialize at round 2390
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.64387705, 20.79088366]), 'dynamicTrap': False, 'previousTarget': array([93.07518824, 20.73259233]), 'currentState': array([111.48399301,  18.26703766,   2.58349526]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8232124156227957
running average episode reward sum: 0.6895538043418686
{'scaleFactor': 20, 'currentTarget': array([90., 21.]), 'dynamicTrap': False, 'previousTarget': array([90., 21.]), 'currentState': array([90.58313196, 20.59983266,  3.07743345]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 0.7072317799009628}
episode index:2391
target Thresh 75.99956039201676
target distance 67.0
model initialize at round 2391
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.52708891,  9.50487604]), 'dynamicTrap': False, 'previousTarget': array([63.54699538,  9.64549364]), 'currentState': array([82.96657895,  4.80315008,  4.44321327]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.3559719793554399
running average episode reward sum: 0.6894143470571753
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'dynamicTrap': False, 'previousTarget': array([16., 21.]), 'currentState': array([15.85131154, 20.65396555,  2.4439938 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.37662726488697246}
episode index:2392
target Thresh 75.99956258457073
target distance 61.0
model initialize at round 2392
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.68526207,  7.0099173 ]), 'dynamicTrap': False, 'previousTarget': array([97.37611087,  6.86043721]), 'currentState': array([115.29182101,   3.06240582,   2.30878901]), 'targetState': array([56, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.4567368872382461
running average episode reward sum: 0.689317114520686
{'scaleFactor': 20, 'currentTarget': array([56., 15.]), 'dynamicTrap': False, 'previousTarget': array([56., 15.]), 'currentState': array([56.99115118, 14.12006974,  1.580176  ]), 'targetState': array([56, 15], dtype=int32), 'currentDistance': 1.325389724044265}
episode index:2393
target Thresh 75.99956476618928
target distance 72.0
model initialize at round 2393
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.40627222, 17.54690525]), 'dynamicTrap': False, 'previousTarget': array([63.72787848, 18.71202025]), 'currentState': array([44.61686433, 20.44161344,  6.09929991]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.47436379609118695
running average episode reward sum: 0.6892273261671231
{'scaleFactor': 20, 'currentTarget': array([116.,  10.]), 'dynamicTrap': False, 'previousTarget': array([116.,  10.]), 'currentState': array([115.52222716,   9.29050008,   0.23530961]), 'targetState': array([116,  10], dtype=int32), 'currentDistance': 0.8553695231541405}
episode index:2394
target Thresh 75.99956693692697
target distance 15.0
model initialize at round 2394
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  14.]), 'dynamicTrap': False, 'previousTarget': array([112.,  14.]), 'currentState': array([96.97049452,  4.0030021 ,  2.93919449]), 'targetState': array([112,  14], dtype=int32), 'currentDistance': 18.050651015101053}
done in step count: 14
reward sum = 0.8405004572968984
running average episode reward sum: 0.68929048822605
{'scaleFactor': 20, 'currentTarget': array([112.,  14.]), 'dynamicTrap': False, 'previousTarget': array([112.,  14.]), 'currentState': array([111.860238  ,  14.69133661,   6.19431389]), 'targetState': array([112,  14], dtype=int32), 'currentDistance': 0.7053224261292322}
episode index:2395
target Thresh 75.99956909683806
target distance 61.0
model initialize at round 2395
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.09425368,  8.05989408]), 'dynamicTrap': False, 'previousTarget': array([30.97585674,  8.98241918]), 'currentState': array([10.14329691,  6.66013555,  4.96674263]), 'targetState': array([72, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6644128300823817
running average episode reward sum: 0.6892801052301637
{'scaleFactor': 20, 'currentTarget': array([72., 11.]), 'dynamicTrap': False, 'previousTarget': array([72., 11.]), 'currentState': array([72.27053816, 10.44840993,  0.26319417]), 'targetState': array([72, 11], dtype=int32), 'currentDistance': 0.6143634876932853}
episode index:2396
target Thresh 75.99957124597654
target distance 20.0
model initialize at round 2396
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.36784126,  6.92306853]), 'dynamicTrap': False, 'previousTarget': array([25.38838649,  6.9223227 ]), 'currentState': array([44.94427851,  2.82879147,  4.30032707]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.689362335003416
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'dynamicTrap': False, 'previousTarget': array([25.,  7.]), 'currentState': array([25.89966626,  7.20496961,  1.3867987 ]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.9227198526797622}
episode index:2397
target Thresh 75.99957338439616
target distance 25.0
model initialize at round 2397
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.47951587,  7.98659944]), 'dynamicTrap': False, 'previousTarget': array([22.56953382,  7.57218647]), 'currentState': array([ 4.29615418, 16.31518621,  0.33931017]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7836646163171661
running average episode reward sum: 0.6894016603917871
{'scaleFactor': 20, 'currentTarget': array([29.,  5.]), 'dynamicTrap': False, 'previousTarget': array([29.,  5.]), 'currentState': array([28.96602633,  4.35944711,  4.3001069 ]), 'targetState': array([29,  5], dtype=int32), 'currentDistance': 0.6414532025584838}
episode index:2398
target Thresh 75.99957551215036
target distance 15.0
model initialize at round 2398
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'dynamicTrap': False, 'previousTarget': array([19., 18.]), 'currentState': array([ 5.54617467, 17.26859793,  0.6886643 ]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 13.473691590054845}
done in step count: 52
reward sum = 0.28510732002912814
running average episode reward sum: 0.6892331341973882
{'scaleFactor': 20, 'currentTarget': array([19., 18.]), 'dynamicTrap': False, 'previousTarget': array([19., 18.]), 'currentState': array([19.0844191 , 17.23820466,  3.75356467]), 'targetState': array([19, 18], dtype=int32), 'currentDistance': 0.7664585576461501}
episode index:2399
target Thresh 75.99957762929235
target distance 10.0
model initialize at round 2399
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 23.]), 'currentState': array([ 8.93269105, 13.0626871 ,  2.115713  ]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 11.094216000790778}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6893382371203897
{'scaleFactor': 20, 'currentTarget': array([ 4., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 23.]), 'currentState': array([ 4.17814856, 22.79438601,  1.64598957]), 'targetState': array([ 4, 23], dtype=int32), 'currentDistance': 0.272055178705836}
episode index:2400
target Thresh 75.99957973587503
target distance 7.0
model initialize at round 2400
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  14.]), 'dynamicTrap': False, 'previousTarget': array([113.,  14.]), 'currentState': array([1.05995449e+02, 2.09926586e+01, 5.84864616e-03]), 'targetState': array([113,  14], dtype=int32), 'currentDistance': 9.897525525499592}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6894472133023053
{'scaleFactor': 20, 'currentTarget': array([113.,  14.]), 'dynamicTrap': False, 'previousTarget': array([113.,  14.]), 'currentState': array([112.0897622 ,  14.9192633 ,   5.67254824]), 'targetState': array([113,  14], dtype=int32), 'currentDistance': 1.2936683748306057}
episode index:2401
target Thresh 75.9995818319511
target distance 18.0
model initialize at round 2401
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39., 19.]), 'dynamicTrap': False, 'previousTarget': array([39., 19.]), 'currentState': array([21.15134652, 11.10469169,  0.55064093]), 'targetState': array([39, 19], dtype=int32), 'currentDistance': 19.516924052006782}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6895292023357831
{'scaleFactor': 20, 'currentTarget': array([39., 19.]), 'dynamicTrap': False, 'previousTarget': array([39., 19.]), 'currentState': array([39.43067501, 19.38221682,  4.25193962]), 'targetState': array([39, 19], dtype=int32), 'currentDistance': 0.5758217276655853}
episode index:2402
target Thresh 75.99958391757295
target distance 12.0
model initialize at round 2402
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 15.]), 'currentState': array([12.24509592,  4.77862795,  2.43006217]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 11.067849200112313}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6896340508364345
{'scaleFactor': 20, 'currentTarget': array([ 8., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 15.]), 'currentState': array([ 8.88147746, 15.84156508,  1.39365977]), 'targetState': array([ 8, 15], dtype=int32), 'currentDistance': 1.2187018944709616}
episode index:2403
target Thresh 75.9995859927927
target distance 42.0
model initialize at round 2403
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.47752365, 19.3586984 ]), 'dynamicTrap': False, 'previousTarget': array([84.99433347, 18.52394444]), 'currentState': array([65.51725798, 20.61877557,  0.27938795]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7272820529207462
running average episode reward sum: 0.6896497114030253
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([106.09199828,  18.06044393,   1.08607448]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 0.9100113177282274}
episode index:2404
target Thresh 75.99958805766227
target distance 49.0
model initialize at round 2404
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.75019061,  7.87742704]), 'dynamicTrap': False, 'previousTarget': array([49.96262067,  8.77779873]), 'currentState': array([30.75983061,  8.4983191 ,  6.1908145 ]), 'targetState': array([79,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 38
reward sum = 0.5750473963308084
running average episode reward sum: 0.6896020597127666
{'scaleFactor': 20, 'currentTarget': array([79.,  7.]), 'dynamicTrap': False, 'previousTarget': array([79.,  7.]), 'currentState': array([78.65661544,  6.92143522,  0.6684869 ]), 'targetState': array([79,  7], dtype=int32), 'currentDistance': 0.3522575480333255}
episode index:2405
target Thresh 75.99959011223325
target distance 11.0
model initialize at round 2405
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([68.0491582 ,  8.1147992 ,  0.15620971]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 12.542396322275872}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6897028341467625
{'scaleFactor': 20, 'currentTarget': array([79.,  2.]), 'dynamicTrap': False, 'previousTarget': array([79.,  2.]), 'currentState': array([78.89617952,  1.15669681,  5.44354433]), 'targetState': array([79,  2], dtype=int32), 'currentDistance': 0.849669917240004}
episode index:2406
target Thresh 75.99959215655701
target distance 49.0
model initialize at round 2406
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.12497087,  4.99313864]), 'dynamicTrap': False, 'previousTarget': array([94.,  5.]), 'currentState': array([112.12497023,   4.98807957,   3.77224565]), 'targetState': array([65,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6529355492726685
running average episode reward sum: 0.689687558997251
{'scaleFactor': 20, 'currentTarget': array([65.,  5.]), 'dynamicTrap': False, 'previousTarget': array([65.,  5.]), 'currentState': array([64.86312628,  5.83862681,  5.82980936]), 'targetState': array([65,  5], dtype=int32), 'currentDistance': 0.849723096943166}
episode index:2407
target Thresh 75.99959419068468
target distance 17.0
model initialize at round 2407
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([9.16923061, 6.81889859, 2.03232609]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 16.78879709078571}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.689772962109901
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.96611139, 22.37867999,  2.03493061]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 1.037675165629459}
episode index:2408
target Thresh 75.99959621466708
target distance 40.0
model initialize at round 2408
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.47721678,  8.19457705]), 'dynamicTrap': False, 'previousTarget': array([93.70060934,  7.55239336]), 'currentState': array([74.92345398, 12.39581023,  1.82521074]), 'targetState': array([114,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.776990682679529
running average episode reward sum: 0.6898091670582488
{'scaleFactor': 20, 'currentTarget': array([114.,   4.]), 'dynamicTrap': False, 'previousTarget': array([114.,   4.]), 'currentState': array([113.67334474,   4.01826188,   4.71721743]), 'targetState': array([114,   4], dtype=int32), 'currentDistance': 0.32716533059185177}
episode index:2409
target Thresh 75.99959822855483
target distance 4.0
model initialize at round 2409
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,   4.]), 'dynamicTrap': False, 'previousTarget': array([105.,   4.]), 'currentState': array([100.88125715,   3.16057926,   2.11389415]), 'targetState': array([105,   4], dtype=int32), 'currentDistance': 4.203411694658573}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6899255528810463
{'scaleFactor': 20, 'currentTarget': array([105.,   4.]), 'dynamicTrap': False, 'previousTarget': array([105.,   4.]), 'currentState': array([104.88514826,   3.65900038,   4.83961414]), 'targetState': array([105,   4], dtype=int32), 'currentDistance': 0.3598217120564927}
episode index:2410
target Thresh 75.99960023239827
target distance 58.0
model initialize at round 2410
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.00076097, 14.33325271]), 'dynamicTrap': False, 'previousTarget': array([72.64973049, 13.27332698]), 'currentState': array([53.46500848, 18.61745331,  0.28085184]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.597350461636525
running average episode reward sum: 0.6898871559124671
{'scaleFactor': 20, 'currentTarget': array([111.,   6.]), 'dynamicTrap': False, 'previousTarget': array([111.,   6.]), 'currentState': array([110.56085998,   6.08943651,   0.14494135]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 0.44815493719016847}
episode index:2411
target Thresh 75.9996022262475
target distance 56.0
model initialize at round 2411
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.40157872,  2.86160256]), 'dynamicTrap': False, 'previousTarget': array([77.949174,  3.424941]), 'currentState': array([57.47471014,  1.15282849,  5.26536345]), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6252933982285029
running average episode reward sum: 0.6898603757475897
{'scaleFactor': 20, 'currentTarget': array([114.,   6.]), 'dynamicTrap': False, 'previousTarget': array([114.,   6.]), 'currentState': array([114.60543331,   5.59209732,   0.14365383]), 'targetState': array([114,   6], dtype=int32), 'currentDistance': 0.7300233445100159}
episode index:2412
target Thresh 75.99960421015237
target distance 8.0
model initialize at round 2412
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 19.]), 'currentState': array([12.56934827, 19.35474198,  3.47644675]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 7.57765630212781}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6899685935984611
{'scaleFactor': 20, 'currentTarget': array([ 5., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 19.]), 'currentState': array([ 4.53315074, 19.40558183,  4.39537084]), 'targetState': array([ 5, 19], dtype=int32), 'currentDistance': 0.6184212598268639}
episode index:2413
target Thresh 75.99960618416246
target distance 62.0
model initialize at round 2413
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.95516253, 13.20139876]), 'dynamicTrap': False, 'previousTarget': array([69.87373448, 14.24380873]), 'currentState': array([50.14269615, 10.46896836,  5.81532311]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5024264367452441
running average episode reward sum: 0.68989090422114
{'scaleFactor': 20, 'currentTarget': array([112.,  19.]), 'dynamicTrap': False, 'previousTarget': array([112.,  19.]), 'currentState': array([112.70660413,  18.83577736,   0.37085292]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 0.7254367427159828}
episode index:2414
target Thresh 75.99960814832716
target distance 19.0
model initialize at round 2414
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.28781111, 18.27298936]), 'dynamicTrap': False, 'previousTarget': array([50.14213562, 18.14213562]), 'currentState': array([36.16789936,  4.10866473,  0.56273007]), 'targetState': array([55, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7891315262111038
running average episode reward sum: 0.6899319976463947
{'scaleFactor': 20, 'currentTarget': array([55., 23.]), 'dynamicTrap': False, 'previousTarget': array([55., 23.]), 'currentState': array([54.86025372, 22.08481337,  5.36476258]), 'targetState': array([55, 23], dtype=int32), 'currentDistance': 0.9257945680307391}
episode index:2415
target Thresh 75.99961010269553
target distance 30.0
model initialize at round 2415
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.00014881, 20.92284775]), 'dynamicTrap': True, 'previousTarget': array([82., 21.]), 'currentState': array([102.       ,  21.       ,   2.3170466], dtype=float32), 'targetState': array([72, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.756441442279757
running average episode reward sum: 0.6899595263900343
{'scaleFactor': 20, 'currentTarget': array([72., 21.]), 'dynamicTrap': False, 'previousTarget': array([72., 21.]), 'currentState': array([71.08382297, 21.15521502,  4.46838839]), 'targetState': array([72, 21], dtype=int32), 'currentDistance': 0.9292319642952888}
episode index:2416
target Thresh 75.99961204731645
target distance 75.0
model initialize at round 2416
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.00660728,  4.53544475]), 'dynamicTrap': False, 'previousTarget': array([89.00710732,  4.5331438 ]), 'currentState': array([108.9995221 ,   4.00313182,   3.85381603]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.39423068858276106
running average episode reward sum: 0.6898371727128282
{'scaleFactor': 20, 'currentTarget': array([34.,  6.]), 'dynamicTrap': False, 'previousTarget': array([34.,  6.]), 'currentState': array([33.01847778,  5.68632131,  3.30155199]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 1.030427185889323}
episode index:2417
target Thresh 75.99961398223853
target distance 37.0
model initialize at round 2417
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.36085928,  9.5306609 ]), 'dynamicTrap': False, 'previousTarget': array([70.34859509,  8.71783336]), 'currentState': array([89.13688107,  6.54587244,  2.60236263]), 'targetState': array([53, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6114438136045857
running average episode reward sum: 0.6898047519687801
{'scaleFactor': 20, 'currentTarget': array([53., 12.]), 'dynamicTrap': False, 'previousTarget': array([53., 12.]), 'currentState': array([52.78577572, 12.26722544,  3.68926809]), 'targetState': array([53, 12], dtype=int32), 'currentDistance': 0.34249303229114675}
episode index:2418
target Thresh 75.99961590751015
target distance 41.0
model initialize at round 2418
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.52784521, 16.85465519]), 'dynamicTrap': False, 'previousTarget': array([44.97624702, 16.97445107]), 'currentState': array([26.56235318, 15.68029247,  5.4327308 ]), 'targetState': array([66, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6431018072968763
running average episode reward sum: 0.6897854452533309
{'scaleFactor': 20, 'currentTarget': array([66., 18.]), 'dynamicTrap': False, 'previousTarget': array([66., 18.]), 'currentState': array([65.63184428, 17.62849203,  5.58284226]), 'targetState': array([66, 18], dtype=int32), 'currentDistance': 0.523026582681643}
episode index:2419
target Thresh 75.99961782317943
target distance 43.0
model initialize at round 2419
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.05268185, 18.33427813]), 'dynamicTrap': False, 'previousTarget': array([90.04849798, 18.39196526]), 'currentState': array([110.0006744 ,  16.89289236,   3.7086854 ]), 'targetState': array([67, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.685222555868263
running average episode reward sum: 0.6897835597618494
{'scaleFactor': 20, 'currentTarget': array([67., 20.]), 'dynamicTrap': False, 'previousTarget': array([67., 20.]), 'currentState': array([66.18339253, 20.24399527,  5.605678  ]), 'targetState': array([67, 20], dtype=int32), 'currentDistance': 0.8522801514947799}
episode index:2420
target Thresh 75.99961972929427
target distance 38.0
model initialize at round 2420
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.36328089, 12.09349667]), 'dynamicTrap': False, 'previousTarget': array([63.92524322, 11.56172689]), 'currentState': array([80.35837247,  3.36557141,  3.87555873]), 'targetState': array([44, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.5723602025019813
running average episode reward sum: 0.6897350577555463
{'scaleFactor': 20, 'currentTarget': array([44., 21.]), 'dynamicTrap': False, 'previousTarget': array([44., 21.]), 'currentState': array([44.86174529, 21.31305937,  4.74432616]), 'targetState': array([44, 21], dtype=int32), 'currentDistance': 0.9168484729395179}
episode index:2421
target Thresh 75.99962162590232
target distance 30.0
model initialize at round 2421
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.29336536, 12.1874117 ]), 'dynamicTrap': False, 'previousTarget': array([33.02633404, 12.67544468]), 'currentState': array([50.21158782, 18.67592918,  2.72844493]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6125319500637839
running average episode reward sum: 0.689703181988539
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'dynamicTrap': False, 'previousTarget': array([22.,  9.]), 'currentState': array([21.92178574,  9.09028447,  4.05702605]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.11945189961838854}
episode index:2422
target Thresh 75.999623513051
target distance 31.0
model initialize at round 2422
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.5810559 ,  5.60192754]), 'dynamicTrap': False, 'previousTarget': array([71.83555733,  5.55942675]), 'currentState': array([53.79780141,  2.66546002,  0.21152204]), 'targetState': array([83,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 26
reward sum = 0.6821975750622864
running average episode reward sum: 0.689700084338136
{'scaleFactor': 20, 'currentTarget': array([83.,  7.]), 'dynamicTrap': False, 'previousTarget': array([83.,  7.]), 'currentState': array([82.63932961,  6.99137849,  1.11817908]), 'targetState': array([83,  7], dtype=int32), 'currentDistance': 0.3607734229589327}
episode index:2423
target Thresh 75.9996253907875
target distance 18.0
model initialize at round 2423
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'dynamicTrap': False, 'previousTarget': array([13., 18.]), 'currentState': array([29.09721499, 21.2734722 ,  2.50842369]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 16.426684099601196}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6897962248538496
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'dynamicTrap': False, 'previousTarget': array([13., 18.]), 'currentState': array([13.95070816, 18.51795284,  2.64665414]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 1.0826454467578654}
episode index:2424
target Thresh 75.99962725915874
target distance 55.0
model initialize at round 2424
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.34338829, 10.92869781]), 'dynamicTrap': False, 'previousTarget': array([45.46369226, 11.39949092]), 'currentState': array([27.87627006, 15.51468819,  0.184484  ]), 'targetState': array([81,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.596184385866874
running average episode reward sum: 0.6897576220336488
{'scaleFactor': 20, 'currentTarget': array([81.,  3.]), 'dynamicTrap': False, 'previousTarget': array([81.,  3.]), 'currentState': array([80.27167473,  2.81294933,  5.88043123]), 'targetState': array([81,  3], dtype=int32), 'currentDistance': 0.7519612047380266}
episode index:2425
target Thresh 75.99962911821144
target distance 63.0
model initialize at round 2425
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.69647576, 14.22893187]), 'dynamicTrap': False, 'previousTarget': array([62.15932542, 13.51945074]), 'currentState': array([80.56681955, 11.95529214,  3.49573839]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5237213148996678
running average episode reward sum: 0.6896891816762151
{'scaleFactor': 20, 'currentTarget': array([19., 19.]), 'dynamicTrap': False, 'previousTarget': array([19., 19.]), 'currentState': array([18.54575939, 18.78529003,  3.0954641 ]), 'targetState': array([19, 19], dtype=int32), 'currentDistance': 0.5024289977959409}
episode index:2426
target Thresh 75.99963096799209
target distance 38.0
model initialize at round 2426
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.9515899, 12.7301246]), 'dynamicTrap': False, 'previousTarget': array([81.49516929, 11.67372223]), 'currentState': array([97.53570722,  3.20128322,  3.0287503 ]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 31
reward sum = 0.6804322593883558
running average episode reward sum: 0.6896853675343577
{'scaleFactor': 20, 'currentTarget': array([61., 23.]), 'dynamicTrap': False, 'previousTarget': array([61., 23.]), 'currentState': array([61.54560912, 23.84395782,  2.26458827]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 1.0049647335380587}
episode index:2427
target Thresh 75.9996328085469
target distance 6.0
model initialize at round 2427
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,  16.]), 'dynamicTrap': False, 'previousTarget': array([102.,  16.]), 'currentState': array([108.10940636,  18.91348045,   4.60407368]), 'targetState': array([102,  16], dtype=int32), 'currentDistance': 6.768545953180196}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6897969452289482
{'scaleFactor': 20, 'currentTarget': array([102.,  16.]), 'dynamicTrap': False, 'previousTarget': array([102.,  16.]), 'currentState': array([101.32973271,  16.0406442 ,   3.71791204]), 'targetState': array([102,  16], dtype=int32), 'currentDistance': 0.6714984631582741}
episode index:2428
target Thresh 75.99963463992192
target distance 16.0
model initialize at round 2428
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.01646153, 18.99404927]), 'dynamicTrap': True, 'previousTarget': array([73., 19.]), 'currentState': array([57.       , 17.       ,  2.8914714], dtype=float32), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 16.140113768355405}
done in step count: 15
reward sum = 0.8303573546412885
running average episode reward sum: 0.6898548128326586
{'scaleFactor': 20, 'currentTarget': array([73., 19.]), 'dynamicTrap': False, 'previousTarget': array([73., 19.]), 'currentState': array([72.64832201, 18.84761496,  5.36391678]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 0.38327354361766763}
episode index:2429
target Thresh 75.99963646216291
target distance 44.0
model initialize at round 2429
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.05301284, 17.31793549]), 'dynamicTrap': False, 'previousTarget': array([35.8721051 , 17.25819376]), 'currentState': array([16.17728508, 15.09185237,  0.57902683]), 'targetState': array([60, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5227020604861062
running average episode reward sum: 0.6897860256917753
{'scaleFactor': 20, 'currentTarget': array([60., 20.]), 'dynamicTrap': False, 'previousTarget': array([60., 20.]), 'currentState': array([60.43183522, 19.35821184,  1.55443529]), 'targetState': array([60, 20], dtype=int32), 'currentDistance': 0.773546186391582}
episode index:2430
target Thresh 75.99963827531543
target distance 7.0
model initialize at round 2430
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([109.14314128,  15.4067811 ,   4.19158444]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 6.254008234547433}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6898934728428275
{'scaleFactor': 20, 'currentTarget': array([106.,  10.]), 'dynamicTrap': False, 'previousTarget': array([106.,  10.]), 'currentState': array([105.84681111,   9.47785411,   4.38228464]), 'targetState': array([106,  10], dtype=int32), 'currentDistance': 0.5441536238909161}
episode index:2431
target Thresh 75.99964007942482
target distance 18.0
model initialize at round 2431
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.27035724,  16.99961217]), 'dynamicTrap': False, 'previousTarget': array([115.80368799,  17.36442559]), 'currentState': array([101.81145595,   2.20576291,   1.962744  ]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6899706223289115
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([118.5983657 ,  19.16403715,   0.12783194]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 1.0280444535377566}
episode index:2432
target Thresh 75.99964187453618
target distance 14.0
model initialize at round 2432
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  19.]), 'dynamicTrap': False, 'previousTarget': array([106.,  19.]), 'currentState': array([93.8673557 , 19.70398475,  0.13836163]), 'targetState': array([106,  19], dtype=int32), 'currentDistance': 12.153051148169402}
done in step count: 11
reward sum = 0.8482594917423573
running average episode reward sum: 0.6900356814614282
{'scaleFactor': 20, 'currentTarget': array([106.,  19.]), 'dynamicTrap': False, 'previousTarget': array([106.,  19.]), 'currentState': array([105.00530475,  18.67878416,   5.51826002]), 'targetState': array([106,  19], dtype=int32), 'currentDistance': 1.0452742498749343}
episode index:2433
target Thresh 75.99964366069439
target distance 42.0
model initialize at round 2433
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.55943538, 10.41120917]), 'dynamicTrap': False, 'previousTarget': array([25.72787848, 10.71202025]), 'currentState': array([ 7.83225353, 13.70336597,  6.26040247]), 'targetState': array([48,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6595360832581096
running average episode reward sum: 0.6900231508130292
{'scaleFactor': 20, 'currentTarget': array([48.,  7.]), 'dynamicTrap': False, 'previousTarget': array([48.,  7.]), 'currentState': array([47.90867807,  6.23813297,  0.9180985 ]), 'targetState': array([48,  7], dtype=int32), 'currentDistance': 0.7673207032285196}
episode index:2434
target Thresh 75.99964543794408
target distance 18.0
model initialize at round 2434
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.21023506,  6.41135234]), 'dynamicTrap': False, 'previousTarget': array([22.27881227,  6.21295565]), 'currentState': array([10.29030253, 22.47109108,  0.08847582]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6369571676860415
running average episode reward sum: 0.6900013578014781
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([23.92526959,  4.003157  ,  0.11200306]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.0747970667545651}
episode index:2435
target Thresh 75.99964720632973
target distance 20.0
model initialize at round 2435
at step 0:
{'scaleFactor': 20, 'currentTarget': array([100.99952703,  19.13754442]), 'dynamicTrap': True, 'previousTarget': array([101.,  19.]), 'currentState': array([81.      , 19.      ,  3.828087], dtype=float32), 'targetState': array([101,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6674130022941788
running average episode reward sum: 0.6899920850775424
{'scaleFactor': 20, 'currentTarget': array([101.,  19.]), 'dynamicTrap': False, 'previousTarget': array([101.,  19.]), 'currentState': array([100.06714875,  18.68365976,   5.39012704]), 'targetState': array([101,  19], dtype=int32), 'currentDistance': 0.9850292349571688}
episode index:2436
target Thresh 75.99964896589549
target distance 11.0
model initialize at round 2436
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65., 23.]), 'dynamicTrap': False, 'previousTarget': array([65., 23.]), 'currentState': array([66.92159406, 12.02427339,  1.83771098]), 'targetState': array([65, 23], dtype=int32), 'currentDistance': 11.142670167304745}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.690076346944256
{'scaleFactor': 20, 'currentTarget': array([65., 23.]), 'dynamicTrap': False, 'previousTarget': array([65., 23.]), 'currentState': array([65.21443979, 22.63348381,  2.33203057]), 'targetState': array([65, 23], dtype=int32), 'currentDistance': 0.42463930337628025}
episode index:2437
target Thresh 75.9996507166854
target distance 62.0
model initialize at round 2437
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.62163169, 19.20499756]), 'dynamicTrap': False, 'previousTarget': array([58.63559701, 19.19956187]), 'currentState': array([38.98627094, 23.00665613,  2.58916216]), 'targetState': array([101,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.4118358283470639
running average episode reward sum: 0.6899622203984819
{'scaleFactor': 20, 'currentTarget': array([101.,  11.]), 'dynamicTrap': False, 'previousTarget': array([101.,  11.]), 'currentState': array([100.18733568,  11.35816148,   0.55644948]), 'targetState': array([101,  11], dtype=int32), 'currentDistance': 0.8880894946562313}
episode index:2438
target Thresh 75.99965245874318
target distance 5.0
model initialize at round 2438
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.97971326, 22.02512767]), 'dynamicTrap': True, 'previousTarget': array([53., 22.]), 'currentState': array([55.       , 17.       ,  5.4490128], dtype=float32), 'targetState': array([53, 22], dtype=int32), 'currentDistance': 5.4160379095055005}
done in step count: 22
reward sum = 0.6197829074792421
running average episode reward sum: 0.6899334465924469
{'scaleFactor': 20, 'currentTarget': array([53.25981248, 20.29722689]), 'dynamicTrap': True, 'previousTarget': array([53., 22.]), 'currentState': array([52.96423663, 19.44327315,  2.55677557]), 'targetState': array([53, 22], dtype=int32), 'currentDistance': 0.9036603776705663}
episode index:2439
target Thresh 75.99965419211243
target distance 21.0
model initialize at round 2439
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.58774022, 11.91290267]), 'dynamicTrap': False, 'previousTarget': array([88.6170994 , 11.87838597]), 'currentState': array([107.02675535,   4.16676032,   3.64288414]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6265574757971911
running average episode reward sum: 0.6899074728339243
{'scaleFactor': 20, 'currentTarget': array([86., 13.]), 'dynamicTrap': False, 'previousTarget': array([86., 13.]), 'currentState': array([86.75108126, 12.31905338,  3.54060784]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 1.0138103152687548}
episode index:2440
target Thresh 75.99965591683647
target distance 5.0
model initialize at round 2440
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.99724584, 15.00241192]), 'dynamicTrap': True, 'previousTarget': array([24., 15.]), 'currentState': array([19.       , 17.       ,  2.5172625], dtype=float32), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 5.381712000614561}
done in step count: 29
reward sum = 0.5409475138210277
running average episode reward sum: 0.6898464486802934
{'scaleFactor': 20, 'currentTarget': array([24., 15.]), 'dynamicTrap': False, 'previousTarget': array([24., 15.]), 'currentState': array([23.61135468, 15.0549109 ,  0.38221586]), 'targetState': array([24, 15], dtype=int32), 'currentDistance': 0.39250527378599426}
episode index:2441
target Thresh 75.99965763295842
target distance 16.0
model initialize at round 2441
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54., 23.]), 'dynamicTrap': False, 'previousTarget': array([54., 23.]), 'currentState': array([69.81972796, 14.42459288,  3.58845568]), 'targetState': array([54, 23], dtype=int32), 'currentDistance': 17.99448249017947}
done in step count: 12
reward sum = 0.8769700702221193
running average episode reward sum: 0.6899230758799421
{'scaleFactor': 20, 'currentTarget': array([54., 23.]), 'dynamicTrap': False, 'previousTarget': array([54., 23.]), 'currentState': array([54.79627188, 23.85193305,  1.72899315]), 'targetState': array([54, 23], dtype=int32), 'currentDistance': 1.1661212693545806}
episode index:2442
target Thresh 75.99965934052115
target distance 63.0
model initialize at round 2442
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.60528779,  5.33963538]), 'dynamicTrap': False, 'previousTarget': array([66.93730785,  6.41767398]), 'currentState': array([47.63567441,  6.44169768,  6.10971069]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5072897629405181
running average episode reward sum: 0.6898483180768559
{'scaleFactor': 20, 'currentTarget': array([110.,   3.]), 'dynamicTrap': False, 'previousTarget': array([110.,   3.]), 'currentState': array([110.52163258,   3.16948692,   1.10754923]), 'targetState': array([110,   3], dtype=int32), 'currentDistance': 0.5484764045311025}
episode index:2443
target Thresh 75.99966103956739
target distance 67.0
model initialize at round 2443
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.72946508,  3.77506121]), 'dynamicTrap': False, 'previousTarget': array([23.89172986,  4.07824043]), 'currentState': array([5.86135711, 1.48196597, 0.18677908]), 'targetState': array([71,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.663629893017377
running average episode reward sum: 0.6898375904070279
{'scaleFactor': 20, 'currentTarget': array([71.,  9.]), 'dynamicTrap': False, 'previousTarget': array([71.,  9.]), 'currentState': array([71.83182958,  8.05474264,  0.19064097]), 'targetState': array([71,  9], dtype=int32), 'currentDistance': 1.2591472992422312}
episode index:2444
target Thresh 75.9996627301396
target distance 21.0
model initialize at round 2444
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([46.37646489, 21.85383134,  2.07258928]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 19.39526796620593}
done in step count: 13
reward sum = 0.8578200229989679
running average episode reward sum: 0.6899062948784357
{'scaleFactor': 20, 'currentTarget': array([27., 21.]), 'dynamicTrap': False, 'previousTarget': array([27., 21.]), 'currentState': array([26.08540879, 21.49021169,  2.91606198]), 'targetState': array([27, 21], dtype=int32), 'currentDistance': 1.037682314302472}
episode index:2445
target Thresh 75.99966441228005
target distance 53.0
model initialize at round 2445
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.29507556,  6.17559033]), 'dynamicTrap': False, 'previousTarget': array([68.77598173,  5.98505385]), 'currentState': array([50.52267537,  3.16690444,  5.38236207]), 'targetState': array([102,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.49554346968697793
running average episode reward sum: 0.6898268333799927
{'scaleFactor': 20, 'currentTarget': array([102.,  11.]), 'dynamicTrap': False, 'previousTarget': array([102.,  11.]), 'currentState': array([102.3465469 ,  10.87303458,   1.15282653]), 'targetState': array([102,  11], dtype=int32), 'currentDistance': 0.3690731272202132}
episode index:2446
target Thresh 75.99966608603079
target distance 22.0
model initialize at round 2446
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.54607663, 11.05398147]), 'dynamicTrap': False, 'previousTarget': array([27.81660336, 11.29773591]), 'currentState': array([ 9.6860184 , 13.41577891,  0.14475482]), 'targetState': array([30, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6557041158053262
running average episode reward sum: 0.6898128886650051
{'scaleFactor': 20, 'currentTarget': array([30., 11.]), 'dynamicTrap': False, 'previousTarget': array([30., 11.]), 'currentState': array([29.50625301, 10.4440129 ,  0.97742607]), 'targetState': array([30, 11], dtype=int32), 'currentDistance': 0.7435776650522751}
episode index:2447
target Thresh 75.99966775143365
target distance 2.0
model initialize at round 2447
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.,  3.]), 'dynamicTrap': False, 'previousTarget': array([91.,  3.]), 'currentState': array([88.99881782,  2.0019198 ,  1.22567701]), 'targetState': array([91,  3], dtype=int32), 'currentDistance': 2.2362679187113974}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6899355141189818
{'scaleFactor': 20, 'currentTarget': array([91.,  3.]), 'dynamicTrap': False, 'previousTarget': array([91.,  3.]), 'currentState': array([90.6433758 ,  2.5444215 ,  5.67645735]), 'targetState': array([91,  3], dtype=int32), 'currentDistance': 0.5785607896269532}
episode index:2448
target Thresh 75.9996694085303
target distance 11.0
model initialize at round 2448
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.,  6.]), 'dynamicTrap': False, 'previousTarget': array([69.,  6.]), 'currentState': array([75.30766624, 15.04739045,  4.63036296]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 11.029139916141343}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6900230790683038
{'scaleFactor': 20, 'currentTarget': array([69.,  6.]), 'dynamicTrap': False, 'previousTarget': array([69.,  6.]), 'currentState': array([68.2278717 ,  5.86927031,  4.12806335]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 0.7831170854511332}
episode index:2449
target Thresh 75.99967105736214
target distance 16.0
model initialize at round 2449
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 21.]), 'dynamicTrap': False, 'previousTarget': array([37., 21.]), 'currentState': array([51.300614  , 16.6296915 ,  3.55575323]), 'targetState': array([37, 21], dtype=int32), 'currentDistance': 14.953499829050456}
done in step count: 9
reward sum = 0.9043820750088044
running average episode reward sum: 0.6901105725360346
{'scaleFactor': 20, 'currentTarget': array([38.45660887, 20.21283368]), 'dynamicTrap': True, 'previousTarget': array([37., 21.]), 'currentState': array([39.11981656, 20.66887774,  3.157584  ]), 'targetState': array([37, 21], dtype=int32), 'currentDistance': 0.8048730482868803}
episode index:2450
target Thresh 75.99967269797038
target distance 56.0
model initialize at round 2450
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.1904396 , 14.11443279]), 'dynamicTrap': False, 'previousTarget': array([69.25339257, 15.17356191]), 'currentState': array([88.83799382, 10.37630314,  3.60294199]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5593252828868802
running average episode reward sum: 0.6900572125647375
{'scaleFactor': 20, 'currentTarget': array([33., 21.]), 'dynamicTrap': False, 'previousTarget': array([33., 21.]), 'currentState': array([33.35308698, 21.05734989,  3.65791802]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 0.35771416559380237}
episode index:2451
target Thresh 75.99967433039606
target distance 21.0
model initialize at round 2451
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,   5.]), 'dynamicTrap': False, 'previousTarget': array([106.79898987,   4.82842712]), 'currentState': array([88.87719695,  1.57709966,  5.57730654]), 'targetState': array([108,   5], dtype=int32), 'currentDistance': 19.426730121333097}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6901446207468109
{'scaleFactor': 20, 'currentTarget': array([108.,   5.]), 'dynamicTrap': False, 'previousTarget': array([108.,   5.]), 'currentState': array([107.4020202 ,   5.79630224,   0.16475145]), 'targetState': array([108,   5], dtype=int32), 'currentDistance': 0.9958298551798055}
episode index:2452
target Thresh 75.99967595467999
target distance 4.0
model initialize at round 2452
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.,  6.]), 'dynamicTrap': False, 'previousTarget': array([44.,  6.]), 'currentState': array([47.20214804,  9.9844333 ,  5.19633055]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 5.1116984271595385}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6902588296254303
{'scaleFactor': 20, 'currentTarget': array([44.,  6.]), 'dynamicTrap': False, 'previousTarget': array([44.,  6.]), 'currentState': array([44.01977845,  6.75963434,  2.66446925]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 0.7598917804920449}
episode index:2453
target Thresh 75.99967757086276
target distance 22.0
model initialize at round 2453
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.44792644,  9.03111571]), 'dynamicTrap': False, 'previousTarget': array([92.0206292 ,  8.90815322]), 'currentState': array([111.4433099 ,   9.46081341,   2.94488871]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7470415284346269
running average episode reward sum: 0.6902819684595008
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([89.35351295,  8.97841375,  4.21329533]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 0.6468473329672262}
episode index:2454
target Thresh 75.99967917898479
target distance 16.0
model initialize at round 2454
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.,  2.]), 'dynamicTrap': False, 'previousTarget': array([59.,  2.]), 'currentState': array([63.76496341, 16.01586081,  4.5164589 ]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 14.803689765103769}
done in step count: 25
reward sum = 0.6348411874953775
running average episode reward sum: 0.6902593856566641
{'scaleFactor': 20, 'currentTarget': array([59.,  2.]), 'dynamicTrap': False, 'previousTarget': array([59.,  2.]), 'currentState': array([58.74037654,  1.91680679,  4.19280139]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 0.2726269431982424}
episode index:2455
target Thresh 75.99968077908629
target distance 17.0
model initialize at round 2455
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96., 19.]), 'dynamicTrap': False, 'previousTarget': array([96., 19.]), 'currentState': array([97.03815559,  2.00930275,  2.37073803]), 'targetState': array([96, 19], dtype=int32), 'currentDistance': 17.022384083908243}
done in step count: 17
reward sum = 0.7878479156990561
running average episode reward sum: 0.6902991204001667
{'scaleFactor': 20, 'currentTarget': array([96., 19.]), 'dynamicTrap': False, 'previousTarget': array([96., 19.]), 'currentState': array([96.02678208, 19.25774762,  0.22062458]), 'targetState': array([96, 19], dtype=int32), 'currentDistance': 0.25913532584390475}
episode index:2456
target Thresh 75.99968237120723
target distance 53.0
model initialize at round 2456
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.61147725,  8.94426624]), 'dynamicTrap': False, 'previousTarget': array([90.34676693,  9.29184282]), 'currentState': array([108.26699081,  12.6403164 ,   2.78832555]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 75
reward sum = 0.17820744900214902
running average episode reward sum: 0.6900906988814862
{'scaleFactor': 20, 'currentTarget': array([57.,  3.]), 'dynamicTrap': False, 'previousTarget': array([57.,  3.]), 'currentState': array([57.35307568,  3.63738133,  5.03282757]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 0.7286407849293528}
episode index:2457
target Thresh 75.99968395538744
target distance 3.0
model initialize at round 2457
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'dynamicTrap': False, 'previousTarget': array([19.,  3.]), 'currentState': array([18.68057246,  5.88673372,  4.49235201]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 2.9043528533006215}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6902086847647728
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'dynamicTrap': False, 'previousTarget': array([19.,  3.]), 'currentState': array([19.55926317,  3.93751339,  4.31567407]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 1.0916531744640667}
episode index:2458
target Thresh 75.99968553166653
target distance 21.0
model initialize at round 2458
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.0920904,  5.0352771]), 'dynamicTrap': False, 'previousTarget': array([73.02263725,  4.95130299]), 'currentState': array([93.08166413,  5.68098752,  2.53225344]), 'targetState': array([72,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 20
reward sum = 0.7907745618297192
running average episode reward sum: 0.6902495818274264
{'scaleFactor': 20, 'currentTarget': array([72.,  5.]), 'dynamicTrap': False, 'previousTarget': array([72.,  5.]), 'currentState': array([72.1261934 ,  5.94416514,  6.15597389]), 'targetState': array([72,  5], dtype=int32), 'currentDistance': 0.9525610690630477}
episode index:2459
target Thresh 75.99968710008388
target distance 56.0
model initialize at round 2459
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.94543598, 17.93051321]), 'dynamicTrap': False, 'previousTarget': array([46.02863735, 16.93010557]), 'currentState': array([66.88281465, 19.51194675,  1.90583955]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6609012751189897
running average episode reward sum: 0.6902376516214472
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([ 9.97290692, 15.5121984 ,  2.01542185]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.5129144518643917}
episode index:2460
target Thresh 75.99968866067873
target distance 8.0
model initialize at round 2460
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.89697233, 14.98373365]), 'dynamicTrap': True, 'previousTarget': array([28., 15.]), 'currentState': array([36.        ,  9.        ,  0.06615347], dtype=float32), 'targetState': array([28, 15], dtype=int32), 'currentDistance': 10.072940277756919}
done in step count: 9
reward sum = 0.8939112873836408
running average episode reward sum: 0.6903204121398391
{'scaleFactor': 20, 'currentTarget': array([28., 15.]), 'dynamicTrap': False, 'previousTarget': array([28., 15.]), 'currentState': array([28.5345015 , 14.15865591,  2.01705274]), 'targetState': array([28, 15], dtype=int32), 'currentDistance': 0.996770646036057}
episode index:2461
target Thresh 75.99969021349007
target distance 27.0
model initialize at round 2461
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.42915404, 10.54189414]), 'dynamicTrap': False, 'previousTarget': array([88.78406925, 10.06902678]), 'currentState': array([68.83146348, 14.53320031,  0.67028141]), 'targetState': array([96,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.649325888140442
running average episode reward sum: 0.6903037612365087
{'scaleFactor': 20, 'currentTarget': array([96.,  9.]), 'dynamicTrap': False, 'previousTarget': array([96.,  9.]), 'currentState': array([95.18758272,  8.48607869,  0.5550165 ]), 'targetState': array([96,  9], dtype=int32), 'currentDistance': 0.9613204185321195}
episode index:2462
target Thresh 75.99969175855674
target distance 38.0
model initialize at round 2462
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.19409633, 15.1021671 ]), 'dynamicTrap': False, 'previousTarget': array([52.99307839, 14.52613364]), 'currentState': array([34.19446589, 15.22374962,  1.76122444]), 'targetState': array([71, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5332074726992531
running average episode reward sum: 0.6902399787401478
{'scaleFactor': 20, 'currentTarget': array([71., 15.]), 'dynamicTrap': False, 'previousTarget': array([71., 15.]), 'currentState': array([70.56376497, 15.40618267,  0.76329879]), 'targetState': array([71, 15], dtype=int32), 'currentDistance': 0.5960581869059092}
episode index:2463
target Thresh 75.99969329591734
target distance 14.0
model initialize at round 2463
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  17.]), 'dynamicTrap': False, 'previousTarget': array([112.,  17.]), 'currentState': array([117.48570528,   3.60987319,   1.37973004]), 'targetState': array([112,  17], dtype=int32), 'currentDistance': 14.470261173766508}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6903343394202157
{'scaleFactor': 20, 'currentTarget': array([112.,  17.]), 'dynamicTrap': False, 'previousTarget': array([112.,  17.]), 'currentState': array([112.32476401,  16.84083219,   2.99790851]), 'targetState': array([112,  17], dtype=int32), 'currentDistance': 0.36167119176651985}
episode index:2464
target Thresh 75.99969482561035
target distance 40.0
model initialize at round 2464
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.40307904, 13.78438019]), 'dynamicTrap': False, 'previousTarget': array([40.56953382, 14.57218647]), 'currentState': array([23.81581018, 21.16770167,  5.94140922]), 'targetState': array([62,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6624245620681912
running average episode reward sum: 0.6903230169953265
{'scaleFactor': 20, 'currentTarget': array([62.,  6.]), 'dynamicTrap': False, 'previousTarget': array([62.,  6.]), 'currentState': array([62.56847723,  5.52546479,  5.41732367]), 'targetState': array([62,  6], dtype=int32), 'currentDistance': 0.7405065989944268}
episode index:2465
target Thresh 75.99969634767396
target distance 68.0
model initialize at round 2465
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.69819336, 10.02150639]), 'dynamicTrap': False, 'previousTarget': array([89.03451254, 10.82555956]), 'currentState': array([107.6794805 ,  10.88647132,   4.77538311]), 'targetState': array([41,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.38262634666946105
running average episode reward sum: 0.6901982413788116
{'scaleFactor': 20, 'currentTarget': array([41.,  8.]), 'dynamicTrap': False, 'previousTarget': array([41.,  8.]), 'currentState': array([40.85570423,  7.2159359 ,  0.39221576]), 'targetState': array([41,  8], dtype=int32), 'currentDistance': 0.7972313221286427}
episode index:2466
target Thresh 75.99969786214625
target distance 12.0
model initialize at round 2466
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.,  6.]), 'dynamicTrap': False, 'previousTarget': array([52.,  6.]), 'currentState': array([41.07448741, 13.98585782,  1.08274889]), 'targetState': array([52,  6], dtype=int32), 'currentDistance': 13.532950547080167}
done in step count: 14
reward sum = 0.8202259621699782
running average episode reward sum: 0.6902509481971298
{'scaleFactor': 20, 'currentTarget': array([52.,  6.]), 'dynamicTrap': False, 'previousTarget': array([52.,  6.]), 'currentState': array([51.69522589,  5.3162668 ,  5.32375333]), 'targetState': array([52,  6], dtype=int32), 'currentDistance': 0.7485842316422247}
episode index:2467
target Thresh 75.99969936906508
target distance 66.0
model initialize at round 2467
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.16352455,  4.53833024]), 'dynamicTrap': False, 'previousTarget': array([59.99082358,  4.60578253]), 'currentState': array([40.17368575,  3.90087846,  5.78961459]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5597313453904282
running average episode reward sum: 0.6901980634310008
{'scaleFactor': 20, 'currentTarget': array([106.,   6.]), 'dynamicTrap': False, 'previousTarget': array([106.,   6.]), 'currentState': array([105.81246045,   5.39711038,   0.46740818]), 'targetState': array([106,   6], dtype=int32), 'currentDistance': 0.6313849696100798}
episode index:2468
target Thresh 75.99970086846812
target distance 18.0
model initialize at round 2468
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.,  3.]), 'dynamicTrap': False, 'previousTarget': array([87.,  3.]), 'currentState': array([91.46842935, 21.29284727,  5.47003168]), 'targetState': array([87,  3], dtype=int32), 'currentDistance': 18.830696267223114}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6902775234586577
{'scaleFactor': 20, 'currentTarget': array([87.,  3.]), 'dynamicTrap': False, 'previousTarget': array([87.,  3.]), 'currentState': array([86.23831536,  2.51146016,  4.73970592]), 'targetState': array([87,  3], dtype=int32), 'currentDistance': 0.9048948413721103}
episode index:2469
target Thresh 75.99970236039286
target distance 3.0
model initialize at round 2469
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.55751251, 10.72300386]), 'dynamicTrap': True, 'previousTarget': array([36., 12.]), 'currentState': array([33.       , 10.       ,  1.1920474], dtype=float32), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 1.7171429200220047}
done in step count: 2
reward sum = 0.9701
running average episode reward sum: 0.6903908119106987
{'scaleFactor': 20, 'currentTarget': array([36., 12.]), 'dynamicTrap': False, 'previousTarget': array([36., 12.]), 'currentState': array([36.46072673, 12.14646066,  0.47654526]), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 0.48344580617992466}
episode index:2470
target Thresh 75.9997038448766
target distance 4.0
model initialize at round 2470
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([31.92817226, 22.07701926,  4.48396553]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 5.019589508376195}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6905040892025196
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([28.59664769, 18.43411133,  3.14787483]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.5925755038981153}
episode index:2471
target Thresh 75.99970532195644
target distance 16.0
model initialize at round 2471
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  18.]), 'dynamicTrap': False, 'previousTarget': array([106.,  18.]), 'currentState': array([104.51509908,   1.97712544,   0.97027212]), 'targetState': array([106,  18], dtype=int32), 'currentDistance': 16.091533176930763}
done in step count: 17
reward sum = 0.8149802914665677
running average episode reward sum: 0.6905544436532737
{'scaleFactor': 20, 'currentTarget': array([106.,  18.]), 'dynamicTrap': False, 'previousTarget': array([106.,  18.]), 'currentState': array([106.35871979,  18.44778708,   5.40879073]), 'targetState': array([106,  18], dtype=int32), 'currentDistance': 0.5737535711546768}
episode index:2472
target Thresh 75.99970679166931
target distance 6.0
model initialize at round 2472
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([108.34858182,   6.00683423,   1.65131348]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 5.65346997948878}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6906675631665559
{'scaleFactor': 20, 'currentTarget': array([111.,  11.]), 'dynamicTrap': False, 'previousTarget': array([111.,  11.]), 'currentState': array([110.23686603,  10.83018224,   0.41969615]), 'targetState': array([111,  11], dtype=int32), 'currentDistance': 0.7818001840702595}
episode index:2473
target Thresh 75.99970825405197
target distance 9.0
model initialize at round 2473
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([112.69330104,  12.95495551,   1.50048327]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 9.85578092287167}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6907651370488276
{'scaleFactor': 20, 'currentTarget': array([107.,  21.]), 'dynamicTrap': False, 'previousTarget': array([107.,  21.]), 'currentState': array([107.11206351,  21.19935011,   1.89322174]), 'targetState': array([107,  21], dtype=int32), 'currentDistance': 0.22868907806208955}
episode index:2474
target Thresh 75.99970970914094
target distance 62.0
model initialize at round 2474
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.34800359, 16.34852613]), 'dynamicTrap': False, 'previousTarget': array([69.43917548, 16.29697367]), 'currentState': array([51.96614505, 21.28244543,  6.24134764]), 'targetState': array([112,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5379629016391289
running average episode reward sum: 0.6907033987718944
{'scaleFactor': 20, 'currentTarget': array([112.,   6.]), 'dynamicTrap': False, 'previousTarget': array([112.,   6.]), 'currentState': array([112.68705545,   5.3058592 ,   0.68523552]), 'targetState': array([112,   6], dtype=int32), 'currentDistance': 0.9766660864664755}
episode index:2475
target Thresh 75.99971115697265
target distance 35.0
model initialize at round 2475
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.49421908, 11.61340007]), 'dynamicTrap': True, 'previousTarget': array([32.74850544, 10.96373059]), 'currentState': array([14.      ,  4.      ,  4.660611], dtype=float32), 'targetState': array([49, 17], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 34
reward sum = 0.6155987380327204
running average episode reward sum: 0.6906730657102065
{'scaleFactor': 20, 'currentTarget': array([49., 17.]), 'dynamicTrap': False, 'previousTarget': array([49., 17.]), 'currentState': array([49.13669008, 16.54487369,  1.39154338]), 'targetState': array([49, 17], dtype=int32), 'currentDistance': 0.4752095744433186}
episode index:2476
target Thresh 75.99971259758325
target distance 20.0
model initialize at round 2476
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.76494707,  4.1061053 ]), 'dynamicTrap': False, 'previousTarget': array([45.2384301 ,  4.79270645]), 'currentState': array([28.53614956, 12.33477112,  0.3443281 ]), 'targetState': array([47,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8138889090775164
running average episode reward sum: 0.6907228096921877
{'scaleFactor': 20, 'currentTarget': array([47.,  4.]), 'dynamicTrap': False, 'previousTarget': array([47.,  4.]), 'currentState': array([46.00357448,  3.54177235,  0.14207525]), 'targetState': array([47,  4], dtype=int32), 'currentDistance': 1.096738981920105}
episode index:2477
target Thresh 75.99971403100879
target distance 36.0
model initialize at round 2477
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.33512883, 17.0645504 ]), 'dynamicTrap': False, 'previousTarget': array([45.81107799, 16.79288928]), 'currentState': array([28.63264648,  9.97814412,  5.89473984]), 'targetState': array([63, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5298899958935208
running average episode reward sum: 0.690657905408976
{'scaleFactor': 20, 'currentTarget': array([63., 23.]), 'dynamicTrap': False, 'previousTarget': array([63., 23.]), 'currentState': array([62.72124718, 23.50431503,  0.72667487]), 'targetState': array([63, 23], dtype=int32), 'currentDistance': 0.5762263301117247}
episode index:2478
target Thresh 75.99971545728508
target distance 15.0
model initialize at round 2478
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.8330036 , 19.88648106]), 'dynamicTrap': True, 'previousTarget': array([91., 20.]), 'currentState': array([94.       ,  5.       ,  4.6321373], dtype=float32), 'targetState': array([91, 20], dtype=int32), 'currentDistance': 15.219631554604598}
done in step count: 12
reward sum = 0.8664848717161292
running average episode reward sum: 0.6907288319786845
{'scaleFactor': 20, 'currentTarget': array([91., 20.]), 'dynamicTrap': False, 'previousTarget': array([91., 20.]), 'currentState': array([91.36288843, 19.36660093,  1.86621336]), 'targetState': array([91, 20], dtype=int32), 'currentDistance': 0.7299879439079423}
episode index:2479
target Thresh 75.99971687644779
target distance 72.0
model initialize at round 2479
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.23970409, 13.7090349 ]), 'dynamicTrap': False, 'previousTarget': array([91.15444247, 13.51930531]), 'currentState': array([109.06372369,  16.35634428,   2.67303336]), 'targetState': array([39,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 56
reward sum = 0.5036968358623414
running average episode reward sum: 0.6906534158512182
{'scaleFactor': 20, 'currentTarget': array([38.58655703,  5.17782991]), 'dynamicTrap': True, 'previousTarget': array([39.,  7.]), 'currentState': array([39.29722969,  5.74153703,  0.88816145]), 'targetState': array([39,  7], dtype=int32), 'currentDistance': 0.9070950067248478}
episode index:2480
target Thresh 75.9997182885324
target distance 5.0
model initialize at round 2480
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([24.96923201, 11.97245203,  4.80164015]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 5.422200408465184}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6907583479890854
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([26.6862292 , 17.31957535,  0.71316105]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.44786216539452245}
episode index:2481
target Thresh 75.9997196935742
target distance 39.0
model initialize at round 2481
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.89477212,  3.93439153]), 'dynamicTrap': False, 'previousTarget': array([28.94108971,  4.46607002]), 'currentState': array([9.92135385, 4.96519837, 0.15791839]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6654892351044095
running average episode reward sum: 0.6907481670411061
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([47.18094445,  3.60733848,  4.87351864]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 1.0196626987498643}
episode index:2482
target Thresh 75.99972109160835
target distance 18.0
model initialize at round 2482
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4.35604135, 18.80618933]), 'dynamicTrap': False, 'previousTarget': array([ 5.21295565, 18.27881227]), 'currentState': array([19.36759773,  5.59054803,  2.37735033]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.69082338768386
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.26288532, 20.27905282,  0.18329664]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.3833786264303248}
episode index:2483
target Thresh 75.99972248266975
target distance 51.0
model initialize at round 2483
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.49656428, 21.63033699]), 'dynamicTrap': False, 'previousTarget': array([68.00384357, 22.6079185 ]), 'currentState': array([87.49509515, 21.38792551,  3.5735606 ]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6714549366258264
running average episode reward sum: 0.6908155904008253
{'scaleFactor': 20, 'currentTarget': array([37., 22.]), 'dynamicTrap': False, 'previousTarget': array([37., 22.]), 'currentState': array([36.90185448, 21.96196301,  3.88884093]), 'targetState': array([37, 22], dtype=int32), 'currentDistance': 0.10525852127991374}
episode index:2484
target Thresh 75.9997238667932
target distance 49.0
model initialize at round 2484
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.71597606, 10.88063934]), 'dynamicTrap': False, 'previousTarget': array([21.73865762, 11.77736202]), 'currentState': array([ 1.88930484, 13.50801597,  5.6483314 ]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6181669528616115
running average episode reward sum: 0.6907863555366244
{'scaleFactor': 20, 'currentTarget': array([51.,  7.]), 'dynamicTrap': False, 'previousTarget': array([51.,  7.]), 'currentState': array([50.97764617,  6.70241313,  0.22330179]), 'targetState': array([51,  7], dtype=int32), 'currentDistance': 0.2984252666796941}
episode index:2485
target Thresh 75.99972524401332
target distance 58.0
model initialize at round 2485
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.37991494, 11.91510887]), 'dynamicTrap': False, 'previousTarget': array([32.36293823, 12.00765644]), 'currentState': array([13.02880447,  6.8619392 ,  0.76648188]), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 51
reward sum = 0.504825959629534
running average episode reward sum: 0.6907115524811509
{'scaleFactor': 20, 'currentTarget': array([71., 22.]), 'dynamicTrap': False, 'previousTarget': array([71., 22.]), 'currentState': array([70.77327667, 21.0591992 ,  1.93119954]), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 0.9677342663939148}
episode index:2486
target Thresh 75.99972661436452
target distance 11.0
model initialize at round 2486
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'dynamicTrap': False, 'previousTarget': array([27., 11.]), 'currentState': array([30.5131675 , 20.60266646,  4.0908289 ]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 10.225142980561511}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6908123842450913
{'scaleFactor': 20, 'currentTarget': array([27., 11.]), 'dynamicTrap': False, 'previousTarget': array([27., 11.]), 'currentState': array([27.14955046, 11.27373176,  4.09344307]), 'targetState': array([27, 11], dtype=int32), 'currentDistance': 0.3119205316967319}
episode index:2487
target Thresh 75.99972797788107
target distance 14.0
model initialize at round 2487
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 19.]), 'dynamicTrap': False, 'previousTarget': array([37., 19.]), 'currentState': array([29.94396237,  3.6511159 ,  0.049802  ]), 'targetState': array([37, 19], dtype=int32), 'currentDistance': 16.893072848493237}
done in step count: 14
reward sum = 0.8226039176266947
running average episode reward sum: 0.690865355118637
{'scaleFactor': 20, 'currentTarget': array([37., 19.]), 'dynamicTrap': False, 'previousTarget': array([37., 19.]), 'currentState': array([37.43459167, 18.2127688 ,  0.29230176]), 'targetState': array([37, 19], dtype=int32), 'currentDistance': 0.8992234850611348}
episode index:2488
target Thresh 75.99972933459705
target distance 61.0
model initialize at round 2488
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.04159302,  6.7108178 ]), 'dynamicTrap': True, 'previousTarget': array([45.04286102,  6.69133515]), 'currentState': array([65.       ,  8.       ,  2.3206124], dtype=float32), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.3502020347020122
running average episode reward sum: 0.6907284875732708
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([3.72315715, 3.63299352, 4.648124  ]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.4597126487991364}
episode index:2489
target Thresh 75.99973068454638
target distance 66.0
model initialize at round 2489
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.61479938,  8.54415928]), 'dynamicTrap': False, 'previousTarget': array([49.00917642,  9.60578253]), 'currentState': array([69.58710128,  7.49194426,  4.07364607]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.47847163919834756
running average episode reward sum: 0.6906432438590641
{'scaleFactor': 20, 'currentTarget': array([ 3., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 11.]), 'currentState': array([ 3.55650771, 11.16904087,  5.38837289]), 'targetState': array([ 3, 11], dtype=int32), 'currentDistance': 0.5816146876872662}
episode index:2490
target Thresh 75.9997320277628
target distance 31.0
model initialize at round 2490
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.29194703,  5.65488034]), 'dynamicTrap': False, 'previousTarget': array([50.15144509,  6.3118031 ]), 'currentState': array([67.13882819, 12.34779231,  3.77127129]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6004638997478167
running average episode reward sum: 0.6906070417939852
{'scaleFactor': 20, 'currentTarget': array([38.,  2.]), 'dynamicTrap': False, 'previousTarget': array([38.,  2.]), 'currentState': array([37.95583688,  1.2831543 ,  5.06788201]), 'targetState': array([38,  2], dtype=int32), 'currentDistance': 0.7182048048498214}
episode index:2491
target Thresh 75.99973336427992
target distance 6.0
model initialize at round 2491
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93., 13.]), 'dynamicTrap': False, 'previousTarget': array([93., 13.]), 'currentState': array([86.75489846, 15.46936685,  1.04658806]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 6.7155838041334395}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6907115293574306
{'scaleFactor': 20, 'currentTarget': array([93., 13.]), 'dynamicTrap': False, 'previousTarget': array([93., 13.]), 'currentState': array([93.18959763, 12.30252322,  0.53696573]), 'targetState': array([93, 13], dtype=int32), 'currentDistance': 0.7227870524421075}
episode index:2492
target Thresh 75.99973469413112
target distance 9.0
model initialize at round 2492
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39., 18.]), 'dynamicTrap': False, 'previousTarget': array([39., 18.]), 'currentState': array([46.92172181, 22.03091164,  2.86370945]), 'targetState': array([39, 18], dtype=int32), 'currentDistance': 8.888302709202634}
done in step count: 6
reward sum = 0.93206534790699
running average episode reward sum: 0.6908083419601381
{'scaleFactor': 20, 'currentTarget': array([39.80982583, 19.68409671]), 'dynamicTrap': True, 'previousTarget': array([39., 18.]), 'currentState': array([39.44116085, 19.04603797,  4.53809571]), 'targetState': array([39, 18], dtype=int32), 'currentDistance': 0.7369076109638251}
episode index:2493
target Thresh 75.99973601734966
target distance 14.0
model initialize at round 2493
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,  17.]), 'dynamicTrap': False, 'previousTarget': array([118.,  17.]), 'currentState': array([105.64489658,  14.41900672,   0.562617  ]), 'targetState': array([118,  17], dtype=int32), 'currentDistance': 12.621810758459414}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6909050769264359
{'scaleFactor': 20, 'currentTarget': array([118.,  17.]), 'dynamicTrap': False, 'previousTarget': array([118.,  17.]), 'currentState': array([118.59566898,  17.76423018,   5.63951989]), 'targetState': array([118,  17], dtype=int32), 'currentDistance': 0.9689526860204831}
episode index:2494
target Thresh 75.99973733396861
target distance 47.0
model initialize at round 2494
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.33852425, 17.09212517]), 'dynamicTrap': False, 'previousTarget': array([73.04061834, 16.72599692]), 'currentState': array([91.2706978 , 18.73786434,  3.34954393]), 'targetState': array([46, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7246799425109264
running average episode reward sum: 0.6909186139467102
{'scaleFactor': 20, 'currentTarget': array([46., 15.]), 'dynamicTrap': False, 'previousTarget': array([46., 15.]), 'currentState': array([45.74641753, 14.44052285,  1.25196069]), 'targetState': array([46, 15], dtype=int32), 'currentDistance': 0.6142627710381016}
episode index:2495
target Thresh 75.99973864402091
target distance 47.0
model initialize at round 2495
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.96049795, 19.01327723]), 'dynamicTrap': False, 'previousTarget': array([83.88777826, 19.11572109]), 'currentState': array([64.08140188, 16.81747685,  5.19912523]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6076389508832947
running average episode reward sum: 0.6908852486970855
{'scaleFactor': 20, 'currentTarget': array([111.,  22.]), 'dynamicTrap': False, 'previousTarget': array([111.,  22.]), 'currentState': array([110.43863442,  22.0785835 ,   0.83077208]), 'targetState': array([111,  22], dtype=int32), 'currentDistance': 0.566839198824669}
episode index:2496
target Thresh 75.9997399475393
target distance 49.0
model initialize at round 2496
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.26460689, 10.24256979]), 'dynamicTrap': True, 'previousTarget': array([80.26134238, 10.22263798]), 'currentState': array([100.       ,   7.       ,   3.2570596], dtype=float32), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.4844694190874103
running average episode reward sum: 0.6908025831666049
{'scaleFactor': 20, 'currentTarget': array([51., 15.]), 'dynamicTrap': False, 'previousTarget': array([51., 15.]), 'currentState': array([51.80412493, 14.29462491,  2.12370846]), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 1.06965925687196}
episode index:2497
target Thresh 75.99974124455635
target distance 21.0
model initialize at round 2497
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.17335415, 16.5902836 ]), 'dynamicTrap': False, 'previousTarget': array([47.28336929, 17.28013989]), 'currentState': array([65.49345022,  6.58957001,  4.08288181]), 'targetState': array([44, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7679086044288275
running average episode reward sum: 0.6908334502687915
{'scaleFactor': 20, 'currentTarget': array([44., 19.]), 'dynamicTrap': False, 'previousTarget': array([44., 19.]), 'currentState': array([43.56627519, 18.9036086 ,  4.86565941]), 'targetState': array([44, 19], dtype=int32), 'currentDistance': 0.4443067775583346}
episode index:2498
target Thresh 75.99974253510452
target distance 74.0
model initialize at round 2498
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.28713488, 11.06141078]), 'dynamicTrap': False, 'previousTarget': array([65.21736847, 10.05933856]), 'currentState': array([86.02380522, 14.29619728,  0.30816877]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 82
reward sum = 0.21514341935012232
running average episode reward sum: 0.6906430981155628
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'dynamicTrap': False, 'previousTarget': array([11.,  2.]), 'currentState': array([10.79414855,  2.62858661,  2.84674701]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.6614347634373478}
episode index:2499
target Thresh 75.99974381921604
target distance 70.0
model initialize at round 2499
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.93003133, 13.32851823]), 'dynamicTrap': True, 'previousTarget': array([58.92693298, 13.29197717]), 'currentState': array([39.       , 15.       ,  3.6620727], dtype=float32), 'targetState': array([109,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.45014537635967405
running average episode reward sum: 0.6905468990268604
{'scaleFactor': 20, 'currentTarget': array([109.,   9.]), 'dynamicTrap': False, 'previousTarget': array([109.,   9.]), 'currentState': array([109.06874084,   8.17427957,   5.81813445]), 'targetState': array([109,   9], dtype=int32), 'currentDistance': 0.8285768085597808}
episode index:2500
target Thresh 75.99974509692302
target distance 33.0
model initialize at round 2500
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.68663114, 10.37118475]), 'dynamicTrap': False, 'previousTarget': array([23.79586847, 10.16513874]), 'currentState': array([ 6.13919885, 17.85401546,  5.9854607 ]), 'targetState': array([38,  5], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 75
reward sum = 0.15582100736444898
running average episode reward sum: 0.6903330941921293
{'scaleFactor': 20, 'currentTarget': array([38.,  5.]), 'dynamicTrap': False, 'previousTarget': array([38.,  5.]), 'currentState': array([37.84107818,  5.71677007,  4.62826778]), 'targetState': array([38,  5], dtype=int32), 'currentDistance': 0.734176738229087}
episode index:2501
target Thresh 75.99974636825743
target distance 4.0
model initialize at round 2501
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   7.]), 'dynamicTrap': False, 'previousTarget': array([106.,   7.]), 'currentState': array([109.92005985,   4.63364139,   2.62897056]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 4.5789215214885095}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6904489083031635
{'scaleFactor': 20, 'currentTarget': array([106.,   7.]), 'dynamicTrap': False, 'previousTarget': array([106.,   7.]), 'currentState': array([106.81674695,   6.72145763,   2.63559702]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 0.8629376745186483}
episode index:2502
target Thresh 75.99974763325102
target distance 40.0
model initialize at round 2502
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.49154772, 18.05872713]), 'dynamicTrap': False, 'previousTarget': array([48.29939066, 17.44760664]), 'currentState': array([66.24324735, 14.91701399,  2.89755964]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.3930951470408034
running average episode reward sum: 0.6903301093573935
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.3072246 , 20.17185957,  0.75196042]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.8832913047381908}
episode index:2503
target Thresh 75.99974889193544
target distance 13.0
model initialize at round 2503
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'dynamicTrap': False, 'previousTarget': array([13., 15.]), 'currentState': array([3.06704547, 3.19269922, 6.1211651 ]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 15.429709563013214}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6904155933692351
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'dynamicTrap': False, 'previousTarget': array([13., 15.]), 'currentState': array([12.5707829 , 14.53388131,  6.04742235]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.6336355044835467}
episode index:2504
target Thresh 75.99975014434213
target distance 19.0
model initialize at round 2504
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.30974129,  9.43912207]), 'dynamicTrap': False, 'previousTarget': array([24.90977806, 10.32014017]), 'currentState': array([ 9.43503383, 20.17430945,  4.75106508]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6402540659156462
running average episode reward sum: 0.6903955688073773
{'scaleFactor': 20, 'currentTarget': array([27.,  9.]), 'dynamicTrap': False, 'previousTarget': array([27.,  9.]), 'currentState': array([27.30240838,  8.05547678,  5.06787729]), 'targetState': array([27,  9], dtype=int32), 'currentDistance': 0.9917534707438449}
episode index:2505
target Thresh 75.99975139050242
target distance 42.0
model initialize at round 2505
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.49331767,  4.82287127]), 'dynamicTrap': False, 'previousTarget': array([21.949174,  4.424941]), 'currentState': array([3.5261866 , 3.67671397, 5.77140886]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6969376463685705
running average episode reward sum: 0.6903981793730442
{'scaleFactor': 20, 'currentTarget': array([44.,  6.]), 'dynamicTrap': False, 'previousTarget': array([44.,  6.]), 'currentState': array([44.48245762,  5.30596579,  0.23273712]), 'targetState': array([44,  6], dtype=int32), 'currentDistance': 0.8452507536369591}
episode index:2506
target Thresh 75.99975263044746
target distance 68.0
model initialize at round 2506
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.43936622,  7.31047864]), 'dynamicTrap': False, 'previousTarget': array([30.10949382,  7.90146133]), 'currentState': array([11.40803576,  1.16162014,  5.8268981 ]), 'targetState': array([79, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6328106020847182
running average episode reward sum: 0.6903752086601249
{'scaleFactor': 20, 'currentTarget': array([79., 23.]), 'dynamicTrap': False, 'previousTarget': array([79., 23.]), 'currentState': array([78.75816509, 22.05497174,  2.47923328]), 'targetState': array([79, 23], dtype=int32), 'currentDistance': 0.9754806674092074}
episode index:2507
target Thresh 75.99975386420826
target distance 9.0
model initialize at round 2507
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,  12.]), 'dynamicTrap': False, 'previousTarget': array([102.,  12.]), 'currentState': array([92.52888831, 14.36695547,  5.41325498]), 'targetState': array([102,  12], dtype=int32), 'currentDistance': 9.762399034153024}
done in step count: 10
reward sum = 0.8567184036597164
running average episode reward sum: 0.6904415336980035
{'scaleFactor': 20, 'currentTarget': array([101.35985352,  13.53883492]), 'dynamicTrap': True, 'previousTarget': array([102.,  12.]), 'currentState': array([101.030227  ,  14.51137037,   6.00524737]), 'targetState': array([102,  12], dtype=int32), 'currentDistance': 1.0268782010581827}
episode index:2508
target Thresh 75.99975509181564
target distance 17.0
model initialize at round 2508
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.07460819, 11.18322685]), 'dynamicTrap': True, 'previousTarget': array([59., 11.]), 'currentState': array([76.      ,  4.      ,  4.415999], dtype=float32), 'targetState': array([59, 11], dtype=int32), 'currentDistance': 18.38661566802693}
done in step count: 22
reward sum = 0.7387871830224018
running average episode reward sum: 0.6904608025897231
{'scaleFactor': 20, 'currentTarget': array([57.97640317,  9.3699522 ]), 'dynamicTrap': True, 'previousTarget': array([59., 11.]), 'currentState': array([57.90355303,  8.92417801,  0.67645882]), 'targetState': array([59, 11], dtype=int32), 'currentDistance': 0.4516876914143008}
episode index:2509
target Thresh 75.99975631330031
target distance 25.0
model initialize at round 2509
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.07609249, 10.25704006]), 'dynamicTrap': True, 'previousTarget': array([57.06369443, 10.40509555]), 'currentState': array([77.       , 12.       ,  1.6717064], dtype=float32), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6979370417913888
running average episode reward sum: 0.6904637811710783
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([52.14641623,  9.09945789,  1.90583127]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 0.9123671450705524}
episode index:2510
target Thresh 75.9997575286928
target distance 10.0
model initialize at round 2510
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.0586259 , 18.80913392]), 'dynamicTrap': True, 'previousTarget': array([93., 19.]), 'currentState': array([103.       ,  20.       ,   1.7698064], dtype=float32), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 10.012446308066208}
done in step count: 15
reward sum = 0.7553966089000049
running average episode reward sum: 0.69048964052103
{'scaleFactor': 20, 'currentTarget': array([93., 19.]), 'dynamicTrap': False, 'previousTarget': array([93., 19.]), 'currentState': array([93.8260231 , 18.80027968,  4.28788611]), 'targetState': array([93, 19], dtype=int32), 'currentDistance': 0.8498249072952808}
episode index:2511
target Thresh 75.99975873802349
target distance 37.0
model initialize at round 2511
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.82795826,  9.36551566]), 'dynamicTrap': False, 'previousTarget': array([19.45171669,  9.22665585]), 'currentState': array([37.32066037,  4.88951695,  2.19728756]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7213694749465447
running average episode reward sum: 0.6905019334487472
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.22577259, 13.99156028,  0.67491084]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 1.2580222130529892}
episode index:2512
target Thresh 75.99975994132261
target distance 18.0
model initialize at round 2512
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68., 23.]), 'dynamicTrap': False, 'previousTarget': array([68., 23.]), 'currentState': array([50.64763517, 13.51099983,  6.09694302]), 'targetState': array([68, 23], dtype=int32), 'currentDistance': 19.777403506131183}
done in step count: 17
reward sum = 0.7956713413665677
running average episode reward sum: 0.6905437835911737
{'scaleFactor': 20, 'currentTarget': array([68., 23.]), 'dynamicTrap': False, 'previousTarget': array([68., 23.]), 'currentState': array([68.08735615, 22.5849195 ,  0.79545509]), 'targetState': array([68, 23], dtype=int32), 'currentDistance': 0.42417321560186394}
episode index:2513
target Thresh 75.99976113862026
target distance 41.0
model initialize at round 2513
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.51028272, 14.13989364]), 'dynamicTrap': False, 'previousTarget': array([51.62981184, 14.83020719]), 'currentState': array([33.05022835,  9.52402058,  5.40202275]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.674956835004245
running average episode reward sum: 0.69053758353207
{'scaleFactor': 20, 'currentTarget': array([73., 19.]), 'dynamicTrap': False, 'previousTarget': array([73., 19.]), 'currentState': array([72.75244381, 19.72402479,  2.27947075]), 'targetState': array([73, 19], dtype=int32), 'currentDistance': 0.7651770786581649}
episode index:2514
target Thresh 75.99976232994636
target distance 40.0
model initialize at round 2514
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.11291503, 13.32713829]), 'dynamicTrap': False, 'previousTarget': array([66.40285  , 14.1492875]), 'currentState': array([46.52876253, 17.38435049,  5.43859124]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5575239589487876
running average episode reward sum: 0.6904846954109634
{'scaleFactor': 20, 'currentTarget': array([87.,  9.]), 'dynamicTrap': False, 'previousTarget': array([87.,  9.]), 'currentState': array([86.47604976,  8.61586227,  0.17146514]), 'targetState': array([87,  9], dtype=int32), 'currentDistance': 0.6496811900231545}
episode index:2515
target Thresh 75.9997635153307
target distance 36.0
model initialize at round 2515
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.06055447,  9.9931648 ]), 'dynamicTrap': False, 'previousTarget': array([83.5237412 , 10.66139084]), 'currentState': array([63.36565002, 13.47321347,  5.14413953]), 'targetState': array([100,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6985635656605841
running average episode reward sum: 0.6904879064086779
{'scaleFactor': 20, 'currentTarget': array([100.,   7.]), 'dynamicTrap': False, 'previousTarget': array([100.,   7.]), 'currentState': array([99.53857549,  7.41924055,  6.0995273 ]), 'targetState': array([100,   7], dtype=int32), 'currentDistance': 0.6234382229498788}
episode index:2516
target Thresh 75.9997646948029
target distance 45.0
model initialize at round 2516
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.49294689, 14.74976993]), 'dynamicTrap': False, 'previousTarget': array([28.00493644, 14.55566525]), 'currentState': array([46.48276919, 15.38773875,  3.40661716]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6736032228641496
running average episode reward sum: 0.6904811981514094
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([ 2.62494976, 13.92449631,  3.99413958]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.3825748165840765}
episode index:2517
target Thresh 75.99976586839247
target distance 14.0
model initialize at round 2517
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  5.]), 'dynamicTrap': False, 'previousTarget': array([34.,  5.]), 'currentState': array([46.2813414 ,  2.54293907,  2.56366253]), 'targetState': array([34,  5], dtype=int32), 'currentDistance': 12.524715369738795}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6905771410226388
{'scaleFactor': 20, 'currentTarget': array([34.,  5.]), 'dynamicTrap': False, 'previousTarget': array([34.,  5.]), 'currentState': array([34.27719617,  4.27142125,  1.50040016]), 'targetState': array([34,  5], dtype=int32), 'currentDistance': 0.7795285173056463}
episode index:2518
target Thresh 75.99976703612874
target distance 36.0
model initialize at round 2518
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.44919008,  4.01485107]), 'dynamicTrap': False, 'previousTarget': array([84.87767469,  3.79136948]), 'currentState': array([66.63821559,  6.75807727,  5.93254191]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6906276887783254
{'scaleFactor': 20, 'currentTarget': array([101.,   2.]), 'dynamicTrap': False, 'previousTarget': array([101.,   2.]), 'currentState': array([100.02061434,   1.15105397,   4.08898066]), 'targetState': array([101,   2], dtype=int32), 'currentDistance': 1.296111734339652}
episode index:2519
target Thresh 75.9997681980409
target distance 19.0
model initialize at round 2519
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.60587508,  4.12860607]), 'dynamicTrap': True, 'previousTarget': array([67.69147429,  3.97927459]), 'currentState': array([85.      , 14.      ,  5.360923], dtype=float32), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8587458127689782
running average episode reward sum: 0.6906944023195916
{'scaleFactor': 20, 'currentTarget': array([66.,  3.]), 'dynamicTrap': False, 'previousTarget': array([66.,  3.]), 'currentState': array([66.29921979,  3.57303285,  4.65026346]), 'targetState': array([66,  3], dtype=int32), 'currentDistance': 0.6464511806079221}
episode index:2520
target Thresh 75.99976935415798
target distance 54.0
model initialize at round 2520
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.6117901 ,  9.21398868]), 'dynamicTrap': False, 'previousTarget': array([49.44447274, 10.31892323]), 'currentState': array([30.03785792, 13.32022525,  5.57699049]), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5640207291103401
running average episode reward sum: 0.6906441549283939
{'scaleFactor': 20, 'currentTarget': array([84.,  2.]), 'dynamicTrap': False, 'previousTarget': array([84.,  2.]), 'currentState': array([83.04916772,  1.2349304 ,  2.40834389]), 'targetState': array([84,  2], dtype=int32), 'currentDistance': 1.220415308647373}
episode index:2521
target Thresh 75.99977050450893
target distance 21.0
model initialize at round 2521
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.81913488,  5.00114956]), 'dynamicTrap': False, 'previousTarget': array([43.,  5.]), 'currentState': array([62.81911518,  5.02921718,  2.99890317]), 'targetState': array([42,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6907182536072481
{'scaleFactor': 20, 'currentTarget': array([42.,  5.]), 'dynamicTrap': False, 'previousTarget': array([42.,  5.]), 'currentState': array([42.18858344,  4.5905252 ,  3.45991172]), 'targetState': array([42,  5], dtype=int32), 'currentDistance': 0.45081406729679574}
episode index:2522
target Thresh 75.99977164912245
target distance 51.0
model initialize at round 2522
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.0856121 , 20.43457744]), 'dynamicTrap': False, 'previousTarget': array([91.03451254, 19.82555956]), 'currentState': array([110.02044882,  22.04773736,   3.27126551]), 'targetState': array([60, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.639016896020741
running average episode reward sum: 0.6906977615907652
{'scaleFactor': 20, 'currentTarget': array([60., 18.]), 'dynamicTrap': False, 'previousTarget': array([60., 18.]), 'currentState': array([60.62581245, 18.99707239,  3.22460098]), 'targetState': array([60, 18], dtype=int32), 'currentDistance': 1.177197756631434}
episode index:2523
target Thresh 75.99977278802722
target distance 13.0
model initialize at round 2523
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([6.55943616, 3.82803855, 1.409841  ]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 11.68974774152514}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6907933905869286
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([ 9.92195241, 15.05825532,  1.10111558]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.09739152449867929}
episode index:2524
target Thresh 75.99977392125164
target distance 62.0
model initialize at round 2524
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.15893193,  5.57197937]), 'dynamicTrap': False, 'previousTarget': array([46.12626552,  5.24380873]), 'currentState': array([64.03845142,  3.38001564,  3.02808443]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.43204966457005567
running average episode reward sum: 0.6903487002680545
{'scaleFactor': 20, 'currentTarget': array([5.61550781, 7.88510969]), 'dynamicTrap': True, 'previousTarget': array([5.62042238, 7.91094203]), 'currentState': array([25.26555377,  4.16010046,  3.76187111]), 'targetState': array([ 4, 10], dtype=int32), 'currentDistance': 20.0}
episode index:2525
target Thresh 75.99977504882412
target distance 57.0
model initialize at round 2525
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.94588532,  9.38744764]), 'dynamicTrap': False, 'previousTarget': array([50.95093522,  9.40006563]), 'currentState': array([30.99541227,  7.98081153,  0.35110211]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 60
reward sum = 0.37739210426603453
running average episode reward sum: 0.690224806128703
{'scaleFactor': 20, 'currentTarget': array([88., 12.]), 'dynamicTrap': False, 'previousTarget': array([88., 12.]), 'currentState': array([87.82276205, 11.64376379,  1.88284174]), 'targetState': array([88, 12], dtype=int32), 'currentDistance': 0.39789135055062286}
episode index:2526
target Thresh 75.99977617077278
target distance 18.0
model initialize at round 2526
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.86656079, 19.76136644]), 'dynamicTrap': False, 'previousTarget': array([48.64100589, 20.09400392]), 'currentState': array([30.57029749,  9.71949034,  1.66535258]), 'targetState': array([50, 21], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 17
reward sum = 0.806013922630588
running average episode reward sum: 0.6902706269108565
{'scaleFactor': 20, 'currentTarget': array([50., 21.]), 'dynamicTrap': False, 'previousTarget': array([50., 21.]), 'currentState': array([49.80486791, 20.93059948,  0.91158528]), 'targetState': array([50, 21], dtype=int32), 'currentDistance': 0.20710616587608313}
episode index:2527
target Thresh 75.99977728712571
target distance 12.0
model initialize at round 2527
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.2349846 ,  7.13615366]), 'dynamicTrap': True, 'previousTarget': array([84.,  7.]), 'currentState': array([72.      , 10.      ,  5.369444], dtype=float32), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 10.628100755388237}
done in step count: 14
reward sum = 0.7941406345851075
running average episode reward sum: 0.6903117147303479
{'scaleFactor': 20, 'currentTarget': array([84.,  7.]), 'dynamicTrap': False, 'previousTarget': array([84.,  7.]), 'currentState': array([84.06439451,  6.44822192,  5.03450435]), 'targetState': array([84,  7], dtype=int32), 'currentDistance': 0.5555229080073393}
episode index:2528
target Thresh 75.9997783979108
target distance 38.0
model initialize at round 2528
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.86438468,  8.05651131]), 'dynamicTrap': False, 'previousTarget': array([52.75525931,  8.11925147]), 'currentState': array([33.12271874,  4.85235367,  5.45028816]), 'targetState': array([71, 11], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 32
reward sum = 0.6967349804857735
running average episode reward sum: 0.6903142545744585
{'scaleFactor': 20, 'currentTarget': array([71., 11.]), 'dynamicTrap': False, 'previousTarget': array([71., 11.]), 'currentState': array([71.23574754, 10.00189659,  1.70304418]), 'targetState': array([71, 11], dtype=int32), 'currentDistance': 1.0255668311537758}
episode index:2529
target Thresh 75.99977950315584
target distance 44.0
model initialize at round 2529
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.10248724, 17.25135158]), 'dynamicTrap': False, 'previousTarget': array([28.91786413, 17.81071492]), 'currentState': array([10.24504763, 14.86763765,  0.27196472]), 'targetState': array([53, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6056027587960956
running average episode reward sum: 0.690280771769803
{'scaleFactor': 20, 'currentTarget': array([53., 20.]), 'dynamicTrap': False, 'previousTarget': array([53., 20.]), 'currentState': array([52.00250173, 20.0203103 ,  3.37691284]), 'targetState': array([53, 20], dtype=int32), 'currentDistance': 0.9977050200659291}
episode index:2530
target Thresh 75.99978060288844
target distance 59.0
model initialize at round 2530
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.99729856,  5.32871031]), 'dynamicTrap': True, 'previousTarget': array([40.99712788,  5.33893437]), 'currentState': array([21.       ,  5.       ,  2.9664896], dtype=float32), 'targetState': array([80,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.33730083812439493
running average episode reward sum: 0.6901413091330406
{'scaleFactor': 20, 'currentTarget': array([81.57688548,  6.73338695]), 'dynamicTrap': True, 'previousTarget': array([80.,  6.]), 'currentState': array([81.76254627,  7.45729162,  2.88238968]), 'targetState': array([80,  6], dtype=int32), 'currentDistance': 0.7473338620242198}
episode index:2531
target Thresh 75.9997816971361
target distance 62.0
model initialize at round 2531
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.54666579, 11.53469115]), 'dynamicTrap': False, 'previousTarget': array([71.09299973, 11.0735161 ]), 'currentState': array([89.42274784, 13.75760866,  1.81484389]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.4981643377403281
running average episode reward sum: 0.690065488844181
{'scaleFactor': 20, 'currentTarget': array([29.,  7.]), 'dynamicTrap': False, 'previousTarget': array([29.,  7.]), 'currentState': array([28.86066648,  7.88537922,  5.41385413]), 'targetState': array([29,  7], dtype=int32), 'currentDistance': 0.89627573209433}
episode index:2532
target Thresh 75.99978278592617
target distance 8.0
model initialize at round 2532
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.94828719, 15.80708856]), 'dynamicTrap': True, 'previousTarget': array([73., 16.]), 'currentState': array([65.       , 20.       ,  1.3992572], dtype=float32), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 8.98642173533347}
done in step count: 6
reward sum = 0.931480149401
running average episode reward sum: 0.6901607966454272
{'scaleFactor': 20, 'currentTarget': array([73., 16.]), 'dynamicTrap': False, 'previousTarget': array([73., 16.]), 'currentState': array([73.01622657, 16.74725457,  4.67613007]), 'targetState': array([73, 16], dtype=int32), 'currentDistance': 0.7474307270081358}
episode index:2533
target Thresh 75.99978386928588
target distance 42.0
model initialize at round 2533
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.25339183, 18.75646379]), 'dynamicTrap': False, 'previousTarget': array([47.00566653, 18.47605556]), 'currentState': array([65.25194611, 18.51599187,  3.35216105]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6902016228044635
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'dynamicTrap': False, 'previousTarget': array([25., 19.]), 'currentState': array([25.11892987, 19.20240902,  1.88894095]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 0.2347631246620464}
episode index:2534
target Thresh 75.99978494724232
target distance 2.0
model initialize at round 2534
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.,  4.]), 'dynamicTrap': False, 'previousTarget': array([71.,  4.]), 'currentState': array([70.58112376,  6.73720794,  5.81742675]), 'targetState': array([71,  4], dtype=int32), 'currentDistance': 2.7690728764798953}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6903159811386629
{'scaleFactor': 20, 'currentTarget': array([71.,  4.]), 'dynamicTrap': False, 'previousTarget': array([71.,  4.]), 'currentState': array([70.06013425,  3.26388672,  4.51002163]), 'targetState': array([71,  4], dtype=int32), 'currentDistance': 1.1938217583291815}
episode index:2535
target Thresh 75.99978601982242
target distance 35.0
model initialize at round 2535
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.07036866, 15.67624425]), 'dynamicTrap': True, 'previousTarget': array([61.07306702, 15.70802283]), 'currentState': array([81.        , 14.        ,  0.10540006], dtype=float32), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.46651725705746877
running average episode reward sum: 0.6902277324304289
{'scaleFactor': 20, 'currentTarget': array([46., 17.]), 'dynamicTrap': False, 'previousTarget': array([46., 17.]), 'currentState': array([45.94315816, 17.01756176,  2.22349857]), 'targetState': array([46, 17], dtype=int32), 'currentDistance': 0.059492943691804065}
episode index:2536
target Thresh 75.99978708705301
target distance 8.0
model initialize at round 2536
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58., 10.]), 'dynamicTrap': False, 'previousTarget': array([58., 10.]), 'currentState': array([55.66151202,  1.73210961,  0.85014218]), 'targetState': array([58, 10], dtype=int32), 'currentDistance': 8.59223705051889}
done in step count: 7
reward sum = 0.90362883382892
running average episode reward sum: 0.6903118479611339
{'scaleFactor': 20, 'currentTarget': array([57.15865665,  9.1838454 ]), 'dynamicTrap': True, 'previousTarget': array([58., 10.]), 'currentState': array([56.69956734,  9.70253142,  1.64456534]), 'targetState': array([58, 10], dtype=int32), 'currentDistance': 0.6926746548795778}
episode index:2537
target Thresh 75.99978814896076
target distance 23.0
model initialize at round 2537
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.69220615,  7.88880754]), 'dynamicTrap': False, 'previousTarget': array([78.04268443,  7.37089005]), 'currentState': array([60.12546359, 17.44964201,  0.50145018]), 'targetState': array([83,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.791241523214333
running average episode reward sum: 0.6903516153666709
{'scaleFactor': 20, 'currentTarget': array([83.,  5.]), 'dynamicTrap': False, 'previousTarget': array([83.,  5.]), 'currentState': array([82.25535419,  4.14443326,  4.71064354]), 'targetState': array([83,  5], dtype=int32), 'currentDistance': 1.1342362301167337}
episode index:2538
target Thresh 75.99978920557223
target distance 19.0
model initialize at round 2538
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.98259507,  4.71389308]), 'dynamicTrap': False, 'previousTarget': array([12.97927459,  4.69147429]), 'currentState': array([22.92999572, 22.0646631 ,  4.59697928]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6904083944710756
{'scaleFactor': 20, 'currentTarget': array([12.,  3.]), 'dynamicTrap': False, 'previousTarget': array([12.,  3.]), 'currentState': array([12.69395869,  2.63052278,  6.21158538]), 'targetState': array([12,  3], dtype=int32), 'currentDistance': 0.7861883214083881}
episode index:2539
target Thresh 75.99979025691383
target distance 58.0
model initialize at round 2539
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.13930346, 13.66454297]), 'dynamicTrap': False, 'previousTarget': array([97.72014764, 12.68142004]), 'currentState': array([116.26481133,  19.51389752,   3.03287911]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.47761056003615654
running average episode reward sum: 0.6903246157961012
{'scaleFactor': 20, 'currentTarget': array([59.,  2.]), 'dynamicTrap': False, 'previousTarget': array([59.,  2.]), 'currentState': array([58.85412103,  1.97590355,  3.26442781]), 'targetState': array([59,  2], dtype=int32), 'currentDistance': 0.14785571456748658}
episode index:2540
target Thresh 75.99979130301183
target distance 9.0
model initialize at round 2540
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 13.]), 'dynamicTrap': False, 'previousTarget': array([86., 13.]), 'currentState': array([90.12483391,  5.53433673,  2.41730422]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 8.52938350111288}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6904234570135765
{'scaleFactor': 20, 'currentTarget': array([86., 13.]), 'dynamicTrap': False, 'previousTarget': array([86., 13.]), 'currentState': array([86.81074387, 13.09799477,  2.20057505]), 'targetState': array([86, 13], dtype=int32), 'currentDistance': 0.8166447229484654}
episode index:2541
target Thresh 75.9997923438924
target distance 33.0
model initialize at round 2541
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.40579159, 11.36787213]), 'dynamicTrap': False, 'previousTarget': array([77.14048809, 12.19985209]), 'currentState': array([57.99267662, 16.17733855,  5.17427611]), 'targetState': array([91,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5504406639731318
running average episode reward sum: 0.6903683890383443
{'scaleFactor': 20, 'currentTarget': array([91.,  8.]), 'dynamicTrap': False, 'previousTarget': array([91.,  8.]), 'currentState': array([91.55863953,  7.95043309,  4.38128339]), 'targetState': array([91,  8], dtype=int32), 'currentDistance': 0.5608342031979402}
episode index:2542
target Thresh 75.99979337958156
target distance 13.0
model initialize at round 2542
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.,  2.]), 'dynamicTrap': False, 'previousTarget': array([54.,  2.]), 'currentState': array([66.69952775,  2.5535333 ,  3.51242197]), 'targetState': array([54,  2], dtype=int32), 'currentDistance': 12.711585430114148}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6904561392776071
{'scaleFactor': 20, 'currentTarget': array([54.,  2.]), 'dynamicTrap': False, 'previousTarget': array([54.,  2.]), 'currentState': array([54.35992139,  1.7364776 ,  2.13720796]), 'targetState': array([54,  2], dtype=int32), 'currentDistance': 0.4460801077423095}
episode index:2543
target Thresh 75.9997944101052
target distance 72.0
model initialize at round 2543
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.95274716,  9.37400172]), 'dynamicTrap': True, 'previousTarget': array([57.95194842,  9.38555197]), 'currentState': array([38.        ,  8.        ,  0.42346507], dtype=float32), 'targetState': array([110,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.06663284475330356
running average episode reward sum: 0.6902109257184388
{'scaleFactor': 20, 'currentTarget': array([110.,  13.]), 'dynamicTrap': False, 'previousTarget': array([110.,  13.]), 'currentState': array([110.87923067,  12.36561935,   5.66112661]), 'targetState': array([110,  13], dtype=int32), 'currentDistance': 1.0841980363921009}
episode index:2544
target Thresh 75.99979543548908
target distance 11.0
model initialize at round 2544
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.98996729,  4.00668489]), 'dynamicTrap': True, 'previousTarget': array([67.,  4.]), 'currentState': array([56.      ,  6.      ,  2.649973], dtype=float32), 'targetState': array([67,  4], dtype=int32), 'currentDistance': 11.169274197559378}
done in step count: 18
reward sum = 0.7572584558780076
running average episode reward sum: 0.6902372705240025
{'scaleFactor': 20, 'currentTarget': array([67.,  4.]), 'dynamicTrap': False, 'previousTarget': array([67.,  4.]), 'currentState': array([67.57283126,  4.792263  ,  3.05301854]), 'targetState': array([67,  4], dtype=int32), 'currentDistance': 0.977658585822036}
episode index:2545
target Thresh 75.99979645575883
target distance 58.0
model initialize at round 2545
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.03124387,  4.89101452]), 'dynamicTrap': False, 'previousTarget': array([69.,  4.]), 'currentState': array([88.02545699,  5.37209839,  3.19837332]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5248078304349001
running average episode reward sum: 0.6901722943102989
{'scaleFactor': 20, 'currentTarget': array([31.,  4.]), 'dynamicTrap': False, 'previousTarget': array([31.,  4.]), 'currentState': array([30.45127966,  4.38811829,  3.62309917]), 'targetState': array([31,  4], dtype=int32), 'currentDistance': 0.6721084859727188}
episode index:2546
target Thresh 75.99979747093997
target distance 45.0
model initialize at round 2546
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.11763077,  5.39172003]), 'dynamicTrap': False, 'previousTarget': array([91.99506356,  4.44433475]), 'currentState': array([72.12010869,  5.70653851,  0.53802967]), 'targetState': array([117,   5], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6478770835320988
running average episode reward sum: 0.6901556884167858
{'scaleFactor': 20, 'currentTarget': array([117.,   5.]), 'dynamicTrap': False, 'previousTarget': array([117.,   5.]), 'currentState': array([116.91953473,   5.46407163,   5.67728169]), 'targetState': array([117,   5], dtype=int32), 'currentDistance': 0.47099589528978286}
episode index:2547
target Thresh 75.99979848105787
target distance 3.0
model initialize at round 2547
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([107.87715883,  10.04779378,   5.00758684]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 4.191311233848263}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6902694813177211
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([105.95616424,   7.61969354,   4.55606318]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 1.1394165778193446}
episode index:2548
target Thresh 75.99979948613777
target distance 67.0
model initialize at round 2548
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.80849   , 17.52110619]), 'dynamicTrap': False, 'previousTarget': array([51.54699538, 16.35450636]), 'currentState': array([71.15595083, 22.58823154,  2.48291859]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 56
reward sum = 0.4447797764127459
running average episode reward sum: 0.690173173077272
{'scaleFactor': 20, 'currentTarget': array([4., 5.]), 'dynamicTrap': False, 'previousTarget': array([4., 5.]), 'currentState': array([3.33341773, 5.57372288, 3.39090639]), 'targetState': array([4, 5], dtype=int32), 'currentDistance': 0.8794827288998492}
episode index:2549
target Thresh 75.99980048620485
target distance 23.0
model initialize at round 2549
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.31200386, 10.61095194]), 'dynamicTrap': False, 'previousTarget': array([71.24989998, 10.67383477]), 'currentState': array([87.17880442, 22.7865275 ,  4.3996396 ]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6902232647496328
{'scaleFactor': 20, 'currentTarget': array([64.,  5.]), 'dynamicTrap': False, 'previousTarget': array([64.,  5.]), 'currentState': array([63.42672681,  4.44091648,  2.10865587]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 0.8007599710174025}
episode index:2550
target Thresh 75.99980148128404
target distance 16.0
model initialize at round 2550
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([32.96376342,  5.6127565 ,  0.03595131]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 15.464165677666847}
done in step count: 14
reward sum = 0.8211915071968984
running average episode reward sum: 0.6902746047113918
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([48.2082028 ,  1.85129491,  5.83144145]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 0.2558546675463008}
episode index:2551
target Thresh 75.99980247140027
target distance 37.0
model initialize at round 2551
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.57127123,  4.56405794]), 'dynamicTrap': False, 'previousTarget': array([80.25789045,  4.79857683]), 'currentState': array([98.30551494,  7.81361564,  4.22415638]), 'targetState': array([63,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5136235590486704
running average episode reward sum: 0.6902053840822137
{'scaleFactor': 20, 'currentTarget': array([63.,  2.]), 'dynamicTrap': False, 'previousTarget': array([63.,  2.]), 'currentState': array([63.09125244,  2.09221161,  4.1297615 ]), 'targetState': array([63,  2], dtype=int32), 'currentDistance': 0.12973044016420165}
episode index:2552
target Thresh 75.99980345657828
target distance 20.0
model initialize at round 2552
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.53900817, 18.00034486]), 'dynamicTrap': False, 'previousTarget': array([22.23112767, 17.89976701]), 'currentState': array([39.95164283,  8.16163792,  1.89444226]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6902753176618012
{'scaleFactor': 20, 'currentTarget': array([19., 20.]), 'dynamicTrap': False, 'previousTarget': array([19., 20.]), 'currentState': array([19.99072837, 20.6072523 ,  2.10022842]), 'targetState': array([19, 20], dtype=int32), 'currentDistance': 1.162023258095891}
episode index:2553
target Thresh 75.99980443684268
target distance 25.0
model initialize at round 2553
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.31414289, 10.01781307]), 'dynamicTrap': False, 'previousTarget': array([40.85014149, 10.71008489]), 'currentState': array([56.18770895, 20.7547944 ,  3.33886239]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.5315768865730051
running average episode reward sum: 0.6902131804530741
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([32.92382839,  5.83935591,  3.62944273]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 0.1777881853639954}
episode index:2554
target Thresh 75.999805412218
target distance 41.0
model initialize at round 2554
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.39055478,  5.55603785]), 'dynamicTrap': False, 'previousTarget': array([25.62981184,  6.16979281]), 'currentState': array([7.71151325, 9.1246965 , 5.4000414 ]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7278582351937961
running average episode reward sum: 0.6902279143296849
{'scaleFactor': 20, 'currentTarget': array([47.,  2.]), 'dynamicTrap': False, 'previousTarget': array([47.,  2.]), 'currentState': array([46.96855533,  1.74285415,  5.50143598]), 'targetState': array([47,  2], dtype=int32), 'currentDistance': 0.2590613016180545}
episode index:2555
target Thresh 75.99980638272861
target distance 49.0
model initialize at round 2555
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.88799734, 12.17676931]), 'dynamicTrap': False, 'previousTarget': array([92.10486776, 11.55545404]), 'currentState': array([111.94736289,   6.11536809,   1.87666672]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6065944510787762
running average episode reward sum: 0.6901951938824036
{'scaleFactor': 20, 'currentTarget': array([62., 22.]), 'dynamicTrap': False, 'previousTarget': array([62., 22.]), 'currentState': array([62.31337004, 22.54285544,  2.28963246]), 'targetState': array([62, 22], dtype=int32), 'currentDistance': 0.6268116207013729}
episode index:2556
target Thresh 75.99980734839878
target distance 23.0
model initialize at round 2556
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.54384336,  20.21790878]), 'dynamicTrap': False, 'previousTarget': array([104.54352728,  20.24859289]), 'currentState': array([85.03704897, 15.80373019,  4.98886809]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7478976322950015
running average episode reward sum: 0.690217760342479
{'scaleFactor': 20, 'currentTarget': array([108.,  21.]), 'dynamicTrap': False, 'previousTarget': array([108.,  21.]), 'currentState': array([108.76844028,  20.45876222,   3.92393807]), 'targetState': array([108,  21], dtype=int32), 'currentDistance': 0.9399142504430683}
episode index:2557
target Thresh 75.99980830925266
target distance 12.0
model initialize at round 2557
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 17.]), 'dynamicTrap': False, 'previousTarget': array([29., 17.]), 'currentState': array([17.07677767, 17.3187822 ,  5.76802516]), 'targetState': array([29, 17], dtype=int32), 'currentDistance': 11.927483093411317}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6903123059201038
{'scaleFactor': 20, 'currentTarget': array([29., 17.]), 'dynamicTrap': False, 'previousTarget': array([29., 17.]), 'currentState': array([28.39549743, 16.56388592,  0.16718798]), 'targetState': array([29, 17], dtype=int32), 'currentDistance': 0.7453984515227063}
episode index:2558
target Thresh 75.99980926531424
target distance 36.0
model initialize at round 2558
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.81331246, 11.21330167]), 'dynamicTrap': True, 'previousTarget': array([53.97366596, 11.67544468]), 'currentState': array([35.     , 18.     ,  2.21691], dtype=float32), 'targetState': array([71,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5507453572865639
running average episode reward sum: 0.6902577662762456
{'scaleFactor': 20, 'currentTarget': array([71.,  6.]), 'dynamicTrap': False, 'previousTarget': array([71.,  6.]), 'currentState': array([70.85284164,  5.3634271 ,  3.66872303]), 'targetState': array([71,  6], dtype=int32), 'currentDistance': 0.6533610295280768}
episode index:2559
target Thresh 75.99981021660744
target distance 64.0
model initialize at round 2559
at step 0:
{'scaleFactor': 20, 'currentTarget': array([94.34508646, 11.95642269]), 'dynamicTrap': False, 'previousTarget': array([95.02193651, 13.06352827]), 'currentState': array([114.34021946,  12.39762146,   3.39853501]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5164834826698956
running average episode reward sum: 0.6901898856967119
{'scaleFactor': 20, 'currentTarget': array([51., 11.]), 'dynamicTrap': False, 'previousTarget': array([51., 11.]), 'currentState': array([50.51663591, 11.24898049,  2.02441804]), 'targetState': array([51, 11], dtype=int32), 'currentDistance': 0.5437206317978255}
episode index:2560
target Thresh 75.99981116315607
target distance 9.0
model initialize at round 2560
at step 0:
{'scaleFactor': 20, 'currentTarget': array([118.,   7.]), 'dynamicTrap': False, 'previousTarget': array([118.,   7.]), 'currentState': array([110.53596563,   8.45371248,   0.5625053 ]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 7.604280968360156}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6902843314062824
{'scaleFactor': 20, 'currentTarget': array([118.,   7.]), 'dynamicTrap': False, 'previousTarget': array([118.,   7.]), 'currentState': array([117.08311356,   6.36372187,   1.66084528]), 'targetState': array([118,   7], dtype=int32), 'currentDistance': 1.1160334259656057}
episode index:2561
target Thresh 75.99981210498376
target distance 56.0
model initialize at round 2561
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.76165165,  4.08428841]), 'dynamicTrap': False, 'previousTarget': array([24.88618308,  4.13066247]), 'currentState': array([6.8911788 , 1.8117769 , 0.32055688]), 'targetState': array([61,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.39126729786308284
running average episode reward sum: 0.6898621801068018
{'scaleFactor': 20, 'currentTarget': array([54.20304388,  7.80021015]), 'dynamicTrap': True, 'previousTarget': array([54.10124826,  7.67320287]), 'currentState': array([55.09933427,  4.82110321,  0.285211  ]), 'targetState': array([61,  8], dtype=int32), 'currentDistance': 3.1110150470214397}
episode index:2562
target Thresh 75.99981304211406
target distance 31.0
model initialize at round 2562
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.38494116, 21.82880418]), 'dynamicTrap': False, 'previousTarget': array([31.98960229, 21.64482588]), 'currentState': array([13.38811058, 21.47276123,  5.62699432]), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7474723885013719
running average episode reward sum: 0.6898846577534637
{'scaleFactor': 20, 'currentTarget': array([43., 22.]), 'dynamicTrap': False, 'previousTarget': array([43., 22.]), 'currentState': array([43.27696601, 21.72126803,  6.1643031 ]), 'targetState': array([43, 22], dtype=int32), 'currentDistance': 0.39293979438312915}
episode index:2563
target Thresh 75.99981397457042
target distance 57.0
model initialize at round 2563
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.27142005, 20.36294958]), 'dynamicTrap': False, 'previousTarget': array([89.07650552, 19.74767495]), 'currentState': array([107.21575598,  18.87182109,   2.47875977]), 'targetState': array([52, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6642620732297462
running average episode reward sum: 0.6898746645457712
{'scaleFactor': 20, 'currentTarget': array([52., 23.]), 'dynamicTrap': False, 'previousTarget': array([52., 23.]), 'currentState': array([51.88781291, 22.84755156,  2.9415396 ]), 'targetState': array([52, 23], dtype=int32), 'currentDistance': 0.18927880965620061}
episode index:2564
target Thresh 75.99981490237612
target distance 13.0
model initialize at round 2564
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79., 14.]), 'dynamicTrap': False, 'previousTarget': array([79., 14.]), 'currentState': array([90.12711046, 17.823563  ,  3.78938377]), 'targetState': array([79, 14], dtype=int32), 'currentDistance': 11.765722296954651}
done in step count: 24
reward sum = 0.7166140521389808
running average episode reward sum: 0.6898850892582832
{'scaleFactor': 20, 'currentTarget': array([79., 14.]), 'dynamicTrap': False, 'previousTarget': array([79., 14.]), 'currentState': array([79.26229692, 14.04802437,  4.64210686]), 'targetState': array([79, 14], dtype=int32), 'currentDistance': 0.26665710835654566}
episode index:2565
target Thresh 75.99981582555436
target distance 24.0
model initialize at round 2565
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9.00992344, 10.06883699]), 'dynamicTrap': False, 'previousTarget': array([10.72442198, 10.92257949]), 'currentState': array([26.0646192 , 20.51571943,  3.26365829]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7955038113500875
running average episode reward sum: 0.6899262501008755
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.12048703, 7.29028655, 4.66963816]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.9261799731535202}
episode index:2566
target Thresh 75.99981674412824
target distance 9.0
model initialize at round 2566
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.19559826, 18.04068333]), 'dynamicTrap': True, 'previousTarget': array([34., 18.]), 'currentState': array([43.       , 16.       ,  3.4262965], dtype=float32), 'targetState': array([34, 18], dtype=int32), 'currentDistance': 9.037802742826898}
done in step count: 11
reward sum = 0.8274036021657064
running average episode reward sum: 0.6899798057502969
{'scaleFactor': 20, 'currentTarget': array([34., 18.]), 'dynamicTrap': False, 'previousTarget': array([34., 18.]), 'currentState': array([34.8108704 , 18.19023066,  4.07279889]), 'targetState': array([34, 18], dtype=int32), 'currentDistance': 0.8328856550474414}
episode index:2567
target Thresh 75.9998176581207
target distance 56.0
model initialize at round 2567
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.98488416, 13.86248212]), 'dynamicTrap': False, 'previousTarget': array([89.59715  , 14.1492875]), 'currentState': array([107.37247977,  18.7738095 ,   2.2696867 ]), 'targetState': array([53,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6130198512132433
running average episode reward sum: 0.6899498369206485
{'scaleFactor': 20, 'currentTarget': array([53.,  5.]), 'dynamicTrap': False, 'previousTarget': array([53.,  5.]), 'currentState': array([53.24220827,  4.38564642,  5.88177481]), 'targetState': array([53,  5], dtype=int32), 'currentDistance': 0.6603750255059582}
episode index:2568
target Thresh 75.99981856755463
target distance 28.0
model initialize at round 2568
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.91269281, 19.35419341]), 'dynamicTrap': False, 'previousTarget': array([27., 19.]), 'currentState': array([ 7.93762187, 20.35246235,  6.2377866 ]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8073813856825759
running average episode reward sum: 0.6899955479166633
{'scaleFactor': 20, 'currentTarget': array([35., 19.]), 'dynamicTrap': False, 'previousTarget': array([35., 19.]), 'currentState': array([35.58698878, 19.6583338 ,  0.51284166]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 0.8820199685709785}
episode index:2569
target Thresh 75.99981947245273
target distance 16.0
model initialize at round 2569
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48., 17.]), 'dynamicTrap': False, 'previousTarget': array([48., 17.]), 'currentState': array([6.41988925e+01, 2.00183334e+01, 1.95337236e-03]), 'targetState': array([48, 17], dtype=int32), 'currentDistance': 16.477695657945517}
done in step count: 13
reward sum = 0.849275667526888
running average episode reward sum: 0.6900575246169007
{'scaleFactor': 20, 'currentTarget': array([48., 17.]), 'dynamicTrap': False, 'previousTarget': array([48., 17.]), 'currentState': array([48.72751194, 17.784695  ,  2.10159515]), 'targetState': array([48, 17], dtype=int32), 'currentDistance': 1.0700560109693669}
episode index:2570
target Thresh 75.99982037283763
target distance 2.0
model initialize at round 2570
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.1175493 ,  3.83841245]), 'dynamicTrap': True, 'previousTarget': array([61.,  4.]), 'currentState': array([63.       ,  4.       ,  2.1271288], dtype=float32), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 1.8893732214970487}
done in step count: 4
reward sum = 0.93089501
running average episode reward sum: 0.6901511992514333
{'scaleFactor': 20, 'currentTarget': array([61.,  4.]), 'dynamicTrap': False, 'previousTarget': array([61.,  4.]), 'currentState': array([60.86917737,  3.74088766,  4.52817582]), 'targetState': array([61,  4], dtype=int32), 'currentDistance': 0.290264989447134}
episode index:2571
target Thresh 75.99982126873184
target distance 43.0
model initialize at round 2571
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.13926071, 12.77143687]), 'dynamicTrap': False, 'previousTarget': array([33.85577145, 13.78779003]), 'currentState': array([53.06963938,  6.3184714 ,  3.74377966]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6955762760061239
running average episode reward sum: 0.6901533085347749
{'scaleFactor': 20, 'currentTarget': array([10., 21.]), 'dynamicTrap': False, 'previousTarget': array([10., 21.]), 'currentState': array([10.3644295 , 21.91528966,  3.54118931]), 'targetState': array([10, 21], dtype=int32), 'currentDistance': 0.9851720744892084}
episode index:2572
target Thresh 75.99982216015775
target distance 27.0
model initialize at round 2572
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.71196645,  7.15849832]), 'dynamicTrap': False, 'previousTarget': array([84.94535509,  7.52256629]), 'currentState': array([65.71831702,  7.66246509,  6.2152319 ]), 'targetState': array([92,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6902159997367026
{'scaleFactor': 20, 'currentTarget': array([92.,  7.]), 'dynamicTrap': False, 'previousTarget': array([92.,  7.]), 'currentState': array([92.6439083 ,  7.19550689,  5.52185529]), 'targetState': array([92,  7], dtype=int32), 'currentDistance': 0.6729344970218245}
episode index:2573
target Thresh 75.99982304713767
target distance 31.0
model initialize at round 2573
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.66539504, 12.08026009]), 'dynamicTrap': False, 'previousTarget': array([88.05202505, 11.54875884]), 'currentState': array([72.13420608, 23.33724865,  6.26505202]), 'targetState': array([102,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6888736874460715
running average episode reward sum: 0.6902154782478562
{'scaleFactor': 20, 'currentTarget': array([102.,   3.]), 'dynamicTrap': False, 'previousTarget': array([102.,   3.]), 'currentState': array([102.03475465,   2.40754044,   5.17997445]), 'targetState': array([102,   3], dtype=int32), 'currentDistance': 0.5934780718828276}
episode index:2574
target Thresh 75.99982392969376
target distance 33.0
model initialize at round 2574
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.57366045, 17.02117828]), 'dynamicTrap': False, 'previousTarget': array([30.39183214, 16.33049037]), 'currentState': array([47.16890802,  9.65797463,  2.38736176]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 29
reward sum = 0.7023822942151683
running average episode reward sum: 0.6902202032249308
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([16.45025901, 22.52495927,  0.16902696]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.6916035039429345}
episode index:2575
target Thresh 75.99982480784807
target distance 25.0
model initialize at round 2575
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.69385342,  19.09654747]), 'dynamicTrap': False, 'previousTarget': array([108.93630557,  19.40509555]), 'currentState': array([89.69887848, 19.54485183,  6.17013645]), 'targetState': array([114,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8417547810948756
running average episode reward sum: 0.6902790287598182
{'scaleFactor': 20, 'currentTarget': array([114.,  19.]), 'dynamicTrap': False, 'previousTarget': array([114.,  19.]), 'currentState': array([114.36706738,  19.25132007,   4.48505629]), 'targetState': array([114,  19], dtype=int32), 'currentDistance': 0.4448597974334797}
episode index:2576
target Thresh 75.99982568162257
target distance 52.0
model initialize at round 2576
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59.86102639, 14.66873564]), 'dynamicTrap': False, 'previousTarget': array([58.56699406, 15.13917182]), 'currentState': array([40.39330306, 10.08531461,  0.14780157]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.3827316874572506
running average episode reward sum: 0.6901596855928401
{'scaleFactor': 20, 'currentTarget': array([91., 22.]), 'dynamicTrap': False, 'previousTarget': array([91., 22.]), 'currentState': array([91.4306128, 22.6882764,  1.2161928]), 'targetState': array([91, 22], dtype=int32), 'currentDistance': 0.8118816314892175}
episode index:2577
target Thresh 75.9998265510391
target distance 17.0
model initialize at round 2577
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.73445469, 18.31854329]), 'dynamicTrap': True, 'previousTarget': array([70.20859686, 18.86502556]), 'currentState': array([87.       ,  8.       ,  2.1270666], dtype=float32), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 18.425775678967565}
done in step count: 99
reward sum = -0.2059616017763709
running average episode reward sum: 0.6898120823006101
{'scaleFactor': 20, 'currentTarget': array([70., 19.]), 'dynamicTrap': False, 'previousTarget': array([70., 19.]), 'currentState': array([69.65384243, 19.46074424,  2.84745554]), 'targetState': array([70, 19], dtype=int32), 'currentDistance': 0.576290139958925}
episode index:2578
target Thresh 75.9998274161194
target distance 74.0
model initialize at round 2578
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.59256581, 11.59776729]), 'dynamicTrap': False, 'previousTarget': array([47.65140491, 11.71783336]), 'currentState': array([29.97526792,  7.70397434,  6.22489591]), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 70
reward sum = 0.37582053397786597
running average episode reward sum: 0.6896903329604307
{'scaleFactor': 20, 'currentTarget': array([102.,  22.]), 'dynamicTrap': False, 'previousTarget': array([102.,  22.]), 'currentState': array([101.43820454,  21.77688389,   0.91341954]), 'targetState': array([102,  22], dtype=int32), 'currentDistance': 0.6044790687566073}
episode index:2579
target Thresh 75.9998282768851
target distance 67.0
model initialize at round 2579
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.74729526,  9.87374692]), 'dynamicTrap': False, 'previousTarget': array([45.9977727 ,  8.70152578]), 'currentState': array([25.76300094, 10.66619913,  0.70208788]), 'targetState': array([93,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6332857698105854
running average episode reward sum: 0.6896684707266516
{'scaleFactor': 20, 'currentTarget': array([93.,  8.]), 'dynamicTrap': False, 'previousTarget': array([93.,  8.]), 'currentState': array([92.6085751 ,  8.00007014,  3.69048552]), 'targetState': array([93,  8], dtype=int32), 'currentDistance': 0.39142490784678413}
episode index:2580
target Thresh 75.99982913335771
target distance 42.0
model initialize at round 2580
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.99191906, 14.8752993 ]), 'dynamicTrap': False, 'previousTarget': array([26.44395172, 15.80941823]), 'currentState': array([45.68845222, 18.34611788,  3.5161972 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7221358385449002
running average episode reward sum: 0.6896810501020172
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 3.67159011, 11.20735384,  2.93763387]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.388392419530647}
episode index:2581
target Thresh 75.99982998555865
target distance 56.0
model initialize at round 2581
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.61739721,  5.834227  ]), 'dynamicTrap': False, 'previousTarget': array([71.92075411,  6.77863876]), 'currentState': array([52.75458804,  3.4956779 ,  6.18732643]), 'targetState': array([108,  10], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5079848003407036
running average episode reward sum: 0.6896106797496697
{'scaleFactor': 20, 'currentTarget': array([108.,  10.]), 'dynamicTrap': False, 'previousTarget': array([108.,  10.]), 'currentState': array([108.37410596,   9.26762042,   1.21141581]), 'targetState': array([108,  10], dtype=int32), 'currentDistance': 0.8223959657833101}
episode index:2582
target Thresh 75.99983083350921
target distance 25.0
model initialize at round 2582
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.90216069, 22.85298068]), 'dynamicTrap': False, 'previousTarget': array([16.01598083, 22.79936077]), 'currentState': array([35.89317233, 22.25343587,  4.17256331]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6896800313303973
{'scaleFactor': 20, 'currentTarget': array([11., 23.]), 'dynamicTrap': False, 'previousTarget': array([11., 23.]), 'currentState': array([11.45758206, 22.41366618,  3.10870898]), 'targetState': array([11, 23], dtype=int32), 'currentDistance': 0.7437531118738231}
episode index:2583
target Thresh 75.9998316772306
target distance 31.0
model initialize at round 2583
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.31382012, 16.37057755]), 'dynamicTrap': False, 'previousTarget': array([32.63559701, 15.80043813]), 'currentState': array([12.50544223, 13.60866426,  0.8590591 ]), 'targetState': array([44, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7639666285166967
running average episode reward sum: 0.689708780013519
{'scaleFactor': 20, 'currentTarget': array([42.32781977, 17.80973346]), 'dynamicTrap': True, 'previousTarget': array([44., 18.]), 'currentState': array([42.0574663 , 17.85922858,  1.10329625]), 'targetState': array([44, 18], dtype=int32), 'currentDistance': 0.2748468006938055}
episode index:2584
target Thresh 75.99983251674392
target distance 21.0
model initialize at round 2584
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3.25775784, 3.35322867]), 'currentState': array([ 6.20699811, 21.27033646,  4.83203602]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 19.53537058981171}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6897848636079881
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([3.43757294, 1.3825965 , 2.19759826]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.7567411420808682}
episode index:2585
target Thresh 75.99983335207014
target distance 17.0
model initialize at round 2585
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  17.]), 'dynamicTrap': False, 'previousTarget': array([106.,  17.]), 'currentState': array([87.86735852, 15.2447937 ,  1.29905581]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 18.21739377892155}
done in step count: 22
reward sum = 0.7027132278960526
running average episode reward sum: 0.6897898629754622
{'scaleFactor': 20, 'currentTarget': array([106.,  17.]), 'dynamicTrap': False, 'previousTarget': array([106.,  17.]), 'currentState': array([106.85255361,  16.60928842,   5.91508344]), 'targetState': array([106,  17], dtype=int32), 'currentDistance': 0.9378183120189378}
episode index:2586
target Thresh 75.99983418323015
target distance 63.0
model initialize at round 2586
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.69425868, 10.69642173]), 'dynamicTrap': False, 'previousTarget': array([72.12232531, 11.20863052]), 'currentState': array([90.5343935 ,  8.17272395,  2.64546967]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5821988844572847
running average episode reward sum: 0.6897482738844231
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([29.49617184, 15.69916159,  4.697557  ]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 0.5802501618166447}
episode index:2587
target Thresh 75.99983501024475
target distance 43.0
model initialize at round 2587
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.52125134, 18.02554098]), 'dynamicTrap': False, 'previousTarget': array([87.80809791, 17.23607936]), 'currentState': array([68.83444195, 21.55109569,  0.20646989]), 'targetState': array([111,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6714182764728462
running average episode reward sum: 0.6897411911960879
{'scaleFactor': 20, 'currentTarget': array([111.,  14.]), 'dynamicTrap': False, 'previousTarget': array([111.,  14.]), 'currentState': array([110.38491402,  14.44131013,   0.16080713]), 'targetState': array([111,  14], dtype=int32), 'currentDistance': 0.757024032469913}
episode index:2588
target Thresh 75.99983583313458
target distance 26.0
model initialize at round 2588
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.17842276,  7.08937215]), 'dynamicTrap': False, 'previousTarget': array([81.64012894,  7.22305213]), 'currentState': array([63.67014957, 11.49701067,  5.66668648]), 'targetState': array([88,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.746368975378906
running average episode reward sum: 0.689763063650388
{'scaleFactor': 20, 'currentTarget': array([88.,  6.]), 'dynamicTrap': False, 'previousTarget': array([88.,  6.]), 'currentState': array([87.85733047,  6.8969656 ,  5.09156541]), 'targetState': array([88,  6], dtype=int32), 'currentDistance': 0.9082410893593631}
episode index:2589
target Thresh 75.99983665192023
target distance 18.0
model initialize at round 2589
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.26834921,  9.10056393]), 'dynamicTrap': False, 'previousTarget': array([72.63557441, 10.19631201]), 'currentState': array([86.37430059, 22.20820418,  2.92547905]), 'targetState': array([70,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7727944924537468
running average episode reward sum: 0.6897951221171075
{'scaleFactor': 20, 'currentTarget': array([70.,  8.]), 'dynamicTrap': False, 'previousTarget': array([70.,  8.]), 'currentState': array([70.1422865 ,  7.19490286,  0.47252261]), 'targetState': array([70,  8], dtype=int32), 'currentDistance': 0.8175737606725224}
episode index:2590
target Thresh 75.9998374666222
target distance 67.0
model initialize at round 2590
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.7828839 ,  7.30707254]), 'dynamicTrap': False, 'previousTarget': array([48.94453985,  7.4883985 ]), 'currentState': array([30.84925359,  5.67907286,  0.4477933 ]), 'targetState': array([96, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6247800040727111
running average episode reward sum: 0.6897700294432193
{'scaleFactor': 20, 'currentTarget': array([96., 11.]), 'dynamicTrap': False, 'previousTarget': array([96., 11.]), 'currentState': array([96.72703213, 11.90885271,  3.2851369 ]), 'targetState': array([96, 11], dtype=int32), 'currentDistance': 1.1638681060589875}
episode index:2591
target Thresh 75.99983827726079
target distance 49.0
model initialize at round 2591
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.51421485,  8.85557284]), 'dynamicTrap': False, 'previousTarget': array([35.98789087,  9.20803563]), 'currentState': array([54.42816042,  2.35459885,  3.0096314 ]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6164375348493321
running average episode reward sum: 0.6897417375857371
{'scaleFactor': 20, 'currentTarget': array([ 6., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 19.]), 'currentState': array([ 5.25328224, 18.68148667,  3.40731701]), 'targetState': array([ 6, 19], dtype=int32), 'currentDistance': 0.8118116522758513}
episode index:2592
target Thresh 75.99983908385633
target distance 15.0
model initialize at round 2592
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'dynamicTrap': False, 'previousTarget': array([26.,  4.]), 'currentState': array([30.35541269, 17.35353556,  5.19695568]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 14.04587240561016}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.689824514422383
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'dynamicTrap': False, 'previousTarget': array([26.,  4.]), 'currentState': array([25.50952146,  3.06817599,  2.20035487]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 1.0530266775831403}
episode index:2593
target Thresh 75.99983988642893
target distance 11.0
model initialize at round 2593
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.80297584, 18.96649958]), 'dynamicTrap': True, 'previousTarget': array([91., 19.]), 'currentState': array([80.        , 16.        ,  0.10242929], dtype=float32), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 11.20287493714483}
done in step count: 17
reward sum = 0.7390123168062439
running average episode reward sum: 0.6898434765667099
{'scaleFactor': 20, 'currentTarget': array([91., 19.]), 'dynamicTrap': False, 'previousTarget': array([91., 19.]), 'currentState': array([91.22558808, 19.03410034,  0.87340385]), 'targetState': array([91, 19], dtype=int32), 'currentDistance': 0.2281508569916501}
episode index:2594
target Thresh 75.9998406849987
target distance 16.0
model initialize at round 2594
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91., 12.]), 'currentState': array([105.98762099,  19.34347402,   3.21426499]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 16.689978840845267}
done in step count: 10
reward sum = 0.8944820750088044
running average episode reward sum: 0.6899223353715045
{'scaleFactor': 20, 'currentTarget': array([91., 12.]), 'dynamicTrap': False, 'previousTarget': array([91., 12.]), 'currentState': array([91.26525726, 12.02339256,  3.55120226]), 'targetState': array([91, 12], dtype=int32), 'currentDistance': 0.2662867375663729}
episode index:2595
target Thresh 75.99984147958558
target distance 20.0
model initialize at round 2595
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.29840251,  4.1815918 ]), 'dynamicTrap': False, 'previousTarget': array([88.28991511,  5.85014149]), 'currentState': array([79.08733305, 21.37851189,  5.68651783]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7774409094327284
running average episode reward sum: 0.6899560482274603
{'scaleFactor': 20, 'currentTarget': array([90.,  3.]), 'dynamicTrap': False, 'previousTarget': array([90.,  3.]), 'currentState': array([90.00493633,  3.31267407,  5.9382816 ]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 0.31271303240478576}
episode index:2596
target Thresh 75.99984227020946
target distance 26.0
model initialize at round 2596
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.56320381, 13.15606093]), 'dynamicTrap': False, 'previousTarget': array([68.84081231, 12.38116355]), 'currentState': array([85.91443456,  5.20418504,  3.18119764]), 'targetState': array([61, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6562775613966444
running average episode reward sum: 0.6899430799999551
{'scaleFactor': 20, 'currentTarget': array([62.49879341, 15.70862847]), 'dynamicTrap': True, 'previousTarget': array([61., 16.]), 'currentState': array([62.20526227, 16.11922564,  1.95958354]), 'targetState': array([61, 16], dtype=int32), 'currentDistance': 0.5047282113354621}
episode index:2597
target Thresh 75.99984305689006
target distance 24.0
model initialize at round 2597
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.69602835,  6.29342928]), 'dynamicTrap': False, 'previousTarget': array([24.64100589,  6.90599608]), 'currentState': array([ 7.45422456, 16.42852682,  5.38811994]), 'targetState': array([32,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7825954924537468
running average episode reward sum: 0.6899787429762653
{'scaleFactor': 20, 'currentTarget': array([32.,  2.]), 'dynamicTrap': False, 'previousTarget': array([32.,  2.]), 'currentState': array([31.2362491 ,  2.53051464,  4.2835945 ]), 'targetState': array([32,  2], dtype=int32), 'currentDistance': 0.9299253874193044}
episode index:2598
target Thresh 75.9998438396471
target distance 64.0
model initialize at round 2598
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.76846482, 19.96556451]), 'dynamicTrap': True, 'previousTarget': array([61.76024068, 19.91246239]), 'currentState': array([42.      , 23.      ,  3.961563], dtype=float32), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 50
reward sum = 0.5311257017970235
running average episode reward sum: 0.6899176221447226
{'scaleFactor': 20, 'currentTarget': array([106.,  13.]), 'dynamicTrap': False, 'previousTarget': array([106.,  13.]), 'currentState': array([106.55691981,  13.68462479,   4.43745639]), 'targetState': array([106,  13], dtype=int32), 'currentDistance': 0.8825365599395688}
episode index:2599
target Thresh 75.99984461850009
target distance 20.0
model initialize at round 2599
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.96697396,   6.32428824]), 'dynamicTrap': False, 'previousTarget': array([108.52431817,   6.361625  ]), 'currentState': array([92.20873629, 17.24039883,  6.15653807]), 'targetState': array([111,   5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 20
reward sum = 0.7444379106961987
running average episode reward sum: 0.6899385914864732
{'scaleFactor': 20, 'currentTarget': array([111.,   5.]), 'dynamicTrap': False, 'previousTarget': array([111.,   5.]), 'currentState': array([110.81757278,   4.38471413,   6.12107216]), 'targetState': array([111,   5], dtype=int32), 'currentDistance': 0.6417603813523516}
episode index:2600
target Thresh 75.99984539346856
target distance 43.0
model initialize at round 2600
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.09866822,  5.82428588]), 'dynamicTrap': False, 'previousTarget': array([84.91402432,  5.85246738]), 'currentState': array([65.18831907,  3.93272655,  5.96803659]), 'targetState': array([108,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7373303288390437
running average episode reward sum: 0.6899568120698459
{'scaleFactor': 20, 'currentTarget': array([108.,   8.]), 'dynamicTrap': False, 'previousTarget': array([108.,   8.]), 'currentState': array([108.0331821 ,   8.07154651,   3.7918937 ]), 'targetState': array([108,   8], dtype=int32), 'currentDistance': 0.07886668631364747}
episode index:2601
target Thresh 75.99984616457185
target distance 43.0
model initialize at round 2601
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.74241701,  3.24779576]), 'dynamicTrap': False, 'previousTarget': array([24.91402432,  4.14753262]), 'currentState': array([4.77113946, 4.31927749, 5.56461382]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.689990580151064
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([48.40691073,  2.98978463,  6.23078336]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 1.0701635197616568}
episode index:2602
target Thresh 75.99984693182925
target distance 15.0
model initialize at round 2602
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.59295223, 20.84790332]), 'dynamicTrap': True, 'previousTarget': array([16., 22.]), 'currentState': array([18.     ,  7.     ,  6.22852], dtype=float32), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 14.260869567284118}
done in step count: 22
reward sum = 0.7186775403623978
running average episode reward sum: 0.6900016008810721
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([16.37166812, 21.9719646 ,  2.46439024]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.37272399685980007}
episode index:2603
target Thresh 75.99984769525994
target distance 66.0
model initialize at round 2603
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.30363461, 17.6150318 ]), 'dynamicTrap': False, 'previousTarget': array([66.18339664, 16.70226409]), 'currentState': array([8.71752890e+01, 1.53528753e+01, 4.48583364e-02]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.5387152847378488
running average episode reward sum: 0.6899435032174226
{'scaleFactor': 20, 'currentTarget': array([20., 23.]), 'dynamicTrap': False, 'previousTarget': array([20., 23.]), 'currentState': array([20.25546764, 23.55345563,  5.68428956]), 'targetState': array([20, 23], dtype=int32), 'currentDistance': 0.6095710392151783}
episode index:2604
target Thresh 75.999848454883
target distance 4.0
model initialize at round 2604
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 15.]), 'dynamicTrap': False, 'previousTarget': array([31., 15.]), 'currentState': array([29.11743092, 10.97481023,  1.92028761]), 'targetState': array([31, 15], dtype=int32), 'currentDistance': 4.443671797534397}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6900511252891242
{'scaleFactor': 20, 'currentTarget': array([31., 15.]), 'dynamicTrap': False, 'previousTarget': array([31., 15.]), 'currentState': array([30.0417504 , 14.84522206,  2.50083035]), 'targetState': array([31, 15], dtype=int32), 'currentDistance': 0.9706691041510183}
episode index:2605
target Thresh 75.99984921071743
target distance 36.0
model initialize at round 2605
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.07328745, 13.33891246]), 'dynamicTrap': False, 'previousTarget': array([73.87296858, 13.84437071]), 'currentState': array([91.93745028,  6.69486545,  3.01909113]), 'targetState': array([57, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7086097571030592
running average episode reward sum: 0.6900582467902039
{'scaleFactor': 20, 'currentTarget': array([57., 19.]), 'dynamicTrap': False, 'previousTarget': array([57., 19.]), 'currentState': array([56.15795291, 18.41940251,  3.56467075]), 'targetState': array([57, 19], dtype=int32), 'currentDistance': 1.0228082656175648}
episode index:2606
target Thresh 75.99984996278211
target distance 12.0
model initialize at round 2606
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 23.]), 'dynamicTrap': False, 'previousTarget': array([87., 23.]), 'currentState': array([91.6639513 , 12.80202386,  1.71420544]), 'targetState': array([87, 23], dtype=int32), 'currentDistance': 11.213882426071153}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6901546878729085
{'scaleFactor': 20, 'currentTarget': array([87., 23.]), 'dynamicTrap': False, 'previousTarget': array([87., 23.]), 'currentState': array([87.05956625, 23.37672776,  1.85660511]), 'targetState': array([87, 23], dtype=int32), 'currentDistance': 0.38140784292895574}
episode index:2607
target Thresh 75.99985071109586
target distance 15.0
model initialize at round 2607
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3.11169277, 16.1757132 ]), 'dynamicTrap': True, 'previousTarget': array([ 3., 16.]), 'currentState': array([18.      , 16.      ,  5.136165], dtype=float32), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 14.889344089580051}
done in step count: 12
reward sum = 0.8566838717161293
running average episode reward sum: 0.6902185410875723
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.49645603, 16.86102984,  4.88216708]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.9974612340901817}
episode index:2608
target Thresh 75.99985145567737
target distance 62.0
model initialize at round 2608
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.38761292,  8.91845287]), 'dynamicTrap': True, 'previousTarget': array([55.42566101,  9.10429689]), 'currentState': array([75.       ,  5.       ,  3.0047538], dtype=float32), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5588319541574589
running average episode reward sum: 0.6901681821044638
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'dynamicTrap': False, 'previousTarget': array([13., 18.]), 'currentState': array([13.59698923, 18.70116223,  4.28373087]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.9208825224536188}
episode index:2609
target Thresh 75.99985219654526
target distance 66.0
model initialize at round 2609
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.77734312, 16.04132673]), 'dynamicTrap': False, 'previousTarget': array([65.14048809, 17.19985209]), 'currentState': array([45.49899718, 21.36536448,  5.37935305]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 52
reward sum = 0.49720098396771595
running average episode reward sum: 0.6900942483120743
{'scaleFactor': 20, 'currentTarget': array([112.,   3.]), 'dynamicTrap': False, 'previousTarget': array([112.,   3.]), 'currentState': array([111.12301765,   3.1355165 ,   5.06414452]), 'targetState': array([112,   3], dtype=int32), 'currentDistance': 0.8873909908593088}
episode index:2610
target Thresh 75.99985293371807
target distance 27.0
model initialize at round 2610
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.15061483,  6.90236489]), 'dynamicTrap': False, 'previousTarget': array([54.02633404,  6.67544468]), 'currentState': array([71.68227014, 14.42418324,  2.57870865]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7076110348893507
running average episode reward sum: 0.6901009571541185
{'scaleFactor': 20, 'currentTarget': array([46.,  4.]), 'dynamicTrap': False, 'previousTarget': array([46.,  4.]), 'currentState': array([45.8211816 ,  3.80194753,  3.83525551]), 'targetState': array([46,  4], dtype=int32), 'currentDistance': 0.2668347834770811}
episode index:2611
target Thresh 75.99985366721421
target distance 28.0
model initialize at round 2611
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.37029004, 15.42920977]), 'dynamicTrap': False, 'previousTarget': array([38.9042497 , 14.37956268]), 'currentState': array([54.56159899,  5.20869625,  3.05743682]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.6858514995911972
running average episode reward sum: 0.6900993302561236
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.2245171 , 20.87460854,  0.08441816]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.2571593790789713}
episode index:2612
target Thresh 75.99985439705203
target distance 35.0
model initialize at round 2612
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.02656661,  5.03051371]), 'dynamicTrap': True, 'previousTarget': array([23.03257331,  5.14099581]), 'currentState': array([43.     ,  4.     ,  1.74881], dtype=float32), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.31905141625063255
running average episode reward sum: 0.6899573295236301
{'scaleFactor': 20, 'currentTarget': array([8., 6.]), 'dynamicTrap': False, 'previousTarget': array([8., 6.]), 'currentState': array([7.51561598, 6.23502095, 1.18175396]), 'targetState': array([8, 6], dtype=int32), 'currentDistance': 0.5383890072794041}
episode index:2613
target Thresh 75.99985512324976
target distance 33.0
model initialize at round 2613
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.25075748, 18.61817395]), 'dynamicTrap': False, 'previousTarget': array([88.96336993, 19.20990121]), 'currentState': array([69.3672055 , 16.46309611,  5.90495145]), 'targetState': array([102,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7587898920798499
running average episode reward sum: 0.6899836617969874
{'scaleFactor': 20, 'currentTarget': array([102.,  20.]), 'dynamicTrap': False, 'previousTarget': array([102.,  20.]), 'currentState': array([102.2841038 ,  19.57374866,   0.4854149 ]), 'targetState': array([102,  20], dtype=int32), 'currentDistance': 0.5122549885955975}
episode index:2614
target Thresh 75.99985584582556
target distance 60.0
model initialize at round 2614
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.56973818, 15.28155177]), 'dynamicTrap': False, 'previousTarget': array([91.22127294, 14.96680906]), 'currentState': array([109.35347916,  12.34836363,   1.92309189]), 'targetState': array([51, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.5994452855829481
running average episode reward sum: 0.6899490390909783
{'scaleFactor': 20, 'currentTarget': array([51., 21.]), 'dynamicTrap': False, 'previousTarget': array([51., 21.]), 'currentState': array([50.26456753, 21.60514275,  1.31227802]), 'targetState': array([51, 21], dtype=int32), 'currentDistance': 0.9523962721833665}
episode index:2615
target Thresh 75.99985656479751
target distance 65.0
model initialize at round 2615
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.85003054,  9.86693351]), 'dynamicTrap': False, 'previousTarget': array([22.9787322 , 10.07790467]), 'currentState': array([ 4.86872391, 10.73144793,  0.44798427]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6794692084047854
running average episode reward sum: 0.6899450330394927
{'scaleFactor': 20, 'currentTarget': array([68.,  8.]), 'dynamicTrap': False, 'previousTarget': array([68.,  8.]), 'currentState': array([67.04785994,  7.20220092,  5.5012098 ]), 'targetState': array([68,  8], dtype=int32), 'currentDistance': 1.2421972708051487}
episode index:2616
target Thresh 75.99985728018358
target distance 13.0
model initialize at round 2616
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.,  4.]), 'dynamicTrap': False, 'previousTarget': array([73.,  4.]), 'currentState': array([60.19537488,  3.95733113,  6.04339046]), 'targetState': array([73,  4], dtype=int32), 'currentDistance': 12.80469620788543}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6900269730631722
{'scaleFactor': 20, 'currentTarget': array([73.,  4.]), 'dynamicTrap': False, 'previousTarget': array([73.,  4.]), 'currentState': array([72.84791561,  3.54196879,  4.03840818]), 'targetState': array([73,  4], dtype=int32), 'currentDistance': 0.48262019141940893}
episode index:2617
target Thresh 75.99985799200162
target distance 22.0
model initialize at round 2617
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.68036249, 13.43863339]), 'dynamicTrap': True, 'previousTarget': array([60.6773982 , 13.42229124]), 'currentState': array([41.      , 17.      ,  5.936968], dtype=float32), 'targetState': array([63, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.6942584408851653
running average episode reward sum: 0.6900285893610416
{'scaleFactor': 20, 'currentTarget': array([63., 13.]), 'dynamicTrap': False, 'previousTarget': array([63., 13.]), 'currentState': array([63.31729273, 13.3470747 ,  6.10373734]), 'targetState': array([63, 13], dtype=int32), 'currentDistance': 0.4702504872870792}
episode index:2618
target Thresh 75.99985870026947
target distance 22.0
model initialize at round 2618
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.8629358 , 20.82401346]), 'dynamicTrap': False, 'previousTarget': array([62.91786413, 20.81071492]), 'currentState': array([42.93040752, 19.18257621,  1.83396487]), 'targetState': array([65, 21], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.690100178682782
{'scaleFactor': 20, 'currentTarget': array([65., 21.]), 'dynamicTrap': False, 'previousTarget': array([65., 21.]), 'currentState': array([65.02804935, 21.57220245,  6.20976686]), 'targetState': array([65, 21], dtype=int32), 'currentDistance': 0.5728895314999876}
episode index:2619
target Thresh 75.99985940500483
target distance 15.0
model initialize at round 2619
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66., 22.]), 'dynamicTrap': False, 'previousTarget': array([66., 22.]), 'currentState': array([52.96343367,  9.23954906,  6.14463124]), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 18.242290696854784}
done in step count: 23
reward sum = 0.6849148495701695
running average episode reward sum: 0.6900981995495328
{'scaleFactor': 20, 'currentTarget': array([66., 22.]), 'dynamicTrap': False, 'previousTarget': array([66., 22.]), 'currentState': array([66.94644881, 22.21572114,  4.84925536]), 'targetState': array([66, 22], dtype=int32), 'currentDistance': 0.9707218741454796}
episode index:2620
target Thresh 75.99986010622528
target distance 44.0
model initialize at round 2620
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.54962005, 16.22211892]), 'dynamicTrap': False, 'previousTarget': array([60.0585156 , 16.93592685]), 'currentState': array([42.43555659, 22.10876296,  0.44304829]), 'targetState': array([85,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6365358199823051
running average episode reward sum: 0.6900777636931545
{'scaleFactor': 20, 'currentTarget': array([85.,  9.]), 'dynamicTrap': False, 'previousTarget': array([85.,  9.]), 'currentState': array([84.73635962,  9.5004393 ,  0.55781091]), 'targetState': array([85,  9], dtype=int32), 'currentDistance': 0.5656374632603011}
episode index:2621
target Thresh 75.9998608039484
target distance 63.0
model initialize at round 2621
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.27033246,  2.51271055]), 'dynamicTrap': False, 'previousTarget': array([51.01007049,  2.63460094]), 'currentState': array([69.25735791,  1.79242336,  2.37743688]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6029184288920029
running average episode reward sum: 0.6900445221467011
{'scaleFactor': 20, 'currentTarget': array([8., 4.]), 'dynamicTrap': False, 'previousTarget': array([8., 4.]), 'currentState': array([8.90963566, 3.64023975, 2.76681551]), 'targetState': array([8, 4], dtype=int32), 'currentDistance': 0.9781944967630483}
episode index:2622
target Thresh 75.99986149819159
target distance 20.0
model initialize at round 2622
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.,  5.]), 'dynamicTrap': False, 'previousTarget': array([96.0992562 ,  4.99007438]), 'currentState': array([115.09135506,   4.13654146,   3.25522161]), 'targetState': array([96,  5], dtype=int32), 'currentDistance': 19.110871214534864}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6901227889145668
{'scaleFactor': 20, 'currentTarget': array([96.,  5.]), 'dynamicTrap': False, 'previousTarget': array([96.,  5.]), 'currentState': array([95.17798113,  4.03312353,  3.15784921]), 'targetState': array([96,  5], dtype=int32), 'currentDistance': 1.2690804314391462}
episode index:2623
target Thresh 75.99986218897224
target distance 25.0
model initialize at round 2623
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.19830054, 16.9200524 ]), 'dynamicTrap': False, 'previousTarget': array([28.95151706, 16.90448546]), 'currentState': array([46.95943259, 23.84969339,  2.16194077]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7483140342172068
running average episode reward sum: 0.6901449654562217
{'scaleFactor': 20, 'currentTarget': array([23., 15.]), 'dynamicTrap': False, 'previousTarget': array([23., 15.]), 'currentState': array([22.74882282, 15.6177178 ,  3.68657881]), 'targetState': array([23, 15], dtype=int32), 'currentDistance': 0.66683225633805}
episode index:2624
target Thresh 75.99986287630762
target distance 22.0
model initialize at round 2624
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.0924203 , 22.66347567]), 'dynamicTrap': False, 'previousTarget': array([52.49734288, 22.43242207]), 'currentState': array([71.83866907, 19.48768296,  1.77086405]), 'targetState': array([50, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6902130038742456
{'scaleFactor': 20, 'currentTarget': array([50., 23.]), 'dynamicTrap': False, 'previousTarget': array([50., 23.]), 'currentState': array([50.56970324, 22.73036789,  3.70392361]), 'targetState': array([50, 23], dtype=int32), 'currentDistance': 0.6302882326925608}
episode index:2625
target Thresh 75.99986356021489
target distance 29.0
model initialize at round 2625
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.15590063, 11.83707091]), 'dynamicTrap': False, 'previousTarget': array([91.04739343, 12.37604183]), 'currentState': array([110.99650073,   9.31703362,   3.70030975]), 'targetState': array([82, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7033793678514184
running average episode reward sum: 0.6902180177219138
{'scaleFactor': 20, 'currentTarget': array([82., 13.]), 'dynamicTrap': False, 'previousTarget': array([82., 13.]), 'currentState': array([82.23801849, 12.66167469,  4.67025347]), 'targetState': array([82, 13], dtype=int32), 'currentDistance': 0.4136626835689874}
episode index:2626
target Thresh 75.99986424071115
target distance 31.0
model initialize at round 2626
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.11393532, 15.47444059]), 'dynamicTrap': False, 'previousTarget': array([82.99681497, 15.71121856]), 'currentState': array([99.72173635,  5.98943439,  2.8586812 ]), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.4003528115896155
running average episode reward sum: 0.6901076769506416
{'scaleFactor': 20, 'currentTarget': array([70., 22.]), 'dynamicTrap': False, 'previousTarget': array([70., 22.]), 'currentState': array([70.63870912, 21.45925558,  3.12829263]), 'targetState': array([70, 22], dtype=int32), 'currentDistance': 0.8368714764382327}
episode index:2627
target Thresh 75.99986491781344
target distance 23.0
model initialize at round 2627
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.48649773, 18.17625813]), 'dynamicTrap': False, 'previousTarget': array([39.16799178, 18.41321632]), 'currentState': array([58.43643792, 19.59043161,  3.32204485]), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8158108176164152
running average episode reward sum: 0.6901555091959481
{'scaleFactor': 20, 'currentTarget': array([37.47846664, 18.19007742]), 'dynamicTrap': True, 'previousTarget': array([36., 18.]), 'currentState': array([37.62684621, 18.43979067,  2.27945516]), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 0.2904706555115152}
episode index:2628
target Thresh 75.99986559153865
target distance 39.0
model initialize at round 2628
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.06308893, 11.41368424]), 'dynamicTrap': True, 'previousTarget': array([32.33391929, 12.00829159]), 'currentState': array([14.        , 20.        ,  0.34732106], dtype=float32), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6516050545678435
running average episode reward sum: 0.6901408456529173
{'scaleFactor': 20, 'currentTarget': array([53.,  3.]), 'dynamicTrap': False, 'previousTarget': array([53.,  3.]), 'currentState': array([53.71939698,  2.77602537,  4.57637992]), 'targetState': array([53,  3], dtype=int32), 'currentDistance': 0.7534564654487658}
episode index:2629
target Thresh 75.99986626190365
target distance 11.0
model initialize at round 2629
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  12.]), 'dynamicTrap': False, 'previousTarget': array([108.,  12.]), 'currentState': array([100.07688809,  23.56067109,   0.53288174]), 'targetState': array([108,  12], dtype=int32), 'currentDistance': 14.015163874398839}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6902292881809686
{'scaleFactor': 20, 'currentTarget': array([108.,  12.]), 'dynamicTrap': False, 'previousTarget': array([108.,  12.]), 'currentState': array([107.48365544,  12.50992192,   5.31575814]), 'targetState': array([108,  12], dtype=int32), 'currentDistance': 0.7256941996575654}
episode index:2630
target Thresh 75.99986692892519
target distance 34.0
model initialize at round 2630
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.12547426, 15.76321063]), 'dynamicTrap': True, 'previousTarget': array([81.13698791, 15.66317505]), 'currentState': array([101.       ,  18.       ,   4.4109344], dtype=float32), 'targetState': array([67, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6189987278146494
running average episode reward sum: 0.6902022146118442
{'scaleFactor': 20, 'currentTarget': array([67., 14.]), 'dynamicTrap': False, 'previousTarget': array([67., 14.]), 'currentState': array([67.81858013, 14.45240521,  4.49877713]), 'targetState': array([67, 14], dtype=int32), 'currentDistance': 0.935277451817588}
episode index:2631
target Thresh 75.99986759261994
target distance 16.0
model initialize at round 2631
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.,  9.]), 'dynamicTrap': False, 'previousTarget': array([88.,  9.]), 'currentState': array([73.34885562, 12.21228227,  1.4984495 ]), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 14.99915960600144}
done in step count: 12
reward sum = 0.8477650211171293
running average episode reward sum: 0.69026207890003
{'scaleFactor': 20, 'currentTarget': array([88.,  9.]), 'dynamicTrap': False, 'previousTarget': array([88.,  9.]), 'currentState': array([87.32278846,  9.0713022 ,  5.35218908]), 'targetState': array([88,  9], dtype=int32), 'currentDistance': 0.6809548242893501}
episode index:2632
target Thresh 75.99986825300451
target distance 15.0
model initialize at round 2632
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.19935978, 23.01120551]), 'dynamicTrap': True, 'previousTarget': array([49., 23.]), 'currentState': array([56.       ,  8.       ,  3.2956073], dtype=float32), 'targetState': array([49, 23], dtype=int32), 'currentDistance': 16.479836113380962}
done in step count: 15
reward sum = 0.8015385040422884
running average episode reward sum: 0.6903043411199853
{'scaleFactor': 20, 'currentTarget': array([49., 23.]), 'dynamicTrap': False, 'previousTarget': array([49., 23.]), 'currentState': array([49.60383632, 22.15990192,  2.72495992]), 'targetState': array([49, 23], dtype=int32), 'currentDistance': 1.0345932009562036}
episode index:2633
target Thresh 75.99986891009539
target distance 62.0
model initialize at round 2633
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.22546941, 14.97414508]), 'dynamicTrap': False, 'previousTarget': array([70.87373448, 15.75619127]), 'currentState': array([52.3197948 , 16.91427872,  6.2755549 ]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5753585615197669
running average episode reward sum: 0.6902607018718455
{'scaleFactor': 20, 'currentTarget': array([113.,  11.]), 'dynamicTrap': False, 'previousTarget': array([113.,  11.]), 'currentState': array([112.25402826,  10.84900076,   1.57484174]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 0.7611009200013407}
episode index:2634
target Thresh 75.99986956390902
target distance 29.0
model initialize at round 2634
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.95684995, 15.47126806]), 'dynamicTrap': False, 'previousTarget': array([65.58520839, 15.94788792]), 'currentState': array([47.28328456, 19.06999347,  6.08207273]), 'targetState': array([75, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6830950572937461
running average episode reward sum: 0.6902579824621384
{'scaleFactor': 20, 'currentTarget': array([73.28900533, 13.61876869]), 'dynamicTrap': True, 'previousTarget': array([75., 14.]), 'currentState': array([72.96511661, 14.40345147,  1.08814773]), 'targetState': array([75, 14], dtype=int32), 'currentDistance': 0.848899860805544}
episode index:2635
target Thresh 75.99987021446172
target distance 7.0
model initialize at round 2635
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97., 23.]), 'dynamicTrap': False, 'previousTarget': array([97., 23.]), 'currentState': array([91.7215207 , 19.22801544,  1.04832142]), 'targetState': array([97, 23], dtype=int32), 'currentDistance': 6.4876969072805455}
done in step count: 8
reward sum = 0.8845110423349101
running average episode reward sum: 0.690331674821726
{'scaleFactor': 20, 'currentTarget': array([97., 23.]), 'dynamicTrap': False, 'previousTarget': array([97., 23.]), 'currentState': array([97.77475867, 23.0775851 ,  0.72026342]), 'targetState': array([97, 23], dtype=int32), 'currentDistance': 0.7786337063452032}
episode index:2636
target Thresh 75.99987086176981
target distance 26.0
model initialize at round 2636
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.79838512, 18.45939519]), 'dynamicTrap': False, 'previousTarget': array([45.11558017, 17.88171698]), 'currentState': array([26.38834035, 13.63755301,  0.32795095]), 'targetState': array([52, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = -0.5616809690194038
running average episode reward sum: 0.6898568880777589
{'scaleFactor': 20, 'currentTarget': array([48.4608321 , 18.81996405]), 'dynamicTrap': True, 'previousTarget': array([48.61732952, 18.94448436]), 'currentState': array([36.86219461,  5.03991798,  0.16951593]), 'targetState': array([52, 20], dtype=int32), 'currentDistance': 18.011609070714815}
episode index:2637
target Thresh 75.99987150584941
target distance 75.0
model initialize at round 2637
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.38712907, 17.08683133]), 'dynamicTrap': True, 'previousTarget': array([55.38754875, 17.08848765]), 'currentState': array([36.       , 22.       ,  2.2047207], dtype=float32), 'targetState': array([111,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 64
reward sum = 0.3981792351581572
running average episode reward sum: 0.6897463203548932
{'scaleFactor': 20, 'currentTarget': array([111.,   3.]), 'dynamicTrap': False, 'previousTarget': array([111.,   3.]), 'currentState': array([110.3982475 ,   2.07539451,   4.13305423]), 'targetState': array([111,   3], dtype=int32), 'currentDistance': 1.1031778529922203}
episode index:2638
target Thresh 75.99987214671665
target distance 9.0
model initialize at round 2638
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 10.]), 'currentState': array([10.32457297, 12.60057284,  2.43356133]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 7.772538081164988}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6898453138105753
{'scaleFactor': 20, 'currentTarget': array([ 3., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 10.]), 'currentState': array([ 2.12117612, 10.11305284,  3.59512579]), 'targetState': array([ 3, 10], dtype=int32), 'currentDistance': 0.8860656590594274}
episode index:2639
target Thresh 75.99987278438758
target distance 4.0
model initialize at round 2639
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99., 23.]), 'dynamicTrap': False, 'previousTarget': array([99., 23.]), 'currentState': array([100.31884122,  20.52576834,   3.00068295]), 'targetState': array([99, 23], dtype=int32), 'currentDistance': 2.803776827989878}
done in step count: 2
reward sum = 0.9603989999999999
running average episode reward sum: 0.6899477962674654
{'scaleFactor': 20, 'currentTarget': array([99.92899899, 21.34461209]), 'dynamicTrap': True, 'previousTarget': array([99.99403396, 21.53353413]), 'currentState': array([100.31884122,  20.52576834,   3.00068295]), 'targetState': array([99, 23], dtype=int32), 'currentDistance': 0.9069079706270554}
episode index:2640
target Thresh 75.99987341887808
target distance 36.0
model initialize at round 2640
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.17874952, 15.2952341 ]), 'dynamicTrap': False, 'previousTarget': array([81.12703142, 16.15562929]), 'currentState': array([62.87739998, 20.53526332,  6.22039258]), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5733368076417177
running average episode reward sum: 0.6899036421634799
{'scaleFactor': 20, 'currentTarget': array([98., 11.]), 'dynamicTrap': False, 'previousTarget': array([98., 11.]), 'currentState': array([97.44402445, 11.17076375,  4.04712002]), 'targetState': array([98, 11], dtype=int32), 'currentDistance': 0.5816090347485615}
episode index:2641
target Thresh 75.99987405020407
target distance 38.0
model initialize at round 2641
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.02042009, 12.32535368]), 'dynamicTrap': False, 'previousTarget': array([27.21128529, 12.43883847]), 'currentState': array([ 9.93698535, 18.3105453 ,  6.09888632]), 'targetState': array([46,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 25
reward sum = 0.7450942907935765
running average episode reward sum: 0.6899245318866556
{'scaleFactor': 20, 'currentTarget': array([46.,  7.]), 'dynamicTrap': False, 'previousTarget': array([46.,  7.]), 'currentState': array([45.32039625,  7.62105405,  4.34786769]), 'targetState': array([46,  7], dtype=int32), 'currentDistance': 0.9206353174925267}
episode index:2642
target Thresh 75.99987467838129
target distance 18.0
model initialize at round 2642
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  7.]), 'dynamicTrap': False, 'previousTarget': array([25.,  7.]), 'currentState': array([8.39543855, 3.94081844, 5.86639196]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 16.884017671516517}
done in step count: 18
reward sum = 0.7891233840316279
running average episode reward sum: 0.6899620645586741
{'scaleFactor': 20, 'currentTarget': array([24.62826367,  8.54408631]), 'dynamicTrap': True, 'previousTarget': array([25.,  7.]), 'currentState': array([24.18318771,  8.52322532,  4.02219611]), 'targetState': array([25,  7], dtype=int32), 'currentDistance': 0.445564579060136}
episode index:2643
target Thresh 75.99987530342547
target distance 74.0
model initialize at round 2643
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.12369516, 10.09331218]), 'dynamicTrap': False, 'previousTarget': array([67.01641512,  9.18985467]), 'currentState': array([86.08987557, 11.25591257,  3.11181664]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 56
reward sum = 0.5004967116935879
running average episode reward sum: 0.6898904059532033
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'dynamicTrap': False, 'previousTarget': array([13.,  7.]), 'currentState': array([13.3378386 ,  6.90716386,  5.67310019]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.3503619141420481}
episode index:2644
target Thresh 75.99987592535223
target distance 17.0
model initialize at round 2644
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.83115265, 19.92238898]), 'dynamicTrap': True, 'previousTarget': array([53., 20.]), 'currentState': array([36.       , 20.       ,  0.4851668], dtype=float32), 'targetState': array([53, 20], dtype=int32), 'currentDistance': 16.831331588510558}
done in step count: 11
reward sum = 0.8754382542587164
running average episode reward sum: 0.6899605563684417
{'scaleFactor': 20, 'currentTarget': array([53., 20.]), 'dynamicTrap': False, 'previousTarget': array([53., 20.]), 'currentState': array([53.10968346, 20.24319667,  5.49376159]), 'targetState': array([53, 20], dtype=int32), 'currentDistance': 0.2667865893058014}
episode index:2645
target Thresh 75.99987654417713
target distance 44.0
model initialize at round 2645
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.01109615, 17.00633598]), 'dynamicTrap': False, 'previousTarget': array([26.49734288, 16.56757793]), 'currentState': array([44.36273454, 22.05748354,  2.77336422]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.6899908218973293
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.42688247, 10.71541561,  2.91367568]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.5130466975296155}
episode index:2646
target Thresh 75.99987715991561
target distance 9.0
model initialize at round 2646
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 18.]), 'currentState': array([ 9.42850264, 11.23707919,  2.47958661]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 10.04588220811026}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6900894237968392
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 18.]), 'currentState': array([ 2.81411173, 17.57314812,  1.02668572]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.9192281763386972}
episode index:2647
target Thresh 75.99987777258308
target distance 27.0
model initialize at round 2647
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.21611814,  8.22283402]), 'dynamicTrap': False, 'previousTarget': array([89.01370332,  8.25976679]), 'currentState': array([107.19789287,   9.07646099,   2.46055996]), 'targetState': array([82,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6784359749917911
running average episode reward sum: 0.6900850229475927
{'scaleFactor': 20, 'currentTarget': array([82.,  8.]), 'dynamicTrap': False, 'previousTarget': array([82.,  8.]), 'currentState': array([81.68600345,  7.34591978,  3.18255994]), 'targetState': array([82,  8], dtype=int32), 'currentDistance': 0.7255444633922198}
episode index:2648
target Thresh 75.99987838219486
target distance 23.0
model initialize at round 2648
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.39378743,  5.53378066]), 'dynamicTrap': False, 'previousTarget': array([64.88993593,  5.4295875 ]), 'currentState': array([46.98926467, 13.36152249,  6.21563393]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8506435531472785
running average episode reward sum: 0.6901456339442704
{'scaleFactor': 20, 'currentTarget': array([69.,  4.]), 'dynamicTrap': False, 'previousTarget': array([69.,  4.]), 'currentState': array([69.00847063,  4.18105145,  0.91242085]), 'targetState': array([69,  4], dtype=int32), 'currentDistance': 0.1812494927060885}
episode index:2649
target Thresh 75.9998789887662
target distance 43.0
model initialize at round 2649
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.4215143 ,  9.60385267]), 'dynamicTrap': False, 'previousTarget': array([48.62394591,  9.043335  ]), 'currentState': array([69.77902865, 14.63243481,  0.19532618]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 40
reward sum = 0.6187373106066546
running average episode reward sum: 0.6901186874071619
{'scaleFactor': 20, 'currentTarget': array([25.,  3.]), 'dynamicTrap': False, 'previousTarget': array([25.,  3.]), 'currentState': array([24.79355813,  3.23796114,  3.90898103]), 'targetState': array([25,  3], dtype=int32), 'currentDistance': 0.3150297560328397}
episode index:2650
target Thresh 75.99987959231225
target distance 13.0
model initialize at round 2650
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  15.]), 'dynamicTrap': False, 'previousTarget': array([108.,  15.]), 'currentState': array([112.68605586,   3.96165696,   1.93205792]), 'targetState': array([108,  15], dtype=int32), 'currentDistance': 11.991836252131469}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6902064376927223
{'scaleFactor': 20, 'currentTarget': array([108.,  15.]), 'dynamicTrap': False, 'previousTarget': array([108.,  15.]), 'currentState': array([107.50351693,  15.46897525,   4.95743543]), 'targetState': array([108,  15], dtype=int32), 'currentDistance': 0.682959161279586}
episode index:2651
target Thresh 75.9998801928481
target distance 6.0
model initialize at round 2651
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.01750932,  5.34054573]), 'dynamicTrap': True, 'previousTarget': array([73.,  4.]), 'currentState': array([67.      ,  9.      ,  3.073136], dtype=float32), 'targetState': array([73,  4], dtype=int32), 'currentDistance': 7.914356743336636}
done in step count: 7
reward sum = 0.9121653479069899
running average episode reward sum: 0.6902901326060761
{'scaleFactor': 20, 'currentTarget': array([73.,  4.]), 'dynamicTrap': False, 'previousTarget': array([73.,  4.]), 'currentState': array([73.37203325,  4.02539125,  4.12570556]), 'targetState': array([73,  4], dtype=int32), 'currentDistance': 0.37289871877808317}
episode index:2652
target Thresh 75.99988079038876
target distance 23.0
model initialize at round 2652
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.17333839, 12.62744922]), 'dynamicTrap': True, 'previousTarget': array([82.16799178, 12.58678368]), 'currentState': array([102.      ,  10.      ,   3.151386], dtype=float32), 'targetState': array([79, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6968345915916051
running average episode reward sum: 0.6902925994206202
{'scaleFactor': 20, 'currentTarget': array([79., 13.]), 'dynamicTrap': False, 'previousTarget': array([79., 13.]), 'currentState': array([78.29324216, 13.20567143,  2.51701583]), 'targetState': array([79, 13], dtype=int32), 'currentDistance': 0.7360756654841547}
episode index:2653
target Thresh 75.99988138494918
target distance 15.0
model initialize at round 2653
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86., 17.]), 'dynamicTrap': False, 'previousTarget': array([86., 17.]), 'currentState': array([86.47701489,  1.90530227,  2.19369173]), 'targetState': array([86, 17], dtype=int32), 'currentDistance': 15.10223303576219}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6903801849876915
{'scaleFactor': 20, 'currentTarget': array([86., 17.]), 'dynamicTrap': False, 'previousTarget': array([86., 17.]), 'currentState': array([85.41152789, 16.46739213,  1.35748687]), 'targetState': array([86, 17], dtype=int32), 'currentDistance': 0.7937068583932096}
episode index:2654
target Thresh 75.99988197654422
target distance 5.0
model initialize at round 2654
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 17.]), 'dynamicTrap': False, 'previousTarget': array([38., 17.]), 'currentState': array([33.19149607, 16.0570781 ,  0.36359128]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 4.900082831214936}
done in step count: 4
reward sum = 0.9509900498999999
running average episode reward sum: 0.6904783431289013
{'scaleFactor': 20, 'currentTarget': array([36.32593814, 17.271743  ]), 'dynamicTrap': True, 'previousTarget': array([38., 17.]), 'currentState': array([36.35867284, 17.72927279,  0.79822436]), 'targetState': array([38, 17], dtype=int32), 'currentDistance': 0.4586993221964049}
episode index:2655
target Thresh 75.99988256518866
target distance 14.0
model initialize at round 2655
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.94001974,  4.19071196]), 'dynamicTrap': True, 'previousTarget': array([26.,  4.]), 'currentState': array([21.      , 18.      ,  4.969112], dtype=float32), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 14.666295756638439}
done in step count: 24
reward sum = 0.6459093758652483
running average episode reward sum: 0.6904615626442389
{'scaleFactor': 20, 'currentTarget': array([26.,  4.]), 'dynamicTrap': False, 'previousTarget': array([26.,  4.]), 'currentState': array([25.42500182,  3.41723479,  3.1826116 ]), 'targetState': array([26,  4], dtype=int32), 'currentDistance': 0.8186807696191655}
episode index:2656
target Thresh 75.99988315089722
target distance 36.0
model initialize at round 2656
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.9856111 , 10.31197419]), 'dynamicTrap': True, 'previousTarget': array([80.72376903, 10.87723068]), 'currentState': array([99.       , 19.       ,  1.5857056], dtype=float32), 'targetState': array([63,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6692199145425348
running average episode reward sum: 0.6904535680457813
{'scaleFactor': 20, 'currentTarget': array([64.58655568,  3.57853063]), 'dynamicTrap': True, 'previousTarget': array([63.,  3.]), 'currentState': array([64.7749763 ,  3.98476054,  4.47172731]), 'targetState': array([63,  3], dtype=int32), 'currentDistance': 0.4478002599637724}
episode index:2657
target Thresh 75.99988373368454
target distance 14.0
model initialize at round 2657
at step 0:
{'scaleFactor': 20, 'currentTarget': array([59., 15.]), 'dynamicTrap': False, 'previousTarget': array([59., 15.]), 'currentState': array([46.8183998 , 11.97220797,  5.56235314]), 'targetState': array([59, 15], dtype=int32), 'currentDistance': 12.552247133655651}
done in step count: 15
reward sum = 0.8225855522249294
running average episode reward sum: 0.6905032791007771
{'scaleFactor': 20, 'currentTarget': array([59., 15.]), 'dynamicTrap': False, 'previousTarget': array([59., 15.]), 'currentState': array([58.50757094, 14.6765703 ,  1.11127704]), 'targetState': array([59, 15], dtype=int32), 'currentDistance': 0.589146117907409}
episode index:2658
target Thresh 75.99988431356522
target distance 54.0
model initialize at round 2658
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.65464009,  7.97388751]), 'dynamicTrap': False, 'previousTarget': array([30.91481348,  8.84396421]), 'currentState': array([11.79884632,  5.57650271,  6.15830708]), 'targetState': array([65, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7152773459578534
running average episode reward sum: 0.6905125961624007
{'scaleFactor': 20, 'currentTarget': array([65., 12.]), 'dynamicTrap': False, 'previousTarget': array([65., 12.]), 'currentState': array([64.80645444, 11.25169748,  6.23218531]), 'targetState': array([65, 12], dtype=int32), 'currentDistance': 0.7729272573748555}
episode index:2659
target Thresh 75.99988489055372
target distance 62.0
model initialize at round 2659
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.37905246,  9.94492931]), 'dynamicTrap': True, 'previousTarget': array([33.43917548,  9.70302633]), 'currentState': array([14.       ,  5.       ,  0.5958942], dtype=float32), 'targetState': array([76, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6005352153720201
running average episode reward sum: 0.6904787700793968
{'scaleFactor': 20, 'currentTarget': array([76., 20.]), 'dynamicTrap': False, 'previousTarget': array([76., 20.]), 'currentState': array([76.56327657, 19.41171689,  4.13006754]), 'targetState': array([76, 20], dtype=int32), 'currentDistance': 0.8144676294756467}
episode index:2660
target Thresh 75.99988546466447
target distance 14.0
model initialize at round 2660
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'dynamicTrap': False, 'previousTarget': array([26., 19.]), 'currentState': array([11.83476114, 20.67232274,  5.25532866]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 14.263612983311052}
done in step count: 12
reward sum = 0.8581395162440494
running average episode reward sum: 0.6905417767483802
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'dynamicTrap': False, 'previousTarget': array([26., 19.]), 'currentState': array([26.1589496 , 19.02769085,  6.05226433]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.1613436036251678}
episode index:2661
target Thresh 75.99988603591184
target distance 20.0
model initialize at round 2661
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.08622551,  15.39523438]), 'dynamicTrap': True, 'previousTarget': array([113.14985851,  15.28991511]), 'currentState': array([96.       ,  5.       ,  4.3910847], dtype=float32), 'targetState': array([116,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8132421933839268
running average episode reward sum: 0.6905878700679279
{'scaleFactor': 20, 'currentTarget': array([116.,  17.]), 'dynamicTrap': False, 'previousTarget': array([116.,  17.]), 'currentState': array([116.73328525,  16.17336641,   1.67166571]), 'targetState': array([116,  17], dtype=int32), 'currentDistance': 1.1050024208868419}
episode index:2662
target Thresh 75.9998866043101
target distance 40.0
model initialize at round 2662
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.8002538 , 20.57910362]), 'dynamicTrap': False, 'previousTarget': array([41.22127294, 19.96680906]), 'currentState': array([59.63647237, 18.02480674,  1.7520107 ]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7432268537481035
running average episode reward sum: 0.6906076368661556
{'scaleFactor': 20, 'currentTarget': array([21., 23.]), 'dynamicTrap': False, 'previousTarget': array([21., 23.]), 'currentState': array([21.82773006, 23.67577615,  3.26735515]), 'targetState': array([21, 23], dtype=int32), 'currentDistance': 1.0685553091244424}
episode index:2663
target Thresh 75.99988716987346
target distance 52.0
model initialize at round 2663
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.1989359 ,  8.60364705]), 'dynamicTrap': True, 'previousTarget': array([77.21647182,  8.54321303]), 'currentState': array([58.       ,  3.       ,  5.4343133], dtype=float32), 'targetState': array([110,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6172091984612831
running average episode reward sum: 0.6905800848997874
{'scaleFactor': 20, 'currentTarget': array([110.,  18.]), 'dynamicTrap': False, 'previousTarget': array([110.,  18.]), 'currentState': array([109.2409087 ,  18.18374556,   0.30111237]), 'targetState': array([110,  18], dtype=int32), 'currentDistance': 0.7810134655349478}
episode index:2664
target Thresh 75.99988773261607
target distance 36.0
model initialize at round 2664
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.87772296, 16.87002554]), 'dynamicTrap': False, 'previousTarget': array([98.99228841, 17.44465866]), 'currentState': array([7.98784616e+01, 1.66981339e+01, 3.27960809e-02]), 'targetState': array([115,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7502111593616851
running average episode reward sum: 0.6906024605374842
{'scaleFactor': 20, 'currentTarget': array([115.,  17.]), 'dynamicTrap': False, 'previousTarget': array([115.,  17.]), 'currentState': array([114.0692116 ,  16.33278556,   1.19502797]), 'targetState': array([115,  17], dtype=int32), 'currentDistance': 1.1452258114561624}
episode index:2665
target Thresh 75.99988829255199
target distance 47.0
model initialize at round 2665
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.14083422, 16.35297949]), 'dynamicTrap': False, 'previousTarget': array([27.56211858, 17.16215289]), 'currentState': array([ 8.72651285, 11.54838529,  5.95762777]), 'targetState': array([55, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 91
reward sum = -0.007938464431938963
running average episode reward sum: 0.6903404421860329
{'scaleFactor': 20, 'currentTarget': array([53.350167 , 22.1169571]), 'dynamicTrap': True, 'previousTarget': array([55., 23.]), 'currentState': array([54.01983992, 21.45781627,  0.14527366]), 'targetState': array([55, 23], dtype=int32), 'currentDistance': 0.9396427257659327}
episode index:2666
target Thresh 75.99988884969521
target distance 40.0
model initialize at round 2666
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.74862356, 13.67980317]), 'dynamicTrap': False, 'previousTarget': array([70.29204194, 13.70332202]), 'currentState': array([86.17700992, 23.49058033,  3.13806832]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7067434226145054
running average episode reward sum: 0.6903465925348999
{'scaleFactor': 20, 'currentTarget': array([48.,  2.]), 'dynamicTrap': False, 'previousTarget': array([48.,  2.]), 'currentState': array([47.51092791,  2.94946652,  4.83533316]), 'targetState': array([48,  2], dtype=int32), 'currentDistance': 1.068025364376399}
episode index:2667
target Thresh 75.99988940405966
target distance 22.0
model initialize at round 2667
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.45669189, 16.51641228]), 'dynamicTrap': False, 'previousTarget': array([69.0585156 , 16.06407315]), 'currentState': array([51.3716887 , 10.53622388,  5.64584178]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.6904134588093505
{'scaleFactor': 20, 'currentTarget': array([72., 17.]), 'dynamicTrap': False, 'previousTarget': array([72., 17.]), 'currentState': array([71.86138178, 16.34856413,  6.2095252 ]), 'targetState': array([72, 17], dtype=int32), 'currentDistance': 0.6660207980304695}
episode index:2668
target Thresh 75.99988995565921
target distance 69.0
model initialize at round 2668
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.50396621,  9.27315674]), 'dynamicTrap': False, 'previousTarget': array([69.2957649, 10.4268235]), 'currentState': array([89.12872892,  5.41716286,  3.78353834]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.5049306734528276
running average episode reward sum: 0.6903439635731735
{'scaleFactor': 20, 'currentTarget': array([20., 19.]), 'dynamicTrap': False, 'previousTarget': array([20., 19.]), 'currentState': array([20.03716902, 19.03705122,  1.7211427 ]), 'targetState': array([20, 19], dtype=int32), 'currentDistance': 0.05248170081666943}
episode index:2669
target Thresh 75.99989050450766
target distance 4.0
model initialize at round 2669
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 13.]), 'currentState': array([5.81013449, 8.27419152, 3.74386692]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 6.070452256830889}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6904451815680899
{'scaleFactor': 20, 'currentTarget': array([ 2., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 13.]), 'currentState': array([ 1.73547366, 13.04561314,  2.40014064]), 'targetState': array([ 2, 13], dtype=int32), 'currentDistance': 0.2684301463928178}
episode index:2670
target Thresh 75.9998910506187
target distance 22.0
model initialize at round 2670
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.34355435, 11.29500106]), 'dynamicTrap': False, 'previousTarget': array([22.9793708 , 11.09184678]), 'currentState': array([ 2.46574864, 13.50245212,  0.92210317]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.772866303207725
running average episode reward sum: 0.6904760393448176
{'scaleFactor': 20, 'currentTarget': array([25., 11.]), 'dynamicTrap': False, 'previousTarget': array([25., 11.]), 'currentState': array([24.33832826, 10.85935181,  0.1905194 ]), 'targetState': array([25, 11], dtype=int32), 'currentDistance': 0.6764550264056095}
episode index:2671
target Thresh 75.99989159400602
target distance 7.0
model initialize at round 2671
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'dynamicTrap': False, 'previousTarget': array([22.,  7.]), 'currentState': array([27.10958526, 12.60295706,  3.17804885]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 7.582940679713337}
done in step count: 5
reward sum = 0.9411890499
running average episode reward sum: 0.6905698690643368
{'scaleFactor': 20, 'currentTarget': array([22.,  7.]), 'dynamicTrap': False, 'previousTarget': array([22.,  7.]), 'currentState': array([22.40021536,  6.58881551,  4.25651923]), 'targetState': array([22,  7], dtype=int32), 'currentDistance': 0.5737987606018394}
episode index:2672
target Thresh 75.99989213468317
target distance 44.0
model initialize at round 2672
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.16145284,  2.29232897]), 'dynamicTrap': False, 'previousTarget': array([69.0206292 ,  3.09184678]), 'currentState': array([88.15986004,  2.54473601,  3.19288492]), 'targetState': array([45,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7450546726730009
running average episode reward sum: 0.6905902524551368
{'scaleFactor': 20, 'currentTarget': array([45.,  2.]), 'dynamicTrap': False, 'previousTarget': array([45.,  2.]), 'currentState': array([44.302387  ,  1.66661907,  2.39098801]), 'targetState': array([45,  2], dtype=int32), 'currentDistance': 0.7731796258340294}
episode index:2673
target Thresh 75.99989267266366
target distance 8.0
model initialize at round 2673
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 13.]), 'dynamicTrap': False, 'previousTarget': array([49., 13.]), 'currentState': array([52.69536906,  5.05600315,  1.02047711]), 'targetState': array([49, 13], dtype=int32), 'currentDistance': 8.761440432052682}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6906876345783398
{'scaleFactor': 20, 'currentTarget': array([49., 13.]), 'dynamicTrap': False, 'previousTarget': array([49., 13.]), 'currentState': array([48.82498573, 13.05391028,  1.33528529]), 'targetState': array([49, 13], dtype=int32), 'currentDistance': 0.18312922144927157}
episode index:2674
target Thresh 75.999893207961
target distance 56.0
model initialize at round 2674
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.97052801,  6.91463793]), 'dynamicTrap': True, 'previousTarget': array([40.97136265,  6.93010557]), 'currentState': array([21.       ,  8.       ,  2.9556706], dtype=float32), 'targetState': array([77,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4922738612552029
running average episode reward sum: 0.6906134612051349
{'scaleFactor': 20, 'currentTarget': array([77.,  5.]), 'dynamicTrap': False, 'previousTarget': array([77.,  5.]), 'currentState': array([76.68294597,  5.49528246,  5.07034718]), 'targetState': array([77,  5], dtype=int32), 'currentDistance': 0.5880714055398689}
episode index:2675
target Thresh 75.9998937405885
target distance 62.0
model initialize at round 2675
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.72282491, 13.08282855]), 'dynamicTrap': False, 'previousTarget': array([48.30753468, 12.50617551]), 'currentState': array([66.34013683, 16.97655079,  3.08895588]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6081049150020638
running average episode reward sum: 0.690582628415074
{'scaleFactor': 20, 'currentTarget': array([6., 5.]), 'dynamicTrap': False, 'previousTarget': array([6., 5.]), 'currentState': array([6.13507405, 4.58495144, 5.11306682]), 'targetState': array([6, 5], dtype=int32), 'currentDistance': 0.436474867041837}
episode index:2676
target Thresh 75.99989427055954
target distance 10.0
model initialize at round 2676
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([91.12221451, 14.99639574,  5.38122768]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 10.219184025370879}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6906763518072988
{'scaleFactor': 20, 'currentTarget': array([89.,  5.]), 'dynamicTrap': False, 'previousTarget': array([89.,  5.]), 'currentState': array([89.31214005,  4.23752388,  3.21516396]), 'targetState': array([89,  5], dtype=int32), 'currentDistance': 0.8238939544875957}
episode index:2677
target Thresh 75.99989479788732
target distance 29.0
model initialize at round 2677
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.7296109,  15.0342027]), 'dynamicTrap': False, 'previousTarget': array([108.89383588,  15.05798302]), 'currentState': array([90.90377347, 12.40054168,  5.86637814]), 'targetState': array([118,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6907300625651938
{'scaleFactor': 20, 'currentTarget': array([118.,  16.]), 'dynamicTrap': False, 'previousTarget': array([118.,  16.]), 'currentState': array([117.90632432,  16.3494858 ,   0.43566313]), 'targetState': array([118,  16], dtype=int32), 'currentDistance': 0.36182241138676824}
episode index:2678
target Thresh 75.99989532258505
target distance 27.0
model initialize at round 2678
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.42378452, 11.57296402]), 'dynamicTrap': False, 'previousTarget': array([ 9.12232531, 11.79136948]), 'currentState': array([27.31311363, 13.67405596,  2.38280475]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7883173082635068
running average episode reward sum: 0.6907664893086422
{'scaleFactor': 20, 'currentTarget': array([ 2., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 11.]), 'currentState': array([ 2.03682951, 10.8032811 ,  2.60685594]), 'targetState': array([ 2, 11], dtype=int32), 'currentDistance': 0.2001367985919029}
episode index:2679
target Thresh 75.99989584466583
target distance 10.0
model initialize at round 2679
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.,  3.]), 'dynamicTrap': False, 'previousTarget': array([51.,  3.]), 'currentState': array([47.53507828, 11.50365386,  5.37989628]), 'targetState': array([51,  3], dtype=int32), 'currentDistance': 9.182473059603355}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6908635876521464
{'scaleFactor': 20, 'currentTarget': array([51.,  3.]), 'dynamicTrap': False, 'previousTarget': array([51.,  3.]), 'currentState': array([51.05732248,  3.0866328 ,  4.89619391]), 'targetState': array([51,  3], dtype=int32), 'currentDistance': 0.10388026505259645}
episode index:2680
target Thresh 75.99989636414273
target distance 42.0
model initialize at round 2680
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.8047542, 12.3954064]), 'dynamicTrap': False, 'previousTarget': array([92.72787848, 12.28797975]), 'currentState': array([73.06339912,  9.18933412,  1.14667861]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6531979217002122
running average episode reward sum: 0.6908495385413848
{'scaleFactor': 20, 'currentTarget': array([115.,  16.]), 'dynamicTrap': False, 'previousTarget': array([115.,  16.]), 'currentState': array([114.33291141,  15.34784469,   1.87730658]), 'targetState': array([115,  16], dtype=int32), 'currentDistance': 0.9329060683966374}
episode index:2681
target Thresh 75.99989688102873
target distance 31.0
model initialize at round 2681
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.30578177, 14.89182128]), 'dynamicTrap': True, 'previousTarget': array([45.34863803, 14.78011795]), 'currentState': array([64.       , 22.       ,  5.0812087], dtype=float32), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7485740868683092
running average episode reward sum: 0.6908710614900525
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([33.6850906 , 10.26324773,  4.98698112]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 0.7339267702573821}
episode index:2682
target Thresh 75.99989739533675
target distance 12.0
model initialize at round 2682
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.,  5.]), 'dynamicTrap': False, 'previousTarget': array([85.,  5.]), 'currentState': array([74.30159363,  5.27222901,  6.25512457]), 'targetState': array([85,  5], dtype=int32), 'currentDistance': 10.701869346590557}
done in step count: 9
reward sum = 0.894686693505571
running average episode reward sum: 0.6909470270629244
{'scaleFactor': 20, 'currentTarget': array([85.,  5.]), 'dynamicTrap': False, 'previousTarget': array([85.,  5.]), 'currentState': array([85.05527457,  4.34489002,  5.50943504]), 'targetState': array([85,  5], dtype=int32), 'currentDistance': 0.6574377270720033}
episode index:2683
target Thresh 75.99989790707963
target distance 19.0
model initialize at round 2683
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88., 15.]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([105.9142186 ,  19.65270632,   2.41513801]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 18.508562999370696}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6910265483177478
{'scaleFactor': 20, 'currentTarget': array([88., 15.]), 'dynamicTrap': False, 'previousTarget': array([88., 15.]), 'currentState': array([88.73742269, 15.21626364,  4.46350679]), 'targetState': array([88, 15], dtype=int32), 'currentDistance': 0.7684804420153771}
episode index:2684
target Thresh 75.9998984162702
target distance 42.0
model initialize at round 2684
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.88691447, 18.57285952]), 'dynamicTrap': False, 'previousTarget': array([45.45612429, 18.36758945]), 'currentState': array([27.54921258, 23.67710143,  5.80517513]), 'targetState': array([68, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6397281783490264
running average episode reward sum: 0.6910074427795844
{'scaleFactor': 20, 'currentTarget': array([68., 13.]), 'dynamicTrap': False, 'previousTarget': array([68., 13.]), 'currentState': array([68.27403228, 13.0171585 ,  1.09753507]), 'targetState': array([68, 13], dtype=int32), 'currentDistance': 0.2745689393620012}
episode index:2685
target Thresh 75.99989892292116
target distance 36.0
model initialize at round 2685
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.18390677,  5.74463056]), 'dynamicTrap': False, 'previousTarget': array([93.72787848,  5.28797975]), 'currentState': array([75.41167823,  2.73481689,  5.76785571]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7444985526139541
running average episode reward sum: 0.6910273575635882
{'scaleFactor': 20, 'currentTarget': array([110.,   8.]), 'dynamicTrap': False, 'previousTarget': array([110.,   8.]), 'currentState': array([109.93981882,   7.85108871,   1.14390229]), 'targetState': array([110,   8], dtype=int32), 'currentDistance': 0.16061241202572943}
episode index:2686
target Thresh 75.9998994270452
target distance 24.0
model initialize at round 2686
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.26953623,  7.91223849]), 'dynamicTrap': False, 'previousTarget': array([68.41416067,  7.47433703]), 'currentState': array([85.23706561, 18.5001064 ,  3.0512898 ]), 'targetState': array([62,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7626206394390459
running average episode reward sum: 0.6910540018813686
{'scaleFactor': 20, 'currentTarget': array([62.85035112,  2.58452371]), 'dynamicTrap': True, 'previousTarget': array([62.,  4.]), 'currentState': array([62.363808  ,  2.82193836,  2.8789998 ]), 'targetState': array([62,  4], dtype=int32), 'currentDistance': 0.5413778030782844}
episode index:2687
target Thresh 75.9998999286549
target distance 43.0
model initialize at round 2687
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.05186148, 10.56368366]), 'dynamicTrap': False, 'previousTarget': array([73.62394591,  9.956665  ]), 'currentState': array([91.47048897,  5.77652307,  2.42841572]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.605453029744017
running average episode reward sum: 0.6910221562816151
{'scaleFactor': 20, 'currentTarget': array([50., 16.]), 'dynamicTrap': False, 'previousTarget': array([50., 16.]), 'currentState': array([49.61744504, 15.52740171,  2.70074161]), 'targetState': array([50, 16], dtype=int32), 'currentDistance': 0.6080275053514242}
episode index:2688
target Thresh 75.99990042776281
target distance 12.0
model initialize at round 2688
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.99759681,  9.8103853 ]), 'dynamicTrap': True, 'previousTarget': array([32., 10.]), 'currentState': array([44.       , 10.       ,  1.5723895], dtype=float32), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 12.003900865085779}
done in step count: 10
reward sum = 0.8844820750088044
running average episode reward sum: 0.6910941012123429
{'scaleFactor': 20, 'currentTarget': array([32., 10.]), 'dynamicTrap': False, 'previousTarget': array([32., 10.]), 'currentState': array([31.94279024, 10.21515131,  4.52074164]), 'targetState': array([32, 10], dtype=int32), 'currentDistance': 0.222627591705431}
episode index:2689
target Thresh 75.99990092438142
target distance 67.0
model initialize at round 2689
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.98302049, 13.41466011]), 'dynamicTrap': False, 'previousTarget': array([52.05546015, 14.4883985 ]), 'currentState': array([72.8923208 , 11.51209316,  4.3330791 ]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 81
reward sum = 0.15905359379419048
running average episode reward sum: 0.6908963166370945
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 18.]), 'currentState': array([ 5.46280398, 17.47127615,  1.80948236]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.7026638130917261}
episode index:2690
target Thresh 75.99990141852314
target distance 11.0
model initialize at round 2690
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([21.61732223,  8.5244286 ,  4.1465193 ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 14.9916186312065}
done in step count: 11
reward sum = 0.8758342642587164
running average episode reward sum: 0.6909650412553113
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([ 9.00997965, 17.99275951,  1.55730593]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.9900468266748453}
episode index:2691
target Thresh 75.9999019102003
target distance 64.0
model initialize at round 2691
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.88074239,  6.81915563]), 'dynamicTrap': True, 'previousTarget': array([40.88143384,  6.82546817]), 'currentState': array([21.        ,  9.        ,  0.08091115], dtype=float32), 'targetState': array([85,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6119596114436522
running average episode reward sum: 0.6909356930272981
{'scaleFactor': 20, 'currentTarget': array([85.,  2.]), 'dynamicTrap': False, 'previousTarget': array([85.,  2.]), 'currentState': array([84.98688789,  1.45129828,  5.06733295]), 'targetState': array([85,  2], dtype=int32), 'currentDistance': 0.5488583681644008}
episode index:2692
target Thresh 75.99990239942521
target distance 12.0
model initialize at round 2692
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.,  9.]), 'dynamicTrap': False, 'previousTarget': array([93.,  9.]), 'currentState': array([91.78779507, 19.83254667,  4.91818714]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 10.900160918445692}
done in step count: 8
reward sum = 0.9034357443279201
running average episode reward sum: 0.6910146013270755
{'scaleFactor': 20, 'currentTarget': array([93.,  9.]), 'dynamicTrap': False, 'previousTarget': array([93.,  9.]), 'currentState': array([92.64202451,  9.83405543,  2.90058586]), 'targetState': array([93,  9], dtype=int32), 'currentDistance': 0.9076314850934271}
episode index:2693
target Thresh 75.99990288621011
target distance 23.0
model initialize at round 2693
at step 0:
{'scaleFactor': 20, 'currentTarget': array([74.73304412, 10.15786537]), 'dynamicTrap': False, 'previousTarget': array([72.92481176, 10.26740767]), 'currentState': array([54.88651619, 12.63078458,  0.14054364]), 'targetState': array([76, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8687458127689781
running average episode reward sum: 0.6910805743083085
{'scaleFactor': 20, 'currentTarget': array([74.29330642, 10.54889903]), 'dynamicTrap': True, 'previousTarget': array([76., 10.]), 'currentState': array([74.13235281, 10.73355884,  0.29141836]), 'targetState': array([76, 10], dtype=int32), 'currentDistance': 0.24495981113161613}
episode index:2694
target Thresh 75.99990337056715
target distance 32.0
model initialize at round 2694
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.99281239, 10.4638538 ]), 'dynamicTrap': True, 'previousTarget': array([75.99024152, 10.37530495]), 'currentState': array([56.       , 11.       ,  3.8600729], dtype=float32), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7038228637481035
running average episode reward sum: 0.6910853024305496
{'scaleFactor': 20, 'currentTarget': array([88., 10.]), 'dynamicTrap': False, 'previousTarget': array([88., 10.]), 'currentState': array([87.94670019,  9.42468909,  0.14572692]), 'targetState': array([88, 10], dtype=int32), 'currentDistance': 0.5777746190039307}
episode index:2695
target Thresh 75.99990385250847
target distance 27.0
model initialize at round 2695
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.75853424, 10.20783394]), 'dynamicTrap': False, 'previousTarget': array([63.17596225,  9.68176659]), 'currentState': array([45.53530464,  4.68810173,  5.72981567]), 'targetState': array([71, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7829372729508458
running average episode reward sum: 0.6911193721525526
{'scaleFactor': 20, 'currentTarget': array([71., 12.]), 'dynamicTrap': False, 'previousTarget': array([71., 12.]), 'currentState': array([70.86864225, 12.42884149,  5.54507747]), 'targetState': array([71, 12], dtype=int32), 'currentDistance': 0.4485085055697833}
episode index:2696
target Thresh 75.99990433204609
target distance 24.0
model initialize at round 2696
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.96601388, 16.82829005]), 'dynamicTrap': False, 'previousTarget': array([82.64100589, 16.09400392]), 'currentState': array([67.514945  ,  5.45453373,  0.74533456]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7326450198542003
running average episode reward sum: 0.6911347691298242
{'scaleFactor': 20, 'currentTarget': array([90., 21.]), 'dynamicTrap': False, 'previousTarget': array([90., 21.]), 'currentState': array([89.39036768, 20.32105011,  0.87337639]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 0.9124826089101269}
episode index:2697
target Thresh 75.99990480919199
target distance 52.0
model initialize at round 2697
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.04159124, 12.66741492]), 'dynamicTrap': False, 'previousTarget': array([42.17878677, 13.66824024]), 'currentState': array([61.77023601,  9.38403627,  5.3799125 ]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.6146487001673591
running average episode reward sum: 0.6911064199567469
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([10.35453104, 17.98904789,  3.05329992]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.3547001674242625}
episode index:2698
target Thresh 75.99990528395813
target distance 55.0
model initialize at round 2698
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.62800713,  7.84245639]), 'dynamicTrap': False, 'previousTarget': array([47.45968477,  7.2633415 ]), 'currentState': array([65.18981205,  3.67882959,  2.32865477]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.49307009486595826
running average episode reward sum: 0.6910330459941345
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([12.5572332 , 14.13100351,  2.65983616]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 1.0323098980190848}
episode index:2699
target Thresh 75.99990575635636
target distance 14.0
model initialize at round 2699
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  5.]), 'dynamicTrap': False, 'previousTarget': array([37.,  5.]), 'currentState': array([50.05423293,  7.5385838 ,  3.22901547]), 'targetState': array([37,  5], dtype=int32), 'currentDistance': 13.298774569753153}
done in step count: 14
reward sum = 0.8307014076958983
running average episode reward sum: 0.6910847750169871
{'scaleFactor': 20, 'currentTarget': array([37.,  5.]), 'dynamicTrap': False, 'previousTarget': array([37.,  5.]), 'currentState': array([36.5900603 ,  5.99748699,  0.4496389 ]), 'targetState': array([37,  5], dtype=int32), 'currentDistance': 1.078439084595168}
episode index:2700
target Thresh 75.9999062263985
target distance 7.0
model initialize at round 2700
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75., 16.]), 'dynamicTrap': False, 'previousTarget': array([75., 16.]), 'currentState': array([81.09060059, 21.22183351,  4.33192179]), 'targetState': array([75, 16], dtype=int32), 'currentDistance': 8.022652975231475}
done in step count: 12
reward sum = 0.8293120196987701
running average episode reward sum: 0.691135951338602
{'scaleFactor': 20, 'currentTarget': array([75., 16.]), 'dynamicTrap': False, 'previousTarget': array([75., 16.]), 'currentState': array([74.98149573, 16.02899499,  0.95336305]), 'targetState': array([75, 16], dtype=int32), 'currentDistance': 0.034396476154674296}
episode index:2701
target Thresh 75.99990669409628
target distance 60.0
model initialize at round 2701
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.42706163,  9.45702345]), 'dynamicTrap': False, 'previousTarget': array([81.52317581, 10.54459231]), 'currentState': array([101.80923925,   4.5243578 ,   4.20400143]), 'targetState': array([41, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5768082570126372
running average episode reward sum: 0.6910936390905168
{'scaleFactor': 20, 'currentTarget': array([41., 20.]), 'dynamicTrap': False, 'previousTarget': array([41., 20.]), 'currentState': array([41.31905059, 19.77982669,  3.92660207]), 'targetState': array([41, 20], dtype=int32), 'currentDistance': 0.38764618876003243}
episode index:2702
target Thresh 75.99990715946142
target distance 50.0
model initialize at round 2702
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.03941673,  6.76918336]), 'dynamicTrap': False, 'previousTarget': array([38.46711067,  7.29723565]), 'currentState': array([56.44682429,  1.93673763,  3.55951247]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5136892270184461
running average episode reward sum: 0.6910280066776155
{'scaleFactor': 20, 'currentTarget': array([ 8., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 14.]), 'currentState': array([ 7.30802793, 13.33210652,  3.77375387]), 'targetState': array([ 8, 14], dtype=int32), 'currentDistance': 0.9617208772949662}
episode index:2703
target Thresh 75.99990762250553
target distance 48.0
model initialize at round 2703
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.35162394, 10.86600866]), 'dynamicTrap': False, 'previousTarget': array([87.79065962,  9.88613786]), 'currentState': array([68.47888505,  8.61339927,  0.27226186]), 'targetState': array([116,  14], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6590607242342074
running average episode reward sum: 0.691016184457777
{'scaleFactor': 20, 'currentTarget': array([116.,  14.]), 'dynamicTrap': False, 'previousTarget': array([116.,  14.]), 'currentState': array([115.63694137,  14.22022928,   0.38528578]), 'targetState': array([116,  14], dtype=int32), 'currentDistance': 0.4246321983005816}
episode index:2704
target Thresh 75.99990808324021
target distance 9.0
model initialize at round 2704
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,  17.]), 'dynamicTrap': False, 'previousTarget': array([114.,  17.]), 'currentState': array([106.76432049,  20.62690657,   6.008084  ]), 'targetState': array([114,  17], dtype=int32), 'currentDistance': 8.09379448845127}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6911158442823767
{'scaleFactor': 20, 'currentTarget': array([114.,  17.]), 'dynamicTrap': False, 'previousTarget': array([114.,  17.]), 'currentState': array([113.25874202,  17.65576582,   6.05554843]), 'targetState': array([114,  17], dtype=int32), 'currentDistance': 0.9896929889800237}
episode index:2705
target Thresh 75.99990854167696
target distance 2.0
model initialize at round 2705
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'dynamicTrap': False, 'previousTarget': array([9., 5.]), 'currentState': array([10.93625422,  2.47401365,  3.66063976]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 3.1827169908713544}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6912226381314963
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'dynamicTrap': False, 'previousTarget': array([9., 5.]), 'currentState': array([8.99209698, 4.69475524, 2.28168207]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.305347054248405}
episode index:2706
target Thresh 75.99990899782725
target distance 15.0
model initialize at round 2706
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.,  5.]), 'dynamicTrap': False, 'previousTarget': array([71.,  5.]), 'currentState': array([69.01153172, 18.92801079,  4.97750378]), 'targetState': array([71,  5], dtype=int32), 'currentDistance': 14.06923916448922}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6913081653041214
{'scaleFactor': 20, 'currentTarget': array([71.,  5.]), 'dynamicTrap': False, 'previousTarget': array([71.,  5.]), 'currentState': array([70.697606  ,  5.89391901,  4.16130362]), 'targetState': array([71,  5], dtype=int32), 'currentDistance': 0.9436807377501486}
episode index:2707
target Thresh 75.99990945170248
target distance 11.0
model initialize at round 2707
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.,  21.]), 'dynamicTrap': False, 'previousTarget': array([104.,  21.]), 'currentState': array([109.60707211,   9.13795421,   2.78688908]), 'targetState': array([104,  21], dtype=int32), 'currentDistance': 13.120494958623993}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6913936293104448
{'scaleFactor': 20, 'currentTarget': array([104.,  21.]), 'dynamicTrap': False, 'previousTarget': array([104.,  21.]), 'currentState': array([104.00903481,  20.11024967,   0.98844039]), 'targetState': array([104,  21], dtype=int32), 'currentDistance': 0.8897962019269374}
episode index:2708
target Thresh 75.99990990331399
target distance 46.0
model initialize at round 2708
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.52470824, 17.33007468]), 'dynamicTrap': False, 'previousTarget': array([86.7042351, 17.4268235]), 'currentState': array([68.87911301, 13.58166328,  5.58266679]), 'targetState': array([113,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5928248921175572
running average episode reward sum: 0.6913572436562576
{'scaleFactor': 20, 'currentTarget': array([113.,  22.]), 'dynamicTrap': False, 'previousTarget': array([113.,  22.]), 'currentState': array([113.54797823,  21.63104305,   0.68021137]), 'targetState': array([113,  22], dtype=int32), 'currentDistance': 0.6606128782419655}
episode index:2709
target Thresh 75.9999103526731
target distance 23.0
model initialize at round 2709
at step 0:
{'scaleFactor': 20, 'currentTarget': array([55.1127648 , 18.36846211]), 'dynamicTrap': False, 'previousTarget': array([56.45647272, 18.75140711]), 'currentState': array([74.81538452, 21.80456106,  2.92153835]), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6914259387777862
{'scaleFactor': 20, 'currentTarget': array([53., 18.]), 'dynamicTrap': False, 'previousTarget': array([53., 18.]), 'currentState': array([53.21042369, 17.37240748,  3.6715875 ]), 'targetState': array([53, 18], dtype=int32), 'currentDistance': 0.6619293758303316}
episode index:2710
target Thresh 75.999910799791
target distance 10.0
model initialize at round 2710
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89., 16.]), 'dynamicTrap': False, 'previousTarget': array([89., 16.]), 'currentState': array([89.42692251,  6.59817664,  1.76821947]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 9.411511316039856}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6915216835624126
{'scaleFactor': 20, 'currentTarget': array([89., 16.]), 'dynamicTrap': False, 'previousTarget': array([89., 16.]), 'currentState': array([88.2599867 , 16.25721884,  2.23227046]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 0.7834419038394483}
episode index:2711
target Thresh 75.9999112446789
target distance 18.0
model initialize at round 2711
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  3.]), 'dynamicTrap': False, 'previousTarget': array([91.35899411,  3.90599608]), 'currentState': array([106.48127555,  13.89233235,   3.15947294]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 19.75538781384117}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6915935357704338
{'scaleFactor': 20, 'currentTarget': array([90.,  3.]), 'dynamicTrap': False, 'previousTarget': array([90.,  3.]), 'currentState': array([90.61722433,  2.57098467,  4.07025125]), 'targetState': array([90,  3], dtype=int32), 'currentDistance': 0.7516781419149738}
episode index:2712
target Thresh 75.9999116873479
target distance 61.0
model initialize at round 2712
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89.43410892, 10.02335871]), 'dynamicTrap': False, 'previousTarget': array([90.21419273,  9.08078253]), 'currentState': array([109.13905603,  13.4460856 ,   3.11779833]), 'targetState': array([49,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6430019804547129
running average episode reward sum: 0.69157562513449
{'scaleFactor': 20, 'currentTarget': array([49.,  3.]), 'dynamicTrap': False, 'previousTarget': array([49.,  3.]), 'currentState': array([49.71701748,  2.1324145 ,  3.82394591]), 'targetState': array([49,  3], dtype=int32), 'currentDistance': 1.125530395256224}
episode index:2713
target Thresh 75.99991212780911
target distance 58.0
model initialize at round 2713
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.69008967, 16.7136145 ]), 'dynamicTrap': False, 'previousTarget': array([32.27985236, 17.68142004]), 'currentState': array([14.33528773, 21.75262647,  4.79499328]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5561313940610906
running average episode reward sum: 0.6915257193750672
{'scaleFactor': 20, 'currentTarget': array([71.,  7.]), 'dynamicTrap': False, 'previousTarget': array([71.,  7.]), 'currentState': array([70.64676788,  7.91890118,  5.6027647 ]), 'targetState': array([71,  7], dtype=int32), 'currentDistance': 0.9844553325377602}
episode index:2714
target Thresh 75.99991256607348
target distance 63.0
model initialize at round 2714
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.23123395, 10.81822728]), 'dynamicTrap': False, 'previousTarget': array([73.04019095, 10.26728946]), 'currentState': array([91.20329208,  9.76139518,  3.12793863]), 'targetState': array([30, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4916882131799034
running average episode reward sum: 0.6914521144004098
{'scaleFactor': 20, 'currentTarget': array([30., 13.]), 'dynamicTrap': False, 'previousTarget': array([30., 13.]), 'currentState': array([30.06807006, 12.95334329,  3.71323425]), 'targetState': array([30, 13], dtype=int32), 'currentDistance': 0.08252503582971495}
episode index:2715
target Thresh 75.99991300215201
target distance 65.0
model initialize at round 2715
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.39283269,  6.40395681]), 'dynamicTrap': False, 'previousTarget': array([72.0212678 ,  5.92209533]), 'currentState': array([90.37931779,  5.66882865,  2.58286732]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 56
reward sum = 0.4403194790802941
running average episode reward sum: 0.6913596502489664
{'scaleFactor': 20, 'currentTarget': array([27.,  8.]), 'dynamicTrap': False, 'previousTarget': array([27.,  8.]), 'currentState': array([27.6384234 ,  7.88295587,  2.11276807]), 'targetState': array([27,  8], dtype=int32), 'currentDistance': 0.649063765207342}
episode index:2716
target Thresh 75.99991343605559
target distance 60.0
model initialize at round 2716
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.17691466,  7.29067646]), 'dynamicTrap': False, 'previousTarget': array([24.86526278,  6.68238601]), 'currentState': array([6.36008036, 9.99124644, 5.90298933]), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5079666219630375
running average episode reward sum: 0.6912921518947942
{'scaleFactor': 20, 'currentTarget': array([65.,  2.]), 'dynamicTrap': False, 'previousTarget': array([65.,  2.]), 'currentState': array([64.40546224,  1.14680979,  4.8454535 ]), 'targetState': array([65,  2], dtype=int32), 'currentDistance': 1.0399080169358452}
episode index:2717
target Thresh 75.99991386779506
target distance 62.0
model initialize at round 2717
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.32040137, 18.441672  ]), 'dynamicTrap': False, 'previousTarget': array([48.12626552, 18.24380873]), 'currentState': array([66.1938046 , 16.19493122,  2.44134212]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5178607845464083
running average episode reward sum: 0.6912283434447027
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.31370723, 22.83769718,  3.05758214]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 0.35320593167363856}
episode index:2718
target Thresh 75.99991429738122
target distance 42.0
model initialize at round 2718
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.14626776, 11.93956483]), 'dynamicTrap': False, 'previousTarget': array([86.35322867, 12.74224216]), 'currentState': array([105.64373788,   7.48438147,   5.40426338]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.47404106642708177
running average episode reward sum: 0.6911484658143174
{'scaleFactor': 20, 'currentTarget': array([64., 17.]), 'dynamicTrap': False, 'previousTarget': array([64., 17.]), 'currentState': array([63.2324054 , 17.49930936,  4.92501106]), 'targetState': array([64, 17], dtype=int32), 'currentDistance': 0.9157026314641519}
episode index:2719
target Thresh 75.99991472482482
target distance 24.0
model initialize at round 2719
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.52319107, 17.64042399]), 'dynamicTrap': True, 'previousTarget': array([54.58583933, 17.52566297]), 'currentState': array([37.       ,  8.       ,  5.6396427], dtype=float32), 'targetState': array([61, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.620761692859598
running average episode reward sum: 0.6911225883242605
{'scaleFactor': 20, 'currentTarget': array([61., 21.]), 'dynamicTrap': False, 'previousTarget': array([61., 21.]), 'currentState': array([61.87017234, 21.06033544,  0.34071811]), 'targetState': array([61, 21], dtype=int32), 'currentDistance': 0.8722615842496023}
episode index:2720
target Thresh 75.99991515013653
target distance 48.0
model initialize at round 2720
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.62504754, 21.52186165]), 'dynamicTrap': False, 'previousTarget': array([68.01733854, 20.83261089]), 'currentState': array([86.62182334, 21.16275524,  3.16539025]), 'targetState': array([40, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6883265425982749
running average episode reward sum: 0.69112156074406
{'scaleFactor': 20, 'currentTarget': array([40., 22.]), 'dynamicTrap': False, 'previousTarget': array([40., 22.]), 'currentState': array([39.60749347, 21.92923244,  2.68750149]), 'targetState': array([40, 22], dtype=int32), 'currentDistance': 0.39883508373869103}
episode index:2721
target Thresh 75.999915573327
target distance 51.0
model initialize at round 2721
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.96886991,  3.88454744]), 'dynamicTrap': True, 'previousTarget': array([80.96548746,  3.82555956]), 'currentState': array([61.       ,  5.       ,  4.0240464], dtype=float32), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6440044926778768
running average episode reward sum: 0.691104251020303
{'scaleFactor': 20, 'currentTarget': array([112.,   2.]), 'dynamicTrap': False, 'previousTarget': array([112.,   2.]), 'currentState': array([111.69046058,   1.19208883,   3.44055429]), 'targetState': array([112,   2], dtype=int32), 'currentDistance': 0.8651792416499423}
episode index:2722
target Thresh 75.99991599440678
target distance 13.0
model initialize at round 2722
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'dynamicTrap': False, 'previousTarget': array([16.,  2.]), 'currentState': array([11.50188126, 14.2405655 ,  4.80502355]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 13.04087865289214}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6911927420584546
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'dynamicTrap': False, 'previousTarget': array([16.,  2.]), 'currentState': array([15.51574659,  2.64177767,  3.45525371]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8039775779729779}
episode index:2723
target Thresh 75.99991641338642
target distance 65.0
model initialize at round 2723
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.72852664, 13.57990284]), 'dynamicTrap': False, 'previousTarget': array([58.23256605, 14.04114369]), 'currentState': array([76.44666725, 10.23402018,  4.52472782]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.2981717451839305
running average episode reward sum: 0.6910484612225976
{'scaleFactor': 20, 'currentTarget': array([13., 21.]), 'dynamicTrap': False, 'previousTarget': array([13., 21.]), 'currentState': array([12.35353173, 20.89347231,  1.81324097]), 'targetState': array([13, 21], dtype=int32), 'currentDistance': 0.655186516432578}
episode index:2724
target Thresh 75.99991683027639
target distance 4.0
model initialize at round 2724
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  8.]), 'dynamicTrap': False, 'previousTarget': array([37.,  8.]), 'currentState': array([40.07398114,  3.84987248,  5.08287932]), 'targetState': array([37,  8], dtype=int32), 'currentDistance': 5.164583089686774}
done in step count: 6
reward sum = 0.92255544740799
running average episode reward sum: 0.6911334179147757
{'scaleFactor': 20, 'currentTarget': array([38.17518393,  6.32065532]), 'dynamicTrap': True, 'previousTarget': array([38.01690837,  6.44286444]), 'currentState': array([37.19783143,  5.32114362,  1.78999997]), 'targetState': array([37,  8], dtype=int32), 'currentDistance': 1.3979418961831263}
episode index:2725
target Thresh 75.99991724508712
target distance 10.0
model initialize at round 2725
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.,  6.]), 'dynamicTrap': False, 'previousTarget': array([75.,  6.]), 'currentState': array([70.48872298, 15.25943951,  4.60722685]), 'targetState': array([75,  6], dtype=int32), 'currentDistance': 10.299943708161697}
done in step count: 10
reward sum = 0.8667161830914453
running average episode reward sum: 0.6911978283201964
{'scaleFactor': 20, 'currentTarget': array([75.,  6.]), 'dynamicTrap': False, 'previousTarget': array([75.,  6.]), 'currentState': array([74.32908115,  6.91701563,  3.95258559]), 'targetState': array([75,  6], dtype=int32), 'currentDistance': 1.1362437132983572}
episode index:2726
target Thresh 75.99991765782897
target distance 49.0
model initialize at round 2726
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.04994771, 16.46708041]), 'dynamicTrap': False, 'previousTarget': array([49.23047895, 17.50557744]), 'currentState': array([30.68377003, 21.46219422,  6.14077663]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 39
reward sum = 0.6087375371896198
running average episode reward sum: 0.6911675898562688
{'scaleFactor': 20, 'currentTarget': array([79.,  9.]), 'dynamicTrap': False, 'previousTarget': array([79.,  9.]), 'currentState': array([78.34004216,  8.51108084,  1.54867145]), 'targetState': array([79,  9], dtype=int32), 'currentDistance': 0.8213320252261361}
episode index:2727
target Thresh 75.99991806851227
target distance 66.0
model initialize at round 2727
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.42797754, 16.5775989 ]), 'dynamicTrap': False, 'previousTarget': array([65.99082358, 15.60578253]), 'currentState': array([45.42880011, 16.39620943,  0.94912934]), 'targetState': array([112,  17], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5617267572756807
running average episode reward sum: 0.6911201408707187
{'scaleFactor': 20, 'currentTarget': array([112.,  17.]), 'dynamicTrap': False, 'previousTarget': array([112.,  17.]), 'currentState': array([111.73021068,  17.85558288,   1.33352644]), 'targetState': array([112,  17], dtype=int32), 'currentDistance': 0.8971111125282968}
episode index:2728
target Thresh 75.99991847714726
target distance 21.0
model initialize at round 2728
at step 0:
{'scaleFactor': 20, 'currentTarget': array([62.0289864 , 19.77739541]), 'dynamicTrap': True, 'previousTarget': array([61.94278962, 19.59867161]), 'currentState': array([80.       , 11.       ,  1.8347993], dtype=float32), 'targetState': array([59, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7093273827300007
running average episode reward sum: 0.6911268126339504
{'scaleFactor': 20, 'currentTarget': array([59., 21.]), 'dynamicTrap': False, 'previousTarget': array([59., 21.]), 'currentState': array([59.07437594, 20.10560097,  2.78721799]), 'targetState': array([59, 21], dtype=int32), 'currentDistance': 0.8974861633728859}
episode index:2729
target Thresh 75.99991888374419
target distance 35.0
model initialize at round 2729
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.3846159 , 16.92307488]), 'dynamicTrap': True, 'previousTarget': array([39.23047895, 17.49442256]), 'currentState': array([20.      , 12.      ,  6.236259], dtype=float32), 'targetState': array([55, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.658324703742744
running average episode reward sum: 0.691114797209448
{'scaleFactor': 20, 'currentTarget': array([55., 22.]), 'dynamicTrap': False, 'previousTarget': array([55., 22.]), 'currentState': array([54.84424316, 22.13980686,  0.72971363]), 'targetState': array([55, 22], dtype=int32), 'currentDistance': 0.20929918691570512}
episode index:2730
target Thresh 75.99991928831321
target distance 56.0
model initialize at round 2730
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.89892248, 14.00820422]), 'dynamicTrap': True, 'previousTarget': array([52.949174, 13.424941]), 'currentState': array([33.       , 12.       ,  4.0408316], dtype=float32), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.5115526256766924
running average episode reward sum: 0.6910490476043463
{'scaleFactor': 20, 'currentTarget': array([89., 16.]), 'dynamicTrap': False, 'previousTarget': array([89., 16.]), 'currentState': array([89.15216437, 16.71384608,  5.922782  ]), 'targetState': array([89, 16], dtype=int32), 'currentDistance': 0.7298837043224696}
episode index:2731
target Thresh 75.99991969086442
target distance 73.0
model initialize at round 2731
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.46833539, 10.26107646]), 'dynamicTrap': False, 'previousTarget': array([31.77673548, 10.01994397]), 'currentState': array([13.72048081, 13.42686856,  0.86530894]), 'targetState': array([85,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5177990708725109
running average episode reward sum: 0.690985632532336
{'scaleFactor': 20, 'currentTarget': array([85.,  2.]), 'dynamicTrap': False, 'previousTarget': array([85.,  2.]), 'currentState': array([84.12949428,  2.75369793,  0.7394722 ]), 'targetState': array([85,  2], dtype=int32), 'currentDistance': 1.1514515963693472}
episode index:2732
target Thresh 75.9999200914079
target distance 21.0
model initialize at round 2732
at step 0:
{'scaleFactor': 20, 'currentTarget': array([109.84480029,  16.05089808]), 'dynamicTrap': False, 'previousTarget': array([107.97366596,  16.67544468]), 'currentState': array([90.84067834, 22.28334133,  6.18751973]), 'targetState': array([110,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7595858713192275
running average episode reward sum: 0.6910107332417348
{'scaleFactor': 20, 'currentTarget': array([110.,  16.]), 'dynamicTrap': False, 'previousTarget': array([110.,  16.]), 'currentState': array([110.28291841,  15.48065286,   5.60151898]), 'targetState': array([110,  16], dtype=int32), 'currentDistance': 0.591408722627082}
episode index:2733
target Thresh 75.99992048995367
target distance 17.0
model initialize at round 2733
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.6987526,  16.8372883]), 'dynamicTrap': False, 'previousTarget': array([107.88715666,  17.14900215]), 'currentState': array([92.78503196,  3.51133855,  6.20766926]), 'targetState': array([109,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8320003537189393
running average episode reward sum: 0.6910623022323995
{'scaleFactor': 20, 'currentTarget': array([109.,  18.]), 'dynamicTrap': False, 'previousTarget': array([109.,  18.]), 'currentState': array([108.44065514,  18.43182229,   6.09316583]), 'targetState': array([109,  18], dtype=int32), 'currentDistance': 0.7066379244434945}
episode index:2734
target Thresh 75.99992088651169
target distance 24.0
model initialize at round 2734
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.39590009, 15.538781  ]), 'dynamicTrap': False, 'previousTarget': array([43.88854382, 14.94427191]), 'currentState': array([27.7579149 ,  6.11002294,  0.58985227]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6630767578542427
running average episode reward sum: 0.6910520698578554
{'scaleFactor': 20, 'currentTarget': array([50., 18.]), 'dynamicTrap': False, 'previousTarget': array([50., 18.]), 'currentState': array([49.60609804, 17.0634747 ,  1.04060581]), 'targetState': array([50, 18], dtype=int32), 'currentDistance': 1.0159913385175083}
episode index:2735
target Thresh 75.99992128109184
target distance 38.0
model initialize at round 2735
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.76120122, 12.0790477 ]), 'dynamicTrap': False, 'previousTarget': array([92.0716533 , 13.02262736]), 'currentState': array([72.94188999,  5.30900156,  5.62235236]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7505391558051551
running average episode reward sum: 0.6910738122138302
{'scaleFactor': 20, 'currentTarget': array([111.,  19.]), 'dynamicTrap': False, 'previousTarget': array([111.,  19.]), 'currentState': array([110.48114323,  18.00037   ,   2.24255347]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 1.1262648367110182}
episode index:2736
target Thresh 75.99992167370404
target distance 47.0
model initialize at round 2736
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.93244881, 17.64240208]), 'dynamicTrap': True, 'previousTarget': array([40.92796014, 17.69599661]), 'currentState': array([21.       , 16.       ,  0.8533576], dtype=float32), 'targetState': array([68, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5635557589830389
running average episode reward sum: 0.6910272217669063
{'scaleFactor': 20, 'currentTarget': array([68., 20.]), 'dynamicTrap': False, 'previousTarget': array([68., 20.]), 'currentState': array([68.30834662, 20.29067385,  4.14689067]), 'targetState': array([68, 20], dtype=int32), 'currentDistance': 0.4237557404929084}
episode index:2737
target Thresh 75.99992206435807
target distance 30.0
model initialize at round 2737
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.70172207,  9.34819628]), 'dynamicTrap': False, 'previousTarget': array([69.38838649,  8.9223227 ]), 'currentState': array([87.35084167,  5.61830367,  2.21709347]), 'targetState': array([59, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.43917008078569564
running average episode reward sum: 0.6909352359593895
{'scaleFactor': 20, 'currentTarget': array([58.91473737, 12.29660676]), 'dynamicTrap': True, 'previousTarget': array([59., 11.]), 'currentState': array([59.63791814, 13.11300873,  5.27758163]), 'targetState': array([59, 11], dtype=int32), 'currentDistance': 1.0906432030617463}
episode index:2738
target Thresh 75.99992245306372
target distance 18.0
model initialize at round 2738
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.10639433,   5.61636551]), 'dynamicTrap': False, 'previousTarget': array([103.36442559,   5.19631201]), 'currentState': array([88.27144884, 19.02995535,  0.38175344]), 'targetState': array([106,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7905323615957605
running average episode reward sum: 0.6909715985463323
{'scaleFactor': 20, 'currentTarget': array([106.,   3.]), 'dynamicTrap': False, 'previousTarget': array([106.,   3.]), 'currentState': array([105.90964598,   2.44039764,   4.319296  ]), 'targetState': array([106,   3], dtype=int32), 'currentDistance': 0.566849763766094}
episode index:2739
target Thresh 75.99992283983066
target distance 11.0
model initialize at round 2739
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([28.06388064, 13.93231762,  1.0838908 ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 10.908414739787862}
done in step count: 9
reward sum = 0.8849865853906308
running average episode reward sum: 0.6910424069356915
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([21.42417888, 22.4181219 ,  1.4633557 ]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.8186281754215342}
episode index:2740
target Thresh 75.99992322466862
target distance 74.0
model initialize at round 2740
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.02031062,  6.56853817]), 'dynamicTrap': False, 'previousTarget': array([69.50774611,  7.47795022]), 'currentState': array([89.44975172,  1.82545705,  4.10805666]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.3527812807466556
running average episode reward sum: 0.6909189990093183
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'dynamicTrap': False, 'previousTarget': array([15., 20.]), 'currentState': array([15.15844646, 20.43342095,  4.13789306]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 0.461474808068705}
episode index:2741
target Thresh 75.99992360758718
target distance 34.0
model initialize at round 2741
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.29442911, 10.40171639]), 'dynamicTrap': False, 'previousTarget': array([91.6810367 ,  9.85725067]), 'currentState': array([73.9566731 , 18.38461715,  6.2393691 ]), 'targetState': array([107,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.732379184782806
running average episode reward sum: 0.6909341194271788
{'scaleFactor': 20, 'currentTarget': array([107.,   4.]), 'dynamicTrap': False, 'previousTarget': array([107.,   4.]), 'currentState': array([106.77219001,   3.01915215,   0.97120268]), 'targetState': array([107,   4], dtype=int32), 'currentDistance': 1.0069557612915692}
episode index:2742
target Thresh 75.99992398859592
target distance 64.0
model initialize at round 2742
at step 0:
{'scaleFactor': 20, 'currentTarget': array([57.02257863, 15.04992866]), 'dynamicTrap': True, 'previousTarget': array([57.02193651, 15.06352827]), 'currentState': array([77.       , 16.       ,  2.9740245], dtype=float32), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.6134049936176754
running average episode reward sum: 0.6909058550721625
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'dynamicTrap': False, 'previousTarget': array([13., 13.]), 'currentState': array([13.4228943 , 13.24499773,  4.40579159]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 0.4887366118174107}
episode index:2743
target Thresh 75.9999243677044
target distance 42.0
model initialize at round 2743
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.46183136,  8.0896704 ]), 'dynamicTrap': False, 'previousTarget': array([26.02633404,  8.32455532]), 'currentState': array([43.22774641,  1.17299256,  3.40234706]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 77
reward sum = 0.07859561694192596
running average episode reward sum: 0.6906827099416486
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.0375173 , 15.69966735,  3.10055434]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.3026668944238202}
episode index:2744
target Thresh 75.99992474492204
target distance 38.0
model initialize at round 2744
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.47247712, 12.67844354]), 'dynamicTrap': True, 'previousTarget': array([66.33093623, 13.37675141]), 'currentState': array([86.       , 17.       ,  2.4290028], dtype=float32), 'targetState': array([48, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7327982803783863
running average episode reward sum: 0.6906980525902594
{'scaleFactor': 20, 'currentTarget': array([48., 10.]), 'dynamicTrap': False, 'previousTarget': array([48., 10.]), 'currentState': array([47.65255793, 10.3070939 ,  2.03213448]), 'targetState': array([48, 10], dtype=int32), 'currentDistance': 0.46370535556815523}
episode index:2745
target Thresh 75.99992512025831
target distance 39.0
model initialize at round 2745
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.55157922, 14.22686724]), 'dynamicTrap': False, 'previousTarget': array([66.24899352, 13.4292033 ]), 'currentState': array([48.18914723,  9.21725372,  6.10206992]), 'targetState': array([86, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7214432365406714
running average episode reward sum: 0.690709248942754
{'scaleFactor': 20, 'currentTarget': array([86., 19.]), 'dynamicTrap': False, 'previousTarget': array([86., 19.]), 'currentState': array([8.59307946e+01, 1.88688491e+01, 2.82680988e-04]), 'targetState': array([86, 19], dtype=int32), 'currentDistance': 0.14829006338592432}
episode index:2746
target Thresh 75.99992549372257
target distance 23.0
model initialize at round 2746
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.35956391, 18.86716332]), 'dynamicTrap': False, 'previousTarget': array([83.41125677, 18.84114513]), 'currentState': array([65.87544608,  9.15605505,  2.14340262]), 'targetState': array([89, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6907615986014752
{'scaleFactor': 20, 'currentTarget': array([89., 22.]), 'dynamicTrap': False, 'previousTarget': array([89., 22.]), 'currentState': array([89.55375776, 21.27394296,  2.72209288]), 'targetState': array([89, 22], dtype=int32), 'currentDistance': 0.9131300435279798}
episode index:2747
target Thresh 75.99992586532419
target distance 21.0
model initialize at round 2747
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.7122291 ,  19.28853811]), 'dynamicTrap': True, 'previousTarget': array([111.36486284,  19.92277877]), 'currentState': array([94.       , 10.       ,  2.0193014], dtype=float32), 'targetState': array([115,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7702429765748817
running average episode reward sum: 0.6907905219559052
{'scaleFactor': 20, 'currentTarget': array([115.,  22.]), 'dynamicTrap': False, 'previousTarget': array([115.,  22.]), 'currentState': array([115.30472371,  22.23397441,   1.40794461]), 'targetState': array([115,  22], dtype=int32), 'currentDistance': 0.3841881869975285}
episode index:2748
target Thresh 75.99992623507242
target distance 7.0
model initialize at round 2748
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.,   6.]), 'dynamicTrap': False, 'previousTarget': array([101.,   6.]), 'currentState': array([92.33056725,  3.34369752,  2.03430831]), 'targetState': array([101,   6], dtype=int32), 'currentDistance': 9.067249145084972}
done in step count: 10
reward sum = 0.8568277694367246
running average episode reward sum: 0.6908509211001325
{'scaleFactor': 20, 'currentTarget': array([101.,   6.]), 'dynamicTrap': False, 'previousTarget': array([101.,   6.]), 'currentState': array([100.67739307,   6.98729822,   0.532655  ]), 'targetState': array([101,   6], dtype=int32), 'currentDistance': 1.0386688644361568}
episode index:2749
target Thresh 75.99992660297653
target distance 32.0
model initialize at round 2749
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.35631541,  3.22357839]), 'dynamicTrap': False, 'previousTarget': array([67.00975848,  3.37530495]), 'currentState': array([85.35165636,  3.65524989,  2.33951509]), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6671644069256304
running average episode reward sum: 0.6908423078222509
{'scaleFactor': 20, 'currentTarget': array([55.,  3.]), 'dynamicTrap': False, 'previousTarget': array([55.,  3.]), 'currentState': array([54.75913846,  2.08825802,  3.99410876]), 'targetState': array([55,  3], dtype=int32), 'currentDistance': 0.9430205288654785}
episode index:2750
target Thresh 75.99992696904572
target distance 52.0
model initialize at round 2750
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.65172106,  4.13270797]), 'dynamicTrap': False, 'previousTarget': array([30.9963028 ,  4.61545572]), 'currentState': array([12.65191227,  4.2201638 ,  5.70554322]), 'targetState': array([63,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6908573790915464
{'scaleFactor': 20, 'currentTarget': array([63.,  4.]), 'dynamicTrap': False, 'previousTarget': array([63.,  4.]), 'currentState': array([63.14042324,  4.64926455,  0.67149835]), 'targetState': array([63,  4], dtype=int32), 'currentDistance': 0.6642764027132021}
episode index:2751
target Thresh 75.99992733328912
target distance 65.0
model initialize at round 2751
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.6729387 , 19.65374921]), 'dynamicTrap': False, 'previousTarget': array([46.99763356, 18.6923441 ]), 'currentState': array([26.68623684, 20.38296023,  0.81725705]), 'targetState': array([92, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 60
reward sum = 0.43367670215892
running average episode reward sum: 0.6907639268106843
{'scaleFactor': 20, 'currentTarget': array([92., 18.]), 'dynamicTrap': False, 'previousTarget': array([92., 18.]), 'currentState': array([91.08900553, 17.7852247 ,  5.75926618]), 'targetState': array([92, 18], dtype=int32), 'currentDistance': 0.935969734450074}
episode index:2752
target Thresh 75.99992769571585
target distance 60.0
model initialize at round 2752
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.14716139,  8.61332484]), 'dynamicTrap': False, 'previousTarget': array([30.9007438 ,  7.99007438]), 'currentState': array([12.22271153,  6.87657557,  0.24268633]), 'targetState': array([71, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.585647169919167
running average episode reward sum: 0.69072574418922
{'scaleFactor': 20, 'currentTarget': array([71., 12.]), 'dynamicTrap': False, 'previousTarget': array([71., 12.]), 'currentState': array([71.58137662, 12.28656491,  5.3337671 ]), 'targetState': array([71, 12], dtype=int32), 'currentDistance': 0.6481652690169893}
episode index:2753
target Thresh 75.99992805633497
target distance 44.0
model initialize at round 2753
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.91446976,  6.39266209]), 'dynamicTrap': False, 'previousTarget': array([76.3226018 ,  5.57770876]), 'currentState': array([96.70807116,  3.52676892,  2.14654963]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5992425809938413
running average episode reward sum: 0.6906925259019304
{'scaleFactor': 20, 'currentTarget': array([52., 10.]), 'dynamicTrap': False, 'previousTarget': array([52., 10.]), 'currentState': array([51.86877852, 10.40661384,  4.90372434]), 'targetState': array([52, 10], dtype=int32), 'currentDistance': 0.4272632616112032}
episode index:2754
target Thresh 75.9999284151555
target distance 57.0
model initialize at round 2754
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.96512822, 19.2059036 ]), 'dynamicTrap': False, 'previousTarget': array([50.10989094, 18.09369569]), 'currentState': array([68.90504419, 17.65679061,  2.36258841]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6306034453715615
running average episode reward sum: 0.6906707149834077
{'scaleFactor': 20, 'currentTarget': array([13., 22.]), 'dynamicTrap': False, 'previousTarget': array([13., 22.]), 'currentState': array([13.54479582, 21.384573  ,  3.71390553]), 'targetState': array([13, 22], dtype=int32), 'currentDistance': 0.8219202388725948}
episode index:2755
target Thresh 75.9999287721864
target distance 36.0
model initialize at round 2755
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.13035996, 10.52879798]), 'dynamicTrap': False, 'previousTarget': array([72.46153846, 11.30769231]), 'currentState': array([53.34489423, 17.39219928,  5.24286175]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6427900061712218
running average episode reward sum: 0.6906533417218648
{'scaleFactor': 20, 'currentTarget': array([90.,  4.]), 'dynamicTrap': False, 'previousTarget': array([90.,  4.]), 'currentState': array([89.89192039,  4.94870979,  6.22568322]), 'targetState': array([90,  4], dtype=int32), 'currentDistance': 0.9548463078346267}
episode index:2756
target Thresh 75.9999291274366
target distance 25.0
model initialize at round 2756
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.0987387 ,  8.28140117]), 'dynamicTrap': False, 'previousTarget': array([34.92324388,  8.89833465]), 'currentState': array([20.44315332, 20.72739033,  5.08648658]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7926177903379419
running average episode reward sum: 0.6906903255624945
{'scaleFactor': 20, 'currentTarget': array([44.,  2.]), 'dynamicTrap': False, 'previousTarget': array([44.,  2.]), 'currentState': array([44.41186204,  2.68129063,  0.73248914]), 'targetState': array([44,  2], dtype=int32), 'currentDistance': 0.7961075665811345}
episode index:2757
target Thresh 75.99992948091499
target distance 39.0
model initialize at round 2757
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.68543912, 10.74226528]), 'dynamicTrap': False, 'previousTarget': array([27.76743395,  9.95885631]), 'currentState': array([ 7.01639421, 14.36561625,  1.18499398]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7391557884362057
running average episode reward sum: 0.6907078982466401
{'scaleFactor': 20, 'currentTarget': array([47.,  7.]), 'dynamicTrap': False, 'previousTarget': array([47.,  7.]), 'currentState': array([47.32738426,  7.56296451,  5.77560484]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 0.6512368897816007}
episode index:2758
target Thresh 75.99992983263039
target distance 66.0
model initialize at round 2758
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.64564903, 16.48132817]), 'dynamicTrap': False, 'previousTarget': array([43.77431008, 17.00389241]), 'currentState': array([25.85581755, 19.373139  ,  0.0303297 ]), 'targetState': array([90, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5219155054668002
running average episode reward sum: 0.6906467194163466
{'scaleFactor': 20, 'currentTarget': array([90., 10.]), 'dynamicTrap': False, 'previousTarget': array([90., 10.]), 'currentState': array([90.37302159, 10.11237075,  0.55015148]), 'targetState': array([90, 10], dtype=int32), 'currentDistance': 0.3895796342353422}
episode index:2759
target Thresh 75.99993018259161
target distance 55.0
model initialize at round 2759
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.25991273, 14.69138038]), 'dynamicTrap': False, 'previousTarget': array([64.16004177, 15.52508559]), 'currentState': array([83.02405237, 11.62890033,  3.08378792]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7090430744705851
running average episode reward sum: 0.6906533847623808
{'scaleFactor': 20, 'currentTarget': array([29., 20.]), 'dynamicTrap': False, 'previousTarget': array([29., 20.]), 'currentState': array([29.67126836, 20.29729584,  4.50938147]), 'targetState': array([29, 20], dtype=int32), 'currentDistance': 0.7341566817643163}
episode index:2760
target Thresh 75.99993053080738
target distance 58.0
model initialize at round 2760
at step 0:
{'scaleFactor': 20, 'currentTarget': array([86.89700163, 17.42716443]), 'dynamicTrap': False, 'previousTarget': array([85.55835583, 16.30718934]), 'currentState': array([106.24706463,  22.48434368,   1.19610494]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.47492909681857015
running average episode reward sum: 0.6905752520974247
{'scaleFactor': 20, 'currentTarget': array([47.,  7.]), 'dynamicTrap': False, 'previousTarget': array([47.,  7.]), 'currentState': array([47.734173  ,  7.08519794,  3.28606827]), 'targetState': array([47,  7], dtype=int32), 'currentDistance': 0.7390999180817749}
episode index:2761
target Thresh 75.99993087728643
target distance 39.0
model initialize at round 2761
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.14503521,  6.96712519]), 'dynamicTrap': False, 'previousTarget': array([87.37327732,  8.03249299]), 'currentState': array([68.57354977, 11.08500786,  4.94913545]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6906125580465725
{'scaleFactor': 20, 'currentTarget': array([107.,   3.]), 'dynamicTrap': False, 'previousTarget': array([107.,   3.]), 'currentState': array([106.61184762,   2.68551976,   1.14091051]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 0.4995598928282979}
episode index:2762
target Thresh 75.99993122203739
target distance 38.0
model initialize at round 2762
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.63643751,  6.94062882]), 'dynamicTrap': False, 'previousTarget': array([70.78871471,  7.43883847]), 'currentState': array([91.03194004, 11.82063715,  4.4212178 ]), 'targetState': array([52,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.690635759902945
{'scaleFactor': 20, 'currentTarget': array([52.,  2.]), 'dynamicTrap': False, 'previousTarget': array([52.,  2.]), 'currentState': array([51.74014466,  1.36995513,  2.56610178]), 'targetState': array([52,  2], dtype=int32), 'currentDistance': 0.6815286768150456}
episode index:2763
target Thresh 75.99993156506892
target distance 29.0
model initialize at round 2763
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.21496752, 13.02664404]), 'dynamicTrap': False, 'previousTarget': array([43.53574428, 13.35465912]), 'currentState': array([59.33317412,  1.18589169,  3.23422194]), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6906788467728141
{'scaleFactor': 20, 'currentTarget': array([31., 22.]), 'dynamicTrap': False, 'previousTarget': array([31., 22.]), 'currentState': array([31.25752504, 22.07605258,  3.14610383]), 'targetState': array([31, 22], dtype=int32), 'currentDistance': 0.2685202785933745}
episode index:2764
target Thresh 75.99993190638956
target distance 11.0
model initialize at round 2764
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90., 18.]), 'dynamicTrap': False, 'previousTarget': array([90., 18.]), 'currentState': array([87.38568164,  8.17307181,  1.46463161]), 'targetState': array([90, 18], dtype=int32), 'currentDistance': 10.168735325103926}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6907695524880504
{'scaleFactor': 20, 'currentTarget': array([90., 18.]), 'dynamicTrap': False, 'previousTarget': array([90., 18.]), 'currentState': array([89.32931785, 17.08108324,  6.25646936]), 'targetState': array([90, 18], dtype=int32), 'currentDistance': 1.1376390285299414}
episode index:2765
target Thresh 75.99993224600786
target distance 51.0
model initialize at round 2765
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.78256027, 11.74315501]), 'dynamicTrap': False, 'previousTarget': array([66.53165663, 12.41921333]), 'currentState': array([84.28883558, 16.15962682,  3.05900896]), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5879759999908374
running average episode reward sum: 0.6907323892369668
{'scaleFactor': 20, 'currentTarget': array([35.,  5.]), 'dynamicTrap': False, 'previousTarget': array([35.,  5.]), 'currentState': array([35.06895915,  5.91590374,  5.50068928]), 'targetState': array([35,  5], dtype=int32), 'currentDistance': 0.9184960671349883}
episode index:2766
target Thresh 75.9999325839323
target distance 28.0
model initialize at round 2766
at step 0:
{'scaleFactor': 20, 'currentTarget': array([81.83308381, 11.42147592]), 'dynamicTrap': True, 'previousTarget': array([81.68855151, 10.48418723]), 'currentState': array([62.      , 14.      ,  4.081832], dtype=float32), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8329431933839267
running average episode reward sum: 0.6907837845402364
{'scaleFactor': 20, 'currentTarget': array([90.,  9.]), 'dynamicTrap': False, 'previousTarget': array([90.,  9.]), 'currentState': array([90.2682008 ,  8.91912391,  5.35056361]), 'targetState': array([90,  9], dtype=int32), 'currentDistance': 0.28012962825122845}
episode index:2767
target Thresh 75.99993292017135
target distance 37.0
model initialize at round 2767
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.99990041, 17.93688509]), 'dynamicTrap': True, 'previousTarget': array([98., 18.]), 'currentState': array([78.        , 18.        ,  0.64720523], dtype=float32), 'targetState': array([115,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6726175510403952
running average episode reward sum: 0.6907772215946079
{'scaleFactor': 20, 'currentTarget': array([115.,  18.]), 'dynamicTrap': False, 'previousTarget': array([115.,  18.]), 'currentState': array([115.15803301,  18.2619409 ,   5.26844536]), 'targetState': array([115,  18], dtype=int32), 'currentDistance': 0.30592068558608987}
episode index:2768
target Thresh 75.99993325473338
target distance 14.0
model initialize at round 2768
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.,  3.]), 'dynamicTrap': False, 'previousTarget': array([72.,  3.]), 'currentState': array([84.264515  ,  5.10947649,  3.22549742]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 12.444606030405161}
done in step count: 11
reward sum = 0.8477839486866365
running average episode reward sum: 0.6908339231934132
{'scaleFactor': 20, 'currentTarget': array([72.,  3.]), 'dynamicTrap': False, 'previousTarget': array([72.,  3.]), 'currentState': array([72.37197031,  3.3807762 ,  2.55586222]), 'targetState': array([72,  3], dtype=int32), 'currentDistance': 0.5323085748931502}
episode index:2769
target Thresh 75.9999335876268
target distance 16.0
model initialize at round 2769
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.,  5.]), 'dynamicTrap': False, 'previousTarget': array([64.,  5.]), 'currentState': array([78.61976508, 14.03701524,  2.74077082]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 17.187355102274612}
done in step count: 9
reward sum = 0.9043820750088044
running average episode reward sum: 0.6909110163890143
{'scaleFactor': 20, 'currentTarget': array([65.21242067,  6.58955627]), 'dynamicTrap': True, 'previousTarget': array([64.,  5.]), 'currentState': array([66.1437996 ,  6.61119124,  4.11039504]), 'targetState': array([64,  5], dtype=int32), 'currentDistance': 0.9316301722957705}
episode index:2770
target Thresh 75.99993391885988
target distance 58.0
model initialize at round 2770
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.3765111 , 17.75943479]), 'dynamicTrap': False, 'previousTarget': array([57.36293823, 16.99234356]), 'currentState': array([39.14736476, 23.25851934,  6.13885469]), 'targetState': array([96,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.44590305407135833
running average episode reward sum: 0.6908225977811768
{'scaleFactor': 20, 'currentTarget': array([96.,  7.]), 'dynamicTrap': False, 'previousTarget': array([96.,  7.]), 'currentState': array([95.96279576,  7.54606729,  0.91369028]), 'targetState': array([96,  7], dtype=int32), 'currentDistance': 0.5473332060389857}
episode index:2771
target Thresh 75.99993424844095
target distance 68.0
model initialize at round 2771
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.31300355, 10.94764245]), 'dynamicTrap': False, 'previousTarget': array([52.96548746,  9.82555956]), 'currentState': array([32.37842417, 12.5639794 ,  0.93908715]), 'targetState': array([101,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.5038497234358382
running average episode reward sum: 0.6907551472493063
{'scaleFactor': 20, 'currentTarget': array([101.,   7.]), 'dynamicTrap': False, 'previousTarget': array([101.,   7.]), 'currentState': array([101.04316542,   7.17422287,   0.90685274]), 'targetState': array([101,   7], dtype=int32), 'currentDistance': 0.17949056244701928}
episode index:2772
target Thresh 75.99993457637822
target distance 31.0
model initialize at round 2772
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.21182554, 14.85598282]), 'dynamicTrap': True, 'previousTarget': array([13.96582764, 14.1400556 ]), 'currentState': array([33.       ,  8.       ,  6.1013813], dtype=float32), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6583081887118474
running average episode reward sum: 0.6907434462184597
{'scaleFactor': 20, 'currentTarget': array([ 2., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 18.]), 'currentState': array([ 1.85402829, 18.62816856,  1.8915832 ]), 'targetState': array([ 2, 18], dtype=int32), 'currentDistance': 0.644905794309979}
episode index:2773
target Thresh 75.9999349026799
target distance 41.0
model initialize at round 2773
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.42476616, 12.9893639 ]), 'dynamicTrap': False, 'previousTarget': array([49.09450732, 12.94199929]), 'currentState': array([67.31847857, 10.9301877 ,  2.17308617]), 'targetState': array([28, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.690777669251837
{'scaleFactor': 20, 'currentTarget': array([28., 15.]), 'dynamicTrap': False, 'previousTarget': array([28., 15.]), 'currentState': array([28.71634869, 15.25395819,  3.97529257]), 'targetState': array([28, 15], dtype=int32), 'currentDistance': 0.7600330317149433}
episode index:2774
target Thresh 75.99993522735413
target distance 59.0
model initialize at round 2774
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.00197014, 15.71928389]), 'dynamicTrap': True, 'previousTarget': array([43.00287212, 15.66106563]), 'currentState': array([63.       , 16.       ,  5.0178714], dtype=float32), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 43
reward sum = 0.5551885000845315
running average episode reward sum: 0.690728808289975
{'scaleFactor': 20, 'currentTarget': array([ 4., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 15.]), 'currentState': array([ 3.58198431, 15.38565355,  1.72942742]), 'targetState': array([ 4, 15], dtype=int32), 'currentDistance': 0.5687405235466162}
episode index:2775
target Thresh 75.99993555040905
target distance 23.0
model initialize at round 2775
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.78057161,  18.37932385]), 'dynamicTrap': False, 'previousTarget': array([104.04268443,  17.62910995]), 'currentState': array([87.91642452,  9.38642396,  0.57136935]), 'targetState': array([109,  20], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 34
reward sum = 0.5644041124739201
running average episode reward sum: 0.690683302275632
{'scaleFactor': 20, 'currentTarget': array([109.,  20.]), 'dynamicTrap': False, 'previousTarget': array([109.,  20.]), 'currentState': array([108.23595878,  19.5497683 ,   0.29253948]), 'targetState': array([109,  20], dtype=int32), 'currentDistance': 0.8868300694704834}
episode index:2776
target Thresh 75.99993587185273
target distance 58.0
model initialize at round 2776
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.57305689, 18.88959322]), 'dynamicTrap': True, 'previousTarget': array([35.58520839, 18.94788792]), 'currentState': array([16.      , 23.      ,  1.682449], dtype=float32), 'targetState': array([74, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5074621039073184
running average episode reward sum: 0.6906173241703499
{'scaleFactor': 20, 'currentTarget': array([74., 11.]), 'dynamicTrap': False, 'previousTarget': array([74., 11.]), 'currentState': array([74.40865213, 11.03203632,  6.25324769]), 'targetState': array([74, 11], dtype=int32), 'currentDistance': 0.40990594904498384}
episode index:2777
target Thresh 75.9999361916932
target distance 22.0
model initialize at round 2777
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.25532037, 11.60920978]), 'dynamicTrap': False, 'previousTarget': array([53.48906088, 11.42734309]), 'currentState': array([71.18432645, 20.47209536,  3.08677435]), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8062684059780076
running average episode reward sum: 0.690658955229316
{'scaleFactor': 20, 'currentTarget': array([50., 10.]), 'dynamicTrap': False, 'previousTarget': array([50., 10.]), 'currentState': array([50.1983488 , 10.4155309 ,  4.59494855]), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 0.460443458307497}
episode index:2778
target Thresh 75.99993650993845
target distance 2.0
model initialize at round 2778
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  14.]), 'dynamicTrap': False, 'previousTarget': array([112.,  14.]), 'currentState': array([114.74321422,  14.6363187 ,   4.20138979]), 'targetState': array([112,  14], dtype=int32), 'currentDistance': 2.8160478960165447}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.690763108178136
{'scaleFactor': 20, 'currentTarget': array([112.,  14.]), 'dynamicTrap': False, 'previousTarget': array([112.,  14.]), 'currentState': array([111.94532692,  14.33905701,   2.32467699]), 'targetState': array([112,  14], dtype=int32), 'currentDistance': 0.34343674880105657}
episode index:2779
target Thresh 75.99993682659645
target distance 27.0
model initialize at round 2779
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.18511247,  12.84964778]), 'dynamicTrap': False, 'previousTarget': array([109.97366596,  12.32455532]), 'currentState': array([92.11207935,  6.83139158,  5.94433684]), 'targetState': array([118,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6908178492159799
{'scaleFactor': 20, 'currentTarget': array([118.,  15.]), 'dynamicTrap': False, 'previousTarget': array([118.,  15.]), 'currentState': array([117.31323428,  14.97486369,   0.3090196 ]), 'targetState': array([118,  15], dtype=int32), 'currentDistance': 0.6872255729657074}
episode index:2780
target Thresh 75.99993714167512
target distance 4.0
model initialize at round 2780
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([102.53652698,   4.7999768 ,   1.40434771]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 3.3028474652638415}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6909218701260065
{'scaleFactor': 20, 'currentTarget': array([105.,   7.]), 'dynamicTrap': False, 'previousTarget': array([105.,   7.]), 'currentState': array([105.24476919,   6.82707639,   5.74250805]), 'targetState': array([105,   7], dtype=int32), 'currentDistance': 0.29969072760175725}
episode index:2781
target Thresh 75.99993745518232
target distance 47.0
model initialize at round 2781
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.57022432, 10.95149667]), 'dynamicTrap': False, 'previousTarget': array([32.07203986, 10.30400339]), 'currentState': array([50.43830692, 13.2448117 ,  2.56272514]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6388425168156985
running average episode reward sum: 0.690903150013386
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([4.8614713 , 7.60465082, 3.51507052]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.4189166697974947}
episode index:2782
target Thresh 75.9999377671259
target distance 12.0
model initialize at round 2782
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([ 6.06208269, 12.2362674 ,  4.13808118]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 10.68445228582205}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6909931884608843
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([3.10625657, 1.77290412, 3.04602225]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.2507249421394013}
episode index:2783
target Thresh 75.99993807751366
target distance 32.0
model initialize at round 2783
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.46751038,  9.11424823]), 'dynamicTrap': False, 'previousTarget': array([32.47066846,  9.47245906]), 'currentState': array([52.32401442, 15.78000065,  4.7706357 ]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.704872962455028
running average episode reward sum: 0.6909981740118879
{'scaleFactor': 20, 'currentTarget': array([19.,  4.]), 'dynamicTrap': False, 'previousTarget': array([19.,  4.]), 'currentState': array([19.57911901,  4.71710674,  2.91240382]), 'targetState': array([19,  4], dtype=int32), 'currentDistance': 0.9217488272796462}
episode index:2784
target Thresh 75.99993838635335
target distance 32.0
model initialize at round 2784
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.08791657,  5.87321475]), 'dynamicTrap': True, 'previousTarget': array([84.08731548,  5.86681417]), 'currentState': array([104.       ,   4.       ,   5.9666963], dtype=float32), 'targetState': array([72,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7422933755481026
running average episode reward sum: 0.6910165923966406
{'scaleFactor': 20, 'currentTarget': array([72.,  7.]), 'dynamicTrap': False, 'previousTarget': array([72.,  7.]), 'currentState': array([71.84342084,  6.4145178 ,  2.5006329 ]), 'targetState': array([72,  7], dtype=int32), 'currentDistance': 0.6060581153662142}
episode index:2785
target Thresh 75.9999386936527
target distance 19.0
model initialize at round 2785
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5.02559797, 2.052279  ]), 'dynamicTrap': False, 'previousTarget': array([5.68507134, 3.30163555]), 'currentState': array([13.82071173, 20.01462779,  2.98630297]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6910835358390679
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.76602441, 1.84916519, 3.58339027]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 0.27838052278329134}
episode index:2786
target Thresh 75.99993899941937
target distance 48.0
model initialize at round 2786
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.41996436, 13.89463376]), 'dynamicTrap': False, 'previousTarget': array([30.97366596, 13.67544468]), 'currentState': array([13.59479244, 20.64836621,  5.72579104]), 'targetState': array([60,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5780914564897305
running average episode reward sum: 0.6910429932917592
{'scaleFactor': 20, 'currentTarget': array([60.,  4.]), 'dynamicTrap': False, 'previousTarget': array([60.,  4.]), 'currentState': array([59.96379389,  3.99782766,  5.61291966]), 'targetState': array([60,  4], dtype=int32), 'currentDistance': 0.03627121750887999}
episode index:2787
target Thresh 75.99993930366104
target distance 27.0
model initialize at round 2787
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.49522674, 12.36314066]), 'dynamicTrap': False, 'previousTarget': array([29.0200334 , 13.32368762]), 'currentState': array([12.24407085, 20.54209588,  4.91024578]), 'targetState': array([38,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7489112743763775
running average episode reward sum: 0.6910637494901396
{'scaleFactor': 20, 'currentTarget': array([38.,  9.]), 'dynamicTrap': False, 'previousTarget': array([38.,  9.]), 'currentState': array([38.64017519,  8.71456945,  5.85313837]), 'targetState': array([38,  9], dtype=int32), 'currentDistance': 0.7009242956199982}
episode index:2788
target Thresh 75.9999396063853
target distance 64.0
model initialize at round 2788
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.92346948, 12.74795992]), 'dynamicTrap': True, 'previousTarget': array([63.96105157, 12.24756572]), 'currentState': array([44.       , 11.       ,  4.1079316], dtype=float32), 'targetState': array([108,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 45
reward sum = 0.5740312721282459
running average episode reward sum: 0.6910217873254347
{'scaleFactor': 20, 'currentTarget': array([108.,  15.]), 'dynamicTrap': False, 'previousTarget': array([108.,  15.]), 'currentState': array([107.87039407,  15.35564549,   0.3298923 ]), 'targetState': array([108,  15], dtype=int32), 'currentDistance': 0.37852531467709827}
episode index:2789
target Thresh 75.99993990759971
target distance 26.0
model initialize at round 2789
at step 0:
{'scaleFactor': 20, 'currentTarget': array([104.60410472,  15.1059922 ]), 'dynamicTrap': False, 'previousTarget': array([103.41934502,  15.20720018]), 'currentState': array([86.61329435, 23.84273886,  0.64694775]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6910823739087021
{'scaleFactor': 20, 'currentTarget': array([111.,  12.]), 'dynamicTrap': False, 'previousTarget': array([111.,  12.]), 'currentState': array([111.10830666,  11.33825698,   4.28945344]), 'targetState': array([111,  12], dtype=int32), 'currentDistance': 0.6705476558087564}
episode index:2790
target Thresh 75.9999402073118
target distance 52.0
model initialize at round 2790
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.73517115, 14.39682928]), 'dynamicTrap': False, 'previousTarget': array([83.21647182, 13.45678697]), 'currentState': array([64.69907248, 20.5309101 ,  0.13243556]), 'targetState': array([116,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.49823028643763884
running average episode reward sum: 0.6910132760629583
{'scaleFactor': 20, 'currentTarget': array([116.,   4.]), 'dynamicTrap': False, 'previousTarget': array([116.,   4.]), 'currentState': array([115.02307213,   3.44434343,   0.97614059]), 'targetState': array([116,   4], dtype=int32), 'currentDistance': 1.123896030127158}
episode index:2791
target Thresh 75.99994050552907
target distance 10.0
model initialize at round 2791
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 12.]), 'currentState': array([14.99188944,  3.11435536,  1.78678513]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 10.717155387273607}
done in step count: 9
reward sum = 0.8841132574836408
running average episode reward sum: 0.6910824379474212
{'scaleFactor': 20, 'currentTarget': array([ 9., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 12.]), 'currentState': array([ 8.7873229 , 12.47580786,  1.07351287]), 'targetState': array([ 9, 12], dtype=int32), 'currentDistance': 0.5211762352505609}
episode index:2792
target Thresh 75.99994080225899
target distance 16.0
model initialize at round 2792
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.91197642, 20.82065748]), 'dynamicTrap': True, 'previousTarget': array([38., 21.]), 'currentState': array([22.      , 17.      ,  1.161342], dtype=float32), 'targetState': array([38, 21], dtype=int32), 'currentDistance': 16.364242024762746}
done in step count: 10
reward sum = 0.8943820750088044
running average episode reward sum: 0.6911552269331216
{'scaleFactor': 20, 'currentTarget': array([38., 21.]), 'dynamicTrap': False, 'previousTarget': array([38., 21.]), 'currentState': array([38.45205725, 21.21116479,  6.25026543]), 'targetState': array([38, 21], dtype=int32), 'currentDistance': 0.4989452181908006}
episode index:2793
target Thresh 75.99994109750895
target distance 15.0
model initialize at round 2793
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.81257344,  13.06971821]), 'dynamicTrap': True, 'previousTarget': array([118.,  13.]), 'currentState': array([103.      ,  18.      ,   5.955061], dtype=float32), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 15.61153453686659}
done in step count: 9
reward sum = 0.9035172474836408
running average episode reward sum: 0.6912312333828534
{'scaleFactor': 20, 'currentTarget': array([118.,  13.]), 'dynamicTrap': False, 'previousTarget': array([118.,  13.]), 'currentState': array([117.42018934,  13.53286816,   5.3371887 ]), 'targetState': array([118,  13], dtype=int32), 'currentDistance': 0.7874826211559005}
episode index:2794
target Thresh 75.99994139128636
target distance 65.0
model initialize at round 2794
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.86281823,  6.54726278]), 'dynamicTrap': False, 'previousTarget': array([46.85022022,  7.44310403]), 'currentState': array([28.07318842,  3.65407215,  0.04611903]), 'targetState': array([92, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6220144811507068
running average episode reward sum: 0.6912064688918939
{'scaleFactor': 20, 'currentTarget': array([92., 13.]), 'dynamicTrap': False, 'previousTarget': array([92., 13.]), 'currentState': array([91.18138122, 12.19253751,  1.77609503]), 'targetState': array([92, 13], dtype=int32), 'currentDistance': 1.1498401533514566}
episode index:2795
target Thresh 75.99994168359852
target distance 15.0
model initialize at round 2795
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96., 17.]), 'dynamicTrap': False, 'previousTarget': array([96., 17.]), 'currentState': array([85.02981072,  3.68270598,  0.54308224]), 'targetState': array([96, 17], dtype=int32), 'currentDistance': 17.253850957806627}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6912794773988205
{'scaleFactor': 20, 'currentTarget': array([96., 17.]), 'dynamicTrap': False, 'previousTarget': array([96., 17.]), 'currentState': array([95.78853375, 16.80198475,  0.51716002]), 'targetState': array([96, 17], dtype=int32), 'currentDistance': 0.28970332101270446}
episode index:2796
target Thresh 75.9999419744528
target distance 75.0
model initialize at round 2796
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.7594893 , 18.42412754]), 'dynamicTrap': False, 'previousTarget': array([47.93630557, 17.59490445]), 'currentState': array([26.79979359, 17.15505432,  1.36536288]), 'targetState': array([103,  22], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 70
reward sum = 0.32101468144242173
running average episode reward sum: 0.6911470981367696
{'scaleFactor': 20, 'currentTarget': array([103.,  22.]), 'dynamicTrap': False, 'previousTarget': array([103.,  22.]), 'currentState': array([102.49936944,  21.20974223,   1.28655505]), 'targetState': array([103,  22], dtype=int32), 'currentDistance': 0.9354882679792329}
episode index:2797
target Thresh 75.99994226385643
target distance 18.0
model initialize at round 2797
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.95636427,  8.8051596 ]), 'dynamicTrap': True, 'previousTarget': array([73.,  9.]), 'currentState': array([55.       , 14.       ,  1.4494755], dtype=float32), 'targetState': array([73,  9], dtype=int32), 'currentDistance': 18.692709393712587}
done in step count: 19
reward sum = 0.7335875828402968
running average episode reward sum: 0.6911622662871282
{'scaleFactor': 20, 'currentTarget': array([71.06884351,  9.51599975]), 'dynamicTrap': True, 'previousTarget': array([73.,  9.]), 'currentState': array([71.50809577,  8.99087201,  6.0788374 ]), 'targetState': array([73,  9], dtype=int32), 'currentDistance': 0.6846179126610018}
episode index:2798
target Thresh 75.99994255181663
target distance 58.0
model initialize at round 2798
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.00511859,  3.55913462]), 'dynamicTrap': False, 'previousTarget': array([53.98811999,  3.68924552]), 'currentState': array([34.01948434,  2.80122704,  4.90175514]), 'targetState': array([92,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.6009701098804776
running average episode reward sum: 0.6911300432944856
{'scaleFactor': 20, 'currentTarget': array([92.,  5.]), 'dynamicTrap': False, 'previousTarget': array([92.,  5.]), 'currentState': array([9.24865810e+01, 5.24049587e+00, 5.73621516e-02]), 'targetState': array([92,  5], dtype=int32), 'currentDistance': 0.5427700284606799}
episode index:2799
target Thresh 75.99994283834064
target distance 2.0
model initialize at round 2799
at step 0:
{'scaleFactor': 20, 'currentTarget': array([115.,   4.]), 'dynamicTrap': False, 'previousTarget': array([115.,   4.]), 'currentState': array([116.69881704,   3.51175074,   3.50271177]), 'targetState': array([115,   4], dtype=int32), 'currentDistance': 1.7675878157199623}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6912367825647375
{'scaleFactor': 20, 'currentTarget': array([115.,   4.]), 'dynamicTrap': False, 'previousTarget': array([115.,   4.]), 'currentState': array([115.07999133,   3.65212057,   2.59844172]), 'targetState': array([115,   4], dtype=int32), 'currentDistance': 0.35695757261458194}
episode index:2800
target Thresh 75.99994312343561
target distance 57.0
model initialize at round 2800
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.18647673, 17.77180696]), 'dynamicTrap': False, 'previousTarget': array([66.02764341, 16.94882334]), 'currentState': array([85.12806133, 19.29928982,  3.14921355]), 'targetState': array([29, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5877506362663859
running average episode reward sum: 0.6911998364218249
{'scaleFactor': 20, 'currentTarget': array([29., 15.]), 'dynamicTrap': False, 'previousTarget': array([29., 15.]), 'currentState': array([29.58540981, 15.63012179,  3.49526054]), 'targetState': array([29, 15], dtype=int32), 'currentDistance': 0.8600919250727194}
episode index:2801
target Thresh 75.99994340710866
target distance 36.0
model initialize at round 2801
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.85408414, 12.05995007]), 'dynamicTrap': False, 'previousTarget': array([71.5237412 , 12.33860916]), 'currentState': array([53.49829143,  7.02474516,  0.24237698]), 'targetState': array([88, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6785619807214321
running average episode reward sum: 0.6911953261235736
{'scaleFactor': 20, 'currentTarget': array([88., 16.]), 'dynamicTrap': False, 'previousTarget': array([88., 16.]), 'currentState': array([8.88641401e+01, 1.61750784e+01, 2.76820769e-02]), 'targetState': array([88, 16], dtype=int32), 'currentDistance': 0.8816975809711264}
episode index:2802
target Thresh 75.99994368936689
target distance 6.0
model initialize at round 2802
at step 0:
{'scaleFactor': 20, 'currentTarget': array([88.,  5.]), 'dynamicTrap': False, 'previousTarget': array([88.,  5.]), 'currentState': array([81.91775919,  7.82898387,  4.35238923]), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 6.707965639095941}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6912914376768652
{'scaleFactor': 20, 'currentTarget': array([88.,  5.]), 'dynamicTrap': False, 'previousTarget': array([88.,  5.]), 'currentState': array([87.71062458,  4.99420842,  0.99264888]), 'targetState': array([88,  5], dtype=int32), 'currentDistance': 0.2894333678651073}
episode index:2803
target Thresh 75.99994397021733
target distance 4.0
model initialize at round 2803
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46., 14.]), 'dynamicTrap': False, 'previousTarget': array([46., 14.]), 'currentState': array([48.68812138, 12.05421182,  1.45466685]), 'targetState': array([46, 14], dtype=int32), 'currentDistance': 3.3184466508706834}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6913944364508748
{'scaleFactor': 20, 'currentTarget': array([46., 14.]), 'dynamicTrap': False, 'previousTarget': array([46., 14.]), 'currentState': array([46.30933728, 14.48928046,  1.3169198 ]), 'targetState': array([46, 14], dtype=int32), 'currentDistance': 0.5788652026062099}
episode index:2804
target Thresh 75.99994424966704
target distance 37.0
model initialize at round 2804
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.96436763,  5.6644838 ]), 'dynamicTrap': False, 'previousTarget': array([89.81984861,  5.32164208]), 'currentState': array([71.23487115,  8.94274079,  5.92522794]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7340147352924687
running average episode reward sum: 0.691409630853314
{'scaleFactor': 20, 'currentTarget': array([107.,   3.]), 'dynamicTrap': False, 'previousTarget': array([107.,   3.]), 'currentState': array([107.82949076,   3.10840141,   0.16083856]), 'targetState': array([107,   3], dtype=int32), 'currentDistance': 0.8365439567652616}
episode index:2805
target Thresh 75.99994452772299
target distance 62.0
model initialize at round 2805
at step 0:
{'scaleFactor': 20, 'currentTarget': array([75.72989287,  6.30964055]), 'dynamicTrap': False, 'previousTarget': array([74.87373448,  7.24380873]), 'currentState': array([55.91733586,  3.57786739,  6.27948403]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6531506050103869
running average episode reward sum: 0.6913959961327711
{'scaleFactor': 20, 'currentTarget': array([117.,  12.]), 'dynamicTrap': False, 'previousTarget': array([117.,  12.]), 'currentState': array([116.5530211 ,  12.74865554,   0.29793179]), 'targetState': array([117,  12], dtype=int32), 'currentDistance': 0.8719376489777798}
episode index:2806
target Thresh 75.99994480439213
target distance 40.0
model initialize at round 2806
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.43974401, 11.40931474]), 'dynamicTrap': False, 'previousTarget': array([92.06555009, 12.41886371]), 'currentState': array([75.25371456, 19.73207468,  6.10466921]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.662539668873675
running average episode reward sum: 0.6913857160019343
{'scaleFactor': 20, 'currentTarget': array([114.,   2.]), 'dynamicTrap': False, 'previousTarget': array([114.,   2.]), 'currentState': array([113.00353853,   2.01123763,   0.3509849 ]), 'targetState': array([114,   2], dtype=int32), 'currentDistance': 0.9965248312387368}
episode index:2807
target Thresh 75.99994507968137
target distance 8.0
model initialize at round 2807
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 13.]), 'currentState': array([6.45925451, 6.47880488, 2.01332681]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 7.381898640301854}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6914781676877955
{'scaleFactor': 20, 'currentTarget': array([ 3., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 13.]), 'currentState': array([ 2.93404351, 12.81601429,  0.68911315]), 'targetState': array([ 3, 13], dtype=int32), 'currentDistance': 0.19545076339557235}
episode index:2808
target Thresh 75.99994535359761
target distance 11.0
model initialize at round 2808
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,   3.]), 'dynamicTrap': False, 'previousTarget': array([108.,   3.]), 'currentState': array([117.40809236,  12.84491126,   3.64600194]), 'targetState': array([108,   3], dtype=int32), 'currentDistance': 13.617432927090878}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6915604982419927
{'scaleFactor': 20, 'currentTarget': array([108.,   3.]), 'dynamicTrap': False, 'previousTarget': array([108.,   3.]), 'currentState': array([107.47247973,   3.46614145,   4.72184412]), 'targetState': array([108,   3], dtype=int32), 'currentDistance': 0.7039641220106916}
episode index:2809
target Thresh 75.99994562614768
target distance 75.0
model initialize at round 2809
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.19174244, 22.14397372]), 'dynamicTrap': False, 'previousTarget': array([82.00710732, 21.5331438 ]), 'currentState': array([100.18915303,  21.82215104,   2.52963307]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 51
reward sum = 0.5915590027322781
running average episode reward sum: 0.6915249105211708
{'scaleFactor': 20, 'currentTarget': array([27., 23.]), 'dynamicTrap': False, 'previousTarget': array([27., 23.]), 'currentState': array([26.68402213, 22.86495217,  3.14438174]), 'targetState': array([27, 23], dtype=int32), 'currentDistance': 0.3436276073904371}
episode index:2810
target Thresh 75.99994589733839
target distance 5.0
model initialize at round 2810
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'dynamicTrap': False, 'previousTarget': array([15., 17.]), 'currentState': array([12.87923152, 13.32040416,  3.19959283]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 4.247008890651449}
done in step count: 9
reward sum = 0.885271892011561
running average episode reward sum: 0.6915938350965855
{'scaleFactor': 20, 'currentTarget': array([15., 17.]), 'dynamicTrap': False, 'previousTarget': array([15., 17.]), 'currentState': array([14.43635363, 16.88761444,  5.45340416]), 'targetState': array([15, 17], dtype=int32), 'currentDistance': 0.574741462303129}
episode index:2811
target Thresh 75.99994616717655
target distance 54.0
model initialize at round 2811
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.52983538, 11.70849085]), 'dynamicTrap': False, 'previousTarget': array([68.01370332, 10.74023321]), 'currentState': array([88.5291227 , 11.53965207,  0.59021139]), 'targetState': array([34, 12], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 74
reward sum = 0.2111198150950646
running average episode reward sum: 0.6914229695133701
{'scaleFactor': 20, 'currentTarget': array([34., 12.]), 'dynamicTrap': False, 'previousTarget': array([34., 12.]), 'currentState': array([33.13519657, 11.86131241,  4.67904619]), 'targetState': array([34, 12], dtype=int32), 'currentDistance': 0.8758534238410496}
episode index:2812
target Thresh 75.99994643566887
target distance 10.0
model initialize at round 2812
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72., 23.]), 'dynamicTrap': False, 'previousTarget': array([72., 23.]), 'currentState': array([80.26395112, 22.5179338 ,  2.08130336]), 'targetState': array([72, 23], dtype=int32), 'currentDistance': 8.277999513683811}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6915152436265541
{'scaleFactor': 20, 'currentTarget': array([72., 23.]), 'dynamicTrap': False, 'previousTarget': array([72., 23.]), 'currentState': array([71.65879961, 22.37977494,  3.99252903]), 'targetState': array([72, 23], dtype=int32), 'currentDistance': 0.7078819382038277}
episode index:2813
target Thresh 75.99994670282209
target distance 22.0
model initialize at round 2813
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.53811377, 17.82497556]), 'dynamicTrap': False, 'previousTarget': array([35.82527831, 17.76343395]), 'currentState': array([53.39449937,  7.06104207,  1.53817361]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7646823691049038
running average episode reward sum: 0.691541244737243
{'scaleFactor': 20, 'currentTarget': array([30., 22.]), 'dynamicTrap': False, 'previousTarget': array([30., 22.]), 'currentState': array([30.60015667, 21.29561945,  3.01579238]), 'targetState': array([30, 22], dtype=int32), 'currentDistance': 0.9253864041553803}
episode index:2814
target Thresh 75.99994696864287
target distance 8.0
model initialize at round 2814
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43., 11.]), 'dynamicTrap': False, 'previousTarget': array([43., 11.]), 'currentState': array([36.88963029,  8.45947054,  6.21364355]), 'targetState': array([43, 11], dtype=int32), 'currentDistance': 6.617469909131967}
done in step count: 6
reward sum = 0.912370199301
running average episode reward sum: 0.6916196919679939
{'scaleFactor': 20, 'currentTarget': array([43., 11.]), 'dynamicTrap': False, 'previousTarget': array([43., 11.]), 'currentState': array([42.54947104, 10.65853351,  6.22537051]), 'targetState': array([43, 11], dtype=int32), 'currentDistance': 0.565310277540876}
episode index:2815
target Thresh 75.99994723313787
target distance 9.0
model initialize at round 2815
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([114.04750523,  16.34509353,   2.7366305 ]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 7.239201983647421}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.691715209126386
{'scaleFactor': 20, 'currentTarget': array([107.,  18.]), 'dynamicTrap': False, 'previousTarget': array([107.,  18.]), 'currentState': array([107.04305677,  18.22229387,   2.11065543]), 'targetState': array([107,  18], dtype=int32), 'currentDistance': 0.2264253697017697}
episode index:2816
target Thresh 75.99994749631368
target distance 36.0
model initialize at round 2816
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.00343925,  8.14155276]), 'dynamicTrap': False, 'previousTarget': array([19.72964181,  7.35287728]), 'currentState': array([37.37316711,  3.16022336,  2.59682253]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7256172744819471
running average episode reward sum: 0.6917272439383687
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 12.]), 'currentState': array([ 2.367796  , 11.85335818,  5.64767604]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.6489882297346683}
episode index:2817
target Thresh 75.99994775817692
target distance 13.0
model initialize at round 2817
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.17348116,  7.95414516]), 'dynamicTrap': True, 'previousTarget': array([26.,  8.]), 'currentState': array([22.       , 21.       ,  2.7849834], dtype=float32), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 13.697162968337073}
done in step count: 15
reward sum = 0.7921237025482785
running average episode reward sum: 0.6917628707867044
{'scaleFactor': 20, 'currentTarget': array([26.,  8.]), 'dynamicTrap': False, 'previousTarget': array([26.,  8.]), 'currentState': array([25.4788337 ,  7.88855165,  3.03531312]), 'targetState': array([26,  8], dtype=int32), 'currentDistance': 0.5329493803692367}
episode index:2818
target Thresh 75.99994801873409
target distance 10.0
model initialize at round 2818
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92., 13.]), 'dynamicTrap': False, 'previousTarget': array([92., 13.]), 'currentState': array([87.94812814,  4.85692414,  0.93141651]), 'targetState': array([92, 13], dtype=int32), 'currentDistance': 9.095457656763552}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6918582355044104
{'scaleFactor': 20, 'currentTarget': array([92., 13.]), 'dynamicTrap': False, 'previousTarget': array([92., 13.]), 'currentState': array([91.0544081 , 12.19882979,  1.30444138]), 'targetState': array([92, 13], dtype=int32), 'currentDistance': 1.2393618275320957}
episode index:2819
target Thresh 75.99994827799173
target distance 5.0
model initialize at round 2819
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.,  6.]), 'dynamicTrap': False, 'previousTarget': array([91.,  6.]), 'currentState': array([85.88092333, 11.74302575,  0.74165082]), 'targetState': array([91,  6], dtype=int32), 'currentDistance': 7.693327676779936}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6919535325875649
{'scaleFactor': 20, 'currentTarget': array([91.,  6.]), 'dynamicTrap': False, 'previousTarget': array([91.,  6.]), 'currentState': array([90.54944124,  6.96062729,  3.91875783]), 'targetState': array([91,  6], dtype=int32), 'currentDistance': 1.06104099072464}
episode index:2820
target Thresh 75.99994853595634
target distance 17.0
model initialize at round 2820
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.16740094, 10.58583176]), 'dynamicTrap': False, 'previousTarget': array([20.88715666,  9.85099785]), 'currentState': array([ 5.04373626, 23.67303033,  0.67217243]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.772873611899361
running average episode reward sum: 0.6919822174792032
{'scaleFactor': 20, 'currentTarget': array([22.,  9.]), 'dynamicTrap': False, 'previousTarget': array([22.,  9.]), 'currentState': array([21.33903189,  8.42449606,  5.56399702]), 'targetState': array([22,  9], dtype=int32), 'currentDistance': 0.8764038061457987}
episode index:2821
target Thresh 75.99994879263433
target distance 49.0
model initialize at round 2821
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.14230002, 13.39746852]), 'dynamicTrap': False, 'previousTarget': array([82.59608118, 14.00079976]), 'currentState': array([64.50744706, 17.20174888,  4.82499421]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7150803359578534
running average episode reward sum: 0.6919904024963821
{'scaleFactor': 20, 'currentTarget': array([112.,   8.]), 'dynamicTrap': False, 'previousTarget': array([112.,   8.]), 'currentState': array([111.70221386,   7.59967692,   0.43811259]), 'targetState': array([112,   8], dtype=int32), 'currentDistance': 0.49893401324939574}
episode index:2822
target Thresh 75.99994904803212
target distance 25.0
model initialize at round 2822
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.10396605,   9.7651306 ]), 'dynamicTrap': False, 'previousTarget': array([111.44774604,   9.33254095]), 'currentState': array([91.94415251, 15.50111743,  0.59798312]), 'targetState': array([117,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.552273161863032
running average episode reward sum: 0.6919409100271531
{'scaleFactor': 20, 'currentTarget': array([117.,   8.]), 'dynamicTrap': False, 'previousTarget': array([117.,   8.]), 'currentState': array([116.61189195,   8.08998243,   0.41125101]), 'targetState': array([117,   8], dtype=int32), 'currentDistance': 0.39840267742712865}
episode index:2823
target Thresh 75.99994930215612
target distance 46.0
model initialize at round 2823
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.28360145, 10.92623494]), 'dynamicTrap': False, 'previousTarget': array([30.35234545,  9.95156206]), 'currentState': array([10.05445427, 16.42531658,  1.25134897]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.620240248281419
running average episode reward sum: 0.6919155202744103
{'scaleFactor': 20, 'currentTarget': array([55.03711799,  2.623048  ]), 'dynamicTrap': True, 'previousTarget': array([57.,  3.]), 'currentState': array([54.6809684 ,  3.10898414,  0.12907863]), 'targetState': array([57,  3], dtype=int32), 'currentDistance': 0.6024752792472261}
episode index:2824
target Thresh 75.99994955501268
target distance 61.0
model initialize at round 2824
at step 0:
{'scaleFactor': 20, 'currentTarget': array([61.99129394,  7.59005648]), 'dynamicTrap': True, 'previousTarget': array([61.98925886,  7.65538554]), 'currentState': array([42.       ,  7.       ,  1.6447654], dtype=float32), 'targetState': array([103,   9], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.4911368501139658
running average episode reward sum: 0.6918444481787782
{'scaleFactor': 20, 'currentTarget': array([103.,   9.]), 'dynamicTrap': False, 'previousTarget': array([103.,   9.]), 'currentState': array([103.84792903,   9.48628079,   0.71697763]), 'targetState': array([103,   9], dtype=int32), 'currentDistance': 0.9774725795379948}
episode index:2825
target Thresh 75.9999498066081
target distance 8.0
model initialize at round 2825
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73., 12.]), 'dynamicTrap': False, 'previousTarget': array([73., 12.]), 'currentState': array([68.32911558, 19.53889653,  4.9392525 ]), 'targetState': array([73, 12], dtype=int32), 'currentDistance': 8.86860317312583}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6919327835295291
{'scaleFactor': 20, 'currentTarget': array([73., 12.]), 'dynamicTrap': False, 'previousTarget': array([73., 12.]), 'currentState': array([72.82548411, 11.19257603,  3.97531626]), 'targetState': array([73, 12], dtype=int32), 'currentDistance': 0.8260685592477722}
episode index:2826
target Thresh 75.99995005694868
target distance 15.0
model initialize at round 2826
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   5.]), 'dynamicTrap': False, 'previousTarget': array([106.,   5.]), 'currentState': array([113.62195008,  18.55067707,   3.97057346]), 'targetState': array([106,   5], dtype=int32), 'currentDistance': 15.547185347802705}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6920047345273109
{'scaleFactor': 20, 'currentTarget': array([106.,   5.]), 'dynamicTrap': False, 'previousTarget': array([106.,   5.]), 'currentState': array([106.59520459,   4.70276029,   2.27252108]), 'targetState': array([106,   5], dtype=int32), 'currentDistance': 0.6652968854197865}
episode index:2827
target Thresh 75.99995030604069
target distance 49.0
model initialize at round 2827
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.97357577, 15.02775053]), 'dynamicTrap': True, 'previousTarget': array([34.93369232, 15.62724019]), 'currentState': array([15.        , 14.        ,  0.78677744], dtype=float32), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.4410607391466849
running average episode reward sum: 0.6919159990268228
{'scaleFactor': 20, 'currentTarget': array([64.59457535, 16.31634011]), 'dynamicTrap': True, 'previousTarget': array([64., 18.]), 'currentState': array([64.07087714, 16.76179786,  1.10285916]), 'targetState': array([64, 18], dtype=int32), 'currentDistance': 0.6875263035602495}
episode index:2828
target Thresh 75.99995055389034
target distance 21.0
model initialize at round 2828
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.9900845 ,  8.50078902]), 'dynamicTrap': False, 'previousTarget': array([19.02633404,  8.32455532]), 'currentState': array([38.38905246,  3.63457468,  2.313687  ]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.7962511617098241
running average episode reward sum: 0.6919528796074813
{'scaleFactor': 20, 'currentTarget': array([19.16818125,  9.10269076]), 'dynamicTrap': True, 'previousTarget': array([18.97236077,  9.07023452]), 'currentState': array([20.14565139,  8.26344185,  2.98530532]), 'targetState': array([17,  9], dtype=int32), 'currentDistance': 1.2883270601461445}
episode index:2829
target Thresh 75.99995080050385
target distance 51.0
model initialize at round 2829
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.92447776, 18.72655872]), 'dynamicTrap': False, 'previousTarget': array([78.55042088, 17.78324255]), 'currentState': array([59.51542412, 23.55238889,  0.33738935]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5187636780323924
running average episode reward sum: 0.6918916820097516
{'scaleFactor': 20, 'currentTarget': array([110.,  11.]), 'dynamicTrap': False, 'previousTarget': array([110.,  11.]), 'currentState': array([109.56915297,  10.49268877,   4.76543356]), 'targetState': array([110,  11], dtype=int32), 'currentDistance': 0.6655778327289242}
episode index:2830
target Thresh 75.99995104588736
target distance 16.0
model initialize at round 2830
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.,  8.]), 'dynamicTrap': False, 'previousTarget': array([90.,  8.]), 'currentState': array([75.34631203, 18.00986732,  5.91674978]), 'targetState': array([90,  8], dtype=int32), 'currentDistance': 17.746211283387353}
done in step count: 15
reward sum = 0.803265131643103
running average episode reward sum: 0.6919310226842953
{'scaleFactor': 20, 'currentTarget': array([90.,  8.]), 'dynamicTrap': False, 'previousTarget': array([90.,  8.]), 'currentState': array([89.71466688,  8.20268481,  5.8293018 ]), 'targetState': array([90,  8], dtype=int32), 'currentDistance': 0.34999445444828253}
episode index:2831
target Thresh 75.99995129004702
target distance 8.0
model initialize at round 2831
at step 0:
{'scaleFactor': 20, 'currentTarget': array([117.,  18.]), 'dynamicTrap': False, 'previousTarget': array([117.,  18.]), 'currentState': array([110.82426433,  18.79876557,   0.25106281]), 'targetState': array([117,  18], dtype=int32), 'currentDistance': 6.227177328348225}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.692029316461596
{'scaleFactor': 20, 'currentTarget': array([117.,  18.]), 'dynamicTrap': False, 'previousTarget': array([117.,  18.]), 'currentState': array([116.58048422,  18.47055679,   5.41995305]), 'targetState': array([117,  18], dtype=int32), 'currentDistance': 0.6304103285615595}
episode index:2832
target Thresh 75.99995153298892
target distance 26.0
model initialize at round 2832
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97.26606825,  9.68894795]), 'dynamicTrap': False, 'previousTarget': array([95.85217755,  9.19966821]), 'currentState': array([113.785469  ,  20.96322836,   5.4394396 ]), 'targetState': array([86,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6983835879450715
running average episode reward sum: 0.6920315594095252
{'scaleFactor': 20, 'currentTarget': array([86.,  2.]), 'dynamicTrap': False, 'previousTarget': array([86.,  2.]), 'currentState': array([85.60425683,  2.73044214,  3.81781042]), 'targetState': array([86,  2], dtype=int32), 'currentDistance': 0.8307577184553927}
episode index:2833
target Thresh 75.99995177471915
target distance 4.0
model initialize at round 2833
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71., 14.]), 'dynamicTrap': False, 'previousTarget': array([71., 14.]), 'currentState': array([6.98403195e+01, 1.94581671e+01, 3.79970074e-02]), 'targetState': array([71, 14], dtype=int32), 'currentDistance': 5.58000415529531}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6921263245649911
{'scaleFactor': 20, 'currentTarget': array([71., 14.]), 'dynamicTrap': False, 'previousTarget': array([71., 14.]), 'currentState': array([70.28636169, 14.40770273,  5.02883834]), 'targetState': array([71, 14], dtype=int32), 'currentDistance': 0.8218887713553837}
episode index:2834
target Thresh 75.99995201524374
target distance 23.0
model initialize at round 2834
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.64467974,  9.48898357]), 'dynamicTrap': True, 'previousTarget': array([66.58189596,  9.57871024]), 'currentState': array([83.       , 21.       ,  2.8660202], dtype=float32), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.5229345043275951
running average episode reward sum: 0.6920666449105864
{'scaleFactor': 20, 'currentTarget': array([60.,  5.]), 'dynamicTrap': False, 'previousTarget': array([60.,  5.]), 'currentState': array([60.23048272,  5.49515014,  0.29514827]), 'targetState': array([60,  5], dtype=int32), 'currentDistance': 0.5461647591421821}
episode index:2835
target Thresh 75.99995225456871
target distance 14.0
model initialize at round 2835
at step 0:
{'scaleFactor': 20, 'currentTarget': array([110.,   2.]), 'dynamicTrap': False, 'previousTarget': array([110.,   2.]), 'currentState': array([105.92550174,  14.33782103,   4.39099109]), 'targetState': array([110,   2], dtype=int32), 'currentDistance': 12.99320452543672}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6921512706873835
{'scaleFactor': 20, 'currentTarget': array([110.,   2.]), 'dynamicTrap': False, 'previousTarget': array([110.,   2.]), 'currentState': array([109.2893693 ,   2.69191348,   5.68417647]), 'targetState': array([110,   2], dtype=int32), 'currentDistance': 0.9918368112074422}
episode index:2836
target Thresh 75.99995249270003
target distance 42.0
model initialize at round 2836
at step 0:
{'scaleFactor': 20, 'currentTarget': array([54.42940795, 19.28196125]), 'dynamicTrap': False, 'previousTarget': array([56.00566653, 19.52394444]), 'currentState': array([74.42750334, 19.55796964,  2.40562201]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7429107700376435
running average episode reward sum: 0.6921691626504961
{'scaleFactor': 20, 'currentTarget': array([34., 19.]), 'dynamicTrap': False, 'previousTarget': array([34., 19.]), 'currentState': array([34.04227857, 19.58295316,  2.89249377]), 'targetState': array([34, 19], dtype=int32), 'currentDistance': 0.5844842722857642}
episode index:2837
target Thresh 75.99995272964368
target distance 17.0
model initialize at round 2837
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([33.60743318,  9.15897056,  2.66942716]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 16.77870343197892}
done in step count: 17
reward sum = 0.7787016134271739
running average episode reward sum: 0.69219965329559
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([17.72133609,  3.10255094,  2.86450414]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.29693479013867446}
episode index:2838
target Thresh 75.99995296540557
target distance 21.0
model initialize at round 2838
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.51921469,  9.26104804]), 'dynamicTrap': False, 'previousTarget': array([30.6170994 ,  9.12161403]), 'currentState': array([48.40366551, 18.21350127,  3.03755355]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7786143182635068
running average episode reward sum: 0.6922300917122747
{'scaleFactor': 20, 'currentTarget': array([28.,  8.]), 'dynamicTrap': False, 'previousTarget': array([28.,  8.]), 'currentState': array([28.82845183,  7.17120476,  3.72731247]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 1.1718506653963372}
episode index:2839
target Thresh 75.9999531999916
target distance 25.0
model initialize at round 2839
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.32850186,   3.8610297 ]), 'dynamicTrap': True, 'previousTarget': array([112.44774604,   4.33254095]), 'currentState': array([93.      ,  9.      ,  1.049329], dtype=float32), 'targetState': array([118,   3], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7962684059780076
running average episode reward sum: 0.6922667249215232
{'scaleFactor': 20, 'currentTarget': array([118.,   3.]), 'dynamicTrap': False, 'previousTarget': array([118.,   3.]), 'currentState': array([117.59833655,   3.28508261,   0.33996618]), 'targetState': array([118,   3], dtype=int32), 'currentDistance': 0.49255011559645}
episode index:2840
target Thresh 75.99995343340761
target distance 9.0
model initialize at round 2840
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65., 14.]), 'dynamicTrap': False, 'previousTarget': array([65., 14.]), 'currentState': array([67.82784414, 22.89685077,  5.92008382]), 'targetState': array([65, 14], dtype=int32), 'currentDistance': 9.33545157561843}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6923577926177493
{'scaleFactor': 20, 'currentTarget': array([65., 14.]), 'dynamicTrap': False, 'previousTarget': array([65., 14.]), 'currentState': array([64.97259256, 14.79139748,  3.22429918]), 'targetState': array([65, 14], dtype=int32), 'currentDistance': 0.7918719247320574}
episode index:2841
target Thresh 75.99995366565946
target distance 18.0
model initialize at round 2841
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.27246248, 10.42905541]), 'dynamicTrap': True, 'previousTarget': array([49.88854382, 10.05572809]), 'currentState': array([32.        , 19.        ,  0.30172756], dtype=float32), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 18.39168634001117}
done in step count: 15
reward sum = 0.8017296626482784
running average episode reward sum: 0.6923962767380979
{'scaleFactor': 20, 'currentTarget': array([50., 10.]), 'dynamicTrap': False, 'previousTarget': array([50., 10.]), 'currentState': array([49.47426748, 10.22410172,  5.95222172]), 'targetState': array([50, 10], dtype=int32), 'currentDistance': 0.5715035052789741}
episode index:2842
target Thresh 75.99995389675294
target distance 20.0
model initialize at round 2842
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11.38838649, 16.0776773 ]), 'currentState': array([29.28857876, 20.40639472,  2.0333488 ]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 18.811922481304798}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6924708408598955
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([11.89037513, 16.65038813,  3.93036848]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 1.1026207875673357}
episode index:2843
target Thresh 75.99995412669385
target distance 23.0
model initialize at round 2843
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.2899909 ,  8.62876851]), 'dynamicTrap': False, 'previousTarget': array([12.45647272,  8.24859289]), 'currentState': array([31.03226169,  5.42833979,  3.10315025]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.6400052346613722
running average episode reward sum: 0.6924523930377442
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([8.2767181 , 8.96716307, 3.73034891]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.7240269082596068}
episode index:2844
target Thresh 75.99995435548792
target distance 5.0
model initialize at round 2844
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.13595358,  2.85372479]), 'dynamicTrap': True, 'previousTarget': array([18.,  3.]), 'currentState': array([23.       ,  4.       ,  2.2255852], dtype=float32), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 4.99728870529591}
done in step count: 7
reward sum = 0.89266135790699
running average episode reward sum: 0.6925227652573819
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([17.43047524,  3.87557104,  2.60648026]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 1.0445013642354328}
episode index:2845
target Thresh 75.99995458314086
target distance 25.0
model initialize at round 2845
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.4151747 ,  7.37861279]), 'dynamicTrap': False, 'previousTarget': array([52.44774604,  7.33254095]), 'currentState': array([34.26229204, 13.13770004,  6.03446573]), 'targetState': array([58,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.6983882396835318
running average episode reward sum: 0.6925248262111507
{'scaleFactor': 20, 'currentTarget': array([57.0768553 ,  4.56216639]), 'dynamicTrap': True, 'previousTarget': array([58.,  6.]), 'currentState': array([56.5353448 ,  5.10942119,  1.94704121]), 'targetState': array([58,  6], dtype=int32), 'currentDistance': 0.769884043701144}
episode index:2846
target Thresh 75.9999548096584
target distance 36.0
model initialize at round 2846
at step 0:
{'scaleFactor': 20, 'currentTarget': array([96.22703381, 15.73359599]), 'dynamicTrap': False, 'previousTarget': array([96.64009343, 14.75107478]), 'currentState': array([77.88798009, 23.71351509,  0.68484223]), 'targetState': array([114,   8], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6897602744868893
running average episode reward sum: 0.6925238551708542
{'scaleFactor': 20, 'currentTarget': array([114.,   8.]), 'dynamicTrap': False, 'previousTarget': array([114.,   8.]), 'currentState': array([114.08671516,   7.26082442,   0.97573043]), 'targetState': array([114,   8], dtype=int32), 'currentDistance': 0.7442446208570876}
episode index:2847
target Thresh 75.99995503504617
target distance 68.0
model initialize at round 2847
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.13779774, 15.76624341]), 'dynamicTrap': False, 'previousTarget': array([84.17290468, 15.62417438]), 'currentState': array([103.97218013,  13.19772724,   1.61078008]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5574037323689466
running average episode reward sum: 0.6924764113075109
{'scaleFactor': 20, 'currentTarget': array([36., 22.]), 'dynamicTrap': False, 'previousTarget': array([36., 22.]), 'currentState': array([36.28069345, 22.60455847,  2.97708884]), 'targetState': array([36, 22], dtype=int32), 'currentDistance': 0.6665431384031643}
episode index:2848
target Thresh 75.99995525930981
target distance 62.0
model initialize at round 2848
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.33346222, 14.25641244]), 'dynamicTrap': False, 'previousTarget': array([66.79255473, 13.12688722]), 'currentState': array([46.61658211, 17.60971509,  0.7946291 ]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.5603117454664969
running average episode reward sum: 0.6924300214634109
{'scaleFactor': 20, 'currentTarget': array([109.,   7.]), 'dynamicTrap': False, 'previousTarget': array([109.,   7.]), 'currentState': array([109.19503427,   7.28401453,   1.06573883]), 'targetState': array([109,   7], dtype=int32), 'currentDistance': 0.3445324638653505}
episode index:2849
target Thresh 75.99995548245494
target distance 6.0
model initialize at round 2849
at step 0:
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([50.96673229,  7.52187039,  4.43012643]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 5.408217105083852}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6925275193506168
{'scaleFactor': 20, 'currentTarget': array([48.,  3.]), 'dynamicTrap': False, 'previousTarget': array([48.,  3.]), 'currentState': array([48.48844434,  2.55869628,  5.05105343]), 'targetState': array([48,  3], dtype=int32), 'currentDistance': 0.6582756630358654}
episode index:2850
target Thresh 75.99995570448712
target distance 47.0
model initialize at round 2850
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.72709622,  2.81515661]), 'dynamicTrap': False, 'previousTarget': array([82.95938166,  3.72599692]), 'currentState': array([63.73671573,  3.43538843,  5.44531894]), 'targetState': array([110,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5105331785854095
running average episode reward sum: 0.692463684085529
{'scaleFactor': 20, 'currentTarget': array([110.,   2.]), 'dynamicTrap': False, 'previousTarget': array([110.,   2.]), 'currentState': array([109.24582175,   2.51281357,   5.90393686]), 'targetState': array([110,   2], dtype=int32), 'currentDistance': 0.91201019149921}
episode index:2851
target Thresh 75.99995592541191
target distance 50.0
model initialize at round 2851
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.89883391, 17.43247279]), 'dynamicTrap': False, 'previousTarget': array([86.9960012 , 16.60007998]), 'currentState': array([67.92301989, 18.41576019,  6.27819497]), 'targetState': array([117,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.43912316749903274
running average episode reward sum: 0.6923748550123922
{'scaleFactor': 20, 'currentTarget': array([117.,  16.]), 'dynamicTrap': False, 'previousTarget': array([117.,  16.]), 'currentState': array([116.53302711,  15.58690688,   0.38031428]), 'targetState': array([117,  16], dtype=int32), 'currentDistance': 0.6234658022752498}
episode index:2852
target Thresh 75.99995614523483
target distance 33.0
model initialize at round 2852
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.03714564,  7.52916516]), 'dynamicTrap': False, 'previousTarget': array([21.5646825 ,  7.15008417]), 'currentState': array([3.45056154, 3.48370804, 5.78294868]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.732782155247834
running average episode reward sum: 0.6923890181039574
{'scaleFactor': 20, 'currentTarget': array([35., 10.]), 'dynamicTrap': False, 'previousTarget': array([35., 10.]), 'currentState': array([35.1224861 , 10.35109486,  6.01522373]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 0.37184733734356396}
episode index:2853
target Thresh 75.99995636396139
target distance 48.0
model initialize at round 2853
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.64571845, 19.56376335]), 'dynamicTrap': False, 'previousTarget': array([35.99566113, 19.41657627]), 'currentState': array([17.64845783, 19.23275307,  1.07649928]), 'targetState': array([64, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 38
reward sum = 0.6257728424940279
running average episode reward sum: 0.6923656767670232
{'scaleFactor': 20, 'currentTarget': array([64., 20.]), 'dynamicTrap': False, 'previousTarget': array([64., 20.]), 'currentState': array([63.56512117, 20.34142829,  5.83823655]), 'targetState': array([64, 20], dtype=int32), 'currentDistance': 0.5528949902280814}
episode index:2854
target Thresh 75.99995658159703
target distance 27.0
model initialize at round 2854
at step 0:
{'scaleFactor': 20, 'currentTarget': array([101.82714716,  13.97327668]), 'dynamicTrap': False, 'previousTarget': array([102.0200334 ,  14.67631238]), 'currentState': array([84.28803736,  4.36184457,  5.77723563]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7585457650397072
running average episode reward sum: 0.6923888571832308
{'scaleFactor': 20, 'currentTarget': array([111.,  19.]), 'dynamicTrap': False, 'previousTarget': array([111.,  19.]), 'currentState': array([111.25305185,  19.45288703,   2.2144972 ]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 0.518788885340588}
episode index:2855
target Thresh 75.99995679814722
target distance 10.0
model initialize at round 2855
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56., 12.]), 'dynamicTrap': False, 'previousTarget': array([56., 12.]), 'currentState': array([49.67181687, 21.00860787,  4.66402054]), 'targetState': array([56, 12], dtype=int32), 'currentDistance': 11.009128825040074}
done in step count: 7
reward sum = 0.92274469442792
running average episode reward sum: 0.6924695139889887
{'scaleFactor': 20, 'currentTarget': array([54.94136602, 13.52152812]), 'dynamicTrap': True, 'previousTarget': array([56., 12.]), 'currentState': array([54.30567974, 13.24126291,  5.20745605]), 'targetState': array([56, 12], dtype=int32), 'currentDistance': 0.6947270220494979}
episode index:2856
target Thresh 75.99995701361736
target distance 74.0
model initialize at round 2856
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.94887487,  6.73090273]), 'dynamicTrap': False, 'previousTarget': array([94.0073006 ,  5.54034323]), 'currentState': array([113.94862608,   6.63114387,   2.59075975]), 'targetState': array([40,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 95
reward sum = 0.07127895869293832
running average episode reward sum: 0.6922520864232569
{'scaleFactor': 20, 'currentTarget': array([40.,  7.]), 'dynamicTrap': False, 'previousTarget': array([40.,  7.]), 'currentState': array([40.62273842,  7.33285252,  4.89485328]), 'targetState': array([40,  7], dtype=int32), 'currentDistance': 0.7061118437303677}
episode index:2857
target Thresh 75.99995722801285
target distance 28.0
model initialize at round 2857
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.28300081,  4.89489505]), 'dynamicTrap': False, 'previousTarget': array([47.20101013,  5.17157288]), 'currentState': array([65.08316981,  7.71505593,  3.67389464]), 'targetState': array([39,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7708281750808716
running average episode reward sum: 0.6922795798062721
{'scaleFactor': 20, 'currentTarget': array([39.,  4.]), 'dynamicTrap': False, 'previousTarget': array([39.,  4.]), 'currentState': array([39.59409589,  3.7648931 ,  4.43866962]), 'targetState': array([39,  4], dtype=int32), 'currentDistance': 0.638925017969624}
episode index:2858
target Thresh 75.99995744133902
target distance 17.0
model initialize at round 2858
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,   5.]), 'dynamicTrap': False, 'previousTarget': array([113.,   5.]), 'currentState': array([104.56188829,  21.11960922,   4.69550431]), 'targetState': array([113,   5], dtype=int32), 'currentDistance': 18.194601687760976}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.692356962691084
{'scaleFactor': 20, 'currentTarget': array([113.,   5.]), 'dynamicTrap': False, 'previousTarget': array([113.,   5.]), 'currentState': array([112.76166048,   5.84405522,   4.46189708]), 'targetState': array([113,   5], dtype=int32), 'currentDistance': 0.8770603937007722}
episode index:2859
target Thresh 75.99995765360123
target distance 66.0
model initialize at round 2859
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.72366998, 11.73407325]), 'dynamicTrap': False, 'previousTarget': array([53.4353175 , 11.15008417]), 'currentState': array([71.30776961,  7.67660586,  2.01397324]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 46
reward sum = 0.6147728483578461
running average episode reward sum: 0.6923298353783802
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.3868458 , 21.34158531,  2.92561747]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.7018821810483833}
episode index:2860
target Thresh 75.99995786480477
target distance 17.0
model initialize at round 2860
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.22759398,  9.19748872]), 'dynamicTrap': False, 'previousTarget': array([64.11284334,  9.85099785]), 'currentState': array([78.33346934, 22.30521658,  1.96951365]), 'targetState': array([63,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.877521022998968
running average episode reward sum: 0.6923945649091808
{'scaleFactor': 20, 'currentTarget': array([64.28791098, 10.36282111]), 'dynamicTrap': True, 'previousTarget': array([63.,  9.]), 'currentState': array([63.42997403, 10.2869988 ,  4.56709866]), 'targetState': array([63,  9], dtype=int32), 'currentDistance': 0.8612809261023862}
episode index:2861
target Thresh 75.99995807495493
target distance 37.0
model initialize at round 2861
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.80684856, 10.28497634]), 'dynamicTrap': False, 'previousTarget': array([70.46521464,  9.51410217]), 'currentState': array([89.64222854,  3.55976548,  2.18374982]), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6055490289283069
running average episode reward sum: 0.6923642205569862
{'scaleFactor': 20, 'currentTarget': array([52., 17.]), 'dynamicTrap': False, 'previousTarget': array([52., 17.]), 'currentState': array([52.61320517, 16.4331144 ,  3.94058723]), 'targetState': array([52, 17], dtype=int32), 'currentDistance': 0.8350927220850232}
episode index:2862
target Thresh 75.99995828405697
target distance 8.0
model initialize at round 2862
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 21.]), 'dynamicTrap': False, 'previousTarget': array([34., 21.]), 'currentState': array([42.90570459, 21.28857071,  1.63919085]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 8.910378628855971}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6924545544128516
{'scaleFactor': 20, 'currentTarget': array([34., 21.]), 'dynamicTrap': False, 'previousTarget': array([34., 21.]), 'currentState': array([34.70290225, 21.47094975,  3.6743627 ]), 'targetState': array([34, 21], dtype=int32), 'currentDistance': 0.8460881954645874}
episode index:2863
target Thresh 75.99995849211611
target distance 35.0
model initialize at round 2863
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83.01983045, 18.60661432]), 'dynamicTrap': False, 'previousTarget': array([84.63014135, 17.9808208 ]), 'currentState': array([102.45853327,  13.90163475,   1.99163926]), 'targetState': array([69, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.6750951952044095
running average episode reward sum: 0.6924484931840778
{'scaleFactor': 20, 'currentTarget': array([69., 22.]), 'dynamicTrap': False, 'previousTarget': array([69., 22.]), 'currentState': array([68.99841476, 22.32154979,  3.27905488]), 'targetState': array([69, 22], dtype=int32), 'currentDistance': 0.3215537023984436}
episode index:2864
target Thresh 75.99995869913754
target distance 32.0
model initialize at round 2864
at step 0:
{'scaleFactor': 20, 'currentTarget': array([99.49545592, 15.38956947]), 'dynamicTrap': False, 'previousTarget': array([100.33768561,  14.38310452]), 'currentState': array([117.45649384,   6.591779  ,   2.19964457]), 'targetState': array([86, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7527851036124507
running average episode reward sum: 0.6924695530830056
{'scaleFactor': 20, 'currentTarget': array([86., 22.]), 'dynamicTrap': False, 'previousTarget': array([86., 22.]), 'currentState': array([85.51068275, 22.02836668,  3.57911267]), 'targetState': array([86, 22], dtype=int32), 'currentDistance': 0.49013879538451616}
episode index:2865
target Thresh 75.99995890512645
target distance 25.0
model initialize at round 2865
at step 0:
{'scaleFactor': 20, 'currentTarget': array([103.51743285,  13.09351436]), 'dynamicTrap': False, 'previousTarget': array([102.30630065,  13.94522771]), 'currentState': array([84.83325881, 20.22805291,  5.51123732]), 'targetState': array([109,  11], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6925220560977652
{'scaleFactor': 20, 'currentTarget': array([109.,  11.]), 'dynamicTrap': False, 'previousTarget': array([109.,  11.]), 'currentState': array([108.35503833,  10.76650639,   1.00330989]), 'targetState': array([109,  11], dtype=int32), 'currentDistance': 0.6859262510079926}
episode index:2866
target Thresh 75.99995911008799
target distance 63.0
model initialize at round 2866
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.88760031, 12.72957449]), 'dynamicTrap': False, 'previousTarget': array([46.00251905, 13.31742033]), 'currentState': array([64.87840793, 12.12326543,  2.79506731]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 42
reward sum = 0.6469717624464538
running average episode reward sum: 0.6925061683078624
{'scaleFactor': 20, 'currentTarget': array([ 3., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 14.]), 'currentState': array([ 3.79949761, 13.95180794,  5.06939217]), 'targetState': array([ 3, 14], dtype=int32), 'currentDistance': 0.8009487537607065}
episode index:2867
target Thresh 75.99995931402728
target distance 20.0
model initialize at round 2867
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.58428342,  6.85193929]), 'dynamicTrap': False, 'previousTarget': array([49.02495322,  6.99875234]), 'currentState': array([70.49751127,  4.99092968,  4.90753301]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6925737689715332
{'scaleFactor': 20, 'currentTarget': array([49.,  7.]), 'dynamicTrap': False, 'previousTarget': array([49.,  7.]), 'currentState': array([49.16404851,  6.76911195,  2.72490463]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 0.28323348317267383}
episode index:2868
target Thresh 75.99995951694942
target distance 9.0
model initialize at round 2868
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'dynamicTrap': False, 'previousTarget': array([10., 10.]), 'currentState': array([13.94591845, 19.01781241,  5.02738917]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 9.843333431142428}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.692641322510308
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'dynamicTrap': False, 'previousTarget': array([10., 10.]), 'currentState': array([9.58686785, 9.47410815, 3.80148097]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 0.6687603572783498}
episode index:2869
target Thresh 75.99995971885947
target distance 5.0
model initialize at round 2869
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.,  7.]), 'dynamicTrap': False, 'previousTarget': array([87.,  7.]), 'currentState': array([90.81854261,  6.80143981,  2.92417669]), 'targetState': array([87,  7], dtype=int32), 'currentDistance': 3.8237015899342524}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6927414823282486
{'scaleFactor': 20, 'currentTarget': array([87.,  7.]), 'dynamicTrap': False, 'previousTarget': array([87.,  7.]), 'currentState': array([86.87460608,  7.14183525,  2.72468868]), 'targetState': array([87,  7], dtype=int32), 'currentDistance': 0.18931686348221116}
episode index:2870
target Thresh 75.9999599197625
target distance 10.0
model initialize at round 2870
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.72466695, 16.97385358]), 'dynamicTrap': True, 'previousTarget': array([26., 18.]), 'currentState': array([20.       ,  8.       ,  1.6035057], dtype=float32), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 10.141623438513268}
done in step count: 11
reward sum = 0.8561293041587164
running average episode reward sum: 0.6927983920537207
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([25.09421987, 17.31691417,  1.39238061]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 1.134479572538887}
episode index:2871
target Thresh 75.99996011966351
target distance 8.0
model initialize at round 2871
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'dynamicTrap': False, 'previousTarget': array([3., 8.]), 'currentState': array([ 9.31024433, 15.01068505,  4.0306567 ]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 9.432332079993925}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6928882916560349
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'dynamicTrap': False, 'previousTarget': array([3., 8.]), 'currentState': array([3.25228463, 8.58424413, 5.33259991]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.6363872530654392}
episode index:2872
target Thresh 75.99996031856752
target distance 8.0
model initialize at round 2872
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42., 15.]), 'dynamicTrap': False, 'previousTarget': array([42., 15.]), 'currentState': array([45.41754374,  5.42103427,  3.34899044]), 'targetState': array([42, 15], dtype=int32), 'currentDistance': 10.170358387847676}
done in step count: 8
reward sum = 0.884881792510561
running average episode reward sum: 0.6929551184923922
{'scaleFactor': 20, 'currentTarget': array([41.84160547, 12.8453566 ]), 'dynamicTrap': True, 'previousTarget': array([41.96256397, 13.00441313]), 'currentState': array([42.0182401 , 11.98177149,  2.30667372]), 'targetState': array([42, 15], dtype=int32), 'currentDistance': 0.8814641450653914}
episode index:2873
target Thresh 75.99996051647949
target distance 14.0
model initialize at round 2873
at step 0:
{'scaleFactor': 20, 'currentTarget': array([97., 14.]), 'dynamicTrap': False, 'previousTarget': array([97., 14.]), 'currentState': array([84.99380862,  7.01271523,  6.15229395]), 'targetState': array([97, 14], dtype=int32), 'currentDistance': 13.891392296303387}
done in step count: 8
reward sum = 0.9135172474836408
running average episode reward sum: 0.6930318624481999
{'scaleFactor': 20, 'currentTarget': array([96.95315141, 12.32620654]), 'dynamicTrap': True, 'previousTarget': array([97., 14.]), 'currentState': array([96.28538166, 11.83233667,  0.8926772 ]), 'targetState': array([97, 14], dtype=int32), 'currentDistance': 0.8305563729448306}
episode index:2874
target Thresh 75.99996071340436
target distance 20.0
model initialize at round 2874
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.59007736, 18.89578035]), 'dynamicTrap': False, 'previousTarget': array([83.61737619, 18.49390095]), 'currentState': array([69.80054054,  5.4321404 ,  6.11488208]), 'targetState': array([88, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.79016160011997
running average episode reward sum: 0.6930656467047814
{'scaleFactor': 20, 'currentTarget': array([88., 22.]), 'dynamicTrap': False, 'previousTarget': array([88., 22.]), 'currentState': array([8.71429160e+01, 2.10082221e+01, 4.47791686e-02]), 'targetState': array([88, 22], dtype=int32), 'currentDistance': 1.3108075117871292}
episode index:2875
target Thresh 75.99996090934708
target distance 12.0
model initialize at round 2875
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,   4.]), 'dynamicTrap': False, 'previousTarget': array([111.,   4.]), 'currentState': array([113.23706047,  16.99744773,   5.95175881]), 'targetState': array([111,   4], dtype=int32), 'currentDistance': 13.188558948214398}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6931487481307905
{'scaleFactor': 20, 'currentTarget': array([111.,   4.]), 'dynamicTrap': False, 'previousTarget': array([111.,   4.]), 'currentState': array([111.77928301,   4.93900683,   3.4330861 ]), 'targetState': array([111,   4], dtype=int32), 'currentDistance': 1.2202523679557091}
episode index:2876
target Thresh 75.99996110431252
target distance 39.0
model initialize at round 2876
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.88456536,  5.62183604]), 'dynamicTrap': False, 'previousTarget': array([91.97375327,  5.02429504]), 'currentState': array([70.88809871,  5.24590863,  1.33345234]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6931864547145264
{'scaleFactor': 20, 'currentTarget': array([111.,   6.]), 'dynamicTrap': False, 'previousTarget': array([111.,   6.]), 'currentState': array([110.6180848 ,   5.4472649 ,   1.21068669]), 'targetState': array([111,   6], dtype=int32), 'currentDistance': 0.6718447132051285}
episode index:2877
target Thresh 75.99996129830558
target distance 23.0
model initialize at round 2877
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.49120912, 13.01177591]), 'dynamicTrap': False, 'previousTarget': array([79.34140113, 12.97452223]), 'currentState': array([61.19143192,  4.94219386,  6.02056754]), 'targetState': array([84, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7498454342747278
running average episode reward sum: 0.6932061416427961
{'scaleFactor': 20, 'currentTarget': array([84., 15.]), 'dynamicTrap': False, 'previousTarget': array([83.66138379, 12.9938218 ]), 'currentState': array([83.70062204, 14.13791122,  1.84338494]), 'targetState': array([84, 15], dtype=int32), 'currentDistance': 0.9125920332324153}
episode index:2878
target Thresh 75.99996149133108
target distance 35.0
model initialize at round 2878
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.13222764,  9.49217764]), 'dynamicTrap': False, 'previousTarget': array([25.6170994 , 10.12161403]), 'currentState': array([44.83444093, 16.579294  ,  4.23344946]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6407539227260881
running average episode reward sum: 0.6931879227407758
{'scaleFactor': 20, 'currentTarget': array([9., 3.]), 'dynamicTrap': False, 'previousTarget': array([9., 3.]), 'currentState': array([8.99862565, 3.45981274, 5.12646826]), 'targetState': array([9, 3], dtype=int32), 'currentDistance': 0.4598147889612196}
episode index:2879
target Thresh 75.99996168339386
target distance 41.0
model initialize at round 2879
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50.99517349, 14.4393601 ]), 'dynamicTrap': True, 'previousTarget': array([50.99405381, 14.48765985]), 'currentState': array([31.     , 14.     ,  6.09276], dtype=float32), 'targetState': array([72, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7077681043315962
running average episode reward sum: 0.6931929853038281
{'scaleFactor': 20, 'currentTarget': array([72., 15.]), 'dynamicTrap': False, 'previousTarget': array([72., 15.]), 'currentState': array([71.35792017, 15.64256008,  0.18231239]), 'targetState': array([72, 15], dtype=int32), 'currentDistance': 0.9083776572231361}
episode index:2880
target Thresh 75.99996187449874
target distance 18.0
model initialize at round 2880
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.,  5.]), 'dynamicTrap': False, 'previousTarget': array([63.,  5.]), 'currentState': array([66.43361669, 21.44493712,  3.97741759]), 'targetState': array([63,  5], dtype=int32), 'currentDistance': 16.799573813324038}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6932600425361822
{'scaleFactor': 20, 'currentTarget': array([63.,  5.]), 'dynamicTrap': False, 'previousTarget': array([63.,  5.]), 'currentState': array([62.32322365,  5.63985434,  3.33224335]), 'targetState': array([63,  5], dtype=int32), 'currentDistance': 0.9313644859404557}
episode index:2881
target Thresh 75.99996206465048
target distance 20.0
model initialize at round 2881
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.,  6.]), 'dynamicTrap': False, 'previousTarget': array([69.0992562 ,  6.00992562]), 'currentState': array([87.37764333,  9.0832835 ,  2.94243175]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 18.634495200676707}
done in step count: 11
reward sum = 0.8855372542587164
running average episode reward sum: 0.6933267591259541
{'scaleFactor': 20, 'currentTarget': array([69.,  6.]), 'dynamicTrap': False, 'previousTarget': array([69.,  6.]), 'currentState': array([69.67759737,  6.55851848,  2.52213029]), 'targetState': array([69,  6], dtype=int32), 'currentDistance': 0.8781122274372042}
episode index:2882
target Thresh 75.99996225385382
target distance 45.0
model initialize at round 2882
at step 0:
{'scaleFactor': 20, 'currentTarget': array([73.64193801, 11.52138167]), 'dynamicTrap': False, 'previousTarget': array([74.78571634, 10.55079306]), 'currentState': array([92.9846071 ,  6.43599551,  2.44261742]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6933480537940352
{'scaleFactor': 20, 'currentTarget': array([49., 18.]), 'dynamicTrap': False, 'previousTarget': array([49., 18.]), 'currentState': array([49.75577649, 18.99276547,  3.94487125]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 1.2477104549949407}
episode index:2883
target Thresh 75.99996244211351
target distance 10.0
model initialize at round 2883
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.,  4.]), 'dynamicTrap': False, 'previousTarget': array([64.,  4.]), 'currentState': array([53.90105137, 12.83873804,  4.18026107]), 'targetState': array([64,  4], dtype=int32), 'currentDistance': 13.420583205697417}
done in step count: 41
reward sum = 0.3804722594278978
running average episode reward sum: 0.693239567041481
{'scaleFactor': 20, 'currentTarget': array([64.,  4.]), 'dynamicTrap': False, 'previousTarget': array([64.,  4.]), 'currentState': array([63.30278948,  4.42500062,  4.50833635]), 'targetState': array([64,  4], dtype=int32), 'currentDistance': 0.8165341595533152}
episode index:2884
target Thresh 75.99996262943425
target distance 15.0
model initialize at round 2884
at step 0:
{'scaleFactor': 20, 'currentTarget': array([114.,  20.]), 'dynamicTrap': False, 'previousTarget': array([114.,  20.]), 'currentState': array([100.37690638,   9.101928  ,   4.69523773]), 'targetState': array([114,  20], dtype=int32), 'currentDistance': 17.445820501575206}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6932944087066641
{'scaleFactor': 20, 'currentTarget': array([114.,  20.]), 'dynamicTrap': False, 'previousTarget': array([114.,  20.]), 'currentState': array([114.55436172,  20.39523087,   5.52459158]), 'targetState': array([114,  20], dtype=int32), 'currentDistance': 0.680826236028245}
episode index:2885
target Thresh 75.99996281582072
target distance 24.0
model initialize at round 2885
at step 0:
{'scaleFactor': 20, 'currentTarget': array([65.50456109,  8.55302762]), 'dynamicTrap': False, 'previousTarget': array([65.97366596,  8.32455532]), 'currentState': array([46.16376714,  3.46051453,  1.08079529]), 'targetState': array([71, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.7993075718257502
running average episode reward sum: 0.6933311423044185
{'scaleFactor': 20, 'currentTarget': array([71., 10.]), 'dynamicTrap': False, 'previousTarget': array([71., 10.]), 'currentState': array([71.0250314 ,  9.06443782,  0.58373613]), 'targetState': array([71, 10], dtype=int32), 'currentDistance': 0.9358969864471155}
episode index:2886
target Thresh 75.99996300127759
target distance 46.0
model initialize at round 2886
at step 0:
{'scaleFactor': 20, 'currentTarget': array([85.46430465, 12.88494743]), 'dynamicTrap': False, 'previousTarget': array([85.75988998, 11.93300282]), 'currentState': array([66.47754143,  6.59982034,  0.87645054]), 'targetState': array([113,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.6088877772444007
running average episode reward sum: 0.6933018927841346
{'scaleFactor': 20, 'currentTarget': array([113.,  22.]), 'dynamicTrap': False, 'previousTarget': array([113.,  22.]), 'currentState': array([112.84239236,  22.09534041,   0.27516185]), 'targetState': array([113,  22], dtype=int32), 'currentDistance': 0.18420087432484047}
episode index:2887
target Thresh 75.99996318580949
target distance 60.0
model initialize at round 2887
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.67695711, 11.22974545]), 'dynamicTrap': False, 'previousTarget': array([52.13473722, 10.31761399]), 'currentState': array([71.58726885,  9.33779232,  2.8800481 ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 48
reward sum = 0.5803872508859736
running average episode reward sum: 0.6932627949164413
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([12.23935579, 15.57223097,  3.15674835]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.6202737095721766}
episode index:2888
target Thresh 75.99996336942104
target distance 20.0
model initialize at round 2888
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.37641422, 10.89205638]), 'dynamicTrap': False, 'previousTarget': array([28.47568183, 11.361625  ]), 'currentState': array([44.15982981, 21.76941641,  2.86319897]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6933265741577298
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'dynamicTrap': False, 'previousTarget': array([26., 10.]), 'currentState': array([25.94307218, 10.31309702,  3.47350553]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.31823029308921136}
episode index:2889
target Thresh 75.99996355211681
target distance 19.0
model initialize at round 2889
at step 0:
{'scaleFactor': 20, 'currentTarget': array([39.99402202,  5.77802492]), 'dynamicTrap': True, 'previousTarget': array([40.09517373,  5.66410281]), 'currentState': array([55.       , 19.       ,  5.5215178], dtype=float32), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7297819163225459
running average episode reward sum: 0.6933391884629772
{'scaleFactor': 20, 'currentTarget': array([36.,  2.]), 'dynamicTrap': False, 'previousTarget': array([36.,  2.]), 'currentState': array([35.27769054,  2.95186156,  3.956068  ]), 'targetState': array([36,  2], dtype=int32), 'currentDistance': 1.1948938785848873}
episode index:2890
target Thresh 75.99996373390138
target distance 21.0
model initialize at round 2890
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.24498976, 13.70023435]), 'dynamicTrap': False, 'previousTarget': array([33.99469707, 13.52709229]), 'currentState': array([49.56255857,  2.13576513,  2.71359396]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8418518109948755
running average episode reward sum: 0.6933905591383601
{'scaleFactor': 20, 'currentTarget': array([30., 16.]), 'dynamicTrap': False, 'previousTarget': array([30., 16.]), 'currentState': array([29.99435028, 16.101043  ,  3.99322238]), 'targetState': array([30, 16], dtype=int32), 'currentDistance': 0.10120082801410205}
episode index:2891
target Thresh 75.9999639147793
target distance 44.0
model initialize at round 2891
at step 0:
{'scaleFactor': 20, 'currentTarget': array([52.85624607, 18.37254678]), 'dynamicTrap': False, 'previousTarget': array([52.49734288, 17.43242207]), 'currentState': array([72.51841515, 14.71206774,  2.09396848]), 'targetState': array([28, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 65
reward sum = 0.3034278687700878
running average episode reward sum: 0.6932557172675551
{'scaleFactor': 20, 'currentTarget': array([28., 23.]), 'dynamicTrap': False, 'previousTarget': array([28., 23.]), 'currentState': array([28.12131894, 22.96723999,  2.91888753]), 'targetState': array([28, 23], dtype=int32), 'currentDistance': 0.12566424832479314}
episode index:2892
target Thresh 75.99996409475509
target distance 13.0
model initialize at round 2892
at step 0:
{'scaleFactor': 20, 'currentTarget': array([82.,  6.]), 'dynamicTrap': False, 'previousTarget': array([82.,  6.]), 'currentState': array([67.42663552, 17.27711965,  4.49808121]), 'targetState': array([82,  6], dtype=int32), 'currentDistance': 18.427055643109327}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.6933194107710917
{'scaleFactor': 20, 'currentTarget': array([82.,  6.]), 'dynamicTrap': False, 'previousTarget': array([82.,  6.]), 'currentState': array([82.19751668,  5.78512607,  6.16823167]), 'targetState': array([82,  6], dtype=int32), 'currentDistance': 0.29186237858975433}
episode index:2893
target Thresh 75.99996427383324
target distance 7.0
model initialize at round 2893
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.00217084,   8.17912831]), 'dynamicTrap': True, 'previousTarget': array([105.,   8.]), 'currentState': array([98.       ,  8.       ,  4.6012707], dtype=float32), 'targetState': array([105,   8], dtype=int32), 'currentDistance': 7.004461677669885}
done in step count: 9
reward sum = 0.8549973968846408
running average episode reward sum: 0.6933752773868876
{'scaleFactor': 20, 'currentTarget': array([105.,   8.]), 'dynamicTrap': False, 'previousTarget': array([105.,   8.]), 'currentState': array([104.50537526,   7.51432302,   0.44725326]), 'targetState': array([105,   8], dtype=int32), 'currentDistance': 0.693206865449792}
episode index:2894
target Thresh 75.99996445201825
target distance 58.0
model initialize at round 2894
at step 0:
{'scaleFactor': 20, 'currentTarget': array([76.69275381, 20.01770685]), 'dynamicTrap': False, 'previousTarget': array([74.98811999, 19.68924552]), 'currentState': array([56.70006953, 19.47680445,  1.13590437]), 'targetState': array([113,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.6137637655354
running average episode reward sum: 0.693347777728217
{'scaleFactor': 20, 'currentTarget': array([113.,  21.]), 'dynamicTrap': False, 'previousTarget': array([113.,  21.]), 'currentState': array([112.58143935,  20.35422595,   1.06136057]), 'targetState': array([113,  21], dtype=int32), 'currentDistance': 0.7695564573575333}
episode index:2895
target Thresh 75.99996462931455
target distance 9.0
model initialize at round 2895
at step 0:
{'scaleFactor': 20, 'currentTarget': array([108.,  11.]), 'dynamicTrap': False, 'previousTarget': array([108.,  11.]), 'currentState': array([98.84959233,  6.87551736,  3.93395848]), 'targetState': array([108,  11], dtype=int32), 'currentDistance': 10.0369974342348}
done in step count: 13
reward sum = 0.8304422604826087
running average episode reward sum: 0.6933951169833117
{'scaleFactor': 20, 'currentTarget': array([108.,  11.]), 'dynamicTrap': False, 'previousTarget': array([108.,  11.]), 'currentState': array([1.08795577e+02, 1.11537478e+01, 9.66508051e-02]), 'targetState': array([108,  11], dtype=int32), 'currentDistance': 0.8102965791553015}
episode index:2896
target Thresh 75.99996480572658
target distance 46.0
model initialize at round 2896
at step 0:
{'scaleFactor': 20, 'currentTarget': array([46.93049026, 17.91568472]), 'dynamicTrap': False, 'previousTarget': array([44.98112317, 17.86874449]), 'currentState': array([26.95075384, 17.01561098,  0.34472578]), 'targetState': array([71, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6197598868756626
running average episode reward sum: 0.6933696992304268
{'scaleFactor': 20, 'currentTarget': array([71., 19.]), 'dynamicTrap': False, 'previousTarget': array([71., 19.]), 'currentState': array([70.93619044, 18.90418532,  1.23896452]), 'targetState': array([71, 19], dtype=int32), 'currentDistance': 0.11511782139191823}
episode index:2897
target Thresh 75.99996498125874
target distance 50.0
model initialize at round 2897
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.86096975, 18.55303397]), 'dynamicTrap': False, 'previousTarget': array([21.53288933, 17.70276435]), 'currentState': array([ 2.46089097, 23.41481793,  0.24587727]), 'targetState': array([52, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5962580317493208
running average episode reward sum: 0.6933361893382664
{'scaleFactor': 20, 'currentTarget': array([52., 11.]), 'dynamicTrap': False, 'previousTarget': array([52., 11.]), 'currentState': array([51.76371686, 10.68780654,  0.23814885]), 'targetState': array([52, 11], dtype=int32), 'currentDistance': 0.3915283855579085}
episode index:2898
target Thresh 75.99996515591545
target distance 5.0
model initialize at round 2898
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90., 21.]), 'dynamicTrap': False, 'previousTarget': array([90., 21.]), 'currentState': array([88.01200743, 17.37589024,  1.20196676]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 4.1335560943506975}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6934351075206264
{'scaleFactor': 20, 'currentTarget': array([90., 21.]), 'dynamicTrap': False, 'previousTarget': array([90., 21.]), 'currentState': array([90.24608333, 20.18855259,  6.07057162]), 'targetState': array([90, 21], dtype=int32), 'currentDistance': 0.8479409784097965}
episode index:2899
target Thresh 75.99996532970104
target distance 37.0
model initialize at round 2899
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.3680956 , 15.69337777]), 'dynamicTrap': False, 'previousTarget': array([31.74210955, 15.20142317]), 'currentState': array([13.58233871, 12.7738188 ,  1.29751354]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.44589951089495716
running average episode reward sum: 0.6933497504183418
{'scaleFactor': 20, 'currentTarget': array([49., 18.]), 'dynamicTrap': False, 'previousTarget': array([49., 18.]), 'currentState': array([49.16312305, 17.52604115,  1.94308654]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 0.5012445753128898}
episode index:2900
target Thresh 75.99996550261989
target distance 12.0
model initialize at round 2900
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([18.06068842, 23.18706935,  1.15680574]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 14.54501040441219}
done in step count: 12
reward sum = 0.8583268707937801
running average episode reward sum: 0.6934066194705222
{'scaleFactor': 20, 'currentTarget': array([26., 11.]), 'dynamicTrap': False, 'previousTarget': array([26., 11.]), 'currentState': array([25.84436309, 10.65352749,  5.16909936]), 'targetState': array([26, 11], dtype=int32), 'currentDistance': 0.3798237015055823}
episode index:2901
target Thresh 75.99996567467628
target distance 46.0
model initialize at round 2901
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.90678894, 10.88661623]), 'dynamicTrap': False, 'previousTarget': array([69.88288926, 11.83881638]), 'currentState': array([50.96307725, 12.3860708 ,  5.86347079]), 'targetState': array([96,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7536552562194138
running average episode reward sum: 0.6934273805445225
{'scaleFactor': 20, 'currentTarget': array([96.,  9.]), 'dynamicTrap': False, 'previousTarget': array([96.,  9.]), 'currentState': array([95.33008768,  8.69842481,  6.22934827]), 'targetState': array([96,  9], dtype=int32), 'currentDistance': 0.7346632632771586}
episode index:2902
target Thresh 75.99996584587454
target distance 19.0
model initialize at round 2902
at step 0:
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([93.12252231, 12.003844  ,  4.75733756]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 18.983533776697236}
done in step count: 28
reward sum = 0.6286370386537202
running average episode reward sum: 0.6934050621353283
{'scaleFactor': 20, 'currentTarget': array([112.,  10.]), 'dynamicTrap': False, 'previousTarget': array([112.,  10.]), 'currentState': array([112.05010506,   9.95548859,   6.14810103]), 'targetState': array([112,  10], dtype=int32), 'currentDistance': 0.06702076781490768}
episode index:2903
target Thresh 75.99996601621896
target distance 15.0
model initialize at round 2903
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'dynamicTrap': False, 'previousTarget': array([14.,  3.]), 'currentState': array([29.18680318,  6.67257067,  2.46957099]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 15.624556515008592}
done in step count: 11
reward sum = 0.8670928987866365
running average episode reward sum: 0.6934648719964341
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'dynamicTrap': False, 'previousTarget': array([14.,  3.]), 'currentState': array([13.67856934,  3.50008554,  3.02264817]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.594477260347711}
episode index:2904
target Thresh 75.99996618571377
target distance 43.0
model initialize at round 2904
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.67674885, 13.95832245]), 'dynamicTrap': False, 'previousTarget': array([90.99459386, 14.46499055]), 'currentState': array([72.70057112, 12.98245175,  5.98061621]), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7313552357737794
running average episode reward sum: 0.6934779151509186
{'scaleFactor': 20, 'currentTarget': array([114.,  15.]), 'dynamicTrap': False, 'previousTarget': array([114.,  15.]), 'currentState': array([114.49239974,  14.5851122 ,   1.82117011]), 'targetState': array([114,  15], dtype=int32), 'currentDistance': 0.6438861646272228}
episode index:2905
target Thresh 75.99996635436324
target distance 52.0
model initialize at round 2905
at step 0:
{'scaleFactor': 20, 'currentTarget': array([60.95700419, 21.17069143]), 'dynamicTrap': False, 'previousTarget': array([59.94108971, 20.46607002]), 'currentState': array([41.06051813, 23.20289278,  6.1214034 ]), 'targetState': array([92, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.62616945276712
running average episode reward sum: 0.6934547532574624
{'scaleFactor': 20, 'currentTarget': array([92., 18.]), 'dynamicTrap': False, 'previousTarget': array([92., 18.]), 'currentState': array([91.94598024, 18.45112138,  0.21852475]), 'targetState': array([92, 18], dtype=int32), 'currentDistance': 0.454344179674868}
episode index:2906
target Thresh 75.99996652217155
target distance 5.0
model initialize at round 2906
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.80444304, 12.95969698]), 'dynamicTrap': True, 'previousTarget': array([17., 13.]), 'currentState': array([20.        , 18.        ,  0.30224776], dtype=float32), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 5.967934214041717}
done in step count: 12
reward sum = 0.8302145828297642
running average episode reward sum: 0.6935017982624753
{'scaleFactor': 20, 'currentTarget': array([18.16111286, 14.51379181]), 'dynamicTrap': True, 'previousTarget': array([17., 13.]), 'currentState': array([17.28692189, 14.88616312,  4.35645052]), 'targetState': array([17, 13], dtype=int32), 'currentDistance': 0.9501948452154757}
episode index:2907
target Thresh 75.99996668914291
target distance 29.0
model initialize at round 2907
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.05256618, 12.64357306]), 'dynamicTrap': False, 'previousTarget': array([13.98896727, 13.30501868]), 'currentState': array([32.49131757, 20.39034316,  3.99863815]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8084921361032208
running average episode reward sum: 0.693541341019642
{'scaleFactor': 20, 'currentTarget': array([3., 8.]), 'dynamicTrap': False, 'previousTarget': array([3., 8.]), 'currentState': array([3.38279882, 8.06968887, 4.18213944]), 'targetState': array([3, 8], dtype=int32), 'currentDistance': 0.38909057099564376}
episode index:2908
target Thresh 75.9999668552815
target distance 19.0
model initialize at round 2908
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.14121955,  20.3345629 ]), 'dynamicTrap': False, 'previousTarget': array([114.76686234,  20.91410718]), 'currentState': array([94.31147865, 13.59357947,  2.46150136]), 'targetState': array([115,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.7188759788159316
running average episode reward sum: 0.6935500500735425
{'scaleFactor': 20, 'currentTarget': array([115.,  21.]), 'dynamicTrap': False, 'previousTarget': array([115.,  21.]), 'currentState': array([114.89465549,  20.95917111,   1.23217009]), 'targetState': array([115,  21], dtype=int32), 'currentDistance': 0.11297992430697243}
episode index:2909
target Thresh 75.99996702059148
target distance 14.0
model initialize at round 2909
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'dynamicTrap': False, 'previousTarget': array([16., 21.]), 'currentState': array([ 1.26528257, 12.38561216,  1.22053146]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 17.068086465572645}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6936256401757454
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'dynamicTrap': False, 'previousTarget': array([16., 21.]), 'currentState': array([16.34726945, 20.70378532,  0.4584012 ]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.4564419015049729}
episode index:2910
target Thresh 75.99996718507695
target distance 1.0
model initialize at round 2910
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.64222322, 19.10606684,  2.74944043]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 1.9274301683418373}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6937274520478939
{'scaleFactor': 20, 'currentTarget': array([ 7., 21.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 21.]), 'currentState': array([ 6.28020472, 20.76573525,  0.80220604]), 'targetState': array([ 7, 21], dtype=int32), 'currentDistance': 0.7569578738176741}
episode index:2911
target Thresh 75.99996734874208
target distance 44.0
model initialize at round 2911
at step 0:
{'scaleFactor': 20, 'currentTarget': array([51.40823843, 15.99620657]), 'dynamicTrap': False, 'previousTarget': array([53.24839387, 15.85769903]), 'currentState': array([71.09758994, 19.50753639,  3.18988198]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7307489212179573
running average episode reward sum: 0.6937401654645045
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([29., 12.]), 'currentState': array([29.56601246, 12.14465711,  3.3524805 ]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 0.5842052548393742}
episode index:2912
target Thresh 75.99996751159091
target distance 17.0
model initialize at round 2912
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69., 19.]), 'dynamicTrap': False, 'previousTarget': array([69., 19.]), 'currentState': array([84.26453999, 23.59749645,  2.09127593]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 15.941867979225906}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6938156124545556
{'scaleFactor': 20, 'currentTarget': array([69., 19.]), 'dynamicTrap': False, 'previousTarget': array([69., 19.]), 'currentState': array([68.4829533 , 18.50834239,  4.38832279]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 0.7134875539027731}
episode index:2913
target Thresh 75.99996767362751
target distance 4.0
model initialize at round 2913
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   7.]), 'dynamicTrap': False, 'previousTarget': array([106.,   7.]), 'currentState': array([107.55830024,   4.94888552,   1.72180103]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 2.5759212444943747}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6939172543171312
{'scaleFactor': 20, 'currentTarget': array([106.,   7.]), 'dynamicTrap': False, 'previousTarget': array([106.,   7.]), 'currentState': array([106.47157488,   6.54752166,   2.62285845]), 'targetState': array([106,   7], dtype=int32), 'currentDistance': 0.6535438089520241}
episode index:2914
target Thresh 75.99996783485598
target distance 62.0
model initialize at round 2914
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.33420002,  8.2914469 ]), 'dynamicTrap': False, 'previousTarget': array([91.49117996,  9.40521743]), 'currentState': array([111.75015609,   3.49346294,   4.16438246]), 'targetState': array([49, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 54
reward sum = 0.4544799332383955
running average episode reward sum: 0.6938351145843427
{'scaleFactor': 20, 'currentTarget': array([49., 19.]), 'dynamicTrap': False, 'previousTarget': array([49., 19.]), 'currentState': array([48.59263044, 19.0023547 ,  2.39772472]), 'targetState': array([49, 19], dtype=int32), 'currentDistance': 0.4073763674189154}
episode index:2915
target Thresh 75.9999679952803
target distance 65.0
model initialize at round 2915
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.00148493, 18.24371107]), 'dynamicTrap': True, 'previousTarget': array([63.00946074, 18.61509352]), 'currentState': array([83.      , 18.      ,  2.334499], dtype=float32), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 58
reward sum = 0.511313318100614
running average episode reward sum: 0.6937725213756719
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'dynamicTrap': False, 'previousTarget': array([18., 20.]), 'currentState': array([17.06576328, 19.68040667,  3.70438724]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 0.9873895601082887}
episode index:2916
target Thresh 75.99996815490451
target distance 17.0
model initialize at round 2916
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([28.79140314, 12.13497444]), 'currentState': array([13.56443515, 23.4251154 ,  5.53854257]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 19.203903880007626}
done in step count: 18
reward sum = 0.7598861000211353
running average episode reward sum: 0.6937951862980735
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([29., 12.]), 'currentState': array([29.13124998, 11.69341159,  4.71180337]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 0.33350113947551013}
episode index:2917
target Thresh 75.99996831373258
target distance 41.0
model initialize at round 2917
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.27481463, 12.88908074]), 'dynamicTrap': False, 'previousTarget': array([47., 13.]), 'currentState': array([65.27448348, 12.77399022,  2.36192822]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7335065418722672
running average episode reward sum: 0.6938087953986815
{'scaleFactor': 20, 'currentTarget': array([26., 13.]), 'dynamicTrap': False, 'previousTarget': array([26., 13.]), 'currentState': array([26.01685478, 13.529476  ,  2.43281882]), 'targetState': array([26, 13], dtype=int32), 'currentDistance': 0.5297441968231704}
episode index:2918
target Thresh 75.9999684717685
target distance 7.0
model initialize at round 2918
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  20.]), 'dynamicTrap': False, 'previousTarget': array([111.,  20.]), 'currentState': array([105.93481538,  22.90132369,   6.13932903]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 5.837274563967126}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6939035162635672
{'scaleFactor': 20, 'currentTarget': array([111.,  20.]), 'dynamicTrap': False, 'previousTarget': array([111.,  20.]), 'currentState': array([110.6818484 ,  20.58166345,   5.71376623]), 'targetState': array([111,  20], dtype=int32), 'currentDistance': 0.6629877905981453}
episode index:2919
target Thresh 75.99996862901621
target distance 40.0
model initialize at round 2919
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90.47964499, 16.38268101]), 'dynamicTrap': False, 'previousTarget': array([89.40285  , 16.8507125]), 'currentState': array([71.25962354, 10.85178818,  0.23434382]), 'targetState': array([110,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.6396314655332144
running average episode reward sum: 0.693884929944824
{'scaleFactor': 20, 'currentTarget': array([110.,  22.]), 'dynamicTrap': False, 'previousTarget': array([110.,  22.]), 'currentState': array([109.71166087,  21.59309255,   3.50711475]), 'targetState': array([110,  22], dtype=int32), 'currentDistance': 0.49871146306159425}
episode index:2920
target Thresh 75.99996878547964
target distance 33.0
model initialize at round 2920
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.08729223, 15.2132098 ]), 'dynamicTrap': False, 'previousTarget': array([48.79586847, 15.83486126]), 'currentState': array([30.62096428,  7.53240695,  6.1225065 ]), 'targetState': array([63, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 36
reward sum = 0.5954841413246722
running average episode reward sum: 0.69385124258138
{'scaleFactor': 20, 'currentTarget': array([63., 21.]), 'dynamicTrap': False, 'previousTarget': array([63., 21.]), 'currentState': array([63.46685816, 20.52417248,  2.85281556]), 'targetState': array([63, 21], dtype=int32), 'currentDistance': 0.666609610558531}
episode index:2921
target Thresh 75.9999689411627
target distance 20.0
model initialize at round 2921
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90., 22.]), 'dynamicTrap': False, 'previousTarget': array([90.03319094, 21.77872706]), 'currentState': array([93.04327099,  3.96683154,  1.84868276]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 18.288156359550204}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6939232928320396
{'scaleFactor': 20, 'currentTarget': array([90., 22.]), 'dynamicTrap': False, 'previousTarget': array([90., 22.]), 'currentState': array([90.36340742, 21.27129789,  1.85910268]), 'targetState': array([90, 22], dtype=int32), 'currentDistance': 0.8142921595074771}
episode index:2922
target Thresh 75.99996909606932
target distance 31.0
model initialize at round 2922
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.4489403,  6.2069708]), 'dynamicTrap': False, 'previousTarget': array([36.63559701,  6.19956187]), 'currentState': array([16.80428807,  9.96032113,  3.4384678 ]), 'targetState': array([48,  4], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 25
reward sum = 0.7204660538270667
running average episode reward sum: 0.6939323734892393
{'scaleFactor': 20, 'currentTarget': array([48.,  4.]), 'dynamicTrap': False, 'previousTarget': array([48.,  4.]), 'currentState': array([47.35852587,  4.50328609,  0.25635145]), 'targetState': array([48,  4], dtype=int32), 'currentDistance': 0.8153440635187397}
episode index:2923
target Thresh 75.9999692502033
target distance 47.0
model initialize at round 2923
at step 0:
{'scaleFactor': 20, 'currentTarget': array([84.74776059, 20.43129958]), 'dynamicTrap': False, 'previousTarget': array([84.83899559, 19.46736226]), 'currentState': array([65.00702833, 23.64120455,  0.7499001 ]), 'targetState': array([112,  16], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5867517915754707
running average episode reward sum: 0.6938957180234685
{'scaleFactor': 20, 'currentTarget': array([112.,  16.]), 'dynamicTrap': False, 'previousTarget': array([112.,  16.]), 'currentState': array([112.45105416,  15.5705542 ,   5.73152346]), 'targetState': array([112,  16], dtype=int32), 'currentDistance': 0.6227949495803641}
episode index:2924
target Thresh 75.99996940356856
target distance 26.0
model initialize at round 2924
at step 0:
{'scaleFactor': 20, 'currentTarget': array([105.73085582,  16.90374571]), 'dynamicTrap': False, 'previousTarget': array([105.86817872,  17.29248216]), 'currentState': array([86.02979525, 13.45871851,  5.74171805]), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7216598693419334
running average episode reward sum: 0.6939052100410132
{'scaleFactor': 20, 'currentTarget': array([112.,  18.]), 'dynamicTrap': False, 'previousTarget': array([112.,  18.]), 'currentState': array([111.38576652,  18.55245016,   1.67630094]), 'targetState': array([112,  18], dtype=int32), 'currentDistance': 0.8261258629036412}
episode index:2925
target Thresh 75.99996955616889
target distance 52.0
model initialize at round 2925
at step 0:
{'scaleFactor': 20, 'currentTarget': array([78.14664289, 10.46116317]), 'dynamicTrap': False, 'previousTarget': array([76.64012894, 10.22305213]), 'currentState': array([58.57127371, 14.56054379,  5.71090335]), 'targetState': array([109,   4], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5884836430165205
running average episode reward sum: 0.6938691807973275
{'scaleFactor': 20, 'currentTarget': array([109.,   4.]), 'dynamicTrap': False, 'previousTarget': array([109.,   4.]), 'currentState': array([108.01039405,   4.09444173,   0.30045873]), 'targetState': array([109,   4], dtype=int32), 'currentDistance': 0.9941022009814844}
episode index:2926
target Thresh 75.99996970800814
target distance 8.0
model initialize at round 2926
at step 0:
{'scaleFactor': 20, 'currentTarget': array([111.,  19.]), 'dynamicTrap': False, 'previousTarget': array([111.,  19.]), 'currentState': array([116.09475301,  12.18136935,   1.2146287 ]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 8.511770211557192}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6939570253033414
{'scaleFactor': 20, 'currentTarget': array([111.,  19.]), 'dynamicTrap': False, 'previousTarget': array([111.,  19.]), 'currentState': array([111.10042657,  19.18115787,   2.28183451]), 'targetState': array([111,  19], dtype=int32), 'currentDistance': 0.2071320074913499}
episode index:2927
target Thresh 75.99996985909009
target distance 41.0
model initialize at round 2927
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.05551881,  8.11736814]), 'dynamicTrap': False, 'previousTarget': array([30.80525186,  8.61797507]), 'currentState': array([49.06566614,  1.90332769,  2.99675131]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.21271912558748907
running average episode reward sum: 0.6937926680971542
{'scaleFactor': 20, 'currentTarget': array([ 9., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 15.]), 'currentState': array([ 9.20365358, 15.26636124,  4.4946012 ]), 'targetState': array([ 9, 15], dtype=int32), 'currentDistance': 0.335295529774134}
episode index:2928
target Thresh 75.99997000941849
target distance 19.0
model initialize at round 2928
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.67420241,  4.63935125]), 'dynamicTrap': False, 'previousTarget': array([53.02072541,  5.69147429]), 'currentState': array([44.59368743, 22.45912251,  4.94599599]), 'targetState': array([54,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 12
reward sum = 0.8679150628989679
running average episode reward sum: 0.6938521158249801
{'scaleFactor': 20, 'currentTarget': array([53.29858317,  5.73326435]), 'dynamicTrap': True, 'previousTarget': array([54.,  4.]), 'currentState': array([53.62322761,  6.44283477,  5.49341626]), 'targetState': array([54,  4], dtype=int32), 'currentDistance': 0.7803103181752378}
episode index:2929
target Thresh 75.99997015899714
target distance 43.0
model initialize at round 2929
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.07061126, 12.17659265]), 'dynamicTrap': False, 'previousTarget': array([86., 13.]), 'currentState': array([67.08469498, 11.42615904,  5.40998408]), 'targetState': array([109,  13], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7278582351937961
running average episode reward sum: 0.6938637220090649
{'scaleFactor': 20, 'currentTarget': array([109.,  13.]), 'dynamicTrap': False, 'previousTarget': array([109.,  13.]), 'currentState': array([109.19381557,  12.74040172,   6.00618524]), 'targetState': array([109,  13], dtype=int32), 'currentDistance': 0.32396873877442534}
episode index:2930
target Thresh 75.99997030782977
target distance 15.0
model initialize at round 2930
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.,  8.]), 'dynamicTrap': False, 'previousTarget': array([63.,  8.]), 'currentState': array([70.0889246 , 21.23056453,  4.0637235 ]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 15.010019644172731}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.693941811730122
{'scaleFactor': 20, 'currentTarget': array([63.,  8.]), 'dynamicTrap': False, 'previousTarget': array([63.,  8.]), 'currentState': array([63.41973559,  8.0592905 ,  3.64486435]), 'targetState': array([63,  8], dtype=int32), 'currentDistance': 0.42390249566998267}
episode index:2931
target Thresh 75.99997045592008
target distance 38.0
model initialize at round 2931
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.50547253, 12.65068312]), 'dynamicTrap': False, 'previousTarget': array([32.93796297, 13.57404971]), 'currentState': array([12.66490594, 10.13038202,  3.9759165 ]), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7264739317315527
running average episode reward sum: 0.6939529072690038
{'scaleFactor': 20, 'currentTarget': array([51., 15.]), 'dynamicTrap': False, 'previousTarget': array([51., 15.]), 'currentState': array([51.22136624, 14.23216153,  1.05788964]), 'targetState': array([51, 15], dtype=int32), 'currentDistance': 0.7991113324138022}
episode index:2932
target Thresh 75.99997060327179
target distance 44.0
model initialize at round 2932
at step 0:
{'scaleFactor': 20, 'currentTarget': array([44.08279865, 16.20366532]), 'dynamicTrap': False, 'previousTarget': array([42.59429865, 15.99207528]), 'currentState': array([24.57926103, 20.63221141,  5.68475408]), 'targetState': array([67, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.639294753529988
running average episode reward sum: 0.6939342716898225
{'scaleFactor': 20, 'currentTarget': array([67., 11.]), 'dynamicTrap': False, 'previousTarget': array([67., 11.]), 'currentState': array([67.04915407, 10.22648291,  1.78968412]), 'targetState': array([67, 11], dtype=int32), 'currentDistance': 0.7750772920220893}
episode index:2933
target Thresh 75.99997074988859
target distance 67.0
model initialize at round 2933
at step 0:
{'scaleFactor': 20, 'currentTarget': array([79.69714622,  9.23448527]), 'dynamicTrap': True, 'previousTarget': array([79.68490416,  9.18913023]), 'currentState': array([99.       ,  4.       ,  4.9909353], dtype=float32), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 70
reward sum = 0.4195725648200226
running average episode reward sum: 0.6938407605422867
{'scaleFactor': 20, 'currentTarget': array([32., 22.]), 'dynamicTrap': False, 'previousTarget': array([32., 22.]), 'currentState': array([31.29945793, 22.77203408,  3.3946312 ]), 'targetState': array([32, 22], dtype=int32), 'currentDistance': 1.042494991621422}
episode index:2934
target Thresh 75.99997089577413
target distance 13.0
model initialize at round 2934
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.65395346, 11.34977666]), 'dynamicTrap': True, 'previousTarget': array([44., 12.]), 'currentState': array([57.       , 15.       ,  1.9434175], dtype=float32), 'targetState': array([44, 12], dtype=int32), 'currentDistance': 11.918762629807244}
done in step count: 8
reward sum = 0.8936172474836408
running average episode reward sum: 0.6939088274884336
{'scaleFactor': 20, 'currentTarget': array([45.65301426, 12.13662623]), 'dynamicTrap': True, 'previousTarget': array([44., 12.]), 'currentState': array([46.5710121 , 12.45547064,  3.77064461]), 'targetState': array([44, 12], dtype=int32), 'currentDistance': 0.9717930772995304}
episode index:2935
target Thresh 75.99997104093207
target distance 41.0
model initialize at round 2935
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.31996115, 14.91515523]), 'dynamicTrap': False, 'previousTarget': array([70.71472851, 15.63407074]), 'currentState': array([50.49834782, 17.58042166,  5.38270521]), 'targetState': array([92, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.5871387758488086
running average episode reward sum: 0.6938724616670305
{'scaleFactor': 20, 'currentTarget': array([92., 12.]), 'dynamicTrap': False, 'previousTarget': array([92., 12.]), 'currentState': array([9.13035926e+01, 1.14436944e+01, 1.88438808e-02]), 'targetState': array([92, 12], dtype=int32), 'currentDistance': 0.8913243768022724}
episode index:2936
target Thresh 75.99997118536601
target distance 15.0
model initialize at round 2936
at step 0:
{'scaleFactor': 20, 'currentTarget': array([107.,   4.]), 'dynamicTrap': False, 'previousTarget': array([107.,   4.]), 'currentState': array([107.53835093,  17.86042504,   4.04464784]), 'targetState': array([107,   4], dtype=int32), 'currentDistance': 13.87087610566493}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6939472470895082
{'scaleFactor': 20, 'currentTarget': array([107.,   4.]), 'dynamicTrap': False, 'previousTarget': array([107.,   4.]), 'currentState': array([106.43013551,   3.22540821,   6.24598674]), 'targetState': array([107,   4], dtype=int32), 'currentDistance': 0.9616329740328693}
episode index:2937
target Thresh 75.9999713290796
target distance 6.0
model initialize at round 2937
at step 0:
{'scaleFactor': 20, 'currentTarget': array([83., 10.]), 'dynamicTrap': False, 'previousTarget': array([83., 10.]), 'currentState': array([88.82702504,  5.59469947,  1.89432019]), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 7.304854108893566}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6940380056881843
{'scaleFactor': 20, 'currentTarget': array([83., 10.]), 'dynamicTrap': False, 'previousTarget': array([83., 10.]), 'currentState': array([83.67151901, 10.95142285,  2.04121503]), 'targetState': array([83, 10], dtype=int32), 'currentDistance': 1.1645356217885308}
episode index:2938
target Thresh 75.99997147207641
target distance 61.0
model initialize at round 2938
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.23514025,  6.72375006]), 'dynamicTrap': False, 'previousTarget': array([51.97585674,  6.01758082]), 'currentState': array([33.28189337,  8.0904758 ,  5.97807163]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6612037303969979
running average episode reward sum: 0.6940268337673638
{'scaleFactor': 20, 'currentTarget': array([93.,  4.]), 'dynamicTrap': False, 'previousTarget': array([93.,  4.]), 'currentState': array([92.75146553,  4.52630191,  0.63868283]), 'targetState': array([93,  4], dtype=int32), 'currentDistance': 0.5820335761004891}
episode index:2939
target Thresh 75.99997161436002
target distance 39.0
model initialize at round 2939
at step 0:
{'scaleFactor': 20, 'currentTarget': array([66.3908617 , 13.38295006]), 'dynamicTrap': False, 'previousTarget': array([68.10437123, 13.95942269]), 'currentState': array([86.32792312, 14.96837812,  3.36487377]), 'targetState': array([49, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6657206171380383
running average episode reward sum: 0.6940172058025239
{'scaleFactor': 20, 'currentTarget': array([49., 12.]), 'dynamicTrap': False, 'previousTarget': array([49., 12.]), 'currentState': array([49.59453889, 11.86737216,  0.90714818]), 'targetState': array([49, 12], dtype=int32), 'currentDistance': 0.6091523888731596}
episode index:2940
target Thresh 75.99997175593398
target distance 52.0
model initialize at round 2940
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.86540931,  8.43999385]), 'dynamicTrap': False, 'previousTarget': array([46.82121323,  9.33175976]), 'currentState': array([27.98637851, 10.63638513,  0.0654332 ]), 'targetState': array([79,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 50
reward sum = 0.4306054779262286
running average episode reward sum: 0.6939276404411243
{'scaleFactor': 20, 'currentTarget': array([79.,  5.]), 'dynamicTrap': False, 'previousTarget': array([79.,  5.]), 'currentState': array([78.87242078,  5.21078195,  1.36733982]), 'targetState': array([79,  5], dtype=int32), 'currentDistance': 0.2463848306487968}
episode index:2941
target Thresh 75.99997189680187
target distance 41.0
model initialize at round 2941
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.41475244,  5.75169007]), 'dynamicTrap': False, 'previousTarget': array([94.05332553,  6.45951277]), 'currentState': array([113.29455499,   3.56229505,   3.24622965]), 'targetState': array([73,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7226500149009389
running average episode reward sum: 0.693937403314836
{'scaleFactor': 20, 'currentTarget': array([73.,  8.]), 'dynamicTrap': False, 'previousTarget': array([73.,  8.]), 'currentState': array([73.03337254,  8.30723842,  4.17272447]), 'targetState': array([73,  8], dtype=int32), 'currentDistance': 0.30904558886194683}
episode index:2942
target Thresh 75.99997203696715
target distance 24.0
model initialize at round 2942
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53.110226  ,  6.51840365]), 'dynamicTrap': False, 'previousTarget': array([53.15444247,  6.48069469]), 'currentState': array([72.97433487,  4.19092033,  1.74322002]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 19
reward sum = 0.7809449524066345
running average episode reward sum: 0.6939669675517003
{'scaleFactor': 20, 'currentTarget': array([49.,  7.]), 'dynamicTrap': False, 'previousTarget': array([49.,  7.]), 'currentState': array([48.61597498,  7.60955918,  2.12766279]), 'targetState': array([49,  7], dtype=int32), 'currentDistance': 0.720442649883402}
episode index:2943
target Thresh 75.99997217643336
target distance 41.0
model initialize at round 2943
at step 0:
{'scaleFactor': 20, 'currentTarget': array([80.86915807, 18.74032967]), 'dynamicTrap': False, 'previousTarget': array([81.28527149, 19.63407074]), 'currentState': array([100.69893302,  21.34417792,   3.52253056]), 'targetState': array([60, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7240700221360821
running average episode reward sum: 0.6939771927740456
{'scaleFactor': 20, 'currentTarget': array([60., 16.]), 'dynamicTrap': False, 'previousTarget': array([60., 16.]), 'currentState': array([59.95069414, 15.35901643,  1.02630071]), 'targetState': array([60, 16], dtype=int32), 'currentDistance': 0.6428771312449173}
episode index:2944
target Thresh 75.99997231520398
target distance 29.0
model initialize at round 2944
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.76790841, 21.00711279]), 'dynamicTrap': False, 'previousTarget': array([18.10616412, 20.94201698]), 'currentState': array([36.6019059 , 23.5785994 ,  1.81773269]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6940277754567654
{'scaleFactor': 20, 'currentTarget': array([ 9., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 20.]), 'currentState': array([ 8.6284261 , 19.7382578 ,  2.62803749]), 'targetState': array([ 9, 20], dtype=int32), 'currentDistance': 0.4545064776802836}
episode index:2945
target Thresh 75.99997245328247
target distance 75.0
model initialize at round 2945
at step 0:
{'scaleFactor': 20, 'currentTarget': array([63.42948989, 13.7052031 ]), 'dynamicTrap': False, 'previousTarget': array([62.91345391, 14.85858903]), 'currentState': array([43.56123654, 11.41336783,  6.06238127]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5718104393683994
running average episode reward sum: 0.6939862895993016
{'scaleFactor': 20, 'currentTarget': array([118.,  20.]), 'dynamicTrap': False, 'previousTarget': array([118.,  20.]), 'currentState': array([118.75209141,  20.03226528,   1.0541317 ]), 'targetState': array([118,  20], dtype=int32), 'currentDistance': 0.7527831901159427}
episode index:2946
target Thresh 75.9999725906723
target distance 43.0
model initialize at round 2946
at step 0:
{'scaleFactor': 20, 'currentTarget': array([68.90997555, 18.53879165]), 'dynamicTrap': False, 'previousTarget': array([68.33739901, 19.34184168]), 'currentState': array([88.69445568, 21.46699009,  4.12773609]), 'targetState': array([45, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7294301399443437
running average episode reward sum: 0.6939983166947699
{'scaleFactor': 20, 'currentTarget': array([45., 15.]), 'dynamicTrap': False, 'previousTarget': array([45., 15.]), 'currentState': array([45.47087274, 15.46164547,  3.14139982]), 'targetState': array([45, 15], dtype=int32), 'currentDistance': 0.6594222339034228}
episode index:2947
target Thresh 75.99997272737689
target distance 44.0
model initialize at round 2947
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.0379668 , 13.67538366]), 'dynamicTrap': False, 'previousTarget': array([90.79586847, 12.83486126]), 'currentState': array([73.14558483,  7.11200774,  6.04371161]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 35
reward sum = 0.6490074492128731
running average episode reward sum: 0.6939830552064788
{'scaleFactor': 20, 'currentTarget': array([116.,  22.]), 'dynamicTrap': False, 'previousTarget': array([116.,  22.]), 'currentState': array([115.80408105,  21.01637248,   1.1514354 ]), 'targetState': array([116,  22], dtype=int32), 'currentDistance': 1.0029493195107535}
episode index:2948
target Thresh 75.99997286339966
target distance 24.0
model initialize at round 2948
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.10792562, 14.63032688]), 'dynamicTrap': False, 'previousTarget': array([67.81870354, 14.66690579]), 'currentState': array([84.88887444, 23.78662849,  3.30190408]), 'targetState': array([62, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8059830993570775
running average episode reward sum: 0.6940210341973743
{'scaleFactor': 20, 'currentTarget': array([62., 12.]), 'dynamicTrap': False, 'previousTarget': array([62., 12.]), 'currentState': array([62.17924162, 12.64477169,  4.08749166]), 'targetState': array([62, 12], dtype=int32), 'currentDistance': 0.6692220044963991}
episode index:2949
target Thresh 75.99997299874403
target distance 67.0
model initialize at round 2949
at step 0:
{'scaleFactor': 20, 'currentTarget': array([77.01559479, 14.78965087]), 'dynamicTrap': True, 'previousTarget': array([77.03554768, 15.1919076 ]), 'currentState': array([97.       , 14.       ,  3.1596625], dtype=float32), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 44
reward sum = 0.5898198155440114
running average episode reward sum: 0.693985711750373
{'scaleFactor': 20, 'currentTarget': array([30., 18.]), 'dynamicTrap': False, 'previousTarget': array([30., 18.]), 'currentState': array([30.10846022, 18.47659999,  2.53045402]), 'targetState': array([30, 18], dtype=int32), 'currentDistance': 0.4887854025149859}
episode index:2950
target Thresh 75.99997313341335
target distance 12.0
model initialize at round 2950
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.87677163, 22.15721915]), 'dynamicTrap': True, 'previousTarget': array([71., 22.]), 'currentState': array([83.       , 18.       ,  5.4616194], dtype=float32), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 12.81620603920613}
done in step count: 99
reward sum = -0.6339676587267706
running average episode reward sum: 0.6935357106082256
{'scaleFactor': 20, 'currentTarget': array([70.77311619, 22.83019364]), 'dynamicTrap': True, 'previousTarget': array([70.85597672, 22.64853189]), 'currentState': array([83.       , 18.       ,  5.4616194], dtype=float32), 'targetState': array([71, 22], dtype=int32), 'currentDistance': 13.146385749046443}
episode index:2951
target Thresh 75.99997326741101
target distance 8.0
model initialize at round 2951
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.,  6.]), 'dynamicTrap': False, 'previousTarget': array([42.,  6.]), 'currentState': array([50.67917964, 13.88711143,  5.20605791]), 'targetState': array([42,  6], dtype=int32), 'currentDistance': 11.727518316870096}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6936165133308878
{'scaleFactor': 20, 'currentTarget': array([42.,  6.]), 'dynamicTrap': False, 'previousTarget': array([42.,  6.]), 'currentState': array([42.30418076,  5.95982599,  2.93490163]), 'targetState': array([42,  6], dtype=int32), 'currentDistance': 0.3068222386504486}
episode index:2952
target Thresh 75.99997340074034
target distance 39.0
model initialize at round 2952
at step 0:
{'scaleFactor': 20, 'currentTarget': array([43.87314743, 11.71413698]), 'dynamicTrap': False, 'previousTarget': array([45.10437123, 12.04057731]), 'currentState': array([63.71155906,  9.17692905,  2.70006227]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.6433694496439535
running average episode reward sum: 0.693599497731942
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'dynamicTrap': False, 'previousTarget': array([26., 14.]), 'currentState': array([26.79945379, 14.26918642,  2.68930112]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 0.8435565763641518}
episode index:2953
target Thresh 75.99997353340471
target distance 7.0
model initialize at round 2953
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,  21.]), 'dynamicTrap': False, 'previousTarget': array([106.,  21.]), 'currentState': array([108.28288875,  13.80532772,   2.32262683]), 'targetState': array([106,  21], dtype=int32), 'currentDistance': 7.548171315938093}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6936898824686609
{'scaleFactor': 20, 'currentTarget': array([106.,  21.]), 'dynamicTrap': False, 'previousTarget': array([106.,  21.]), 'currentState': array([106.27634399,  20.73797847,   2.84072483]), 'targetState': array([106,  21], dtype=int32), 'currentDistance': 0.38081659864728734}
episode index:2954
target Thresh 75.99997366540741
target distance 29.0
model initialize at round 2954
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.12900417,  16.44131967]), 'dynamicTrap': False, 'previousTarget': array([100.76435286,  16.18845838]), 'currentState': array([84.70220667,  6.62772049,  0.69059234]), 'targetState': array([112,  22], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.624388847822867
running average episode reward sum: 0.6936664303418772
{'scaleFactor': 20, 'currentTarget': array([112.,  22.]), 'dynamicTrap': False, 'previousTarget': array([112.,  22.]), 'currentState': array([111.49161751,  21.73946221,   0.76371511]), 'targetState': array([112,  22], dtype=int32), 'currentDistance': 0.5712553712960671}
episode index:2955
target Thresh 75.99997379675173
target distance 19.0
model initialize at round 2955
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.78782941, 9.17678831]), 'dynamicTrap': True, 'previousTarget': array([7., 9.]), 'currentState': array([26.       ,  6.       ,  3.9864588], dtype=float32), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 17.502879777459906}
done in step count: 64
reward sum = 0.1443212677765307
running average episode reward sum: 0.693480589623824
{'scaleFactor': 20, 'currentTarget': array([7., 9.]), 'dynamicTrap': False, 'previousTarget': array([7., 9.]), 'currentState': array([7.81280385, 9.24323844, 0.88754727]), 'targetState': array([7, 9], dtype=int32), 'currentDistance': 0.8484191370593644}
episode index:2956
target Thresh 75.99997392744098
target distance 12.0
model initialize at round 2956
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([4.16178215, 5.11428042, 1.77453106]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 10.947539835378185}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6935644582608809
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 3.25808596, 16.11508325,  1.56616985]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.2825818735329623}
episode index:2957
target Thresh 75.99997405747841
target distance 38.0
model initialize at round 2957
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.03104524,  4.05884999]), 'dynamicTrap': False, 'previousTarget': array([62.93796297,  3.57404971]), 'currentState': array([44.06173595,  2.95129117,  6.0037734 ]), 'targetState': array([81,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.6895215997205383
running average episode reward sum: 0.6935630915068104
{'scaleFactor': 20, 'currentTarget': array([81.,  5.]), 'dynamicTrap': False, 'previousTarget': array([81.,  5.]), 'currentState': array([81.38917472,  4.18009732,  4.93071008]), 'targetState': array([81,  5], dtype=int32), 'currentDistance': 0.9075777490061109}
episode index:2958
target Thresh 75.99997418686728
target distance 16.0
model initialize at round 2958
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'dynamicTrap': False, 'previousTarget': array([17., 15.]), 'currentState': array([31.01396953, 18.98619337,  2.94191045]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 14.569868894105605}
done in step count: 11
reward sum = 0.8672802533363673
running average episode reward sum: 0.6936217995709637
{'scaleFactor': 20, 'currentTarget': array([17., 15.]), 'dynamicTrap': False, 'previousTarget': array([17., 15.]), 'currentState': array([17.1906274 , 15.58101621,  2.57675904]), 'targetState': array([17, 15], dtype=int32), 'currentDistance': 0.6114888705785431}
episode index:2959
target Thresh 75.99997431561081
target distance 19.0
model initialize at round 2959
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.92206798,  3.5495147 ]), 'dynamicTrap': False, 'previousTarget': array([29.89888325,  4.13601924]), 'currentState': array([44.49246663, 16.10191232,  4.02169037]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.6791114505464436
running average episode reward sum: 0.693616897426023
{'scaleFactor': 20, 'currentTarget': array([27.,  2.]), 'dynamicTrap': False, 'previousTarget': array([27.,  2.]), 'currentState': array([26.26112746,  2.31003426,  2.34512137]), 'targetState': array([27,  2], dtype=int32), 'currentDistance': 0.8012826463364959}
episode index:2960
target Thresh 75.99997444371225
target distance 52.0
model initialize at round 2960
at step 0:
{'scaleFactor': 20, 'currentTarget': array([42.464711  , 22.80714063]), 'dynamicTrap': False, 'previousTarget': array([41.98522349, 21.76866244]), 'currentState': array([22.465085  , 22.68482954,  0.4054997 ]), 'targetState': array([74, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 55
reward sum = 0.40203757201526263
running average episode reward sum: 0.6935184241651615
{'scaleFactor': 20, 'currentTarget': array([74., 23.]), 'dynamicTrap': False, 'previousTarget': array([74., 23.]), 'currentState': array([73.68485529, 23.26563051,  5.25546779]), 'targetState': array([74, 23], dtype=int32), 'currentDistance': 0.4121598702830889}
episode index:2961
target Thresh 75.99997457117476
target distance 12.0
model initialize at round 2961
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 13.]), 'dynamicTrap': False, 'previousTarget': array([34., 13.]), 'currentState': array([44.47722127, 16.5372996 ,  2.83287954]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 11.058239190857028}
done in step count: 10
reward sum = 0.8653721249088044
running average episode reward sum: 0.6935764436454935
{'scaleFactor': 20, 'currentTarget': array([34., 13.]), 'dynamicTrap': False, 'previousTarget': array([34., 13.]), 'currentState': array([34.93800827, 12.44191613,  4.70224459]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 1.0914747425847724}
episode index:2962
target Thresh 75.99997469800155
target distance 27.0
model initialize at round 2962
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87.33408477, 14.99332578]), 'dynamicTrap': False, 'previousTarget': array([85.6656401, 14.6417852]), 'currentState': array([67.64247561, 11.49467912,  5.76923031]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.7476683918324931
running average episode reward sum: 0.6935946994498091
{'scaleFactor': 20, 'currentTarget': array([93., 16.]), 'dynamicTrap': False, 'previousTarget': array([93., 16.]), 'currentState': array([93.27874338, 15.69782504,  6.2180431 ]), 'targetState': array([93, 16], dtype=int32), 'currentDistance': 0.4111053129606683}
episode index:2963
target Thresh 75.99997482419579
target distance 40.0
model initialize at round 2963
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.34656202, 12.84245282]), 'dynamicTrap': False, 'previousTarget': array([28.7615699 , 12.79270645]), 'currentState': array([45.35066904, 21.55176567,  3.73062706]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.6874140592980822
running average episode reward sum: 0.6935926142135906
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([7.1538993 , 3.82918051, 2.81431396]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.8433417537791936}
episode index:2964
target Thresh 75.99997494976064
target distance 5.0
model initialize at round 2964
at step 0:
{'scaleFactor': 20, 'currentTarget': array([50., 19.]), 'dynamicTrap': False, 'previousTarget': array([50., 19.]), 'currentState': array([50.57389466, 14.83722667,  1.70930231]), 'targetState': array([50, 19], dtype=int32), 'currentDistance': 4.2021466945023676}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6936892440232995
{'scaleFactor': 20, 'currentTarget': array([50., 19.]), 'dynamicTrap': False, 'previousTarget': array([50., 19.]), 'currentState': array([49.66755064, 18.15929668,  1.24427965]), 'targetState': array([50, 19], dtype=int32), 'currentDistance': 0.9040490312963175}
episode index:2965
target Thresh 75.99997507469924
target distance 16.0
model initialize at round 2965
at step 0:
{'scaleFactor': 20, 'currentTarget': array([113.,  19.]), 'dynamicTrap': False, 'previousTarget': array([113.,  19.]), 'currentState': array([9.78504133e+01, 1.27295549e+01, 2.90839036e-02]), 'targetState': array([113,  19], dtype=int32), 'currentDistance': 16.395989085505995}
done in step count: 12
reward sum = 0.8569808817161292
running average episode reward sum: 0.6937442985201615
{'scaleFactor': 20, 'currentTarget': array([113.,  19.]), 'dynamicTrap': False, 'previousTarget': array([113.,  19.]), 'currentState': array([112.82674528,  19.65723705,   1.62961217]), 'targetState': array([113,  19], dtype=int32), 'currentDistance': 0.6796894409924955}
episode index:2966
target Thresh 75.9999751990147
target distance 62.0
model initialize at round 2966
at step 0:
{'scaleFactor': 20, 'currentTarget': array([45.08765894,  9.46573527]), 'dynamicTrap': False, 'previousTarget': array([43.63559701, 10.19956187]), 'currentState': array([25.41256089, 13.05607182,  4.92431376]), 'targetState': array([86,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6161283911573171
running average episode reward sum: 0.6937181387940534
{'scaleFactor': 20, 'currentTarget': array([86.,  2.]), 'dynamicTrap': False, 'previousTarget': array([86.,  2.]), 'currentState': array([85.30541782,  1.89318196,  0.46250144]), 'targetState': array([86,  2], dtype=int32), 'currentDistance': 0.7027478173492189}
episode index:2967
target Thresh 75.99997532271013
target distance 19.0
model initialize at round 2967
at step 0:
{'scaleFactor': 20, 'currentTarget': array([53., 23.]), 'dynamicTrap': False, 'previousTarget': array([53., 23.]), 'currentState': array([34.47888187, 18.38659979,  6.01092148]), 'targetState': array([53, 23], dtype=int32), 'currentDistance': 19.087044776189416}
done in step count: 44
reward sum = 0.34902482013429187
running average episode reward sum: 0.6936020022311626
{'scaleFactor': 20, 'currentTarget': array([53., 23.]), 'dynamicTrap': False, 'previousTarget': array([53., 23.]), 'currentState': array([52.98853039, 22.53752129,  4.87340653]), 'targetState': array([53, 23], dtype=int32), 'currentDistance': 0.46262091777914216}
episode index:2968
target Thresh 75.99997544578862
target distance 23.0
model initialize at round 2968
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.51379861, 18.94130514]), 'dynamicTrap': False, 'previousTarget': array([72., 19.]), 'currentState': array([90.49878191, 18.16642286,  2.72945786]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.7810548686077351
running average episode reward sum: 0.6936314575583357
{'scaleFactor': 20, 'currentTarget': array([69., 19.]), 'dynamicTrap': False, 'previousTarget': array([69., 19.]), 'currentState': array([69.68715547, 19.17127151,  2.10793681]), 'targetState': array([69, 19], dtype=int32), 'currentDistance': 0.7081783458880522}
episode index:2969
target Thresh 75.99997556825326
target distance 43.0
model initialize at round 2969
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.96627259, 10.10517502]), 'dynamicTrap': False, 'previousTarget': array([24.88400363, 10.58744313]), 'currentState': array([7.30906584, 2.9004102 , 0.26586336]), 'targetState': array([49, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 30
reward sum = 0.7146641176015842
running average episode reward sum: 0.6936385392620539
{'scaleFactor': 20, 'currentTarget': array([49., 19.]), 'dynamicTrap': False, 'previousTarget': array([49., 19.]), 'currentState': array([48.75836127, 18.33855544,  5.64370525]), 'targetState': array([49, 19], dtype=int32), 'currentDistance': 0.7042003805457052}
episode index:2970
target Thresh 75.9999756901071
target distance 48.0
model initialize at round 2970
at step 0:
{'scaleFactor': 20, 'currentTarget': array([72.70436303, 13.48978244]), 'dynamicTrap': False, 'previousTarget': array([72.49464637, 14.46752313]), 'currentState': array([53.37367584,  8.35903856,  5.94628859]), 'targetState': array([101,  21], dtype=int32), 'currentDistance': 20.0}
done in step count: 59
reward sum = 0.3990006651412953
running average episode reward sum: 0.6935393679816362
{'scaleFactor': 20, 'currentTarget': array([101.,  21.]), 'dynamicTrap': False, 'previousTarget': array([101.,  21.]), 'currentState': array([101.47425035,  20.74289189,   1.05965026]), 'targetState': array([101,  21], dtype=int32), 'currentDistance': 0.5394608149037194}
episode index:2971
target Thresh 75.9999758113532
target distance 49.0
model initialize at round 2971
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.32480294, 16.16297483]), 'dynamicTrap': False, 'previousTarget': array([58.26134238, 17.22263798]), 'currentState': array([77.94000165, 12.25862093,  3.89998794]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 40
reward sum = 0.617817626187575
running average episode reward sum: 0.6935138896028359
{'scaleFactor': 20, 'currentTarget': array([29., 22.]), 'dynamicTrap': False, 'previousTarget': array([29., 22.]), 'currentState': array([28.37769008, 22.98819097,  1.90289084]), 'targetState': array([29, 22], dtype=int32), 'currentDistance': 1.1678146432620429}
episode index:2972
target Thresh 75.99997593199457
target distance 34.0
model initialize at round 2972
at step 0:
{'scaleFactor': 20, 'currentTarget': array([91.77376284,  2.92677218]), 'dynamicTrap': False, 'previousTarget': array([93.03451254,  3.17444044]), 'currentState': array([111.7035439,   1.252309 ,   2.8230176]), 'targetState': array([79,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.7989822356042208
running average episode reward sum: 0.6935493649967147
{'scaleFactor': 20, 'currentTarget': array([79.,  4.]), 'dynamicTrap': False, 'previousTarget': array([79.,  4.]), 'currentState': array([79.75441859,  4.22935991,  1.71109872]), 'targetState': array([79,  4], dtype=int32), 'currentDistance': 0.7885133964780097}
episode index:2973
target Thresh 75.99997605203426
target distance 19.0
model initialize at round 2973
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.4313217 ,  6.08738716]), 'dynamicTrap': False, 'previousTarget': array([41.48094632,  7.24510704]), 'currentState': array([55.29893321, 19.46476089,  3.28203678]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.6986942639981869
running average episode reward sum: 0.6935510949560293
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([36.42930908,  2.12430612,  5.31747647]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 1.0452405906466826}
episode index:2974
target Thresh 75.99997617147524
target distance 41.0
model initialize at round 2974
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.01400906, 18.16007884]), 'dynamicTrap': False, 'previousTarget': array([90.94667447, 17.45951277]), 'currentState': array([72.03164712, 17.3203102 ,  6.18070585]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 39
reward sum = 0.5897202746278305
running average episode reward sum: 0.6935161938399526
{'scaleFactor': 20, 'currentTarget': array([112.,  19.]), 'dynamicTrap': False, 'previousTarget': array([112.,  19.]), 'currentState': array([112.48187196,  18.81241552,   1.27108949]), 'targetState': array([112,  19], dtype=int32), 'currentDistance': 0.5170962428232024}
episode index:2975
target Thresh 75.99997629032049
target distance 37.0
model initialize at round 2975
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.97363675, 11.9734354 ]), 'dynamicTrap': True, 'previousTarget': array([95.97084547, 11.92049484]), 'currentState': array([76.       , 13.       ,  5.6790276], dtype=float32), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 32
reward sum = 0.6310378139299013
running average episode reward sum: 0.6934951997606817
{'scaleFactor': 20, 'currentTarget': array([113.,  11.]), 'dynamicTrap': False, 'previousTarget': array([113.,  11.]), 'currentState': array([112.57830322,  10.04131167,   0.80138633]), 'targetState': array([113,  11], dtype=int32), 'currentDistance': 1.047335421039951}
episode index:2976
target Thresh 75.99997640857302
target distance 74.0
model initialize at round 2976
at step 0:
{'scaleFactor': 20, 'currentTarget': array([64.52445661,  7.42087481]), 'dynamicTrap': False, 'previousTarget': array([63.85370283,  8.58536047]), 'currentState': array([44.62643235,  9.43795948,  4.08378696]), 'targetState': array([118,   2], dtype=int32), 'currentDistance': 20.0}
done in step count: 53
reward sum = 0.5494993279926085
running average episode reward sum: 0.6934468303042599
{'scaleFactor': 20, 'currentTarget': array([118.,   2.]), 'dynamicTrap': False, 'previousTarget': array([118.,   2.]), 'currentState': array([117.4422725 ,   1.64968158,   0.5371252 ]), 'targetState': array([118,   2], dtype=int32), 'currentDistance': 0.6586220176333555}
episode index:2977
target Thresh 75.99997652623574
target distance 15.0
model initialize at round 2977
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49.24450121,  4.66511478]), 'dynamicTrap': True, 'previousTarget': array([49.,  3.]), 'currentState': array([64.       , 14.       ,  5.5565934], dtype=float32), 'targetState': array([49,  3], dtype=int32), 'currentDistance': 17.460378765230832}
done in step count: 20
reward sum = 0.7157503701124714
running average episode reward sum: 0.6934543197400584
{'scaleFactor': 20, 'currentTarget': array([49.,  3.]), 'dynamicTrap': False, 'previousTarget': array([49.,  3.]), 'currentState': array([49.77623769,  3.10851257,  3.59690347]), 'targetState': array([49,  3], dtype=int32), 'currentDistance': 0.7837856444539152}
episode index:2978
target Thresh 75.99997664331164
target distance 13.0
model initialize at round 2978
at step 0:
{'scaleFactor': 20, 'currentTarget': array([49., 18.]), 'dynamicTrap': False, 'previousTarget': array([49., 18.]), 'currentState': array([49.2623663 ,  6.92955044,  1.03245831]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 11.07355812240408}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6935375778231939
{'scaleFactor': 20, 'currentTarget': array([49., 18.]), 'dynamicTrap': False, 'previousTarget': array([49., 18.]), 'currentState': array([49.06758012, 17.9904018 ,  1.27215905]), 'targetState': array([49, 18], dtype=int32), 'currentDistance': 0.06825832096363871}
episode index:2979
target Thresh 75.9999767598036
target distance 59.0
model initialize at round 2979
at step 0:
{'scaleFactor': 20, 'currentTarget': array([93.30686488, 12.79211977]), 'dynamicTrap': False, 'previousTarget': array([95.13929528, 12.6436452 ]), 'currentState': array([113.14388135,  15.34021271,   3.11700511]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 63
reward sum = 0.3994251588919931
running average episode reward sum: 0.6934388823805996
{'scaleFactor': 20, 'currentTarget': array([54.64941055,  6.5347319 ]), 'dynamicTrap': True, 'previousTarget': array([56.,  8.]), 'currentState': array([55.21672918,  6.45248647,  0.97206621]), 'targetState': array([56,  8], dtype=int32), 'currentDistance': 0.5732492727920655}
episode index:2980
target Thresh 75.99997687571457
target distance 15.0
model initialize at round 2980
at step 0:
{'scaleFactor': 20, 'currentTarget': array([90., 11.]), 'dynamicTrap': False, 'previousTarget': array([90., 11.]), 'currentState': array([104.22705909,   7.73341335,   2.55351329]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 14.59725312398074}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6935158048267744
{'scaleFactor': 20, 'currentTarget': array([90., 11.]), 'dynamicTrap': False, 'previousTarget': array([90., 11.]), 'currentState': array([90.23828796, 11.41409022,  1.66720414]), 'targetState': array([90, 11], dtype=int32), 'currentDistance': 0.47775711280254823}
episode index:2981
target Thresh 75.99997699104742
target distance 14.0
model initialize at round 2981
at step 0:
{'scaleFactor': 20, 'currentTarget': array([92.,  9.]), 'dynamicTrap': False, 'previousTarget': array([92.,  9.]), 'currentState': array([80.83869325, 23.06131255,  2.872173  ]), 'targetState': array([92,  9], dtype=int32), 'currentDistance': 17.95258418605251}
done in step count: 15
reward sum = 0.8223943936189393
running average episode reward sum: 0.6935590236694278
{'scaleFactor': 20, 'currentTarget': array([92.,  9.]), 'dynamicTrap': False, 'previousTarget': array([92.,  9.]), 'currentState': array([91.6312402 ,  9.21271357,  6.22363747]), 'targetState': array([92,  9], dtype=int32), 'currentDistance': 0.42571217409666245}
episode index:2982
target Thresh 75.99997710580506
target distance 54.0
model initialize at round 2982
at step 0:
{'scaleFactor': 20, 'currentTarget': array([95.7224467 , 18.29154859]), 'dynamicTrap': False, 'previousTarget': array([95.12232531, 19.20863052]), 'currentState': array([115.54106391,  15.60409407,   4.07216692]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 37
reward sum = 0.6652829826893449
running average episode reward sum: 0.6935495446077516
{'scaleFactor': 20, 'currentTarget': array([61., 23.]), 'dynamicTrap': False, 'previousTarget': array([61., 23.]), 'currentState': array([60.47062771, 23.71336397,  3.35755736]), 'targetState': array([61, 23], dtype=int32), 'currentDistance': 0.8883260546615839}
episode index:2983
target Thresh 75.99997721999033
target distance 4.0
model initialize at round 2983
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'dynamicTrap': False, 'previousTarget': array([23., 17.]), 'currentState': array([24.44717767, 20.02326409,  3.78263433]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 3.351782949699561}
done in step count: 11
reward sum = 0.8669017401806465
running average episode reward sum: 0.6936076385070723
{'scaleFactor': 20, 'currentTarget': array([23., 17.]), 'dynamicTrap': False, 'previousTarget': array([23., 17.]), 'currentState': array([23.51409404, 17.74975537,  5.29123158]), 'targetState': array([23, 17], dtype=int32), 'currentDistance': 0.9090796422487414}
episode index:2984
target Thresh 75.9999773336061
target distance 38.0
model initialize at round 2984
at step 0:
{'scaleFactor': 20, 'currentTarget': array([70.27836061, 10.5890239 ]), 'dynamicTrap': False, 'previousTarget': array([71.10989094,  9.90630431]), 'currentState': array([90.05754592, 13.55277634,  1.35600507]), 'targetState': array([53,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 34
reward sum = 0.6749914195709412
running average episode reward sum: 0.693601401917814
{'scaleFactor': 20, 'currentTarget': array([53.,  8.]), 'dynamicTrap': False, 'previousTarget': array([53.,  8.]), 'currentState': array([52.305402  ,  8.70076166,  4.43490314]), 'targetState': array([53,  8], dtype=int32), 'currentDistance': 0.9866779035170743}
episode index:2985
target Thresh 75.99997744665521
target distance 20.0
model initialize at round 2985
at step 0:
{'scaleFactor': 20, 'currentTarget': array([106.,   8.]), 'dynamicTrap': False, 'previousTarget': array([105.77872706,   7.96680906]), 'currentState': array([87.48322566,  4.01358458,  4.86412174]), 'targetState': array([106,   8], dtype=int32), 'currentDistance': 18.941025313890492}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6936659643658376
{'scaleFactor': 20, 'currentTarget': array([106.,   8.]), 'dynamicTrap': False, 'previousTarget': array([106.,   8.]), 'currentState': array([106.16375535,   7.71661964,   1.244036  ]), 'targetState': array([106,   8], dtype=int32), 'currentDistance': 0.32729229272568844}
episode index:2986
target Thresh 75.99997755914049
target distance 62.0
model initialize at round 2986
at step 0:
{'scaleFactor': 20, 'currentTarget': array([47.1672424 , 10.42842177]), 'dynamicTrap': False, 'previousTarget': array([49.00260095, 10.32253869]), 'currentState': array([67.16521779, 10.1438514 ,  2.35584044]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 20.0}
done in step count: 61
reward sum = 0.37300276889777717
running average episode reward sum: 0.6935586114379944
{'scaleFactor': 20, 'currentTarget': array([ 7., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 11.]), 'currentState': array([ 6.90250162, 10.65750454,  3.54188634]), 'targetState': array([ 7, 11], dtype=int32), 'currentDistance': 0.3561026183746415}
episode index:2987
target Thresh 75.99997767106474
target distance 72.0
model initialize at round 2987
at step 0:
{'scaleFactor': 20, 'currentTarget': array([58.89559718, 19.6947857 ]), 'dynamicTrap': False, 'previousTarget': array([57.99807127, 20.72224901]), 'currentState': array([3.88959539e+01, 1.95753405e+01, 5.95267619e-04]), 'targetState': array([110,  20], dtype=int32), 'currentDistance': 20.0}
done in step count: 66
reward sum = 0.360223829918167
running average episode reward sum: 0.6934470536128537
{'scaleFactor': 20, 'currentTarget': array([110.,  20.]), 'dynamicTrap': False, 'previousTarget': array([110.,  20.]), 'currentState': array([110.43425638,  20.43500489,   1.60342879]), 'targetState': array([110,  20], dtype=int32), 'currentDistance': 0.6146607689040947}
episode index:2988
target Thresh 75.99997778243078
target distance 10.0
model initialize at round 2988
at step 0:
{'scaleFactor': 20, 'currentTarget': array([102.,  12.]), 'dynamicTrap': False, 'previousTarget': array([102.,  12.]), 'currentState': array([110.23923867,  12.69609826,   2.28247952]), 'targetState': array([102,  12], dtype=int32), 'currentDistance': 8.26859157972098}
done in step count: 19
reward sum = 0.7825365997723019
running average episode reward sum: 0.6934768594161859
{'scaleFactor': 20, 'currentTarget': array([103.57866682,  10.85198555]), 'dynamicTrap': True, 'previousTarget': array([102.,  12.]), 'currentState': array([104.12214851,  10.31148991,   2.83127839]), 'targetState': array([102,  12], dtype=int32), 'currentDistance': 0.7664906318768092}
episode index:2989
target Thresh 75.99997789324136
target distance 49.0
model initialize at round 2989
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.15066135,  7.52223614]), 'dynamicTrap': False, 'previousTarget': array([32.14827223,  6.56917619]), 'currentState': array([51.91425676, 10.58822645,  2.63473076]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.647766147142924
running average episode reward sum: 0.6934615715525494
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'dynamicTrap': False, 'previousTarget': array([3., 3.]), 'currentState': array([2.59733916, 2.82174025, 5.32415353]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.44035473548934007}
episode index:2990
target Thresh 75.99997800349928
target distance 19.0
model initialize at round 2990
at step 0:
{'scaleFactor': 20, 'currentTarget': array([89., 11.]), 'dynamicTrap': False, 'previousTarget': array([89.5672925, 11.23886  ]), 'currentState': array([106.75706906,  19.99069944,   3.42295635]), 'targetState': array([89, 11], dtype=int32), 'currentDistance': 19.90342126038985}
done in step count: 16
reward sum = 0.8039034655227957
running average episode reward sum: 0.6934984962914227
{'scaleFactor': 20, 'currentTarget': array([89., 11.]), 'dynamicTrap': False, 'previousTarget': array([89., 11.]), 'currentState': array([89.97515077, 11.88651006,  4.89935755]), 'targetState': array([89, 11], dtype=int32), 'currentDistance': 1.3178843358727415}
episode index:2991
target Thresh 75.99997811320729
target distance 63.0
model initialize at round 2991
at step 0:
{'scaleFactor': 20, 'currentTarget': array([71.50742188, 12.87342218]), 'dynamicTrap': False, 'previousTarget': array([70.79898987, 11.82842712]), 'currentState': array([51.65140754, 10.47786495,  0.19494927]), 'targetState': array([114,  18], dtype=int32), 'currentDistance': 20.0}
done in step count: 49
reward sum = 0.5433063861822894
running average episode reward sum: 0.693448298393659
{'scaleFactor': 20, 'currentTarget': array([114.,  18.]), 'dynamicTrap': False, 'previousTarget': array([114.,  18.]), 'currentState': array([114.50987881,  17.7504614 ,   2.71109224]), 'targetState': array([114,  18], dtype=int32), 'currentDistance': 0.5676670801839389}
episode index:2992
target Thresh 75.99997822236811
target distance 19.0
model initialize at round 2992
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'dynamicTrap': False, 'previousTarget': array([16.08589282, 22.76686234]), 'currentState': array([24.4040153 ,  4.98346479,  1.56753796]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 19.88021665451649}
done in step count: 21
reward sum = 0.7089136203699404
running average episode reward sum: 0.6934534655577005
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'dynamicTrap': False, 'previousTarget': array([16., 23.]), 'currentState': array([15.96331721, 22.63028402,  1.29368666]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.3715313403043554}
episode index:2993
target Thresh 75.99997833098452
target distance 54.0
model initialize at round 2993
at step 0:
{'scaleFactor': 20, 'currentTarget': array([40.05290125, 14.54629728]), 'dynamicTrap': True, 'previousTarget': array([40.01370332, 15.25976679]), 'currentState': array([60.      , 16.      ,  1.872509], dtype=float32), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 41
reward sum = 0.5908561733347484
running average episode reward sum: 0.6934191979250275
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 14.]), 'currentState': array([ 6.38900539, 14.77099535,  4.78854005]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.8635734062574222}
episode index:2994
target Thresh 75.99997843905918
target distance 44.0
model initialize at round 2994
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.02680488,  7.10516479]), 'dynamicTrap': False, 'previousTarget': array([37.91786413,  7.18928508]), 'currentState': array([18.10347347,  8.85469779,  5.36347891]), 'targetState': array([62,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.6524357788846066
running average episode reward sum: 0.6934055139787703
{'scaleFactor': 20, 'currentTarget': array([62.,  5.]), 'dynamicTrap': False, 'previousTarget': array([62.,  5.]), 'currentState': array([62.67754402,  4.28771545,  5.35956959]), 'targetState': array([62,  5], dtype=int32), 'currentDistance': 0.9830641802927218}
episode index:2995
target Thresh 75.99997854659482
target distance 15.0
model initialize at round 2995
at step 0:
{'scaleFactor': 20, 'currentTarget': array([87., 15.]), 'dynamicTrap': False, 'previousTarget': array([87., 15.]), 'currentState': array([73.69798813, 18.0505309 ,  6.18373585]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 13.64731689656899}
done in step count: 15
reward sum = 0.795074663301697
running average episode reward sum: 0.6934394489418286
{'scaleFactor': 20, 'currentTarget': array([87., 15.]), 'dynamicTrap': False, 'previousTarget': array([87., 15.]), 'currentState': array([87.72853321, 15.44914534,  3.68157979]), 'targetState': array([87, 15], dtype=int32), 'currentDistance': 0.8558575688863308}
episode index:2996
target Thresh 75.99997865359413
target distance 31.0
model initialize at round 2996
at step 0:
{'scaleFactor': 20, 'currentTarget': array([56.87402064, 15.86486696]), 'dynamicTrap': False, 'previousTarget': array([56.83555733, 16.44057325]), 'currentState': array([36.93417392, 17.41487114,  5.68088555]), 'targetState': array([68, 15], dtype=int32), 'currentDistance': 20.0}
done in step count: 39
reward sum = 0.5504591454203422
running average episode reward sum: 0.693391741132846
{'scaleFactor': 20, 'currentTarget': array([68., 15.]), 'dynamicTrap': False, 'previousTarget': array([68., 15.]), 'currentState': array([67.79196147, 14.62820234,  0.335037  ]), 'targetState': array([68, 15], dtype=int32), 'currentDistance': 0.4260440511692072}
episode index:2997
target Thresh 75.99997876005976
target distance 43.0
model initialize at round 2997
at step 0:
{'scaleFactor': 20, 'currentTarget': array([69.73124867,  5.86508797]), 'dynamicTrap': False, 'previousTarget': array([68.57581251,  6.90273692]), 'currentState': array([50.02585877,  9.28526472,  5.17678137]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7279561705581541
running average episode reward sum: 0.6934032702954295
{'scaleFactor': 20, 'currentTarget': array([92.,  2.]), 'dynamicTrap': False, 'previousTarget': array([92.,  2.]), 'currentState': array([91.37490639,  1.3684183 ,  6.01331965]), 'targetState': array([92,  2], dtype=int32), 'currentDistance': 0.8886154766828716}
episode index:2998
target Thresh 75.99997886599441
target distance 16.0
model initialize at round 2998
at step 0:
{'scaleFactor': 20, 'currentTarget': array([67.50443089,  9.29797666]), 'dynamicTrap': False, 'previousTarget': array([68.,  9.]), 'currentState': array([50.36427241, 19.60404124,  4.38909292]), 'targetState': array([68,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.7971500739134267
running average episode reward sum: 0.6934378640945685
{'scaleFactor': 20, 'currentTarget': array([68.,  9.]), 'dynamicTrap': False, 'previousTarget': array([68.,  9.]), 'currentState': array([67.83443909,  8.50815928,  4.94135634]), 'targetState': array([68,  9], dtype=int32), 'currentDistance': 0.5189582945654242}
episode index:2999
target Thresh 75.99997897140071
target distance 11.0
model initialize at round 2999
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'dynamicTrap': False, 'previousTarget': array([18., 14.]), 'currentState': array([ 7.80059912, 17.32895593,  5.59842759]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 10.728920075080087}
done in step count: 9
reward sum = 0.9039112873836408
running average episode reward sum: 0.6935080219023316
{'scaleFactor': 20, 'currentTarget': array([18., 14.]), 'dynamicTrap': False, 'previousTarget': array([18., 14.]), 'currentState': array([18.72719117, 13.86980435,  2.80023373]), 'targetState': array([18, 14], dtype=int32), 'currentDistance': 0.7387542922137277}
