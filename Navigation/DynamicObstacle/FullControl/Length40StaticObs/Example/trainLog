/home/yangyutu/anaconda3/bin/python /snap/pycharm-community/132/helpers/pydev/pydevconsole.py --mode=client --port=39219
import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel'])
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
Type 'copyright', 'credits' or 'license' for more information
IPython 7.4.0 -- An enhanced Interactive Python. Type '?' for help.
PyDev console: using IPython 7.4.0
Python 3.7.3 (default, Mar 27 2019, 22:11:17)
[GCC 7.3.0] on linux
runfile('/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40StaticObs/DDPGHER_CNN.py', wdir='/home/yangyutu/Dropbox/UniversalNavigationProject/activeParticleModel/Navigation/DynamicObstacle/FullControl/Length40StaticObs')
Backend Qt5Agg is interactive backend. Turning interactive mode on.
episode index:0
target Thresh 15.199999999999996
target distance 5.0
model initialize at round 0
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([34.09161517,  3.71194277,  6.0623908 ]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 4.528871590464791}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.8953382542587164
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([38.82264831,  5.69806607,  1.39608121]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 0.8763072232729727}
episode index:1
target Thresh 15.321478481026144
target distance 3.0
model initialize at round 1
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([27.78576304, 12.4346142 ,  4.29343212]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 4.902213496693392}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.4476691271293582
{'scaleFactor': 20, 'currentTarget': array([26., 17.]), 'dynamicTrap': False, 'previousTarget': array([26., 17.]), 'currentState': array([31.55418256, 13.59708251,  4.73070999]), 'targetState': array([26, 17], dtype=int32), 'currentDistance': 6.513738663397048}
episode index:2
target Thresh 15.442714247885311
target distance 14.0
model initialize at round 2
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([4.4336541 , 9.62614028, 2.32018435]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 16.937909016068}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.29844608475290546
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([7.15574661, 8.56147321, 3.98538203]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 16.087722654517876}
episode index:3
target Thresh 15.563707785520728
target distance 15.0
model initialize at round 3
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([21.34307951, 20.5664832 ,  2.36518806]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 15.534867005540544}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2238345635646791
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([14.2644458 , 18.59474089,  4.49223621]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 9.365221415659002}
episode index:4
target Thresh 15.684459577906708
target distance 4.0
model initialize at round 4
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'dynamicTrap': False, 'previousTarget': array([10.,  2.]), 'currentState': array([9.35510451, 7.36063037, 1.28426519]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 5.399282189998564}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.2703896004005262
{'scaleFactor': 20, 'currentTarget': array([10.,  2.]), 'dynamicTrap': False, 'previousTarget': array([10.,  2.]), 'currentState': array([9.37482621, 1.26855163, 6.08044489]), 'targetState': array([10,  2], dtype=int32), 'currentDistance': 0.9622156690618349}
episode index:5
target Thresh 15.80497010805058
target distance 4.0
model initialize at round 5
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 22.]), 'currentState': array([ 2.01675107, 18.17047158,  3.22592747]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 7.103840910550664}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.37757754158104523
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 22.]), 'currentState': array([ 7.04173033, 21.20419415,  2.09426277]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 1.2456274356279449}
episode index:6
target Thresh 15.925239857994622
target distance 5.0
model initialize at round 6
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([33.20345681, 11.55490425,  6.01689005]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 3.822545018953115}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.32363789278375305
{'scaleFactor': 20, 'currentTarget': array([37., 12.]), 'dynamicTrap': False, 'previousTarget': array([37., 12.]), 'currentState': array([27.2772614 , 13.03867954,  3.56446798]), 'targetState': array([37, 12], dtype=int32), 'currentDistance': 9.77806223935551}
episode index:7
target Thresh 16.045269308817993
target distance 13.0
model initialize at round 7
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'dynamicTrap': False, 'previousTarget': array([19.,  3.]), 'currentState': array([32.90962772, 16.4007236 ,  0.40494925]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 19.31468706616979}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.28318315618578394
{'scaleFactor': 20, 'currentTarget': array([19.,  3.]), 'dynamicTrap': False, 'previousTarget': array([19.,  3.]), 'currentState': array([33.55886877, 13.83938257,  0.71892733]), 'targetState': array([19,  3], dtype=int32), 'currentDistance': 18.15083674074959}
episode index:8
target Thresh 16.16505894063866
target distance 16.0
model initialize at round 8
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.81694488, 19.68246084]), 'dynamicTrap': False, 'previousTarget': array([18., 20.]), 'currentState': array([7.82825015, 2.35543058, 5.35045266]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.25171836105403017
{'scaleFactor': 20, 'currentTarget': array([18., 20.]), 'dynamicTrap': False, 'previousTarget': array([17.38112219, 18.84118832]), 'currentState': array([8.81034637, 2.66028693, 2.0506556 ]), 'targetState': array([18, 20], dtype=int32), 'currentDistance': 19.624356892199778}
episode index:9
target Thresh 16.284609232615313
target distance 15.0
model initialize at round 9
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'dynamicTrap': False, 'previousTarget': array([14.,  5.]), 'currentState': array([ 6.79244945, 21.17226693,  3.38101959]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 17.705677127831283}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.22654652494862715
{'scaleFactor': 20, 'currentTarget': array([14.,  5.]), 'dynamicTrap': False, 'previousTarget': array([14.,  5.]), 'currentState': array([ 2.02192445, 20.58583983,  3.80880823]), 'targetState': array([14,  5], dtype=int32), 'currentDistance': 19.656874043588186}
episode index:10
target Thresh 16.403920662949282
target distance 13.0
model initialize at round 10
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14., 19.]), 'dynamicTrap': False, 'previousTarget': array([14., 19.]), 'currentState': array([27.58333834, 13.29953108,  6.14844108]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 14.731002218886669}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.20595138631693377
{'scaleFactor': 20, 'currentTarget': array([15.5428537 , 20.14117296]), 'dynamicTrap': True, 'previousTarget': array([15.34688066, 20.10294436]), 'currentState': array([23.47820382, 17.93656595,  3.4018219 ]), 'targetState': array([14, 19], dtype=int32), 'currentDistance': 8.23590150815277}
episode index:11
target Thresh 16.522993708886425
target distance 15.0
model initialize at round 11
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 20.]), 'dynamicTrap': False, 'previousTarget': array([17., 20.]), 'currentState': array([20.51846606,  5.7257058 ,  1.45582836]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 14.701533197000542}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.18878877079052261
{'scaleFactor': 20, 'currentTarget': array([18.37746398, 19.15643   ]), 'dynamicTrap': True, 'previousTarget': array([18.37197325, 18.96555101]), 'currentState': array([21.40181113,  6.4949742 ,  1.07355328]), 'targetState': array([17, 20], dtype=int32), 'currentDistance': 13.017647200854409}
episode index:12
target Thresh 16.641828846719115
target distance 10.0
model initialize at round 12
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'dynamicTrap': False, 'previousTarget': array([21., 22.]), 'currentState': array([30.7649317 , 19.59897943,  5.5413183 ]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 10.055783948028386}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.17426655765279012
{'scaleFactor': 20, 'currentTarget': array([21., 22.]), 'dynamicTrap': False, 'previousTarget': array([21., 22.]), 'currentState': array([37.0259225 , 16.78735185,  6.01488991]), 'targetState': array([21, 22], dtype=int32), 'currentDistance': 16.852355699349626}
episode index:13
target Thresh 16.760426551788036
target distance 11.0
model initialize at round 13
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([16.28127217, 22.32330362,  0.82966119]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 13.020007696717371}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16181894639187652
{'scaleFactor': 20, 'currentTarget': array([ 4., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 18.]), 'currentState': array([11.47475102, 16.94734841,  0.56313797]), 'targetState': array([ 4, 18], dtype=int32), 'currentDistance': 7.548508343602918}
episode index:14
target Thresh 16.87878729848419
target distance 10.0
model initialize at round 14
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([3.68273211, 5.02829719, 1.02681464]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 10.237411163142252}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1510310166324181
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([7.15781218, 5.57638296, 0.502656  ]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 9.494476670247211}
episode index:15
target Thresh 16.9969115602507
target distance 8.0
model initialize at round 15
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 13.]), 'dynamicTrap': False, 'previousTarget': array([28., 13.]), 'currentState': array([36.29978764,  8.71411829,  2.30777514]), 'targetState': array([28, 13], dtype=int32), 'currentDistance': 9.341052232494382}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.14159157809289197
{'scaleFactor': 20, 'currentTarget': array([30.07294163, 12.10701457]), 'dynamicTrap': True, 'previousTarget': array([30.15978673, 12.21354108]), 'currentState': array([25.15885948, 11.09972063,  1.81961652]), 'targetState': array([28, 13], dtype=int32), 'currentDistance': 5.016258021323667}
episode index:16
target Thresh 17.11479980958478
target distance 17.0
model initialize at round 16
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.,  7.]), 'dynamicTrap': False, 'previousTarget': array([31.,  7.]), 'currentState': array([14.84772315,  4.28149676,  1.91714974]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 16.37944771251335}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.13326266173448656
{'scaleFactor': 20, 'currentTarget': array([31.,  7.]), 'dynamicTrap': False, 'previousTarget': array([31.,  7.]), 'currentState': array([19.87162349,  4.55830361,  4.07827499]), 'targetState': array([31,  7], dtype=int32), 'currentDistance': 11.393096372024397}
episode index:17
target Thresh 17.23245251803959
target distance 5.0
model initialize at round 17
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([31.729153  , 19.81566709,  5.4975636 ]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 4.691224941324328}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.12585918052701509
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([30.99715478, 22.53681908,  3.34692376]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 6.8351028433619305}
episode index:18
target Thresh 17.34987015622611
target distance 10.0
model initialize at round 18
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 14.]), 'dynamicTrap': False, 'previousTarget': array([33., 14.]), 'currentState': array([21.95510992, 21.64650364,  4.47781515]), 'targetState': array([33, 14], dtype=int32), 'currentDistance': 13.433488560422925}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.1192350131308564
{'scaleFactor': 20, 'currentTarget': array([33.45957002, 14.16720749]), 'dynamicTrap': True, 'previousTarget': array([33.4167196 , 14.11370528]), 'currentState': array([21.5238466 , 17.55248723,  2.37181344]), 'targetState': array([33, 14], dtype=int32), 'currentDistance': 12.406514927633056}
episode index:19
target Thresh 17.46705319381507
target distance 10.0
model initialize at round 19
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'dynamicTrap': False, 'previousTarget': array([12., 16.]), 'currentState': array([ 1.70512121, 14.82743004,  0.60049561]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 10.361440522358311}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.11327326247431357
{'scaleFactor': 20, 'currentTarget': array([12., 16.]), 'dynamicTrap': False, 'previousTarget': array([12., 16.]), 'currentState': array([11.14176326,  8.92325486,  2.35298181]), 'targetState': array([12, 16], dtype=int32), 'currentDistance': 7.128596779219866}
episode index:20
target Thresh 17.58400209953875
target distance 10.0
model initialize at round 20
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 20.]), 'dynamicTrap': False, 'previousTarget': array([15., 20.]), 'currentState': array([ 8.37712908, 11.74634786,  2.5891546 ]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 10.582305650766001}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10787929759458435
{'scaleFactor': 20, 'currentTarget': array([13.45402519, 19.98047341]), 'dynamicTrap': True, 'previousTarget': array([13.45469342, 20.12962488]), 'currentState': array([14.76693493, 17.08240147,  5.02552736]), 'targetState': array([15, 20], dtype=int32), 'currentDistance': 3.181595973437312}
episode index:21
target Thresh 17.70071734119294
target distance 11.0
model initialize at round 21
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 13.]), 'currentState': array([4.61974491, 2.93862934, 1.15712434]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 10.339090568219214}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.10297569315846689
{'scaleFactor': 20, 'currentTarget': array([ 7., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 13.]), 'currentState': array([ 7.8904543 , 10.65085273,  0.75829512]), 'targetState': array([ 7, 13], dtype=int32), 'currentDistance': 2.512250335099435}
episode index:22
target Thresh 17.817199385638755
target distance 8.0
model initialize at round 22
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([20.02983139, 22.0901737 ,  5.74269569]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 10.712084539043456}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.1230161060842895
{'scaleFactor': 20, 'currentTarget': array([12., 15.]), 'dynamicTrap': False, 'previousTarget': array([12., 15.]), 'currentState': array([11.36342729, 15.12282412,  5.6324313 ]), 'targetState': array([12, 15], dtype=int32), 'currentDistance': 0.6483136447674366}
episode index:23
target Thresh 17.933448698804547
target distance 3.0
model initialize at round 23
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'dynamicTrap': False, 'previousTarget': array([27.,  4.]), 'currentState': array([26.85594424,  6.0704822 ,  3.32501078]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 2.0754875602067497}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.1571187745558191
{'scaleFactor': 20, 'currentTarget': array([27.,  4.]), 'dynamicTrap': False, 'previousTarget': array([27.,  4.]), 'currentState': array([27.48644361,  4.95965044,  1.86169194]), 'targetState': array([27,  4], dtype=int32), 'currentDistance': 1.0758979317278168}
episode index:24
target Thresh 18.049465745687712
target distance 13.0
model initialize at round 24
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'dynamicTrap': False, 'previousTarget': array([24.,  6.]), 'currentState': array([36.3137222 , 11.51053178,  3.93509167]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 13.490504622696946}
done in step count: 71
reward sum = 0.4898902730042049
running average episode reward sum: 0.17042963449375453
{'scaleFactor': 20, 'currentTarget': array([25.68512389,  8.05184183]), 'dynamicTrap': True, 'previousTarget': array([25.76425041,  8.00349692]), 'currentState': array([25.57335033,  9.0139929 ,  2.77031039]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9686217099950811}
episode index:25
target Thresh 18.165250990356583
target distance 17.0
model initialize at round 25
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'dynamicTrap': False, 'previousTarget': array([13., 13.]), 'currentState': array([29.9670799 , 20.99958187,  0.20277923]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 18.75833442313579}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16387464855168704
{'scaleFactor': 20, 'currentTarget': array([13., 13.]), 'dynamicTrap': False, 'previousTarget': array([13., 13.]), 'currentState': array([10.83038504, 14.91408547,  4.63242948]), 'targetState': array([13, 13], dtype=int32), 'currentDistance': 2.8932597965726727}
episode index:26
target Thresh 18.280804895952315
target distance 18.0
model initialize at round 26
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'dynamicTrap': False, 'previousTarget': array([13., 20.]), 'currentState': array([31.36560283, 12.17082771,  1.44710558]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 19.96475159072935}
done in step count: 88
reward sum = 0.41294967113388814
running average episode reward sum: 0.17309964938806488
{'scaleFactor': 20, 'currentTarget': array([13., 20.]), 'dynamicTrap': False, 'previousTarget': array([13., 20.]), 'currentState': array([13.46176916, 20.49162057,  5.14730252]), 'targetState': array([13, 20], dtype=int32), 'currentDistance': 0.6744787212318218}
episode index:27
target Thresh 18.39612792469066
target distance 13.0
model initialize at round 27
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 21.]), 'dynamicTrap': False, 'previousTarget': array([11., 21.]), 'currentState': array([6.38953871, 7.57815375, 0.18480509]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 14.191628165288158}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.16691751905277682
{'scaleFactor': 20, 'currentTarget': array([10.03712733, 21.22532412]), 'dynamicTrap': True, 'previousTarget': array([10.09895498, 21.22856058]), 'currentState': array([14.47703766, 17.29614821,  6.28102001]), 'targetState': array([11, 21], dtype=int32), 'currentDistance': 5.928847026843751}
episode index:28
target Thresh 18.5112205378639
target distance 3.0
model initialize at round 28
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'dynamicTrap': False, 'previousTarget': array([6., 3.]), 'currentState': array([7.17001753, 6.46360076, 0.59866269]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 3.6558817338371368}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.19111849469816308
{'scaleFactor': 20, 'currentTarget': array([6., 3.]), 'dynamicTrap': False, 'previousTarget': array([6., 3.]), 'currentState': array([5.11877445, 3.15428641, 1.23973308]), 'targetState': array([6, 3], dtype=int32), 'currentDistance': 0.8946299575791247}
episode index:29
target Thresh 18.626083195842643
target distance 3.0
model initialize at round 29
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 12.]), 'dynamicTrap': False, 'previousTarget': array([35., 12.]), 'currentState': array([34.36809712, 14.07974956,  4.84540159]), 'targetState': array([35, 12], dtype=int32), 'currentDistance': 2.1736281809859754}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.2174178782082243
{'scaleFactor': 20, 'currentTarget': array([35., 12.]), 'dynamicTrap': False, 'previousTarget': array([35., 12.]), 'currentState': array([34.0301163 , 11.73533035,  5.54507452]), 'targetState': array([35, 12], dtype=int32), 'currentDistance': 1.0053479109850436}
episode index:30
target Thresh 18.740716358077673
target distance 4.0
model initialize at round 30
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([12.40145208, 15.42007583,  5.48471761]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 3.696077193300847}
done in step count: 53
reward sum = 0.5870367819374844
running average episode reward sum: 0.22934106865110368
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([10.06238816, 11.17682624,  1.92343744]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 1.2476902683051017}
episode index:31
target Thresh 18.85512048310179
target distance 11.0
model initialize at round 31
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'dynamicTrap': False, 'previousTarget': array([25., 23.]), 'currentState': array([37.05684493, 21.49518015,  0.56437796]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 12.150390629866296}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.2479919297506188
{'scaleFactor': 20, 'currentTarget': array([25., 23.]), 'dynamicTrap': False, 'previousTarget': array([25., 23.]), 'currentState': array([25.94599791, 23.70318854,  3.64104062]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 1.1787222582017396}
episode index:32
target Thresh 18.969296028531648
target distance 10.0
model initialize at round 32
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([19.52342539,  2.60630637,  1.72434425]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 12.313480424598053}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.24047702278847882
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([7.69556354, 8.62410117, 1.52903573]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 1.3575177381934262}
episode index:33
target Thresh 19.083243451069574
target distance 14.0
model initialize at round 33
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'dynamicTrap': False, 'previousTarget': array([26., 19.]), 'currentState': array([33.41020678,  6.47043354,  1.98704648]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 14.556826584400905}
done in step count: 51
reward sum = 0.598956006466161
running average episode reward sum: 0.25102052230841065
{'scaleFactor': 20, 'currentTarget': array([26., 19.]), 'dynamicTrap': False, 'previousTarget': array([26., 19.]), 'currentState': array([26.21782066, 19.68672418,  5.74757291]), 'targetState': array([26, 19], dtype=int32), 'currentDistance': 0.7204414869658166}
episode index:34
target Thresh 19.19696320650541
target distance 8.0
model initialize at round 34
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([ 9.01901618, 10.54702615,  3.64291024]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 9.911719301332269}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.2691737894343455
{'scaleFactor': 20, 'currentTarget': array([4., 2.]), 'dynamicTrap': False, 'previousTarget': array([4., 2.]), 'currentState': array([3.40221157, 2.34945308, 4.06246651]), 'targetState': array([4, 2], dtype=int32), 'currentDistance': 0.6924366175208638}
episode index:35
target Thresh 19.310455749718336
target distance 10.0
model initialize at round 35
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17., 21.]), 'dynamicTrap': False, 'previousTarget': array([17., 21.]), 'currentState': array([12.76286923, 10.40186299,  5.31520319]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 11.413754214178697}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.2616967397278359
{'scaleFactor': 20, 'currentTarget': array([12.43302392, 19.29910413]), 'dynamicTrap': True, 'previousTarget': array([12.61793164, 19.22335196]), 'currentState': array([13.86370284, 12.18591525,  1.15221633]), 'targetState': array([17, 21], dtype=int32), 'currentDistance': 7.255639062694412}
episode index:36
target Thresh 19.42372153467869
target distance 17.0
model initialize at round 36
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 23.]), 'dynamicTrap': False, 'previousTarget': array([36., 23.]), 'currentState': array([37.31176313,  6.13593142,  1.42115801]), 'targetState': array([36, 23], dtype=int32), 'currentDistance': 16.915009057544353}
done in step count: 50
reward sum = 0.6050060671375364
running average episode reward sum: 0.27097537019836837
{'scaleFactor': 20, 'currentTarget': array([36., 23.]), 'dynamicTrap': False, 'previousTarget': array([36., 23.]), 'currentState': array([35.53714735, 23.12339736,  2.23424986]), 'targetState': array([36, 23], dtype=int32), 'currentDistance': 0.4790192931845174}
episode index:37
target Thresh 19.536761014449734
target distance 15.0
model initialize at round 37
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'dynamicTrap': False, 'previousTarget': array([25.,  2.]), 'currentState': array([9.45766675, 2.54477051, 2.75676239]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 15.55187762429047}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.2809260875186324
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'dynamicTrap': False, 'previousTarget': array([25.,  2.]), 'currentState': array([24.48126358,  2.30247652,  0.33551039]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.6004827362474889}
episode index:38
target Thresh 19.64957464118956
target distance 11.0
model initialize at round 38
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([35.47028326, 17.07621457,  4.31225681]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 11.361425791265344}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.29469482726423746
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([38.78814219,  5.0508819 ,  1.60425925]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 1.2336909202434716}
episode index:39
target Thresh 19.76216286615281
target distance 16.0
model initialize at round 39
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([ 5.33065784, 19.33220735,  1.2137318 ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 16.417298808921093}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.30185661693558424
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([7.55963201, 2.52847096, 5.5202857 ]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.7317975281666571}
episode index:40
target Thresh 19.87452613969254
target distance 15.0
model initialize at round 40
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'dynamicTrap': False, 'previousTarget': array([16.,  2.]), 'currentState': array([ 4.71229735, 17.02265703,  3.00570345]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 18.790754517308105}
done in step count: 54
reward sum = 0.5811664141181095
running average episode reward sum: 0.30866905101320685
{'scaleFactor': 20, 'currentTarget': array([16.,  2.]), 'dynamicTrap': False, 'previousTarget': array([16.,  2.]), 'currentState': array([15.11108769,  2.04684731,  1.75923423]), 'targetState': array([16,  2], dtype=int32), 'currentDistance': 0.8901459204686105}
episode index:41
target Thresh 19.986664911261997
target distance 6.0
model initialize at round 41
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 16.]), 'dynamicTrap': False, 'previousTarget': array([36., 16.]), 'currentState': array([29.60446278, 10.75688836,  6.07161593]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 8.270013062093282}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.32242418960137165
{'scaleFactor': 20, 'currentTarget': array([36., 16.]), 'dynamicTrap': False, 'previousTarget': array([36., 16.]), 'currentState': array([35.01731545, 16.2173658 ,  3.69229422]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 1.0064376823439545}
episode index:42
target Thresh 20.09857962941642
target distance 11.0
model initialize at round 42
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 20.]), 'currentState': array([4.20703521, 9.82800097, 1.05762662]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 10.328808596015257}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.33492730971857904
{'scaleFactor': 20, 'currentTarget': array([ 6., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 20.]), 'currentState': array([ 5.97066815, 20.28329032,  2.1017269 ]), 'targetState': array([ 6, 20], dtype=int32), 'currentDistance': 0.2848047817720228}
episode index:43
target Thresh 20.21027074181482
target distance 19.0
model initialize at round 43
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.,  3.]), 'currentState': array([ 8.99501118, 21.96843831,  2.42402771]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 19.38663424696705}
done in step count: 30
reward sum = 0.7397003733882802
running average episode reward sum: 0.344126697529254
{'scaleFactor': 20, 'currentTarget': array([13.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.,  3.]), 'currentState': array([12.30301155,  3.67771515,  5.43548029]), 'targetState': array([13,  3], dtype=int32), 'currentDistance': 0.9721577724543167}
episode index:44
target Thresh 20.3217386952218
target distance 12.0
model initialize at round 44
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  6.]), 'dynamicTrap': False, 'previousTarget': array([37.,  6.]), 'currentState': array([26.20611363,  1.15817179,  5.63197041]), 'targetState': array([37,  6], dtype=int32), 'currentDistance': 11.830100731023093}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.355591845465077
{'scaleFactor': 20, 'currentTarget': array([37.,  6.]), 'dynamicTrap': False, 'previousTarget': array([37.,  6.]), 'currentState': array([36.30124423,  5.51415803,  5.06458657]), 'targetState': array([37,  6], dtype=int32), 'currentDistance': 0.8510593705438745}
episode index:45
target Thresh 20.43298393550932
target distance 4.0
model initialize at round 45
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'dynamicTrap': False, 'previousTarget': array([25.,  4.]), 'currentState': array([29.09394412,  1.73623025,  5.09437859]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 4.678144070686624}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.3631539291506094
{'scaleFactor': 20, 'currentTarget': array([25.,  4.]), 'dynamicTrap': False, 'previousTarget': array([25.,  4.]), 'currentState': array([25.90604048,  3.81696631,  3.93199192]), 'targetState': array([25,  4], dtype=int32), 'currentDistance': 0.9243433768080117}
episode index:46
target Thresh 20.5440069076585
target distance 8.0
model initialize at round 46
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([10.38931137, 14.89332336,  0.78848403]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 10.155188495147785}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.37248321979717186
{'scaleFactor': 20, 'currentTarget': array([4., 7.]), 'dynamicTrap': False, 'previousTarget': array([4., 7.]), 'currentState': array([3.99804836, 7.31956339, 3.23978214]), 'targetState': array([4, 7], dtype=int32), 'currentDistance': 0.31956934581870694}
episode index:47
target Thresh 20.65480805576136
target distance 6.0
model initialize at round 47
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'dynamicTrap': False, 'previousTarget': array([20.,  2.]), 'currentState': array([24.37729112,  5.55367727,  2.40000343]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 5.6382000391041664}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.3830048406972093
{'scaleFactor': 20, 'currentTarget': array([20.,  2.]), 'dynamicTrap': False, 'previousTarget': array([20.,  2.]), 'currentState': array([20.79578917,  1.89804827,  3.91765659]), 'targetState': array([20,  2], dtype=int32), 'currentDistance': 0.8022933142866102}
episode index:48
target Thresh 20.765387823022657
target distance 17.0
model initialize at round 48
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'dynamicTrap': False, 'previousTarget': array([24.,  3.]), 'currentState': array([ 8.079538  , 10.86774687,  0.28204077]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 17.75845013133474}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.3915482233266345
{'scaleFactor': 20, 'currentTarget': array([24.,  3.]), 'dynamicTrap': False, 'previousTarget': array([24.,  3.]), 'currentState': array([24.47507435,  2.04713537,  2.20870766]), 'targetState': array([24,  3], dtype=int32), 'currentDistance': 1.0647284366993381}
episode index:49
target Thresh 20.875746651761602
target distance 1.0
model initialize at round 49
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 21.]), 'dynamicTrap': False, 'previousTarget': array([33., 21.]), 'currentState': array([33.87838208, 20.33959542,  5.05063903]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 1.0989491789784942}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.40371725886010185
{'scaleFactor': 20, 'currentTarget': array([33., 21.]), 'dynamicTrap': False, 'previousTarget': array([33., 21.]), 'currentState': array([33.87838208, 20.33959542,  5.05063903]), 'targetState': array([33, 21], dtype=int32), 'currentDistance': 1.0989491789784942}
episode index:50
target Thresh 20.985884983413655
target distance 7.0
model initialize at round 50
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([12.17632351,  9.21197694,  3.17770898]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 8.46452497554047}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.41353421603948815
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.36520123, 14.22670644,  3.14913554]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.8551928848905674}
episode index:51
target Thresh 21.095803258532285
target distance 13.0
model initialize at round 51
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([ 8.54309912, 21.71407873,  4.38283229]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 13.518201608010921}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.4221212187049074
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([22.52467995, 23.15762189,  1.14434057]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.5478446030698515}
episode index:52
target Thresh 21.205501916790745
target distance 13.0
model initialize at round 52
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'dynamicTrap': False, 'previousTarget': array([7., 8.]), 'currentState': array([20.01018871,  3.9960766 ,  0.76069498]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 13.612362502602767}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.42479638798316177
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'dynamicTrap': False, 'previousTarget': array([7., 8.]), 'currentState': array([7.66217403, 8.08336723, 4.74565895]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.6674013350757995}
episode index:53
target Thresh 21.314981396983814
target distance 4.0
model initialize at round 53
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([32.39674536,  6.66661756,  5.81140223]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 2.624514925384719}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.4352631215390291
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([34.0760534 ,  6.98709728,  0.86226937]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 0.9240366832852911}
episode index:54
target Thresh 21.42424213702956
target distance 17.0
model initialize at round 54
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.,  5.]), 'dynamicTrap': False, 'previousTarget': array([36.,  5.]), 'currentState': array([19.41976388,  1.15251113,  3.95446873]), 'targetState': array([36,  5], dtype=int32), 'currentDistance': 17.020793173917195}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.4416343037075416
{'scaleFactor': 20, 'currentTarget': array([36.,  5.]), 'dynamicTrap': False, 'previousTarget': array([36.,  5.]), 'currentState': array([35.62390407,  4.14184098,  0.63413178]), 'targetState': array([36,  5], dtype=int32), 'currentDistance': 0.9369552055878658}
episode index:55
target Thresh 21.533284573971084
target distance 9.0
model initialize at round 55
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([38.24356253,  1.80625115,  0.33802193]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 11.712961659645512}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.44926129494078154
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([29.31871663,  9.07832928,  3.06890307]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 0.3282008005718782}
episode index:56
target Thresh 21.64210914397828
target distance 9.0
model initialize at round 56
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 11.]), 'dynamicTrap': False, 'previousTarget': array([37., 11.]), 'currentState': array([28.85784733, 16.46412151,  6.24469239]), 'targetState': array([37, 11], dtype=int32), 'currentDistance': 9.805675601199408}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.4570872065077629
{'scaleFactor': 20, 'currentTarget': array([37., 11.]), 'dynamicTrap': False, 'previousTarget': array([37., 11.]), 'currentState': array([37.60479393, 11.95163007,  0.37813288]), 'targetState': array([37, 11], dtype=int32), 'currentDistance': 1.1275528774100976}
episode index:57
target Thresh 21.75071628234957
target distance 20.0
model initialize at round 57
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.26006445, 18.0011504 ]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([31.25986878, 18.08962014,  1.3420946 ]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.4589288958861185
{'scaleFactor': 20, 'currentTarget': array([11., 18.]), 'dynamicTrap': False, 'previousTarget': array([11., 18.]), 'currentState': array([11.27670376, 17.22578165,  0.53110706]), 'targetState': array([11, 18], dtype=int32), 'currentDistance': 0.8221794340464198}
episode index:58
target Thresh 21.85910642351366
target distance 19.0
model initialize at round 58
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.40412678,  6.50568355]), 'dynamicTrap': False, 'previousTarget': array([14.67985983,  6.09022194]), 'currentState': array([ 3.66016157, 23.37480361,  2.7658931 ]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.46487464117993443
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'dynamicTrap': False, 'previousTarget': array([16.,  4.]), 'currentState': array([15.491957  ,  3.06370731,  5.79431415]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 1.0652472425341184}
episode index:59
target Thresh 21.96728000103126
target distance 5.0
model initialize at round 59
at step 0:
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([6.40850359, 9.09469909, 0.63312518]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 5.635321496805203}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.47035363522099644
{'scaleFactor': 20, 'currentTarget': array([4., 4.]), 'dynamicTrap': False, 'previousTarget': array([4., 4.]), 'currentState': array([4.49975597, 4.31606397, 2.21906818]), 'targetState': array([4, 4], dtype=int32), 'currentDistance': 0.5913141866309354}
episode index:60
target Thresh 22.075237447596823
target distance 4.0
model initialize at round 60
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([27.71356232, 19.94054725,  4.06846642]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 4.145218275835986}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.4782329207075375
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([29.04682664, 16.67114113,  4.15243253]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 0.67277273093376}
episode index:61
target Thresh 22.182979195040264
target distance 9.0
model initialize at round 61
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([31.00823549, 11.97792806,  2.50421607]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 10.79372198589195}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.4833197168839265
{'scaleFactor': 20, 'currentTarget': array([37.,  3.]), 'dynamicTrap': False, 'previousTarget': array([37.,  3.]), 'currentState': array([37.11309029,  2.3874023 ,  5.22787617]), 'targetState': array([37,  3], dtype=int32), 'currentDistance': 0.6229489164238342}
episode index:62
target Thresh 22.290505674328745
target distance 9.0
model initialize at round 62
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([8.64949974, 4.22295774, 3.15413821]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 9.0609179578652}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.49014824911566796
{'scaleFactor': 20, 'currentTarget': array([ 4., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 12.]), 'currentState': array([ 3.67891626, 11.28317565,  1.91624996]), 'targetState': array([ 4, 12], dtype=int32), 'currentDistance': 0.78545013491016}
episode index:63
target Thresh 22.397817315568297
target distance 6.0
model initialize at round 63
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'dynamicTrap': False, 'previousTarget': array([21.,  5.]), 'currentState': array([26.45604328,  3.22113104,  1.97114962]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 5.738709177843796}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.49566067011985954
{'scaleFactor': 20, 'currentTarget': array([21.,  5.]), 'dynamicTrap': False, 'previousTarget': array([21.,  5.]), 'currentState': array([20.40027929,  4.82748079,  3.49366496]), 'targetState': array([21,  5], dtype=int32), 'currentDistance': 0.6240415160195636}
episode index:64
target Thresh 22.50491454800565
target distance 15.0
model initialize at round 64
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.,  8.]), 'dynamicTrap': False, 'previousTarget': array([31.,  8.]), 'currentState': array([34.52142613, 23.05908735,  4.26443005]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 15.465333936347221}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5015354447795382
{'scaleFactor': 20, 'currentTarget': array([31.,  8.]), 'dynamicTrap': False, 'previousTarget': array([31.,  8.]), 'currentState': array([30.95196857,  8.69237828,  5.16282725]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 0.6940422889963517}
episode index:65
target Thresh 22.611797800029873
target distance 19.0
model initialize at round 65
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.36399641, 18.85410921]), 'dynamicTrap': False, 'previousTarget': array([22.69765531, 18.39288577]), 'currentState': array([7.85363926, 6.2275966 , 0.79763311]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 80
reward sum = 0.4475232137638106
running average episode reward sum: 0.5007170776429362
{'scaleFactor': 20, 'currentTarget': array([26., 21.]), 'dynamicTrap': False, 'previousTarget': array([26., 21.]), 'currentState': array([25.88396746, 20.80445926,  1.887582  ]), 'targetState': array([26, 21], dtype=int32), 'currentDistance': 0.22737575062600487}
episode index:66
target Thresh 22.718467499174107
target distance 10.0
model initialize at round 66
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([18.00958744,  3.01130009,  1.87720936]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 13.453199123824573}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.5029317873552566
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.98567911, 11.41992747,  2.91993728]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.5802492766455996}
episode index:67
target Thresh 22.82492407211729
target distance 8.0
model initialize at round 67
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([ 7.340308  , 21.28152223,  5.05259824]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 6.422923558498498}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.5091055065769133
{'scaleFactor': 20, 'currentTarget': array([ 6., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 15.]), 'currentState': array([ 6.19383293, 14.10533529,  2.23613843]), 'targetState': array([ 6, 15], dtype=int32), 'currentDistance': 0.9154212898484994}
episode index:68
target Thresh 22.93116794468586
target distance 7.0
model initialize at round 68
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'dynamicTrap': False, 'previousTarget': array([19.,  5.]), 'currentState': array([13.34797725,  7.53950327,  5.26184094]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 6.196324561143185}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5141917797372666
{'scaleFactor': 20, 'currentTarget': array([19.,  5.]), 'dynamicTrap': False, 'previousTarget': array([19.,  5.]), 'currentState': array([18.17888527,  5.79991921,  4.23912786]), 'targetState': array([19,  5], dtype=int32), 'currentDistance': 1.1463420679581107}
episode index:69
target Thresh 23.037199541855454
target distance 11.0
model initialize at round 69
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'dynamicTrap': False, 'previousTarget': array([20., 20.]), 'currentState': array([10.35344328, 11.3563144 ,  0.27644432]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 12.952581104934689}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.5188882285036475
{'scaleFactor': 20, 'currentTarget': array([20., 20.]), 'dynamicTrap': False, 'previousTarget': array([20., 20.]), 'currentState': array([20.54973802, 20.46088738,  2.77231225]), 'targetState': array([20, 20], dtype=int32), 'currentDistance': 0.7173765229997432}
episode index:70
target Thresh 23.143019287752598
target distance 10.0
model initialize at round 70
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([ 5.78438343, 20.27876748,  3.66774273]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 8.570119473390529}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5241903415424513
{'scaleFactor': 20, 'currentTarget': array([ 8., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 12.]), 'currentState': array([ 7.53804667, 12.64756762,  5.23546856]), 'targetState': array([ 8, 12], dtype=int32), 'currentDistance': 0.7954525176505436}
episode index:71
target Thresh 23.24862760565642
target distance 10.0
model initialize at round 71
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([20.69185453, 22.07398857,  3.56075382]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 13.732994264059586}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.525569995061184
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([31.80111809, 12.08531955,  3.36410635]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 1.21590728112111}
episode index:72
target Thresh 23.354024918000334
target distance 12.0
model initialize at round 72
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  3.]), 'dynamicTrap': False, 'previousTarget': array([34.,  3.]), 'currentState': array([31.90285892, 13.34709882,  4.60963285]), 'targetState': array([34,  3], dtype=int32), 'currentDistance': 10.557483353496307}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.5300342111712345
{'scaleFactor': 20, 'currentTarget': array([34.,  3.]), 'dynamicTrap': False, 'previousTarget': array([34.,  3.]), 'currentState': array([34.54927239,  3.70808332,  6.20492684]), 'targetState': array([34,  3], dtype=int32), 'currentDistance': 0.8961485057321994}
episode index:73
target Thresh 23.459211646373713
target distance 21.0
model initialize at round 73
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  5.]), 'dynamicTrap': False, 'previousTarget': array([31.90990945,  5.10381815]), 'currentState': array([13.44836072,  6.62392861,  5.35774675]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 19.618963848845656}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5340360275585906
{'scaleFactor': 20, 'currentTarget': array([33.,  5.]), 'dynamicTrap': False, 'previousTarget': array([33.,  5.]), 'currentState': array([32.58708703,  5.44019492,  0.481406  ]), 'targetState': array([33,  5], dtype=int32), 'currentDistance': 0.6035467603034353}
episode index:74
target Thresh 23.564188211523636
target distance 10.0
model initialize at round 74
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([25.86761488, 20.5579069 ,  6.26401043]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 11.243069016661435}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5390957771575913
{'scaleFactor': 20, 'currentTarget': array([35., 14.]), 'dynamicTrap': False, 'previousTarget': array([35., 14.]), 'currentState': array([35.04390388, 13.4100391 ,  6.04749636]), 'targetState': array([35, 14], dtype=int32), 'currentDistance': 0.5915922727756509}
episode index:75
target Thresh 23.66895503335648
target distance 8.0
model initialize at round 75
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([22.23702159, 20.63550449,  3.33165252]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 11.622853330362826}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5433189689665873
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([31.44237829, 13.41024485,  2.52695321]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 0.6033236183642665}
episode index:76
target Thresh 23.77351253093969
target distance 12.0
model initialize at round 76
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([12.88959153, 20.79107897,  3.38389313]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 10.956490942670456}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5480081002138888
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.3913674 , 22.22364219,  3.37608734]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.4507596646005576}
episode index:77
target Thresh 23.877861122503376
target distance 12.0
model initialize at round 77
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([34.95618382, 20.80467322,  2.98455238]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 10.980328706202059}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.5523462639510971
{'scaleFactor': 20, 'currentTarget': array([33., 10.]), 'dynamicTrap': False, 'previousTarget': array([33., 10.]), 'currentState': array([32.30251559,  9.45439449,  4.83426343]), 'targetState': array([33, 10], dtype=int32), 'currentDistance': 0.8855336674899024}
episode index:78
target Thresh 23.98200122544207
target distance 19.0
model initialize at round 78
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([10.6091017 ,  3.689404  ,  2.02780908]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 17.366385117645816}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.5559180044257679
{'scaleFactor': 20, 'currentTarget': array([12., 21.]), 'dynamicTrap': False, 'previousTarget': array([12., 21.]), 'currentState': array([11.32167994, 20.07266389,  0.75580111]), 'targetState': array([12, 21], dtype=int32), 'currentDistance': 1.1489431496659295}
episode index:79
target Thresh 24.08593325631631
target distance 6.0
model initialize at round 79
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'dynamicTrap': False, 'previousTarget': array([12., 23.]), 'currentState': array([17.63121418, 19.80565444,  1.49500388]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 6.4741344358753885}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.559828352030058
{'scaleFactor': 20, 'currentTarget': array([12., 23.]), 'dynamicTrap': False, 'previousTarget': array([12., 23.]), 'currentState': array([11.79677356, 22.58672006,  0.55026584]), 'targetState': array([12, 23], dtype=int32), 'currentDistance': 0.46054456110430797}
episode index:80
target Thresh 24.18965763085434
target distance 13.0
model initialize at round 80
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'dynamicTrap': False, 'previousTarget': array([26., 10.]), 'currentState': array([29.51693514, 21.78508235,  3.32394361]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 12.298658410291191}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.563859914001491
{'scaleFactor': 20, 'currentTarget': array([26., 10.]), 'dynamicTrap': False, 'previousTarget': array([26., 10.]), 'currentState': array([25.18016006,  9.44816467,  4.87759844]), 'targetState': array([26, 10], dtype=int32), 'currentDistance': 0.9882609722905156}
episode index:81
target Thresh 24.29317476395383
target distance 21.0
model initialize at round 81
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.05452262, 18.56468611]), 'dynamicTrap': False, 'previousTarget': array([30.3829006 , 17.87838597]), 'currentState': array([13.88759108, 10.2003213 ,  5.82635892]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.5569835735868387
{'scaleFactor': 20, 'currentTarget': array([28.45626692, 21.73901193]), 'dynamicTrap': True, 'previousTarget': array([28.28697185, 21.63315335]), 'currentState': array([13.59218421, 15.55591713,  1.60373985]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 16.098807908084044}
episode index:82
target Thresh 24.39648506968343
target distance 10.0
model initialize at round 82
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 12.]), 'dynamicTrap': False, 'previousTarget': array([36., 12.]), 'currentState': array([37.55284775, 21.7086462 ,  5.31703401]), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 9.832046950171208}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5610601360045722
{'scaleFactor': 20, 'currentTarget': array([36., 12.]), 'dynamicTrap': False, 'previousTarget': array([36., 12.]), 'currentState': array([36.00181978, 11.75233192,  1.5759764 ]), 'targetState': array([36, 12], dtype=int32), 'currentDistance': 0.2476747609477267}
episode index:83
target Thresh 24.499588961284495
target distance 12.0
model initialize at round 83
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([ 3.75060818, 13.29297691,  3.63016438]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 10.793698203554202}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5650396374123596
{'scaleFactor': 20, 'currentTarget': array([7., 3.]), 'dynamicTrap': False, 'previousTarget': array([7., 3.]), 'currentState': array([7.88818552, 3.31026646, 4.74959755]), 'targetState': array([7, 3], dtype=int32), 'currentDistance': 0.9408181533316861}
episode index:84
target Thresh 24.602486851172742
target distance 3.0
model initialize at round 84
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([27.82606535, 15.35006324,  5.50458476]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 1.3418421474355078}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5690319013840824
{'scaleFactor': 20, 'currentTarget': array([29., 16.]), 'dynamicTrap': False, 'previousTarget': array([29., 16.]), 'currentState': array([29.77117037, 15.17291306,  3.55375065]), 'targetState': array([29, 16], dtype=int32), 'currentDistance': 1.1308300247169367}
episode index:85
target Thresh 24.705179150939866
target distance 5.0
model initialize at round 85
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'dynamicTrap': False, 'previousTarget': array([17.,  4.]), 'currentState': array([20.31259074,  4.0266647 ,  3.26431358]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 3.3126980591151427}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.5730375449433797
{'scaleFactor': 20, 'currentTarget': array([17.,  4.]), 'dynamicTrap': False, 'previousTarget': array([17.,  4.]), 'currentState': array([17.07937195,  3.8310855 ,  2.52636155]), 'targetState': array([17,  4], dtype=int32), 'currentDistance': 0.18663337200032462}
episode index:86
target Thresh 24.8076662713552
target distance 12.0
model initialize at round 86
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([25.86079516,  6.00447663,  3.76545858]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 12.505485755884756}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.5759470975743245
{'scaleFactor': 20, 'currentTarget': array([38.,  3.]), 'dynamicTrap': False, 'previousTarget': array([38.,  3.]), 'currentState': array([37.34936487,  3.80855409,  6.23962739]), 'targetState': array([38,  3], dtype=int32), 'currentDistance': 1.0378274362309103}
episode index:87
target Thresh 24.90994862236737
target distance 22.0
model initialize at round 87
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.48812513,  8.75397402]), 'dynamicTrap': False, 'previousTarget': array([32.29527642,  8.73765188]), 'currentState': array([13.33246348, 14.50381112,  5.13908768]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5745909913262516
{'scaleFactor': 20, 'currentTarget': array([35.,  8.]), 'dynamicTrap': False, 'previousTarget': array([35.,  8.]), 'currentState': array([35.51340614,  7.37573555,  4.5731442 ]), 'targetState': array([35,  8], dtype=int32), 'currentDistance': 0.8082647919556051}
episode index:88
target Thresh 25.012026613105895
target distance 14.0
model initialize at round 88
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'dynamicTrap': False, 'previousTarget': array([16.,  5.]), 'currentState': array([3.39507903, 7.31009485, 0.2369498 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 12.81485742733397}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.575501870306565
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'dynamicTrap': False, 'previousTarget': array([16.,  5.]), 'currentState': array([15.11849826,  4.67076778,  5.5905741 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.9409777706867257}
episode index:89
target Thresh 25.113900651882894
target distance 12.0
model initialize at round 89
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'dynamicTrap': False, 'previousTarget': array([25., 20.]), 'currentState': array([35.43854679, 11.98062245,  2.14400196]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 13.163345905221089}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5786636090213952
{'scaleFactor': 20, 'currentTarget': array([25., 20.]), 'dynamicTrap': False, 'previousTarget': array([25., 20.]), 'currentState': array([25.2660625 , 19.4820792 ,  4.08074224]), 'targetState': array([25, 20], dtype=int32), 'currentDistance': 0.58226386054184}
episode index:90
target Thresh 25.215571146194662
target distance 18.0
model initialize at round 90
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'dynamicTrap': False, 'previousTarget': array([13.,  7.]), 'currentState': array([29.3519245 ,  9.34093289,  3.94760275]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 16.518638008892626}
done in step count: 78
reward sum = 0.4566097477439145
running average episode reward sum: 0.5773223577985658
{'scaleFactor': 20, 'currentTarget': array([13.,  7.]), 'dynamicTrap': False, 'previousTarget': array([13.,  7.]), 'currentState': array([12.19963405,  7.05371653,  1.15111206]), 'targetState': array([13,  7], dtype=int32), 'currentDistance': 0.8021665182371157}
episode index:91
target Thresh 25.31703850272329
target distance 15.0
model initialize at round 91
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([11.19303459,  5.51104758,  1.10276198]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 15.782194186302222}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.5803955751555518
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 3.01396853, 18.29585083,  4.88891102]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.7042877104083333}
episode index:92
target Thresh 25.418303127338355
target distance 16.0
model initialize at round 92
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([11.8430186 ,  4.35851366,  2.69583988]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 14.665735668778593}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.5835904724441907
{'scaleFactor': 20, 'currentTarget': array([11., 19.]), 'dynamicTrap': False, 'previousTarget': array([11., 19.]), 'currentState': array([10.66056463, 18.20366564,  1.28325772]), 'targetState': array([11, 19], dtype=int32), 'currentDistance': 0.8656585792673354}
episode index:93
target Thresh 25.51936542509849
target distance 5.0
model initialize at round 93
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([35.79950285, 11.93596323,  2.13714027]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 4.444615327933748}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.5858247683080148
{'scaleFactor': 20, 'currentTarget': array([34., 16.]), 'dynamicTrap': False, 'previousTarget': array([34., 16.]), 'currentState': array([33.97332942, 16.40810254,  4.23395251]), 'targetState': array([34, 16], dtype=int32), 'currentDistance': 0.4089731069375091}
episode index:94
target Thresh 25.620225800253017
target distance 18.0
model initialize at round 94
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([12.51595386, 12.58738108,  5.00743866]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 16.86988678227426}
done in step count: 83
reward sum = 0.43423132679181164
running average episode reward sum: 0.5842290478710022
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([28.83056072,  9.56804153,  3.98404817]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 0.5927738557928676}
episode index:95
target Thresh 25.720884656243566
target distance 3.0
model initialize at round 95
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 20.]), 'dynamicTrap': False, 'previousTarget': array([36., 20.]), 'currentState': array([37.95958831, 16.9953877 ,  0.55896443]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 3.5871550598324484}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.5875639752370209
{'scaleFactor': 20, 'currentTarget': array([36., 20.]), 'dynamicTrap': False, 'previousTarget': array([36., 20.]), 'currentState': array([35.20904912, 19.19111324,  2.31213074]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 1.1313271344378073}
episode index:96
target Thresh 25.82134239570571
target distance 16.0
model initialize at round 96
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'dynamicTrap': False, 'previousTarget': array([26., 22.]), 'currentState': array([10.04529313, 14.82439354,  5.97481155]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 17.494056116087904}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.5907369059485847
{'scaleFactor': 20, 'currentTarget': array([26., 22.]), 'dynamicTrap': False, 'previousTarget': array([26., 22.]), 'currentState': array([25.73647506, 21.01842453,  0.3638522 ]), 'targetState': array([26, 22], dtype=int32), 'currentDistance': 1.016334496421169}
episode index:97
target Thresh 25.92159942047052
target distance 5.0
model initialize at round 97
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([28.20942545, 23.16512337,  4.66807449]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 6.34805933161457}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.5942198492338746
{'scaleFactor': 20, 'currentTarget': array([33., 19.]), 'dynamicTrap': False, 'previousTarget': array([33., 19.]), 'currentState': array([32.22966214, 19.81948368,  4.79284632]), 'targetState': array([33, 19], dtype=int32), 'currentDistance': 1.1247105940503594}
episode index:98
target Thresh 26.021656131566253
target distance 5.0
model initialize at round 98
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 18.]), 'dynamicTrap': False, 'previousTarget': array([33., 18.]), 'currentState': array([36.97726889, 17.47827294,  4.43365109]), 'targetState': array([33, 18], dtype=int32), 'currentDistance': 4.011342290443022}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.5979206185345426
{'scaleFactor': 20, 'currentTarget': array([33., 18.]), 'dynamicTrap': False, 'previousTarget': array([33., 18.]), 'currentState': array([33.25269052, 17.83846046,  2.30360717]), 'targetState': array([33, 18], dtype=int32), 'currentDistance': 0.2999125214637116}
episode index:99
target Thresh 26.121512929219865
target distance 14.0
model initialize at round 99
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.,  9.]), 'dynamicTrap': False, 'previousTarget': array([34.,  9.]), 'currentState': array([21.84364313, 21.58348357,  4.23954916]), 'targetState': array([34,  9], dtype=int32), 'currentDistance': 17.496315928752008}
done in step count: 35
reward sum = 0.7034476949995692
running average episode reward sum: 0.5989758892991929
{'scaleFactor': 20, 'currentTarget': array([34.,  9.]), 'dynamicTrap': False, 'previousTarget': array([34.,  9.]), 'currentState': array([34.30770147,  9.39011336,  4.82522189]), 'targetState': array([34,  9], dtype=int32), 'currentDistance': 0.49685875956946857}
episode index:100
target Thresh 26.2211702128587
target distance 15.0
model initialize at round 100
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([28.12819408,  7.68734351,  0.6628291 ]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 15.579625094227351}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6013914071614179
{'scaleFactor': 20, 'currentTarget': array([31., 23.]), 'dynamicTrap': False, 'previousTarget': array([31., 23.]), 'currentState': array([30.23729713, 22.06468316,  2.310454  ]), 'targetState': array([31, 23], dtype=int32), 'currentDistance': 1.2068691960699853}
episode index:101
target Thresh 26.32062838111201
target distance 6.0
model initialize at round 101
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'dynamicTrap': False, 'previousTarget': array([10., 10.]), 'currentState': array([ 4.7220943 , 16.31915161,  2.5430764 ]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 8.233344735859037}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6037595619283053
{'scaleFactor': 20, 'currentTarget': array([10., 10.]), 'dynamicTrap': False, 'previousTarget': array([10., 10.]), 'currentState': array([10.62478638, 10.9839533 ,  5.86815832]), 'targetState': array([10, 10], dtype=int32), 'currentDistance': 1.1655565663807044}
episode index:102
target Thresh 26.419887831812602
target distance 13.0
model initialize at round 102
at step 0:
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'dynamicTrap': False, 'previousTarget': array([2., 3.]), 'currentState': array([10.85650255, 16.10057269,  3.54028475]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 15.813369097988467}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6062478997216352
{'scaleFactor': 20, 'currentTarget': array([2., 3.]), 'dynamicTrap': False, 'previousTarget': array([2., 3.]), 'currentState': array([2.97949823, 2.79740786, 2.93466681]), 'targetState': array([2, 3], dtype=int32), 'currentDistance': 1.0002301482861538}
episode index:103
target Thresh 26.51894896199842
target distance 18.0
model initialize at round 103
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'dynamicTrap': False, 'previousTarget': array([16., 23.]), 'currentState': array([32.8273769 , 15.79279532,  4.95152092]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 18.30585732753781}
done in step count: 36
reward sum = 0.6964132180495735
running average episode reward sum: 0.6071148739363269
{'scaleFactor': 20, 'currentTarget': array([16., 23.]), 'dynamicTrap': False, 'previousTarget': array([16., 23.]), 'currentState': array([16.46408114, 22.18717666,  4.95730577]), 'targetState': array([16, 23], dtype=int32), 'currentDistance': 0.9359770721962768}
episode index:104
target Thresh 26.617812167914103
target distance 11.0
model initialize at round 104
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 19.]), 'dynamicTrap': False, 'previousTarget': array([38., 19.]), 'currentState': array([35.82057212,  8.79518889,  5.12336338]), 'targetState': array([38, 19], dtype=int32), 'currentDistance': 10.434944923936987}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6093608579310659
{'scaleFactor': 20, 'currentTarget': array([38., 19.]), 'dynamicTrap': False, 'previousTarget': array([38., 19.]), 'currentState': array([38.02933467, 18.78532789,  6.27919647]), 'targetState': array([38, 19], dtype=int32), 'currentDistance': 0.216667109283548}
episode index:105
target Thresh 26.71647784501262
target distance 10.0
model initialize at round 105
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  8.]), 'dynamicTrap': False, 'previousTarget': array([38.,  8.]), 'currentState': array([29.90646351, 14.93104787,  0.49906557]), 'targetState': array([38,  8], dtype=int32), 'currentDistance': 10.655738244341233}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6114062142131841
{'scaleFactor': 20, 'currentTarget': array([38.,  8.]), 'dynamicTrap': False, 'previousTarget': array([38.,  8.]), 'currentState': array([37.33873765,  7.13797882,  0.73708781]), 'targetState': array([38,  8], dtype=int32), 'currentDistance': 1.0864384061240027}
episode index:106
target Thresh 26.814946387956805
target distance 7.0
model initialize at round 106
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 18.]), 'currentState': array([13.49779508, 22.24120592,  2.59932804]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 6.943599814227737}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.6146696702485749
{'scaleFactor': 20, 'currentTarget': array([ 8., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 18.]), 'currentState': array([ 7.11358193, 18.72307397,  4.24114254]), 'targetState': array([ 8, 18], dtype=int32), 'currentDistance': 1.1439287360155794}
episode index:107
target Thresh 26.913218190620967
target distance 17.0
model initialize at round 107
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.66057202, 17.93005333]), 'dynamicTrap': False, 'previousTarget': array([19.43860471, 17.71414506]), 'currentState': array([4.03411878, 5.44750715, 5.03390634]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6161803340370061
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'dynamicTrap': False, 'previousTarget': array([21., 19.]), 'currentState': array([20.70148603, 19.63149661,  4.4600454 ]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.6984973568541669}
episode index:108
target Thresh 27.011293646092433
target distance 14.0
model initialize at round 108
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.16061272,  4.80921777]), 'dynamicTrap': True, 'previousTarget': array([12.,  5.]), 'currentState': array([26.       , 11.       ,  2.3070588], dtype=float32), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 14.253969635814697}
done in step count: 73
reward sum = 0.4801414565714212
running average episode reward sum: 0.6149322709409915
{'scaleFactor': 20, 'currentTarget': array([12.,  5.]), 'dynamicTrap': False, 'previousTarget': array([12.,  5.]), 'currentState': array([12.44742773,  4.53694762,  3.16859051]), 'targetState': array([12,  5], dtype=int32), 'currentDistance': 0.6439014514474001}
episode index:109
target Thresh 27.10917314667317
target distance 20.0
model initialize at round 109
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.69755679, 5.97308349]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([24.84026928, 17.78040468,  2.94214582]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 20.0}
done in step count: 47
reward sum = 0.6235253948912
running average episode reward sum: 0.6150103902496298
{'scaleFactor': 20, 'currentTarget': array([6., 4.]), 'dynamicTrap': False, 'previousTarget': array([6., 4.]), 'currentState': array([6.08380413, 3.94690502, 1.15439399]), 'targetState': array([6, 4], dtype=int32), 'currentDistance': 0.09920791438163384}
episode index:110
target Thresh 27.206857083881303
target distance 11.0
model initialize at round 110
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([8.60404309, 4.81849996, 1.26476973]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 10.276752349129007}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6172180295684735
{'scaleFactor': 20, 'currentTarget': array([10., 15.]), 'dynamicTrap': False, 'previousTarget': array([10., 15.]), 'currentState': array([ 9.24079638, 15.49781923,  1.93803203]), 'targetState': array([10, 15], dtype=int32), 'currentDistance': 0.9078623923312223}
episode index:111
target Thresh 27.30434584845272
target distance 25.0
model initialize at round 111
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.64374891, 15.34496991]), 'dynamicTrap': False, 'previousTarget': array([16.25118736, 15.15981002]), 'currentState': array([34.32821051, 11.80633017,  4.26692438]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6117071543044693
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([11.80573066, 17.96861061,  6.05375001]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 2.127117681665694}
episode index:112
target Thresh 27.401639830342603
target distance 5.0
model initialize at round 112
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 11.]), 'dynamicTrap': False, 'previousTarget': array([33., 11.]), 'currentState': array([34.82015799,  6.40497406,  1.46865576]), 'targetState': array([33, 11], dtype=int32), 'currentDistance': 4.942391977452893}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6136788941907136
{'scaleFactor': 20, 'currentTarget': array([33., 11.]), 'dynamicTrap': False, 'previousTarget': array([33., 11.]), 'currentState': array([32.002449  , 10.14226455,  5.40444207]), 'targetState': array([33, 11], dtype=int32), 'currentDistance': 1.3156055982105919}
episode index:113
target Thresh 27.498739418727006
target distance 17.0
model initialize at round 113
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 9.20859686, 15.86502556]), 'currentState': array([25.43327324,  5.12647803,  3.9104228 ]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 19.704972709620723}
done in step count: 43
reward sum = 0.6491026283684022
running average episode reward sum: 0.6139896287010442
{'scaleFactor': 20, 'currentTarget': array([ 9., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 16.]), 'currentState': array([ 8.97875901, 16.91442691,  5.91175929]), 'targetState': array([ 9, 16], dtype=int32), 'currentDistance': 0.9146735792755921}
episode index:114
target Thresh 27.595645002004407
target distance 24.0
model initialize at round 114
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8.88181327, 12.92690954]), 'dynamicTrap': False, 'previousTarget': array([ 8.01733854, 13.16738911]), 'currentState': array([28.87957204, 12.62750327,  4.27230883]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 57
reward sum = 0.5639051904523875
running average episode reward sum: 0.613554111846708
{'scaleFactor': 20, 'currentTarget': array([ 4., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 13.]), 'currentState': array([ 4.99014423, 12.94612783,  5.55667901]), 'targetState': array([ 4, 13], dtype=int32), 'currentDistance': 0.9916086986306448}
episode index:115
target Thresh 27.692356967797288
target distance 10.0
model initialize at round 115
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38., 10.]), 'dynamicTrap': False, 'previousTarget': array([38., 10.]), 'currentState': array([28.87038654,  6.55957806,  6.26593351]), 'targetState': array([38, 10], dtype=int32), 'currentDistance': 9.756348960079038}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.616381060446314
{'scaleFactor': 20, 'currentTarget': array([38., 10.]), 'dynamicTrap': False, 'previousTarget': array([38., 10.]), 'currentState': array([38.06948967, 10.02224219,  5.95087069]), 'targetState': array([38, 10], dtype=int32), 'currentDistance': 0.07296252316029532}
episode index:116
target Thresh 27.788875702953632
target distance 17.0
model initialize at round 116
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 20.]), 'dynamicTrap': False, 'previousTarget': array([33., 20.]), 'currentState': array([35.35041704,  3.33356279,  1.93207264]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 16.83135732929685}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6178958743197955
{'scaleFactor': 20, 'currentTarget': array([33., 20.]), 'dynamicTrap': False, 'previousTarget': array([33., 20.]), 'currentState': array([32.96224304, 20.18022253,  1.62460135]), 'targetState': array([33, 20], dtype=int32), 'currentDistance': 0.18413513946759125}
episode index:117
target Thresh 27.8852015935485
target distance 5.0
model initialize at round 117
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([31.08545913, 16.35139563,  1.15573907]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 3.7614756206908995}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6203237234781769
{'scaleFactor': 20, 'currentTarget': array([32., 20.]), 'dynamicTrap': False, 'previousTarget': array([32., 20.]), 'currentState': array([32.75832108, 19.76783513,  1.23308503]), 'targetState': array([32, 20], dtype=int32), 'currentDistance': 0.7930645551189539}
episode index:118
target Thresh 27.98133502488559
target distance 6.0
model initialize at round 118
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 15.]), 'currentState': array([ 9.10195858, 13.53338614,  4.11846757]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 6.275735389259061}
done in step count: 56
reward sum = 0.5696012024771592
running average episode reward sum: 0.6198974838058995
{'scaleFactor': 20, 'currentTarget': array([ 3., 15.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 15.]), 'currentState': array([ 3.67766266, 15.59943774,  4.07085684]), 'targetState': array([ 3, 15], dtype=int32), 'currentDistance': 0.9047387927819611}
episode index:119
target Thresh 28.077276381498756
target distance 12.0
model initialize at round 119
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'dynamicTrap': False, 'previousTarget': array([22., 19.]), 'currentState': array([11.6023013 ,  8.01981658,  0.06692689]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 15.122055617333752}
done in step count: 48
reward sum = 0.617290140942288
running average episode reward sum: 0.6198757559487027
{'scaleFactor': 20, 'currentTarget': array([22., 19.]), 'dynamicTrap': False, 'previousTarget': array([22., 19.]), 'currentState': array([21.88346211, 19.7554721 ,  0.41796445]), 'targetState': array([22, 19], dtype=int32), 'currentDistance': 0.764407726651367}
episode index:120
target Thresh 28.17302604715355
target distance 17.0
model initialize at round 120
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  4.]), 'dynamicTrap': False, 'previousTarget': array([33.,  4.]), 'currentState': array([29.87259846, 20.20186675,  4.53232809]), 'targetState': array([33,  4], dtype=int32), 'currentDistance': 16.500943206929207}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6217192884894897
{'scaleFactor': 20, 'currentTarget': array([33.,  4.]), 'dynamicTrap': False, 'previousTarget': array([33.,  4.]), 'currentState': array([32.15719778,  4.57984693,  0.73560514]), 'targetState': array([33,  4], dtype=int32), 'currentDistance': 1.023004415301096}
episode index:121
target Thresh 28.268584404848763
target distance 15.0
model initialize at round 121
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'dynamicTrap': False, 'previousTarget': array([23., 21.]), 'currentState': array([33.1220879 ,  7.25481616,  0.90952337]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 17.070053962762024}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.623395102713638
{'scaleFactor': 20, 'currentTarget': array([23., 21.]), 'dynamicTrap': False, 'previousTarget': array([23., 21.]), 'currentState': array([23.66734117, 21.28553462,  5.15881893]), 'targetState': array([23, 21], dtype=int32), 'currentDistance': 0.7258610465862209}
episode index:122
target Thresh 28.363951836817954
target distance 9.0
model initialize at round 122
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([11.92661352, 20.51599892,  5.9223519 ]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 12.38488746369122}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6258288392316403
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.62298069, 10.82391413,  4.89286453]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.6473879623833776}
episode index:123
target Thresh 28.45912872453098
target distance 1.0
model initialize at round 123
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 13.]), 'dynamicTrap': False, 'previousTarget': array([35., 13.]), 'currentState': array([34.82941779, 13.7727625 ,  1.49629778]), 'targetState': array([35, 13], dtype=int32), 'currentDistance': 0.7913660188518595}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6288463485926754
{'scaleFactor': 20, 'currentTarget': array([35., 13.]), 'dynamicTrap': False, 'previousTarget': array([35., 13.]), 'currentState': array([34.82941779, 13.7727625 ,  1.49629778]), 'targetState': array([35, 13], dtype=int32), 'currentDistance': 0.7913660188518595}
episode index:124
target Thresh 28.554115448695512
target distance 11.0
model initialize at round 124
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'dynamicTrap': False, 'previousTarget': array([7., 8.]), 'currentState': array([11.29147318, 18.13866134,  4.6849916 ]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 11.009504796326995}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6311237157838032
{'scaleFactor': 20, 'currentTarget': array([7., 8.]), 'dynamicTrap': False, 'previousTarget': array([7., 8.]), 'currentState': array([7.0004435 , 8.75382475, 1.53371779]), 'targetState': array([7, 8], dtype=int32), 'currentDistance': 0.7538248780000467}
episode index:125
target Thresh 28.648912389258577
target distance 2.0
model initialize at round 125
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 19.]), 'currentState': array([ 6.37853611, 17.03458492,  0.39905995]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 2.061328168325576}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6339719402617094
{'scaleFactor': 20, 'currentTarget': array([ 7., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 7., 19.]), 'currentState': array([ 6.68271212, 18.68983869,  2.39905995]), 'targetState': array([ 7, 19], dtype=int32), 'currentDistance': 0.44370219792795834}
episode index:126
target Thresh 28.74351992540807
target distance 14.0
model initialize at round 126
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([29., 12.]), 'currentState': array([16.6601781 ,  2.72396242,  0.84523743]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 15.437489355566509}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6289800352202787
{'scaleFactor': 20, 'currentTarget': array([29., 12.]), 'dynamicTrap': False, 'previousTarget': array([29., 12.]), 'currentState': array([30.31523126,  8.80812127,  6.11603866]), 'targetState': array([29, 12], dtype=int32), 'currentDistance': 3.45223450670191}
episode index:127
target Thresh 28.837938435574255
target distance 19.0
model initialize at round 127
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.44712323, 12.24905876]), 'dynamicTrap': False, 'previousTarget': array([25.69836445, 12.68507134]), 'currentState': array([ 8.21195569, 20.46359862,  5.38759196]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6240661286951202
{'scaleFactor': 20, 'currentTarget': array([27., 12.]), 'dynamicTrap': False, 'previousTarget': array([27., 12.]), 'currentState': array([19.24396175,  6.16105377,  2.27247094]), 'targetState': array([27, 12], dtype=int32), 'currentDistance': 9.708214172642599}
episode index:128
target Thresh 28.93216829743129
target distance 12.0
model initialize at round 128
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 10.]), 'currentState': array([19.11694866, 22.88016951,  1.43554753]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 17.014267955021136}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6255687706245939
{'scaleFactor': 20, 'currentTarget': array([ 8., 10.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 10.]), 'currentState': array([8.01773268, 9.21540342, 1.0631256 ]), 'targetState': array([ 8, 10], dtype=int32), 'currentDistance': 0.7847969412175855}
episode index:129
target Thresh 29.026209887898773
target distance 11.0
model initialize at round 129
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 13.]), 'dynamicTrap': False, 'previousTarget': array([34., 13.]), 'currentState': array([22.89502558,  4.88092784,  3.85803163]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 13.75644538804064}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.6265622361367403
{'scaleFactor': 20, 'currentTarget': array([34., 13.]), 'dynamicTrap': False, 'previousTarget': array([34., 13.]), 'currentState': array([34.98209195, 12.72961133,  2.08551598]), 'targetState': array([34, 13], dtype=int32), 'currentDistance': 1.0186337079205272}
episode index:130
target Thresh 29.120063583143168
target distance 19.0
model initialize at round 130
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.66632599, 16.96989769]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([ 6.74721951, 15.17290101,  1.43603778]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 68
reward sum = 0.5048858887870696
running average episode reward sum: 0.6256334090577352
{'scaleFactor': 20, 'currentTarget': array([27., 17.]), 'dynamicTrap': False, 'previousTarget': array([27., 17.]), 'currentState': array([27.02129308, 16.98202323,  4.14883978]), 'targetState': array([27, 17], dtype=int32), 'currentDistance': 0.027866818452872418}
episode index:131
target Thresh 29.21372975857939
target distance 14.0
model initialize at round 131
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 14.]), 'currentState': array([18.4033754 ,  3.70594845,  3.62514722]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 16.11866057494366}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6264415148198312
{'scaleFactor': 20, 'currentTarget': array([ 6., 14.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 14.]), 'currentState': array([ 5.76755745, 14.62283521,  4.76230741]), 'targetState': array([ 6, 14], dtype=int32), 'currentDistance': 0.664795638573727}
episode index:132
target Thresh 29.307208788872266
target distance 4.0
model initialize at round 132
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 11.]), 'currentState': array([12.25955144, 12.57786978,  2.91449964]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 3.6213738624417773}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6281333663707713
{'scaleFactor': 20, 'currentTarget': array([ 9., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 11.]), 'currentState': array([ 8.13876282, 11.39829195,  5.12986548]), 'targetState': array([ 9, 11], dtype=int32), 'currentDistance': 0.94887614930457}
episode index:133
target Thresh 29.400501047938032
target distance 7.0
model initialize at round 133
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([10.95820611, 14.13067138,  3.33897984]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 9.345909186684901}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6297364247813173
{'scaleFactor': 20, 'currentTarget': array([16., 22.]), 'dynamicTrap': False, 'previousTarget': array([16., 22.]), 'currentState': array([16.55368591, 21.37917444,  2.81095714]), 'targetState': array([16, 22], dtype=int32), 'currentDistance': 0.8318608432530151}
episode index:134
target Thresh 29.493606908945868
target distance 12.0
model initialize at round 134
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'dynamicTrap': False, 'previousTarget': array([5., 3.]), 'currentState': array([15.51219016,  4.21335526,  4.63795662]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 10.581983408500932}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.6317038457404092
{'scaleFactor': 20, 'currentTarget': array([5., 3.]), 'dynamicTrap': False, 'previousTarget': array([5., 3.]), 'currentState': array([5.58236329, 3.61275951, 3.99522477]), 'targetState': array([5, 3], dtype=int32), 'currentDistance': 0.8453527194480626}
episode index:135
target Thresh 29.586526744319322
target distance 19.0
model initialize at round 135
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([22.34069985,  3.7819284 ,  2.64236355]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 17.846344141605012}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.633257076237788
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([5.23703358, 7.37833446, 0.10547438]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.6653216946711976}
episode index:136
target Thresh 29.67926092573787
target distance 4.0
model initialize at round 136
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([26.14151228,  5.76333602,  4.44779811]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 4.318210946995187}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6353701245457452
{'scaleFactor': 20, 'currentTarget': array([29.,  9.]), 'dynamicTrap': False, 'previousTarget': array([29.,  9.]), 'currentState': array([28.9267511 ,  9.02168303,  2.90149776]), 'targetState': array([29,  9], dtype=int32), 'currentDistance': 0.07639080864439843}
episode index:137
target Thresh 29.771809824138362
target distance 25.0
model initialize at round 137
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.59777778, 11.78455865]), 'dynamicTrap': False, 'previousTarget': array([20.74433602, 11.77294527]), 'currentState': array([ 4.38092859, 21.96198976,  5.88164276]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 62
reward sum = 0.536268225207185
running average episode reward sum: 0.6346519948403931
{'scaleFactor': 20, 'currentTarget': array([28.,  8.]), 'dynamicTrap': False, 'previousTarget': array([28.,  8.]), 'currentState': array([27.38281007,  8.88626519,  0.05820098]), 'targetState': array([28,  8], dtype=int32), 'currentDistance': 1.0799950869084434}
episode index:138
target Thresh 29.864173809716526
target distance 1.0
model initialize at round 138
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'dynamicTrap': False, 'previousTarget': array([18., 22.]), 'currentState': array([19.17043036, 20.76931109,  0.74113899]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 1.6983822952938707}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.6372084553091673
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'dynamicTrap': False, 'previousTarget': array([18., 22.]), 'currentState': array([18.90173328, 22.43069303,  2.74113899]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.9993094601740243}
episode index:139
target Thresh 29.956353251928398
target distance 14.0
model initialize at round 139
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([15.42952011, 20.60496362,  3.78390145]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 17.004944890742763}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6388002403043966
{'scaleFactor': 20, 'currentTarget': array([3., 9.]), 'dynamicTrap': False, 'previousTarget': array([3., 9.]), 'currentState': array([2.71621224, 8.93344971, 3.24964285]), 'targetState': array([3, 9], dtype=int32), 'currentDistance': 0.29148659481284367}
episode index:140
target Thresh 30.048348519491885
target distance 13.0
model initialize at round 140
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([25.74990023,  3.20708638,  1.45924652]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 11.858986515561705}
done in step count: 42
reward sum = 0.6556592205741436
running average episode reward sum: 0.6389198075403522
{'scaleFactor': 20, 'currentTarget': array([27., 15.]), 'dynamicTrap': False, 'previousTarget': array([27., 15.]), 'currentState': array([27.81777729, 14.90533395,  4.30094286]), 'targetState': array([27, 15], dtype=int32), 'currentDistance': 0.8232383345425474}
episode index:141
target Thresh 30.14015998038819
target distance 7.0
model initialize at round 141
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([ 2.59738282, 20.42352437,  5.47234321]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 7.789236903175663}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6411174853034483
{'scaleFactor': 20, 'currentTarget': array([10., 18.]), 'dynamicTrap': False, 'previousTarget': array([10., 18.]), 'currentState': array([10.66774429, 17.83420881,  6.26539397]), 'targetState': array([10, 18], dtype=int32), 'currentDistance': 0.688018283911099}
episode index:142
target Thresh 30.23178800186326
target distance 8.0
model initialize at round 142
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([28.38002818, 15.18553668,  1.46377819]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 8.185541148340038}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6430223787452678
{'scaleFactor': 20, 'currentTarget': array([35., 20.]), 'dynamicTrap': False, 'previousTarget': array([35., 20.]), 'currentState': array([35.31701157, 20.82598587,  3.73986053]), 'targetState': array([35, 20], dtype=int32), 'currentDistance': 0.8847310262210503}
episode index:143
target Thresh 30.32323295042933
target distance 21.0
model initialize at round 143
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32.30414089,  8.27498245]), 'dynamicTrap': False, 'previousTarget': array([33.45612429,  8.63241055]), 'currentState': array([12.99040445,  3.08079487,  4.88348234]), 'targetState': array([35,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6445295730223235
{'scaleFactor': 20, 'currentTarget': array([35.,  9.]), 'dynamicTrap': False, 'previousTarget': array([35.,  9.]), 'currentState': array([34.31256484,  8.22719345,  5.95773917]), 'targetState': array([35,  9], dtype=int32), 'currentDistance': 1.034309947917724}
episode index:144
target Thresh 30.414495191866287
target distance 8.0
model initialize at round 144
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'dynamicTrap': False, 'previousTarget': array([16., 21.]), 'currentState': array([23.64420914, 20.72330666,  2.79257822]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 7.649215158363423}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6465775080318316
{'scaleFactor': 20, 'currentTarget': array([16., 21.]), 'dynamicTrap': False, 'previousTarget': array([16., 21.]), 'currentState': array([16.62867607, 21.27482214,  1.25217322]), 'targetState': array([16, 21], dtype=int32), 'currentDistance': 0.6861201174860714}
episode index:145
target Thresh 30.505575091223232
target distance 21.0
model initialize at round 145
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.17900894,  2.48308368]), 'dynamicTrap': False, 'previousTarget': array([12.67544468,  4.02633404]), 'currentState': array([19.12833636, 21.23693258,  3.92653751]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6472665120475835
{'scaleFactor': 20, 'currentTarget': array([12.,  2.]), 'dynamicTrap': False, 'previousTarget': array([12.,  2.]), 'currentState': array([11.28452064,  1.62897828,  0.05084732]), 'targetState': array([12,  2], dtype=int32), 'currentDistance': 0.8059577113556555}
episode index:146
target Thresh 30.59647301281989
target distance 17.0
model initialize at round 146
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'dynamicTrap': False, 'previousTarget': array([21., 19.]), 'currentState': array([27.94587908,  2.71313556,  1.60376048]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 17.706134235351765}
done in step count: 92
reward sum = 0.3966778064220251
running average episode reward sum: 0.6455618269753007
{'scaleFactor': 20, 'currentTarget': array([21., 19.]), 'dynamicTrap': False, 'previousTarget': array([21., 19.]), 'currentState': array([21.19016021, 19.56323683,  3.54522296]), 'targetState': array([21, 19], dtype=int32), 'currentDistance': 0.5944717266545135}
episode index:147
target Thresh 30.68718932024805
target distance 22.0
model initialize at round 147
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.12144315, 15.80490108]), 'dynamicTrap': False, 'previousTarget': array([29.6773982 , 15.57770876]), 'currentState': array([11.59706243, 11.46917075,  0.68910902]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 97
reward sum = 0.37723664692350417
running average episode reward sum: 0.6437488190019778
{'scaleFactor': 20, 'currentTarget': array([32., 16.]), 'dynamicTrap': False, 'previousTarget': array([32., 16.]), 'currentState': array([32.33911705, 15.83797433,  1.81782449]), 'targetState': array([32, 16], dtype=int32), 'currentDistance': 0.3758359868618049}
episode index:148
target Thresh 30.77772437637308
target distance 25.0
model initialize at round 148
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.7794065 , 18.75241526]), 'dynamicTrap': False, 'previousTarget': array([30.25928039, 18.39259851]), 'currentState': array([10.169897  , 14.81958715,  2.00371417]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6450856939978298
{'scaleFactor': 20, 'currentTarget': array([36., 20.]), 'dynamicTrap': False, 'previousTarget': array([36., 20.]), 'currentState': array([36.49047069, 20.00873623,  6.14184952]), 'targetState': array([36, 20], dtype=int32), 'currentDistance': 0.49054848373345084}
episode index:149
target Thresh 30.868078543335315
target distance 11.0
model initialize at round 149
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'dynamicTrap': False, 'previousTarget': array([13., 11.]), 'currentState': array([ 2.77539738, 14.56769275,  0.21759796]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 10.829170344962785}
done in step count: 58
reward sum = 0.5582661385478637
running average episode reward sum: 0.6445068969614968
{'scaleFactor': 20, 'currentTarget': array([13., 11.]), 'dynamicTrap': False, 'previousTarget': array([13., 11.]), 'currentState': array([12.02660736, 11.38182783,  1.98951971]), 'targetState': array([13, 11], dtype=int32), 'currentDistance': 1.0456030415906468}
episode index:150
target Thresh 30.95825218255155
target distance 16.0
model initialize at round 150
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([8.44625376, 8.14499727, 4.18342829]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 16.096584248069494}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6462279246306842
{'scaleFactor': 20, 'currentTarget': array([24.,  4.]), 'dynamicTrap': False, 'previousTarget': array([24.,  4.]), 'currentState': array([23.55914926,  4.36609066,  6.24010348]), 'targetState': array([24,  4], dtype=int32), 'currentDistance': 0.5730372975367914}
episode index:151
target Thresh 31.04824565471646
target distance 28.0
model initialize at round 151
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.64335501,  3.29941567]), 'dynamicTrap': False, 'previousTarget': array([11.44395172,  3.80941823]), 'currentState': array([31.42110336,  6.27274214,  4.22946024]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6474666472413382
{'scaleFactor': 20, 'currentTarget': array([3., 2.]), 'dynamicTrap': False, 'previousTarget': array([3., 2.]), 'currentState': array([3.6075835 , 2.26771665, 3.97065258]), 'targetState': array([3, 2], dtype=int32), 'currentDistance': 0.6639502364632739}
episode index:152
target Thresh 31.138059319804057
target distance 8.0
model initialize at round 152
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'dynamicTrap': False, 'previousTarget': array([26., 15.]), 'currentState': array([21.68382153, 21.94083041,  5.54011762]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 8.173403413353588}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6493883041181987
{'scaleFactor': 20, 'currentTarget': array([26., 15.]), 'dynamicTrap': False, 'previousTarget': array([26.4443043 , 16.79663858]), 'currentState': array([25.02328255, 14.51467225,  5.3061525 ]), 'targetState': array([26, 15], dtype=int32), 'currentDistance': 1.090651184479869}
episode index:153
target Thresh 31.22769353706911
target distance 13.0
model initialize at round 153
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([33.04980559, 17.04785855,  5.11942887]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 11.047970811718708}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6511633456137164
{'scaleFactor': 20, 'currentTarget': array([33.,  6.]), 'dynamicTrap': False, 'previousTarget': array([33.,  6.]), 'currentState': array([32.03993591,  5.6820057 ,  6.23686782]), 'targetState': array([33,  6], dtype=int32), 'currentDistance': 1.0113572198891272}
episode index:154
target Thresh 31.317148665048617
target distance 18.0
model initialize at round 154
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.70556809, 5.29081531]), 'dynamicTrap': False, 'previousTarget': array([8.71272322, 5.05181363]), 'currentState': array([21.40707837, 20.73981647,  3.25641727]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6519804940897513
{'scaleFactor': 20, 'currentTarget': array([6., 2.]), 'dynamicTrap': False, 'previousTarget': array([6., 2.]), 'currentState': array([6.54568746, 2.89010159, 5.21508398]), 'targetState': array([6, 2], dtype=int32), 'currentDistance': 1.0440573028517648}
episode index:155
target Thresh 31.406425061563212
target distance 17.0
model initialize at round 155
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'dynamicTrap': False, 'previousTarget': array([27.,  7.]), 'currentState': array([10.10963795,  9.89363984,  5.82116437]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 17.13643725050222}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.653150579136933
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'dynamicTrap': False, 'previousTarget': array([27.,  7.]), 'currentState': array([26.13677087,  6.6631565 ,  1.72816938]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.9266218626840494}
episode index:156
target Thresh 31.49552308371859
target distance 8.0
model initialize at round 156
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([ 8.20081458, 13.48133316,  1.39122933]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 10.137024881192422}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6549870732150481
{'scaleFactor': 20, 'currentTarget': array([15., 21.]), 'dynamicTrap': False, 'previousTarget': array([15., 21.]), 'currentState': array([14.85163052, 21.04668784,  1.63188764]), 'targetState': array([15, 21], dtype=int32), 'currentDistance': 0.15554182274725203}
episode index:157
target Thresh 31.584443087906962
target distance 17.0
model initialize at round 157
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.53501374,  3.53061209]), 'dynamicTrap': False, 'previousTarget': array([15.23243274,  5.00324289]), 'currentState': array([ 3.35367552, 18.57229825,  5.78769588]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6561766689123195
{'scaleFactor': 20, 'currentTarget': array([17.,  3.]), 'dynamicTrap': False, 'previousTarget': array([17.,  3.]), 'currentState': array([17.04691829,  2.44437074,  0.61467984]), 'targetState': array([17,  3], dtype=int32), 'currentDistance': 0.5576066710589369}
episode index:158
target Thresh 31.67318542980847
target distance 19.0
model initialize at round 158
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'dynamicTrap': False, 'previousTarget': array([25., 19.]), 'currentState': array([ 7.49437611, 16.80778373,  5.20713103]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 17.642354714845663}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.6574048519449142
{'scaleFactor': 20, 'currentTarget': array([25., 19.]), 'dynamicTrap': False, 'previousTarget': array([25., 19.]), 'currentState': array([25.48685919, 19.89139559,  6.2731359 ]), 'targetState': array([25, 19], dtype=int32), 'currentDistance': 1.015685961753213}
episode index:159
target Thresh 31.76175046439259
target distance 29.0
model initialize at round 159
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.15913658, 19.85171185]), 'dynamicTrap': False, 'previousTarget': array([13.01188001, 19.68924552]), 'currentState': array([32.15583428, 19.48828205,  1.98472977]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6582065600003035
{'scaleFactor': 20, 'currentTarget': array([ 4., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 20.]), 'currentState': array([ 4.71365071, 19.55131848,  4.69383144]), 'targetState': array([ 4, 20], dtype=int32), 'currentDistance': 0.8429783137517333}
episode index:160
target Thresh 31.850138545919584
target distance 9.0
model initialize at round 160
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'dynamicTrap': False, 'previousTarget': array([12., 12.]), 'currentState': array([4.23711731, 3.21185077, 0.4427573 ]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 11.72577991836645}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.6599075462605936
{'scaleFactor': 20, 'currentTarget': array([12., 12.]), 'dynamicTrap': False, 'previousTarget': array([12., 12.]), 'currentState': array([11.4259562 , 12.22041317,  1.28014627]), 'targetState': array([12, 12], dtype=int32), 'currentDistance': 0.614905074172608}
episode index:161
target Thresh 31.9383500279419
target distance 24.0
model initialize at round 161
at step 0:
{'scaleFactor': 20, 'currentTarget': array([17.4036189 ,  9.79557527]), 'dynamicTrap': False, 'previousTarget': array([19., 11.]), 'currentState': array([33.41218126, 21.7841503 ,  3.81543776]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 20.0}
done in step count: 99
reward sum = 0.0
running average episode reward sum: 0.6558340428886146
{'scaleFactor': 20, 'currentTarget': array([11.,  5.]), 'dynamicTrap': False, 'previousTarget': array([11.,  5.]), 'currentState': array([ 7.71398331, 12.56532805,  5.35370612]), 'targetState': array([11,  5], dtype=int32), 'currentDistance': 8.248157020686618}
episode index:162
target Thresh 32.02638526330558
target distance 23.0
model initialize at round 162
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.5766102 ,  6.64030776]), 'dynamicTrap': False, 'previousTarget': array([21.34140113,  7.02547777]), 'currentState': array([ 4.54015306, 15.28242636,  5.55262787]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 19.999999999999996}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6564874703208752
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([25.28026143,  4.48710592,  5.79741108]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.8837895385544616}
episode index:163
target Thresh 32.11424460415167
target distance 9.0
model initialize at round 163
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 17.]), 'currentState': array([3.20988102, 8.05755067, 1.15740269]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 9.023924428676724}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6580547250596726
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 17.]), 'currentState': array([ 1.58763333, 17.16346992,  4.91015511]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.4435861640762383}
episode index:164
target Thresh 32.20192840191768
target distance 14.0
model initialize at round 164
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  7.]), 'dynamicTrap': False, 'previousTarget': array([28.,  7.]), 'currentState': array([34.11965641, 20.31880854,  3.50395331]), 'targetState': array([28,  7], dtype=int32), 'currentDistance': 14.657450505595335}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6596029827713329
{'scaleFactor': 20, 'currentTarget': array([28.,  7.]), 'dynamicTrap': False, 'previousTarget': array([28.,  7.]), 'currentState': array([28.4729028 ,  7.54854281,  4.33503857]), 'targetState': array([28,  7], dtype=int32), 'currentDistance': 0.7242487653055875}
episode index:165
target Thresh 32.28943700733888
target distance 22.0
model initialize at round 165
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.6732882 , 14.99109967]), 'dynamicTrap': False, 'previousTarget': array([34.51093912, 15.42734309]), 'currentState': array([17.27307318, 22.82896209,  0.87644928]), 'targetState': array([38, 14], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.66022189681697
{'scaleFactor': 20, 'currentTarget': array([38., 14.]), 'dynamicTrap': False, 'previousTarget': array([38., 14.]), 'currentState': array([37.14397752, 14.64400883,  4.19324809]), 'targetState': array([38, 14], dtype=int32), 'currentDistance': 1.071224463431975}
episode index:166
target Thresh 32.376770770449845
target distance 18.0
model initialize at round 166
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 20.]), 'dynamicTrap': False, 'previousTarget': array([31., 20.]), 'currentState': array([13.25660808, 21.51297933,  4.18731928]), 'targetState': array([31, 20], dtype=int32), 'currentDistance': 17.80778097690279}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.6611661186180494
{'scaleFactor': 20, 'currentTarget': array([31., 20.]), 'dynamicTrap': False, 'previousTarget': array([31., 20.]), 'currentState': array([31.76857788, 19.01074991,  1.76802744]), 'targetState': array([31, 20], dtype=int32), 'currentDistance': 1.2527281033539708}
episode index:167
target Thresh 32.46393004058572
target distance 1.0
model initialize at round 167
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 19.]), 'dynamicTrap': False, 'previousTarget': array([35., 19.]), 'currentState': array([35.28363927, 20.32764467,  0.75184005]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 1.357605094754717}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.6630061952929419
{'scaleFactor': 20, 'currentTarget': array([35., 19.]), 'dynamicTrap': False, 'previousTarget': array([35., 19.]), 'currentState': array([34.97082598, 18.85172994,  3.25092119]), 'targetState': array([35, 19], dtype=int32), 'currentDistance': 0.1511129874812942}
episode index:168
target Thresh 32.55091516638372
target distance 4.0
model initialize at round 168
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  7.]), 'dynamicTrap': False, 'previousTarget': array([37.,  7.]), 'currentState': array([34.87697067,  4.6845186 ,  5.56861678]), 'targetState': array([37,  7], dtype=int32), 'currentDistance': 3.141449932576505}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.6648824899953505
{'scaleFactor': 20, 'currentTarget': array([37.,  7.]), 'dynamicTrap': False, 'previousTarget': array([37.,  7.]), 'currentState': array([37.38380053,  6.47458239,  1.71201441]), 'targetState': array([37,  7], dtype=int32), 'currentDistance': 0.650666205540878}
episode index:169
target Thresh 32.63772649578444
target distance 12.0
model initialize at round 169
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([29.50572874, 10.60875231,  0.858881  ]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 10.4997736897233}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6662913110836649
{'scaleFactor': 20, 'currentTarget': array([28., 21.]), 'dynamicTrap': False, 'previousTarget': array([28., 21.]), 'currentState': array([28.62034402, 21.47639922,  1.90749928]), 'targetState': array([28, 21], dtype=int32), 'currentDistance': 0.7821655332464993}
episode index:170
target Thresh 32.72436437603333
target distance 10.0
model initialize at round 170
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([8.53857427, 5.3769133 , 2.53461075]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 12.474090129443058}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.6675784079294688
{'scaleFactor': 20, 'currentTarget': array([ 2., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 16.]), 'currentState': array([ 2.69937542, 16.29353382,  4.01936534]), 'targetState': array([ 2, 16], dtype=int32), 'currentDistance': 0.7584774730705336}
episode index:171
target Thresh 32.810829153682015
target distance 22.0
model initialize at round 171
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.13528276,  9.34994579]), 'dynamicTrap': False, 'previousTarget': array([25.79586847,  8.83486126]), 'currentState': array([8.24992974, 2.7663722 , 5.82320494]), 'targetState': array([29, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 13
reward sum = 0.8775210229989678
running average episode reward sum: 0.66879900452871
{'scaleFactor': 20, 'currentTarget': array([29., 10.]), 'dynamicTrap': False, 'previousTarget': array([29., 10.]), 'currentState': array([28.66982752,  9.24602853,  2.23099592]), 'targetState': array([29, 10], dtype=int32), 'currentDistance': 0.8230958943621638}
episode index:172
target Thresh 32.89712117458974
target distance 3.0
model initialize at round 172
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'dynamicTrap': False, 'previousTarget': array([16., 20.]), 'currentState': array([15.15561521, 22.65730681,  4.47106671]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 2.788236927994448}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.6693397196143654
{'scaleFactor': 20, 'currentTarget': array([16., 20.]), 'dynamicTrap': False, 'previousTarget': array([16., 20.]), 'currentState': array([15.43499585, 20.76898928,  5.47296515]), 'targetState': array([16, 20], dtype=int32), 'currentDistance': 0.9542401169195576}
episode index:173
target Thresh 32.98324078392469
target distance 8.0
model initialize at round 173
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([29.58877437, 19.63962434,  1.51450389]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 8.299495226489325}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6709037450729093
{'scaleFactor': 20, 'currentTarget': array([22., 23.]), 'dynamicTrap': False, 'previousTarget': array([22., 23.]), 'currentState': array([22.59763766, 23.1281851 ,  3.99126672]), 'targetState': array([22, 23], dtype=int32), 'currentDistance': 0.6112300663432925}
episode index:174
target Thresh 33.06918832616543
target distance 27.0
model initialize at round 174
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.2232181 , 12.27401212]), 'dynamicTrap': False, 'previousTarget': array([28.94535509, 12.47743371]), 'currentState': array([10.37930961,  9.78016048,  6.26448309]), 'targetState': array([36, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6713395642115304
{'scaleFactor': 20, 'currentTarget': array([36., 13.]), 'dynamicTrap': False, 'previousTarget': array([36., 13.]), 'currentState': array([36.10807076, 12.83388576,  1.66935369]), 'targetState': array([36, 13], dtype=int32), 'currentDistance': 0.1981747399557137}
episode index:175
target Thresh 33.15496414510223
target distance 29.0
model initialize at round 175
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.72196482, 15.65892135]), 'dynamicTrap': False, 'previousTarget': array([21.10128274, 16.0720157 ]), 'currentState': array([ 3.68013703, 21.77519427,  0.34935838]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 33
reward sum = 0.7177305325982749
running average episode reward sum: 0.6716031492591824
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([31.41703528, 13.83026427,  2.91724836]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 0.9291163432290415}
episode index:176
target Thresh 33.24056858383847
target distance 25.0
model initialize at round 176
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 7.12165132, 10.81200959]), 'dynamicTrap': False, 'previousTarget': array([ 8.74071961, 10.39259851]), 'currentState': array([26.33929915,  5.27287501,  1.96873474]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 23
reward sum = 0.7936142836436554
running average episode reward sum: 0.6722924777020325
{'scaleFactor': 20, 'currentTarget': array([ 3., 12.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 12.]), 'currentState': array([ 3.40906136, 12.21364646,  2.53556895]), 'targetState': array([ 3, 12], dtype=int32), 'currentDistance': 0.46149323635286155}
episode index:177
target Thresh 33.326001984792036
target distance 10.0
model initialize at round 177
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 10.]), 'dynamicTrap': False, 'previousTarget': array([34., 10.]), 'currentState': array([25.08935332, 18.38738165,  4.76089549]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 12.237148158654378}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6735963518442053
{'scaleFactor': 20, 'currentTarget': array([34., 10.]), 'dynamicTrap': False, 'previousTarget': array([34., 10.]), 'currentState': array([33.36633722,  9.78419453,  3.71369684]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 0.6694031093948006}
episode index:178
target Thresh 33.41126468969666
target distance 17.0
model initialize at round 178
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'dynamicTrap': False, 'previousTarget': array([14.14900215,  6.11284334]), 'currentState': array([ 3.18675209, 20.82434877,  6.27732015]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 19.747476802911795}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6746380390106694
{'scaleFactor': 20, 'currentTarget': array([15.,  5.]), 'dynamicTrap': False, 'previousTarget': array([15.,  5.]), 'currentState': array([1.53096679e+01, 5.40377172e+00, 8.25544795e-03]), 'targetState': array([15,  5], dtype=int32), 'currentDistance': 0.5088475135976094}
episode index:179
target Thresh 33.49635703960323
target distance 30.0
model initialize at round 179
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.84167636,  5.12919649]), 'dynamicTrap': False, 'previousTarget': array([18.,  5.]), 'currentState': array([38.84025644,  5.36751263,  1.05706864]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.6752112796794942
{'scaleFactor': 20, 'currentTarget': array([8., 5.]), 'dynamicTrap': False, 'previousTarget': array([8., 5.]), 'currentState': array([8.39012441, 4.35388947, 1.25109988]), 'targetState': array([8, 5], dtype=int32), 'currentDistance': 0.7547555048118594}
episode index:180
target Thresh 33.58127937488131
target distance 16.0
model initialize at round 180
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34.19864406,  7.84842587]), 'dynamicTrap': False, 'previousTarget': array([33.59074408,  8.32117742]), 'currentState': array([20.46558603, 22.38813416,  1.07161408]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6760452981554947
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([34.50860533,  6.61009024,  6.14971346]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 0.627294465860725}
episode index:181
target Thresh 33.66603203522032
target distance 21.0
model initialize at round 181
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.21620024,  3.59923591]), 'dynamicTrap': False, 'previousTarget': array([21.00530293,  4.47290771]), 'currentState': array([ 4.87417494, 13.56187405,  4.24856567]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.6769623195578488
{'scaleFactor': 20, 'currentTarget': array([25.,  2.]), 'dynamicTrap': False, 'previousTarget': array([25.,  2.]), 'currentState': array([24.32566997,  2.08869468,  1.28204175]), 'targetState': array([25,  2], dtype=int32), 'currentDistance': 0.6801380232512358}
episode index:182
target Thresh 33.750615359631034
target distance 17.0
model initialize at round 182
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'dynamicTrap': False, 'previousTarget': array([13.53366395,  3.66064273]), 'currentState': array([ 3.54644161, 19.15377976,  4.91461981]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 19.241140393589014}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6779628443397255
{'scaleFactor': 20, 'currentTarget': array([14.,  3.]), 'dynamicTrap': False, 'previousTarget': array([14.,  3.]), 'currentState': array([13.83030166,  2.45524037,  5.31068917]), 'targetState': array([14,  3], dtype=int32), 'currentDistance': 0.570579159875149}
episode index:183
target Thresh 33.83502968644684
target distance 13.0
model initialize at round 183
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([10.28252043,  3.48448031,  2.0408293 ]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 14.480101417808704}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.6792430313133337
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.55782526, 15.37746275,  2.48679423]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.7635909437317677}
episode index:184
target Thresh 33.91927535332519
target distance 1.0
model initialize at round 184
at step 0:
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'dynamicTrap': False, 'previousTarget': array([3., 3.]), 'currentState': array([2.41702349, 3.65237414, 4.78455479]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8749020669432835}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.6809768527656941
{'scaleFactor': 20, 'currentTarget': array([3., 3.]), 'dynamicTrap': False, 'previousTarget': array([3., 3.]), 'currentState': array([2.41702349, 3.65237414, 4.78455479]), 'targetState': array([3, 3], dtype=int32), 'currentDistance': 0.8749020669432835}
episode index:185
target Thresh 34.003352697248836
target distance 16.0
model initialize at round 185
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'dynamicTrap': False, 'previousTarget': array([27.,  7.]), 'currentState': array([17.56738623, 21.41555658,  4.04625988]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 17.227375714477795}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6817574536854247
{'scaleFactor': 20, 'currentTarget': array([27.,  7.]), 'dynamicTrap': False, 'previousTarget': array([27.,  7.]), 'currentState': array([27.07513564,  7.23957466,  4.75465983]), 'targetState': array([27,  7], dtype=int32), 'currentDistance': 0.25108043320461243}
episode index:186
target Thresh 34.08726205452727
target distance 8.0
model initialize at round 186
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'dynamicTrap': False, 'previousTarget': array([16.,  5.]), 'currentState': array([12.94650266, 13.47621601,  2.6926657 ]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 9.009444149269306}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6830461555075771
{'scaleFactor': 20, 'currentTarget': array([16.,  5.]), 'dynamicTrap': False, 'previousTarget': array([16.,  5.]), 'currentState': array([15.25905961,  4.52416065,  0.46864132]), 'targetState': array([16,  5], dtype=int32), 'currentDistance': 0.8805769359927592}
episode index:187
target Thresh 34.171003760798044
target distance 24.0
model initialize at round 187
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.09125593, 11.30213083]), 'dynamicTrap': False, 'previousTarget': array([28.57960839, 11.07908508]), 'currentState': array([10.64316188,  6.63612209,  0.79206723]), 'targetState': array([33, 12], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.6833872509268538
{'scaleFactor': 20, 'currentTarget': array([33., 12.]), 'dynamicTrap': False, 'previousTarget': array([33., 12.]), 'currentState': array([32.19419673, 11.70195358,  0.62512024]), 'targetState': array([33, 12], dtype=int32), 'currentDistance': 0.85915689784641}
episode index:188
target Thresh 34.2545781510281
target distance 7.0
model initialize at round 188
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([ 5.72648292, 19.40532799,  5.66624004]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 9.091142116200045}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6847528218182514
{'scaleFactor': 20, 'currentTarget': array([11., 12.]), 'dynamicTrap': False, 'previousTarget': array([11., 12.]), 'currentState': array([10.05402627, 12.31013645,  5.35967607]), 'targetState': array([11, 12], dtype=int32), 'currentDistance': 0.9955154073209607}
episode index:189
target Thresh 34.33798555951508
target distance 31.0
model initialize at round 189
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.38950428,  5.88943472]), 'dynamicTrap': False, 'previousTarget': array([25.83555733,  5.44057325]), 'currentState': array([6.69925411, 9.39572164, 0.09635043]), 'targetState': array([37,  4], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.6854105852203725
{'scaleFactor': 20, 'currentTarget': array([37.,  4.]), 'dynamicTrap': False, 'previousTarget': array([37.,  4.]), 'currentState': array([37.71459655,  4.90824387,  5.09804053]), 'targetState': array([37,  4], dtype=int32), 'currentDistance': 1.1556622163573742}
episode index:190
target Thresh 34.42122631988877
target distance 4.0
model initialize at round 190
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([27.46251577,  7.64891248,  0.77440089]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 4.384159198870904}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6867512635668679
{'scaleFactor': 20, 'currentTarget': array([28., 12.]), 'dynamicTrap': False, 'previousTarget': array([28., 12.]), 'currentState': array([27.05629659, 12.4441613 ,  0.25589631]), 'targetState': array([28, 12], dtype=int32), 'currentDistance': 1.0430030592269892}
episode index:191
target Thresh 34.504300765112276
target distance 6.0
model initialize at round 191
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 12.]), 'dynamicTrap': False, 'previousTarget': array([31., 12.]), 'currentState': array([28.0553335 ,  5.95295183,  0.14246052]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 6.725909036510277}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6881275072456864
{'scaleFactor': 20, 'currentTarget': array([31., 12.]), 'dynamicTrap': False, 'previousTarget': array([31., 12.]), 'currentState': array([30.09575164, 12.48940678,  6.22841263]), 'targetState': array([31, 12], dtype=int32), 'currentDistance': 1.0281945747311427}
episode index:192
target Thresh 34.587209227483534
target distance 18.0
model initialize at round 192
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.73800521, 17.88907437]), 'dynamicTrap': False, 'previousTarget': array([28.36442559, 16.80368799]), 'currentState': array([14.72591919,  4.67403471,  5.97683329]), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 20.000000000000004}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6887156061176727
{'scaleFactor': 20, 'currentTarget': array([31., 19.]), 'dynamicTrap': False, 'previousTarget': array([31., 19.]), 'currentState': array([30.6734139 , 19.22212113,  2.31692766]), 'targetState': array([31, 19], dtype=int32), 'currentDistance': 0.3949636396726119}
episode index:193
target Thresh 34.66995203863648
target distance 13.0
model initialize at round 193
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([35.44147376, 13.38817581,  3.96972942]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 11.494326964846204}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.68987437746492
{'scaleFactor': 20, 'currentTarget': array([37.,  2.]), 'dynamicTrap': False, 'previousTarget': array([37.,  2.]), 'currentState': array([36.26479537,  2.24273054,  5.79126717]), 'targetState': array([37,  2], dtype=int32), 'currentDistance': 0.7742376618693908}
episode index:194
target Thresh 34.75252952954247
target distance 26.0
model initialize at round 194
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.78826315, 15.82381973]), 'dynamicTrap': False, 'previousTarget': array([23.66691212, 15.82041841]), 'currentState': array([ 5.16501668, 23.11591757,  4.76396751]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 24
reward sum = 0.7856781408072188
running average episode reward sum: 0.6903656788153933
{'scaleFactor': 20, 'currentTarget': array([31., 13.]), 'dynamicTrap': False, 'previousTarget': array([31., 13.]), 'currentState': array([30.95232336, 12.48710562,  4.54816514]), 'targetState': array([31, 13], dtype=int32), 'currentDistance': 0.5151055333664342}
episode index:195
target Thresh 34.83494203051159
target distance 25.0
model initialize at round 195
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.0816495 ,  7.39883281]), 'dynamicTrap': False, 'previousTarget': array([11.01598083,  7.20063923]), 'currentState': array([29.98684828,  9.34384214,  3.22680259]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6910585509838638
{'scaleFactor': 20, 'currentTarget': array([6., 7.]), 'dynamicTrap': False, 'previousTarget': array([6., 7.]), 'currentState': array([6.54738967, 7.2212825 , 3.05384191]), 'targetState': array([6, 7], dtype=int32), 'currentDistance': 0.5904247612916573}
episode index:196
target Thresh 34.91718987119393
target distance 14.0
model initialize at round 196
at step 0:
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'dynamicTrap': False, 'previousTarget': array([7., 7.]), 'currentState': array([11.99964533, 21.10896927,  2.11425757]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 14.968616080778975}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.6921414115119092
{'scaleFactor': 20, 'currentTarget': array([7., 7.]), 'dynamicTrap': False, 'previousTarget': array([7., 7.]), 'currentState': array([7.92997236, 7.15280883, 3.70513165]), 'targetState': array([7, 7], dtype=int32), 'currentDistance': 0.9424431717525699}
episode index:197
target Thresh 34.999273380580995
target distance 10.0
model initialize at round 197
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([7.35855840e+00, 6.53293612e+00, 7.51550998e-04]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 10.486699838326258}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.6933060745569396
{'scaleFactor': 20, 'currentTarget': array([ 8., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 17.]), 'currentState': array([ 8.64360819, 16.991422  ,  1.05569619]), 'targetState': array([ 8, 17], dtype=int32), 'currentDistance': 0.6436653475274701}
episode index:198
target Thresh 35.08119288700691
target distance 22.0
model initialize at round 198
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.29629906, 17.62666857]), 'dynamicTrap': False, 'previousTarget': array([15.3226018 , 17.57770876]), 'currentState': array([36.16924688, 15.37590315,  1.24525142]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.6940156609232369
{'scaleFactor': 20, 'currentTarget': array([13., 18.]), 'dynamicTrap': False, 'previousTarget': array([13., 18.]), 'currentState': array([12.98564481, 18.76651689,  3.1281262 ]), 'targetState': array([13, 18], dtype=int32), 'currentDistance': 0.7666513004557258}
episode index:199
target Thresh 35.1629487181498
target distance 13.0
model initialize at round 199
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'dynamicTrap': False, 'previousTarget': array([23., 20.]), 'currentState': array([ 8.96329337, 11.32575543,  1.22444892]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 16.500656105733153}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.6945537355663159
{'scaleFactor': 20, 'currentTarget': array([23., 20.]), 'dynamicTrap': False, 'previousTarget': array([23., 20.]), 'currentState': array([22.13895622, 20.02827553,  1.19023876]), 'targetState': array([23, 20], dtype=int32), 'currentDistance': 0.8615079174769807}
episode index:200
target Thresh 35.24454120103312
target distance 14.0
model initialize at round 200
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([17.0088569 , 20.13467261,  3.95762825]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 15.895678705279613}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.6953771416313657
{'scaleFactor': 20, 'currentTarget': array([ 4., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 11.]), 'currentState': array([ 4.40261571, 11.22805444,  5.31060076]), 'targetState': array([ 4, 11], dtype=int32), 'currentDistance': 0.4627183140567918}
episode index:201
target Thresh 35.32597066202692
target distance 26.0
model initialize at round 201
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.62001755,  9.8956299 ]), 'dynamicTrap': False, 'previousTarget': array([11.88441983, 10.11828302]), 'currentState': array([30.84727378, 15.40131951,  2.33101901]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.6955599447403906
{'scaleFactor': 20, 'currentTarget': array([5., 8.]), 'dynamicTrap': False, 'previousTarget': array([5., 8.]), 'currentState': array([4.5335997 , 7.68572197, 4.84943425]), 'targetState': array([5, 8], dtype=int32), 'currentDistance': 0.5624054817305465}
episode index:202
target Thresh 35.40723742684912
target distance 7.0
model initialize at round 202
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'dynamicTrap': False, 'previousTarget': array([11., 14.]), 'currentState': array([10.67502895, 21.16329754,  5.37036759]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 7.17066509432275}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.6968182211204873
{'scaleFactor': 20, 'currentTarget': array([11., 14.]), 'dynamicTrap': False, 'previousTarget': array([11., 14.]), 'currentState': array([10.37056741, 13.80028744,  5.89563439]), 'targetState': array([11, 14], dtype=int32), 'currentDistance': 0.6603563342355215}
episode index:203
target Thresh 35.48834182056691
target distance 20.0
model initialize at round 203
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'dynamicTrap': False, 'previousTarget': array([21.56953382, 18.42781353]), 'currentState': array([ 4.5342039 , 11.37666062,  5.54788965]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 19.97751057521048}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.6974522917220319
{'scaleFactor': 20, 'currentTarget': array([23., 19.]), 'dynamicTrap': False, 'previousTarget': array([23., 19.]), 'currentState': array([23.04830015, 19.34856394,  0.09735799]), 'targetState': array([23, 19], dtype=int32), 'currentDistance': 0.35189448312368704}
episode index:204
target Thresh 35.569284167597964
target distance 2.0
model initialize at round 204
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 1.63611376, 19.24612315,  4.14538437]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 2.777814054085176}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6986426715155879
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.39634263, 21.64457377,  2.45605206]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.5323676233647233}
episode index:205
target Thresh 35.650064791711785
target distance 7.0
model initialize at round 205
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35., 17.]), 'dynamicTrap': False, 'previousTarget': array([35., 17.]), 'currentState': array([28.62254536, 18.61196843,  0.27019715]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 6.578021729106453}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.6998214942237695
{'scaleFactor': 20, 'currentTarget': array([35., 17.]), 'dynamicTrap': False, 'previousTarget': array([35., 17.]), 'currentState': array([34.11914073, 17.07928755,  0.4615941 ]), 'targetState': array([35, 17], dtype=int32), 'currentDistance': 0.8844204687554577}
episode index:206
target Thresh 35.730684016030956
target distance 10.0
model initialize at round 206
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 23.]), 'dynamicTrap': False, 'previousTarget': array([33., 23.]), 'currentState': array([34.9982316 , 13.97430448,  1.78326863]), 'targetState': array([33, 23], dtype=int32), 'currentDistance': 9.244247356743829}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7009434452077464
{'scaleFactor': 20, 'currentTarget': array([33., 23.]), 'dynamicTrap': False, 'previousTarget': array([33., 23.]), 'currentState': array([33.46883595, 22.88196101,  2.73409372]), 'targetState': array([33, 23], dtype=int32), 'currentDistance': 0.4834670067722226}
episode index:207
target Thresh 35.81114216303252
target distance 18.0
model initialize at round 207
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 15.]), 'dynamicTrap': False, 'previousTarget': array([36., 15.]), 'currentState': array([19.23266562, 21.79319208,  5.17920924]), 'targetState': array([36, 15], dtype=int32), 'currentDistance': 18.091184619908347}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7018349905274982
{'scaleFactor': 20, 'currentTarget': array([36., 15.]), 'dynamicTrap': False, 'previousTarget': array([36., 15.]), 'currentState': array([36.02622474, 15.66759085,  6.20544988]), 'targetState': array([36, 15], dtype=int32), 'currentDistance': 0.6681057374279704}
episode index:208
target Thresh 35.891439554549144
target distance 2.0
model initialize at round 208
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([18.46665436,  3.30628591,  4.57645607]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.5581911428868541}
done in step count: 0
reward sum = 1.0
running average episode reward sum: 0.7032616173670796
{'scaleFactor': 20, 'currentTarget': array([18.,  3.]), 'dynamicTrap': False, 'previousTarget': array([18.,  3.]), 'currentState': array([18.46665436,  3.30628591,  4.57645607]), 'targetState': array([18,  3], dtype=int32), 'currentDistance': 0.5581911428868541}
episode index:209
target Thresh 35.97157651177051
target distance 9.0
model initialize at round 209
at step 0:
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([29.87692316, 10.52900009,  4.29059315]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 9.300334359100907}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7043959913291459
{'scaleFactor': 20, 'currentTarget': array([38.,  6.]), 'dynamicTrap': False, 'previousTarget': array([38.,  6.]), 'currentState': array([37.89524586,  6.13149104,  6.01696718]), 'targetState': array([38,  6], dtype=int32), 'currentDistance': 0.1681169910140134}
episode index:210
target Thresh 36.05155335524454
target distance 19.0
model initialize at round 210
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([21.93805105, 21.16564257,  3.95981753]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 19.06147384648946}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7052584978712643
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.71943243, 19.60684018,  2.97782031]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.6685605209693372}
episode index:211
target Thresh 36.131370404878744
target distance 20.0
model initialize at round 211
at step 0:
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([33.97504678, 22.00124766]), 'currentState': array([14.07081426, 23.03983883,  4.6596806 ]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 19.9562949758309}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.705907953982173
{'scaleFactor': 20, 'currentTarget': array([34., 22.]), 'dynamicTrap': False, 'previousTarget': array([34., 22.]), 'currentState': array([33.87345355, 21.62921281,  5.41201615]), 'targetState': array([34, 22], dtype=int32), 'currentDistance': 0.3917871172085118}
episode index:212
target Thresh 36.21102797994139
target distance 6.0
model initialize at round 212
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37., 13.]), 'dynamicTrap': False, 'previousTarget': array([37., 13.]), 'currentState': array([32.65148889,  7.10737034,  1.99399197]), 'targetState': array([37, 13], dtype=int32), 'currentDistance': 7.32343043052327}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7069259668481156
{'scaleFactor': 20, 'currentTarget': array([37., 13.]), 'dynamicTrap': False, 'previousTarget': array([37., 13.]), 'currentState': array([36.48605302, 13.62556965,  6.13122284]), 'targetState': array([37, 13], dtype=int32), 'currentDistance': 0.8096165046198623}
episode index:213
target Thresh 36.29052639906292
target distance 17.0
model initialize at round 213
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'dynamicTrap': False, 'previousTarget': array([20., 21.]), 'currentState': array([ 4.64214382, 19.42170988,  6.25397718]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 15.438741726682013}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7078063980976979
{'scaleFactor': 20, 'currentTarget': array([20., 21.]), 'dynamicTrap': False, 'previousTarget': array([20., 21.]), 'currentState': array([19.17430866, 20.3970969 ,  1.4205547 ]), 'targetState': array([20, 21], dtype=int32), 'currentDistance': 1.0223787648416793}
episode index:214
target Thresh 36.369865980237094
target distance 17.0
model initialize at round 214
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([33.26818124, 21.92591942,  4.10668707]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 18.420456202521496}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7086786392891444
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([26.28967259,  5.67458354,  5.05530894]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 0.7341479180819531}
episode index:215
target Thresh 36.44904704082236
target distance 9.0
model initialize at round 215
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([32.11101021, 12.59614871,  6.19614363]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 9.74914975800181}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7095428041732629
{'scaleFactor': 20, 'currentTarget': array([26.,  5.]), 'dynamicTrap': False, 'previousTarget': array([26.,  5.]), 'currentState': array([26.96653441,  4.4476011 ,  4.44100177]), 'targetState': array([26,  5], dtype=int32), 'currentDistance': 1.1132534790363104}
episode index:216
target Thresh 36.528069897543034
target distance 17.0
model initialize at round 216
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'dynamicTrap': False, 'previousTarget': array([19., 23.]), 'currentState': array([23.92007987,  5.47561355,  2.76941192]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 18.201958861255765}
done in step count: 39
reward sum = 0.6757290490602831
running average episode reward sum: 0.7093869804169819
{'scaleFactor': 20, 'currentTarget': array([19., 23.]), 'dynamicTrap': False, 'previousTarget': array([19., 23.]), 'currentState': array([19.98481188, 22.03089802,  1.98098974]), 'targetState': array([19, 23], dtype=int32), 'currentDistance': 1.3816703936357306}
episode index:217
target Thresh 36.60693486649068
target distance 5.0
model initialize at round 217
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 16.]), 'currentState': array([ 8.04192929, 14.949946  ,  1.35045266]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 4.17609934578301}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7105393154150691
{'scaleFactor': 20, 'currentTarget': array([ 4., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 4., 16.]), 'currentState': array([ 3.9219277 , 16.86443453,  2.95268917]), 'targetState': array([ 4, 16], dtype=int32), 'currentDistance': 0.8679529614787752}
episode index:218
target Thresh 36.68564226312527
target distance 6.0
model initialize at round 218
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 14.]), 'dynamicTrap': False, 'previousTarget': array([29., 14.]), 'currentState': array([33.29192769, 17.26072493,  3.10468477]), 'targetState': array([29, 14], dtype=int32), 'currentDistance': 5.390080744091287}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7116811268058678
{'scaleFactor': 20, 'currentTarget': array([29., 14.]), 'dynamicTrap': False, 'previousTarget': array([29., 14.]), 'currentState': array([29.76245732, 14.55140552,  2.52976602]), 'targetState': array([29, 14], dtype=int32), 'currentDistance': 0.9409512284828989}
episode index:219
target Thresh 36.764192402276485
target distance 7.0
model initialize at round 219
at step 0:
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([25.37771821, 18.51063124,  5.97076201]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 8.0125158543587}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7126828732654183
{'scaleFactor': 20, 'currentTarget': array([32., 14.]), 'dynamicTrap': False, 'previousTarget': array([32., 14.]), 'currentState': array([32.07856015, 13.45099563,  0.65988795]), 'targetState': array([32, 14], dtype=int32), 'currentDistance': 0.554596696618541}
episode index:220
target Thresh 36.842585598145
target distance 14.0
model initialize at round 220
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([1.52045013, 4.74985377, 2.85746598]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 14.326748948277228}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7135502904678771
{'scaleFactor': 20, 'currentTarget': array([ 3., 19.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 19.]), 'currentState': array([ 2.60869199, 18.69949455,  1.98801816]), 'targetState': array([ 3, 19], dtype=int32), 'currentDistance': 0.4933816801260092}
episode index:221
target Thresh 36.9208221643037
target distance 19.0
model initialize at round 221
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.60155881, 22.0361814 ]), 'dynamicTrap': False, 'previousTarget': array([15.49385478, 21.29367831]), 'currentState': array([31.73779173, 11.72359097,  2.625875  ]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.7141331413819133
{'scaleFactor': 20, 'currentTarget': array([13., 23.]), 'dynamicTrap': False, 'previousTarget': array([13., 23.]), 'currentState': array([12.77396648, 23.41452457,  3.16365227]), 'targetState': array([13, 23], dtype=int32), 'currentDistance': 0.4721459235433928}
episode index:222
target Thresh 36.99890241369895
target distance 9.0
model initialize at round 222
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 23.]), 'currentState': array([16.32302824, 21.85803589,  2.21604633]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 7.411533220188756}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7151952799851335
{'scaleFactor': 20, 'currentTarget': array([ 9., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 23.]), 'currentState': array([ 9.01702188, 23.24101024,  1.6511932 ]), 'targetState': array([ 9, 23], dtype=int32), 'currentDistance': 0.2416105991947809}
episode index:223
target Thresh 37.076826658651854
target distance 14.0
model initialize at round 223
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 18.]), 'currentState': array([17.4473602 , 22.64938259,  3.75545812]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 13.287344900620095}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7160806459114661
{'scaleFactor': 20, 'currentTarget': array([ 5., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 18.]), 'currentState': array([ 4.75690621, 18.25145523,  5.35927767]), 'targetState': array([ 5, 18], dtype=int32), 'currentDistance': 0.3497489447526484}
episode index:224
target Thresh 37.15459521085949
target distance 24.0
model initialize at round 224
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.29381841, 15.08726631]), 'dynamicTrap': False, 'previousTarget': array([10.35899411, 15.09400392]), 'currentState': array([26.88195602,  3.91436647,  1.63759487]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.7162188301266668
{'scaleFactor': 20, 'currentTarget': array([ 3., 20.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 20.]), 'currentState': array([ 3.36353167, 19.81460544,  2.49110816]), 'targetState': array([ 3, 20], dtype=int32), 'currentDistance': 0.4080764868599699}
episode index:225
target Thresh 37.23220838139618
target distance 28.0
model initialize at round 225
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26.93707129,  9.66126823]), 'dynamicTrap': False, 'previousTarget': array([26., 10.]), 'currentState': array([6.96003245, 8.70318725, 6.14834571]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.716668777504855
{'scaleFactor': 20, 'currentTarget': array([34., 10.]), 'dynamicTrap': False, 'previousTarget': array([34., 10.]), 'currentState': array([34.60496682, 10.26731196,  2.26007912]), 'targetState': array([34, 10], dtype=int32), 'currentDistance': 0.6613928753648961}
episode index:226
target Thresh 37.309666480714704
target distance 8.0
model initialize at round 226
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 17.]), 'dynamicTrap': False, 'previousTarget': array([29., 17.]), 'currentState': array([27.54534315,  7.91540725,  4.5466547 ]), 'targetState': array([29, 17], dtype=int32), 'currentDistance': 9.200318035247657}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7174957083308636
{'scaleFactor': 20, 'currentTarget': array([29., 17.]), 'dynamicTrap': False, 'previousTarget': array([29., 17.]), 'currentState': array([29.9698824 , 17.75522392,  3.60625949]), 'targetState': array([29, 17], dtype=int32), 'currentDistance': 1.2292416545355669}
episode index:227
target Thresh 37.38696981864757
target distance 22.0
model initialize at round 227
at step 0:
{'scaleFactor': 20, 'currentTarget': array([6.80975218, 9.23143767]), 'dynamicTrap': False, 'previousTarget': array([6.18339664, 9.29773591]), 'currentState': array([26.74224827, 10.87326587,  4.2848983 ]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.7178647209677417
{'scaleFactor': 20, 'currentTarget': array([4., 9.]), 'dynamicTrap': False, 'previousTarget': array([4., 9.]), 'currentState': array([3.79945666, 8.75892825, 3.03371362]), 'targetState': array([4, 9], dtype=int32), 'currentDistance': 0.31358127596400864}
episode index:228
target Thresh 37.46411870440822
target distance 22.0
model initialize at round 228
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.75841136, 20.01233194]), 'dynamicTrap': False, 'previousTarget': array([35.9793708 , 20.09184678]), 'currentState': array([17.78441666, 21.03190815,  5.47563637]), 'targetState': array([38, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7185235903642537
{'scaleFactor': 20, 'currentTarget': array([38., 20.]), 'dynamicTrap': False, 'previousTarget': array([38., 20.]), 'currentState': array([37.29236712, 19.58288043,  6.11707503]), 'targetState': array([38, 20], dtype=int32), 'currentDistance': 0.8214213483536085}
episode index:229
target Thresh 37.541113446592306
target distance 19.0
model initialize at round 229
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([29.89729814, 14.8769266 ,  1.01315784]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 18.9306410541791}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.7191015650630825
{'scaleFactor': 20, 'currentTarget': array([11., 16.]), 'dynamicTrap': False, 'previousTarget': array([11., 16.]), 'currentState': array([11.16459843, 15.18661367,  3.03164664]), 'targetState': array([11, 16], dtype=int32), 'currentDistance': 0.8298734662748098}
episode index:230
target Thresh 37.6179543531789
target distance 24.0
model initialize at round 230
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.02956846, 21.31895051]), 'dynamicTrap': False, 'previousTarget': array([20.72658355, 21.02246883]), 'currentState': array([ 3.61230733, 13.52122697,  0.72134465]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7196011849608619
{'scaleFactor': 20, 'currentTarget': array([26., 23.]), 'dynamicTrap': False, 'previousTarget': array([26., 23.]), 'currentState': array([25.60644034, 22.74604049,  5.0814334 ]), 'targetState': array([26, 23], dtype=int32), 'currentDistance': 0.46838514366898615}
episode index:231
target Thresh 37.69464173153173
target distance 20.0
model initialize at round 231
at step 0:
{'scaleFactor': 20, 'currentTarget': array([23.14669281,  5.49236935]), 'dynamicTrap': False, 'previousTarget': array([23.361625  ,  5.47568183]), 'currentState': array([31.50602582, 23.66161676,  3.67312396]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.7201695323148878
{'scaleFactor': 20, 'currentTarget': array([22.,  3.]), 'dynamicTrap': False, 'previousTarget': array([22.,  3.]), 'currentState': array([22.05430373,  2.1795151 ,  2.7692005 ]), 'targetState': array([22,  3], dtype=int32), 'currentDistance': 0.8222799837347872}
episode index:232
target Thresh 37.771175888400414
target distance 20.0
model initialize at round 232
at step 0:
{'scaleFactor': 20, 'currentTarget': array([37.53810441, 22.96164229]), 'dynamicTrap': False, 'previousTarget': array([38., 23.]), 'currentState': array([17.606713  , 21.30645764,  5.39711165]), 'targetState': array([38, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.720807198754605
{'scaleFactor': 20, 'currentTarget': array([38., 23.]), 'dynamicTrap': False, 'previousTarget': array([38., 23.]), 'currentState': array([38.56066401, 22.45429583,  6.27890158]), 'targetState': array([38, 23], dtype=int32), 'currentDistance': 0.782391954848883}
episode index:233
target Thresh 37.84755712992168
target distance 14.0
model initialize at round 233
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'dynamicTrap': False, 'previousTarget': array([24.,  6.]), 'currentState': array([11.66476374, 19.35974068,  0.46037727]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 18.183528939572373}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.7209198692485238
{'scaleFactor': 20, 'currentTarget': array([24.,  6.]), 'dynamicTrap': False, 'previousTarget': array([24.,  6.]), 'currentState': array([24.87215339,  5.59608243,  5.2835009 ]), 'targetState': array([24,  6], dtype=int32), 'currentDistance': 0.9611456413369094}
episode index:234
target Thresh 37.92378576162059
target distance 3.0
model initialize at round 234
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'dynamicTrap': False, 'previousTarget': array([19.,  2.]), 'currentState': array([18.38756067,  3.98339269,  0.3137601 ]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 2.0757958698803316}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7220227634219343
{'scaleFactor': 20, 'currentTarget': array([19.,  2.]), 'dynamicTrap': False, 'previousTarget': array([19.,  2.]), 'currentState': array([18.43352503,  1.65302794,  3.77094138]), 'targetState': array([19,  2], dtype=int32), 'currentDistance': 0.6642917335969453}
episode index:235
target Thresh 37.99986208841179
target distance 11.0
model initialize at round 235
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'dynamicTrap': False, 'previousTarget': array([26., 14.]), 'currentState': array([35.96356026, 13.34560507,  3.21083057]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 9.985027081068324}
done in step count: 29
reward sum = 0.7471720943315961
running average episode reward sum: 0.7221293283834159
{'scaleFactor': 20, 'currentTarget': array([26., 14.]), 'dynamicTrap': False, 'previousTarget': array([26., 14.]), 'currentState': array([25.21163077, 14.65038932,  5.12070863]), 'targetState': array([26, 14], dtype=int32), 'currentDistance': 1.0220236345557743}
episode index:236
target Thresh 38.07578641460067
target distance 2.0
model initialize at round 236
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 21.]), 'dynamicTrap': False, 'previousTarget': array([29., 21.]), 'currentState': array([28.52056757, 18.52490766,  3.33048379]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 2.521098483424743}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7231355169134437
{'scaleFactor': 20, 'currentTarget': array([29., 21.]), 'dynamicTrap': False, 'previousTarget': array([29., 21.]), 'currentState': array([29.70841265, 21.1958588 ,  2.62697852]), 'targetState': array([29, 21], dtype=int32), 'currentDistance': 0.7349892190735658}
episode index:237
target Thresh 38.15155904388464
target distance 25.0
model initialize at round 237
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31.05050461, 14.65360863]), 'dynamicTrap': False, 'previousTarget': array([30.61161351, 14.9223227 ]), 'currentState': array([11.75179676,  9.40385859,  6.0175904 ]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 20.0}
done in step count: 21
reward sum = 0.8097278682212584
running average episode reward sum: 0.7234993503223
{'scaleFactor': 20, 'currentTarget': array([36., 16.]), 'dynamicTrap': False, 'previousTarget': array([36., 16.]), 'currentState': array([36.32629862, 15.45865461,  2.47293248]), 'targetState': array([36, 16], dtype=int32), 'currentDistance': 0.632080392366276}
episode index:238
target Thresh 38.22718027935432
target distance 8.0
model initialize at round 238
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([29.91959113, 11.28394533,  5.61548901]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 6.929067848149864}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.724491386555261
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([27.56106586,  5.13570519,  4.07337195]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.5772441435401697}
episode index:239
target Thresh 38.302650423494754
target distance 30.0
model initialize at round 239
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.77079867, 10.76246902]), 'dynamicTrap': False, 'previousTarget': array([14.11145618, 10.94427191]), 'currentState': array([32.74179978,  1.98504804,  0.9898464 ]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 20.0}
done in step count: 28
reward sum = 0.7547192872036326
running average episode reward sum: 0.724617336141296
{'scaleFactor': 20, 'currentTarget': array([ 2., 17.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 17.]), 'currentState': array([ 2.01845933, 16.25999924,  2.26533928]), 'targetState': array([ 2, 17], dtype=int32), 'currentDistance': 0.7402309539966102}
episode index:240
target Thresh 38.37796977818664
target distance 3.0
model initialize at round 240
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'dynamicTrap': False, 'previousTarget': array([13., 15.]), 'currentState': array([11.22320187, 14.54044852,  0.65061968]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 1.835265423838153}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.7248058249780754
{'scaleFactor': 20, 'currentTarget': array([13., 15.]), 'dynamicTrap': False, 'previousTarget': array([13., 15.]), 'currentState': array([12.39549352, 15.38859169,  4.38298998]), 'targetState': array([13, 15], dtype=int32), 'currentDistance': 0.7186317409091227}
episode index:241
target Thresh 38.45313864470746
target distance 7.0
model initialize at round 241
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 13.]), 'dynamicTrap': False, 'previousTarget': array([36., 13.]), 'currentState': array([30.62548571, 13.43610124,  5.53530246]), 'targetState': array([36, 13], dtype=int32), 'currentDistance': 5.392178424579606}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.725780164585604
{'scaleFactor': 20, 'currentTarget': array([36., 13.]), 'dynamicTrap': False, 'previousTarget': array([36., 13.]), 'currentState': array([35.32859149, 12.69237657,  0.58522117]), 'targetState': array([36, 13], dtype=int32), 'currentDistance': 0.7385266175272689}
episode index:242
target Thresh 38.528157323732785
target distance 4.0
model initialize at round 242
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([ 6.83746695, 15.602412  ,  0.9333725 ]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 3.2287733589777154}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7267864149371036
{'scaleFactor': 20, 'currentTarget': array([ 9., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 9., 18.]), 'currentState': array([ 8.58645659, 18.6396346 ,  1.84158137]), 'targetState': array([ 9, 18], dtype=int32), 'currentDistance': 0.7616761545346433}
episode index:243
target Thresh 38.60302611533746
target distance 16.0
model initialize at round 243
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.88591183, 16.94207358]), 'dynamicTrap': False, 'previousTarget': array([30.05153389, 17.17009216]), 'currentState': array([15.38295065,  3.17021398,  6.15476942]), 'targetState': array([31, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7273326114113011
{'scaleFactor': 20, 'currentTarget': array([31., 18.]), 'dynamicTrap': False, 'previousTarget': array([31., 18.]), 'currentState': array([30.35003932, 17.57445056,  6.24167204]), 'targetState': array([31, 18], dtype=int32), 'currentDistance': 0.776879145143399}
episode index:244
target Thresh 38.67774531899671
target distance 27.0
model initialize at round 244
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.66912118,  7.95579112]), 'dynamicTrap': False, 'previousTarget': array([24.,  8.]), 'currentState': array([5.66980888, 7.78993717, 0.88485688]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7277700854930921
{'scaleFactor': 20, 'currentTarget': array([31.,  8.]), 'dynamicTrap': False, 'previousTarget': array([31.,  8.]), 'currentState': array([30.91229884,  7.40885013,  1.56572222]), 'targetState': array([31,  8], dtype=int32), 'currentDistance': 0.5976200021967386}
episode index:245
target Thresh 38.752315233587495
target distance 10.0
model initialize at round 245
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'dynamicTrap': False, 'previousTarget': array([18., 22.]), 'currentState': array([ 8.51238265, 15.60307583,  0.25143385]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 11.442706057500699}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7285251552572813
{'scaleFactor': 20, 'currentTarget': array([18., 22.]), 'dynamicTrap': False, 'previousTarget': array([18., 22.]), 'currentState': array([18.16894348, 21.2479301 ,  0.93619144]), 'targetState': array([18, 22], dtype=int32), 'currentDistance': 0.7708119366749188}
episode index:246
target Thresh 38.82673615738955
target distance 19.0
model initialize at round 246
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.09959134,  6.35723465]), 'dynamicTrap': False, 'previousTarget': array([22.85786438,  6.85786438]), 'currentState': array([35.80451787, 20.92346232,  4.18988848]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 20.0}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.7289204729438332
{'scaleFactor': 20, 'currentTarget': array([18.,  2.]), 'dynamicTrap': False, 'previousTarget': array([18.,  2.]), 'currentState': array([18.10465835,  2.52985119,  2.09844208]), 'targetState': array([18,  2], dtype=int32), 'currentDistance': 0.5400885557783396}
episode index:247
target Thresh 38.901008388086666
target distance 10.0
model initialize at round 247
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([12.15684741,  8.54346926,  3.17764568]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 11.808797783884174}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7295915123846191
{'scaleFactor': 20, 'currentTarget': array([ 3., 16.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 16.]), 'currentState': array([ 2.77880299, 16.67774959,  3.58409381]), 'targetState': array([ 3, 16], dtype=int32), 'currentDistance': 0.712932415520643}
episode index:248
target Thresh 38.975132222767876
target distance 9.0
model initialize at round 248
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'dynamicTrap': False, 'previousTarget': array([24., 22.]), 'currentState': array([16.59103714, 22.50763214,  5.64677274]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 7.426332952536161}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7304046603184439
{'scaleFactor': 20, 'currentTarget': array([24., 22.]), 'dynamicTrap': False, 'previousTarget': array([24., 22.]), 'currentState': array([23.24568379, 21.70713308,  1.3905782 ]), 'targetState': array([24, 22], dtype=int32), 'currentDistance': 0.8091748732926961}
episode index:249
target Thresh 39.0491079579286
target distance 17.0
model initialize at round 249
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'dynamicTrap': False, 'previousTarget': array([27., 10.]), 'currentState': array([11.36538311, 19.69789675,  5.82986957]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 18.398109863282805}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.7308888727615496
{'scaleFactor': 20, 'currentTarget': array([27., 10.]), 'dynamicTrap': False, 'previousTarget': array([27., 10.]), 'currentState': array([27.98535785, 10.07058001,  5.15878743]), 'targetState': array([27, 10], dtype=int32), 'currentDistance': 0.9878823958437452}
episode index:250
target Thresh 39.12293588947188
target distance 13.0
model initialize at round 250
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'dynamicTrap': False, 'previousTarget': array([20.,  4.]), 'currentState': array([ 6.78585925, 11.33070918,  5.59480333]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 15.11134715431828}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7314381035982326
{'scaleFactor': 20, 'currentTarget': array([20.,  4.]), 'dynamicTrap': False, 'previousTarget': array([20.,  4.]), 'currentState': array([20.21849874,  3.3441723 ,  1.9700157 ]), 'targetState': array([20,  4], dtype=int32), 'currentDistance': 0.6912681650282566}
episode index:251
target Thresh 39.196616312709565
target distance 11.0
model initialize at round 251
at step 0:
{'scaleFactor': 20, 'currentTarget': array([31., 17.]), 'dynamicTrap': False, 'previousTarget': array([31., 17.]), 'currentState': array([20.16668786, 19.42733149,  5.73217869]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 11.101918304553013}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7321972567364458
{'scaleFactor': 20, 'currentTarget': array([31., 17.]), 'dynamicTrap': False, 'previousTarget': array([31., 17.]), 'currentState': array([31.30800379, 17.68217095,  5.64220321]), 'targetState': array([31, 17], dtype=int32), 'currentDistance': 0.7484808184584677}
episode index:252
target Thresh 39.270149522363425
target distance 21.0
model initialize at round 252
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13.60752932,  9.66840786]), 'dynamicTrap': False, 'previousTarget': array([14.94278962, 10.40132839]), 'currentState': array([32.07475647, 17.34704844,  3.62867641]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 32
reward sum = 0.7249803359578534
running average episode reward sum: 0.7321687313578743
{'scaleFactor': 20, 'currentTarget': array([12.,  9.]), 'dynamicTrap': False, 'previousTarget': array([12.,  9.]), 'currentState': array([12.07709381,  8.32900022,  0.86361163]), 'targetState': array([12,  9], dtype=int32), 'currentDistance': 0.675414066919799}
episode index:253
target Thresh 39.34353581256641
target distance 16.0
model initialize at round 253
at step 0:
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'dynamicTrap': False, 'previousTarget': array([20.,  3.]), 'currentState': array([ 9.65562161, 19.08558276,  5.48486168]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 19.124647370643817}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.7326048512871106
{'scaleFactor': 20, 'currentTarget': array([20.,  3.]), 'dynamicTrap': False, 'previousTarget': array([20.,  3.]), 'currentState': array([19.70142749,  2.55644197,  1.04029399]), 'targetState': array([20,  3], dtype=int32), 'currentDistance': 0.534686144762985}
episode index:254
target Thresh 39.41677547686375
target distance 7.0
model initialize at round 254
at step 0:
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([27.87480108,  5.67830668,  0.63525677]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 7.24674636782408}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7334612638306907
{'scaleFactor': 20, 'currentTarget': array([35.,  7.]), 'dynamicTrap': False, 'previousTarget': array([35.,  7.]), 'currentState': array([34.70214175,  6.4968243 ,  0.1234628 ]), 'targetState': array([35,  7], dtype=int32), 'currentDistance': 0.5847267088207305}
episode index:255
target Thresh 39.48986880821423
target distance 35.0
model initialize at round 255
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.35017895, 21.18073557]), 'dynamicTrap': False, 'previousTarget': array([22.96742669, 20.85900419]), 'currentState': array([ 4.42458744, 22.90433727,  5.84623427]), 'targetState': array([38, 20], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.7337911297438412
{'scaleFactor': 20, 'currentTarget': array([38., 20.]), 'dynamicTrap': False, 'previousTarget': array([38., 20.]), 'currentState': array([37.00415436, 19.45235138,  1.08382082]), 'targetState': array([38, 20], dtype=int32), 'currentDistance': 1.136497933054332}
episode index:256
target Thresh 39.56281609899126
target distance 23.0
model initialize at round 256
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.76591215,  3.90265196]), 'dynamicTrap': False, 'previousTarget': array([13.,  4.]), 'currentState': array([34.76174128,  3.49421917,  0.44481426]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 20.0}
done in step count: 16
reward sum = 0.8514577710948755
running average episode reward sum: 0.7342489765973472
{'scaleFactor': 20, 'currentTarget': array([10.,  4.]), 'dynamicTrap': False, 'previousTarget': array([10.,  4.]), 'currentState': array([10.3435105 ,  3.43232792,  4.48869818]), 'targetState': array([10,  4], dtype=int32), 'currentDistance': 0.6635141657012834}
episode index:257
target Thresh 39.635617640984115
target distance 3.0
model initialize at round 257
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'dynamicTrap': False, 'previousTarget': array([19., 22.]), 'currentState': array([18.91352048, 19.41198703,  1.95226264]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 2.5894574393517606}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.7352402596337916
{'scaleFactor': 20, 'currentTarget': array([19., 22.]), 'dynamicTrap': False, 'previousTarget': array([19., 22.]), 'currentState': array([19.59147527, 21.08616452,  0.40432143]), 'targetState': array([19, 22], dtype=int32), 'currentDistance': 1.0885487044347557}
episode index:258
target Thresh 39.70827372539903
target distance 10.0
model initialize at round 258
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  3.]), 'dynamicTrap': False, 'previousTarget': array([28.,  3.]), 'currentState': array([36.41667194, 11.57049131,  3.80576134]), 'targetState': array([28,  3], dtype=int32), 'currentDistance': 12.012230761079424}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7358933168360118
{'scaleFactor': 20, 'currentTarget': array([28.,  3.]), 'dynamicTrap': False, 'previousTarget': array([28.,  3.]), 'currentState': array([28.41913338,  2.42686893,  4.04602635]), 'targetState': array([28,  3], dtype=int32), 'currentDistance': 0.710036621693624}
episode index:259
target Thresh 39.78078464286047
target distance 33.0
model initialize at round 259
at step 0:
{'scaleFactor': 20, 'currentTarget': array([21.52749314,  7.19313398]), 'dynamicTrap': False, 'previousTarget': array([21.91786413,  7.81071492]), 'currentState': array([1.70497098, 4.5346353 , 5.36386895]), 'targetState': array([35,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.7360246623320469
{'scaleFactor': 20, 'currentTarget': array([35.,  9.]), 'dynamicTrap': False, 'previousTarget': array([35.,  9.]), 'currentState': array([34.93551882,  8.34595147,  0.04356861]), 'targetState': array([35,  9], dtype=int32), 'currentDistance': 0.6572193725879565}
episode index:260
target Thresh 39.85315068341218
target distance 19.0
model initialize at round 260
at step 0:
{'scaleFactor': 20, 'currentTarget': array([12.30679178,  7.24945996]), 'dynamicTrap': False, 'previousTarget': array([11.49385478,  6.70632169]), 'currentState': array([28.8433647 , 18.49853797,  1.50635093]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.7361550013491852
{'scaleFactor': 20, 'currentTarget': array([9., 5.]), 'dynamicTrap': False, 'previousTarget': array([9., 5.]), 'currentState': array([8.0024713 , 4.99261002, 5.79751408]), 'targetState': array([9, 5], dtype=int32), 'currentDistance': 0.9975560755292406}
episode index:261
target Thresh 39.92537213651844
target distance 31.0
model initialize at round 261
at step 0:
{'scaleFactor': 20, 'currentTarget': array([24.53844582, 19.75893953]), 'dynamicTrap': False, 'previousTarget': array([23.74482241, 19.18464878]), 'currentState': array([ 4.67770981, 17.40284845,  1.88756248]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.7362549544522308
{'scaleFactor': 20, 'currentTarget': array([35., 21.]), 'dynamicTrap': False, 'previousTarget': array([35., 21.]), 'currentState': array([35.55136952, 21.13317488,  0.39429241]), 'targetState': array([35, 21], dtype=int32), 'currentDistance': 0.567224733965025}
episode index:262
target Thresh 39.99744929106513
target distance 22.0
model initialize at round 262
at step 0:
{'scaleFactor': 20, 'currentTarget': array([8.19868946, 9.7176032 ]), 'dynamicTrap': False, 'previousTarget': array([6.78146944, 8.82541376]), 'currentState': array([25.35051364, 20.00424151,  1.6493923 ]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7366285620834013
{'scaleFactor': 20, 'currentTarget': array([2., 6.]), 'dynamicTrap': False, 'previousTarget': array([2., 6.]), 'currentState': array([2.92178646, 6.20353761, 5.25766038]), 'targetState': array([2, 6], dtype=int32), 'currentDistance': 0.9439903758830462}
episode index:263
target Thresh 40.069382435361
target distance 9.0
model initialize at round 263
at step 0:
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([ 2.40272885, 18.66917846,  0.14468408]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 11.70542603855248}
done in step count: 9
reward sum = 0.9135172474836408
running average episode reward sum: 0.7372985949826447
{'scaleFactor': 20, 'currentTarget': array([9., 9.]), 'dynamicTrap': False, 'previousTarget': array([9., 9.]), 'currentState': array([9.21480824, 8.26402857, 0.79128814]), 'targetState': array([9, 9], dtype=int32), 'currentDistance': 0.766678896357234}
episode index:264
target Thresh 40.141171857138694
target distance 7.0
model initialize at round 264
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.,  4.]), 'dynamicTrap': False, 'previousTarget': array([36.,  4.]), 'currentState': array([35.07813928,  9.70771374,  4.39769057]), 'targetState': array([36,  4], dtype=int32), 'currentDistance': 5.781679975861094}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7381049778313895
{'scaleFactor': 20, 'currentTarget': array([36.,  4.]), 'dynamicTrap': False, 'previousTarget': array([36.,  4.]), 'currentState': array([36.3342376 ,  4.21775126,  5.04196051]), 'targetState': array([36,  4], dtype=int32), 'currentDistance': 0.3989114993215724}
episode index:265
target Thresh 40.212817843556
target distance 34.0
model initialize at round 265
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.07023634,  7.30653458]), 'dynamicTrap': False, 'previousTarget': array([16.13698791,  7.33682495]), 'currentState': array([35.92693096,  4.91662266,  1.80857867]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 20.0}
done in step count: 76
reward sum = 0.46588077516979337
running average episode reward sum: 0.7370815785732632
{'scaleFactor': 20, 'currentTarget': array([2., 9.]), 'dynamicTrap': False, 'previousTarget': array([2., 9.]), 'currentState': array([2.54155323, 8.78300413, 1.95814568]), 'targetState': array([2, 9], dtype=int32), 'currentDistance': 0.5834098955740868}
episode index:266
target Thresh 40.284320681196974
target distance 6.0
model initialize at round 266
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.,  9.]), 'dynamicTrap': False, 'previousTarget': array([28.,  9.]), 'currentState': array([27.35929851,  4.64416929,  2.36564964]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 4.4026991199980126}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.7379917599269213
{'scaleFactor': 20, 'currentTarget': array([28.,  9.]), 'dynamicTrap': False, 'previousTarget': array([28.,  9.]), 'currentState': array([27.70023668,  8.09385435,  2.40601617]), 'targetState': array([28,  9], dtype=int32), 'currentDistance': 0.9544411886932472}
episode index:267
target Thresh 40.35568065607305
target distance 11.0
model initialize at round 267
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([35.73470274,  8.47868568,  2.68622482]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 11.662403159230857}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7386811365481938
{'scaleFactor': 20, 'currentTarget': array([29., 18.]), 'dynamicTrap': False, 'previousTarget': array([29., 18.]), 'currentState': array([29.41663371, 17.71272629,  1.12242162]), 'targetState': array([29, 18], dtype=int32), 'currentDistance': 0.5060729561448682}
episode index:268
target Thresh 40.42689805362422
target distance 24.0
model initialize at round 268
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.58340627,  9.14989102]), 'dynamicTrap': False, 'previousTarget': array([28.46153846,  8.30769231]), 'currentState': array([ 9.62997307, 17.96318985,  1.83995619]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.7389151493845911
{'scaleFactor': 20, 'currentTarget': array([34.,  6.]), 'dynamicTrap': False, 'previousTarget': array([34.,  6.]), 'currentState': array([33.46827355,  6.46249834,  4.12166882]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 0.7047252898549575}
episode index:269
target Thresh 40.49797315872017
target distance 18.0
model initialize at round 269
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6.06386567, 15.28291878]), 'dynamicTrap': False, 'previousTarget': array([ 6.05181363, 15.28727678]), 'currentState': array([21.02745336,  2.01298874,  1.41448801]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.739269218318167
{'scaleFactor': 20, 'currentTarget': array([ 3., 18.]), 'dynamicTrap': False, 'previousTarget': array([ 3., 18.]), 'currentState': array([ 3.90122773, 17.78338076,  1.73990273]), 'targetState': array([ 3, 18], dtype=int32), 'currentDistance': 0.9268955232583842}
episode index:270
target Thresh 40.56890625566143
target distance 29.0
model initialize at round 270
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.29867723, 14.07574823]), 'dynamicTrap': False, 'previousTarget': array([28.81242258, 14.26725206]), 'currentState': array([10.49098262, 16.84255766,  5.16797883]), 'targetState': array([38, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 26
reward sum = 0.7700431458051551
running average episode reward sum: 0.7393827752461632
{'scaleFactor': 20, 'currentTarget': array([38., 13.]), 'dynamicTrap': False, 'previousTarget': array([38., 13.]), 'currentState': array([37.47889524, 12.78701323,  3.72443012]), 'targetState': array([38, 13], dtype=int32), 'currentDistance': 0.5629507437484265}
episode index:271
target Thresh 40.639697628180464
target distance 5.0
model initialize at round 271
at step 0:
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([27.21069644, 21.60782573,  4.7311064 ]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 3.8055475799514586}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7402317319548172
{'scaleFactor': 20, 'currentTarget': array([26., 18.]), 'dynamicTrap': False, 'previousTarget': array([26., 18.]), 'currentState': array([25.81444643, 18.93905248,  4.42478418]), 'targetState': array([26, 18], dtype=int32), 'currentDistance': 0.9572093242783571}
episode index:272
target Thresh 40.71034755944286
target distance 26.0
model initialize at round 272
at step 0:
{'scaleFactor': 20, 'currentTarget': array([28.4171762 , 17.60749231]), 'dynamicTrap': False, 'previousTarget': array([29.48782391, 17.49719013]), 'currentState': array([ 8.74611495, 13.995104  ,  3.48075616]), 'targetState': array([36, 19], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.7403694228978367
{'scaleFactor': 20, 'currentTarget': array([36., 19.]), 'dynamicTrap': False, 'previousTarget': array([36., 19.]), 'currentState': array([35.34154634, 19.11365142,  0.39693802]), 'targetState': array([36, 19], dtype=int32), 'currentDistance': 0.6681899906798455}
episode index:273
target Thresh 40.780856332048444
target distance 21.0
model initialize at round 273
at step 0:
{'scaleFactor': 20, 'currentTarget': array([30.69683898,  7.86656367]), 'dynamicTrap': False, 'previousTarget': array([31.05721038,  7.40132839]), 'currentState': array([13.28458786, 17.70594937,  2.33482158]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 20.0}
done in step count: 25
reward sum = 0.7778213593991467
running average episode reward sum: 0.7405061087974766
{'scaleFactor': 20, 'currentTarget': array([34.,  6.]), 'dynamicTrap': False, 'previousTarget': array([34.,  6.]), 'currentState': array([33.8133328 ,  6.16827933,  1.02578985]), 'targetState': array([34,  6], dtype=int32), 'currentDistance': 0.251321654027547}
episode index:274
target Thresh 40.85122422803241
target distance 28.0
model initialize at round 274
at step 0:
{'scaleFactor': 20, 'currentTarget': array([14.45924626, 19.80062798]), 'dynamicTrap': False, 'previousTarget': array([13.31144849, 20.51581277]), 'currentState': array([33.93961246, 15.27124253,  4.89460039]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 27
reward sum = 0.7623427143471035
running average episode reward sum: 0.7405855146358389
{'scaleFactor': 20, 'currentTarget': array([ 5., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 5., 22.]), 'currentState': array([ 4.83329779, 21.74732542,  1.16765838]), 'targetState': array([ 5, 22], dtype=int32), 'currentDistance': 0.3027111979781682}
episode index:275
target Thresh 40.92145152886641
target distance 3.0
model initialize at round 275
at step 0:
{'scaleFactor': 20, 'currentTarget': array([29.,  6.]), 'dynamicTrap': False, 'previousTarget': array([29.,  6.]), 'currentState': array([30.04027914,  3.00947567,  1.16126817]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 3.1662938051497855}
done in step count: 3
reward sum = 0.970299
running average episode reward sum: 0.7414178098726656
{'scaleFactor': 20, 'currentTarget': array([29.,  6.]), 'dynamicTrap': False, 'previousTarget': array([29.,  6.]), 'currentState': array([29.20475565,  5.37547696,  0.77796048]), 'targetState': array([29,  6], dtype=int32), 'currentDistance': 0.6572320026000321}
episode index:276
target Thresh 40.991538515459766
target distance 5.0
model initialize at round 276
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 18.]), 'dynamicTrap': False, 'previousTarget': array([36., 18.]), 'currentState': array([37.18050436, 22.20734396,  4.74511701]), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 4.3698207883848115}
done in step count: 6
reward sum = 0.941480149401
running average episode reward sum: 0.7421400565857642
{'scaleFactor': 20, 'currentTarget': array([36., 18.]), 'dynamicTrap': False, 'previousTarget': array([36., 18.]), 'currentState': array([35.10567787, 18.29007146,  6.24857849]), 'targetState': array([36, 18], dtype=int32), 'currentDistance': 0.9401880276553264}
episode index:277
target Thresh 41.061485468160484
target distance 15.0
model initialize at round 277
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.48564107,  9.44972324]), 'dynamicTrap': False, 'previousTarget': array([33.62110536,  9.35363499]), 'currentState': array([18.42916402, 22.61416393,  4.74597001]), 'targetState': array([34,  9], dtype=int32), 'currentDistance': 20.0}
done in step count: 14
reward sum = 0.8687458127689782
running average episode reward sum: 0.7425954729749126
{'scaleFactor': 20, 'currentTarget': array([34.,  9.]), 'dynamicTrap': False, 'previousTarget': array([34.,  9.]), 'currentState': array([34.59182946,  9.60814889,  4.54742125]), 'targetState': array([34,  9], dtype=int32), 'currentDistance': 0.8485912940443754}
episode index:278
target Thresh 41.13129266675651
target distance 23.0
model initialize at round 278
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.24430174,  9.63883074]), 'dynamicTrap': False, 'previousTarget': array([31.54352728,  9.24859289]), 'currentState': array([13.65450433,  5.60896257,  5.76834006]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 20
reward sum = 0.8179069375972308
running average episode reward sum: 0.7428654065398671
{'scaleFactor': 20, 'currentTarget': array([35., 10.]), 'dynamicTrap': False, 'previousTarget': array([35., 10.]), 'currentState': array([35.93279097,  9.39028613,  0.59540319]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 1.1143832352456235}
episode index:279
target Thresh 41.200960390476695
target distance 3.0
model initialize at round 279
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33., 22.]), 'dynamicTrap': False, 'previousTarget': array([33., 22.]), 'currentState': array([31.54219673, 20.29698651,  0.5621677 ]), 'targetState': array([33, 22], dtype=int32), 'currentDistance': 2.241750506795227}
done in step count: 1
reward sum = 0.99
running average episode reward sum: 0.743748030087939
{'scaleFactor': 20, 'currentTarget': array([33., 22.]), 'dynamicTrap': False, 'previousTarget': array([33., 22.]), 'currentState': array([33.26199323, 21.31744442,  0.50833924]), 'targetState': array([33, 22], dtype=int32), 'currentDistance': 0.7311105023043231}
episode index:280
target Thresh 41.270488917992054
target distance 7.0
model initialize at round 280
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([4.18749016, 7.56932515, 1.18898118]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 6.635319182555272}
done in step count: 5
reward sum = 0.9509900498999999
running average episode reward sum: 0.7444855461726795
{'scaleFactor': 20, 'currentTarget': array([ 8., 13.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 13.]), 'currentState': array([ 7.80310833, 12.06126814,  1.61933863]), 'targetState': array([ 8, 13], dtype=int32), 'currentDistance': 0.9591578825074496}
episode index:281
target Thresh 41.33987852741677
target distance 10.0
model initialize at round 281
at step 0:
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'dynamicTrap': False, 'previousTarget': array([16.,  4.]), 'currentState': array([5.72261832, 6.37241036, 1.79978502]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 10.547649267243786}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7451507227745742
{'scaleFactor': 20, 'currentTarget': array([16.,  4.]), 'dynamicTrap': False, 'previousTarget': array([16.,  4.]), 'currentState': array([16.06901003,  3.69145412,  5.96199835]), 'targetState': array([16,  4], dtype=int32), 'currentDistance': 0.316169166231668}
episode index:282
target Thresh 41.40912949630939
target distance 12.0
model initialize at round 282
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36.,  8.]), 'dynamicTrap': False, 'previousTarget': array([36.,  8.]), 'currentState': array([31.35567186, 19.75007432,  5.0908767 ]), 'targetState': array([36,  8], dtype=int32), 'currentDistance': 12.634636142417985}
done in step count: 8
reward sum = 0.9227446944279201
running average episode reward sum: 0.7457782633104518
{'scaleFactor': 20, 'currentTarget': array([36.,  8.]), 'dynamicTrap': False, 'previousTarget': array([36.,  8.]), 'currentState': array([35.15676719,  8.85373765,  1.04101589]), 'targetState': array([36,  8], dtype=int32), 'currentDistance': 1.1999623150218992}
episode index:283
target Thresh 41.47824210167387
target distance 3.0
model initialize at round 283
at step 0:
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([27.60752285,  3.63656646,  2.81616867]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 1.4926603891925878}
done in step count: 2
reward sum = 0.9801
running average episode reward sum: 0.746603339848091
{'scaleFactor': 20, 'currentTarget': array([27.,  5.]), 'dynamicTrap': False, 'previousTarget': array([27.,  5.]), 'currentState': array([26.93126285,  4.27250171,  2.61667049]), 'targetState': array([27,  5], dtype=int32), 'currentDistance': 0.7307383688177752}
episode index:284
target Thresh 41.547216619960736
target distance 13.0
model initialize at round 284
at step 0:
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([ 4.69441418, 13.40087885,  5.53356934]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 11.40497353502291}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.747254083736017
{'scaleFactor': 20, 'currentTarget': array([5., 2.]), 'dynamicTrap': False, 'previousTarget': array([5., 2.]), 'currentState': array([4.04271875, 1.68474552, 4.75173497]), 'targetState': array([5, 2], dtype=int32), 'currentDistance': 1.007855533077629}
episode index:285
target Thresh 41.61605332706815
target distance 7.0
model initialize at round 285
at step 0:
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([15.12802756,  1.85949127,  3.83525515]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 7.260957009382088}
done in step count: 4
reward sum = 0.96059601
running average episode reward sum: 0.7480000345271497
{'scaleFactor': 20, 'currentTarget': array([10.,  7.]), 'dynamicTrap': False, 'previousTarget': array([10.,  7.]), 'currentState': array([10.44557719,  6.82557464,  3.12854046]), 'targetState': array([10,  7], dtype=int32), 'currentDistance': 0.47850103723870757}
episode index:286
target Thresh 41.68475249834301
target distance 28.0
model initialize at round 286
at step 0:
{'scaleFactor': 20, 'currentTarget': array([19.70600633, 13.62316088]), 'dynamicTrap': False, 'previousTarget': array([19.72533058, 13.62476387]), 'currentState': array([35.97054539,  1.98422756,  1.50162953]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 31
reward sum = 0.7323033696543975
running average episode reward sum: 0.7479453423150496
{'scaleFactor': 20, 'currentTarget': array([ 8., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 22.]), 'currentState': array([ 8.16568047, 21.83423482,  4.10479861]), 'targetState': array([ 8, 22], dtype=int32), 'currentDistance': 0.23436747593469517}
episode index:287
target Thresh 41.75331440858211
target distance 18.0
model initialize at round 287
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 11.]), 'currentState': array([24.56441106,  6.4026763 ,  1.85812449]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 17.190552608720054}
done in step count: 19
reward sum = 0.8261686238355866
running average episode reward sum: 0.7482169509314404
{'scaleFactor': 20, 'currentTarget': array([ 8., 11.]), 'dynamicTrap': False, 'previousTarget': array([ 8., 11.]), 'currentState': array([ 8.41708384, 10.51779871,  3.07327167]), 'targetState': array([ 8, 11], dtype=int32), 'currentDistance': 0.6375554947399511}
episode index:288
target Thresh 41.8217393320332
target distance 17.0
model initialize at round 288
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'dynamicTrap': False, 'previousTarget': array([11.,  2.]), 'currentState': array([14.98396925, 19.8720612 ,  1.47467744]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 18.31072315367216}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7486039454079451
{'scaleFactor': 20, 'currentTarget': array([11.,  2.]), 'dynamicTrap': False, 'previousTarget': array([11.,  2.]), 'currentState': array([11.70937763,  2.46756109,  6.12951308]), 'targetState': array([11,  2], dtype=int32), 'currentDistance': 0.8496057824236255}
episode index:289
target Thresh 41.89002754239602
target distance 24.0
model initialize at round 289
at step 0:
{'scaleFactor': 20, 'currentTarget': array([15.69539658, 14.39487629]), 'dynamicTrap': False, 'previousTarget': array([17.27341645, 14.97753117]), 'currentState': array([34.40678359, 21.45773665,  4.47965169]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.7489292531595864
{'scaleFactor': 20, 'currentTarget': array([12., 13.]), 'dynamicTrap': False, 'previousTarget': array([12., 13.]), 'currentState': array([11.65776732, 12.57839949,  6.10505512]), 'targetState': array([12, 13], dtype=int32), 'currentDistance': 0.5430195152701714}
episode index:290
target Thresh 41.95817931282356
target distance 16.0
model initialize at round 290
at step 0:
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'dynamicTrap': False, 'previousTarget': array([18., 21.]), 'currentState': array([ 1.70698253, 15.65726507,  0.73579693]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 17.146639170276423}
done in step count: 10
reward sum = 0.9043820750088044
running average episode reward sum: 0.7494634552965252
{'scaleFactor': 20, 'currentTarget': array([18., 21.]), 'dynamicTrap': False, 'previousTarget': array([18., 21.]), 'currentState': array([17.7142826 , 21.36274536,  0.84101224]), 'targetState': array([18, 21], dtype=int32), 'currentDistance': 0.4617560263526571}
episode index:291
target Thresh 42.026194915922936
target distance 26.0
model initialize at round 291
at step 0:
{'scaleFactor': 20, 'currentTarget': array([11.26752595, 22.5178874 ]), 'dynamicTrap': False, 'previousTarget': array([12.01477651, 22.76866244]), 'currentState': array([31.18427955, 20.6949979 ,  3.14373779]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 20.0}
done in step count: 18
reward sum = 0.8345137614500875
running average episode reward sum: 0.7497547234682841
{'scaleFactor': 20, 'currentTarget': array([ 6., 23.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 23.]), 'currentState': array([ 6.96658997, 23.30689495,  2.11107802]), 'targetState': array([ 6, 23], dtype=int32), 'currentDistance': 1.0141403636938398}
episode index:292
target Thresh 42.09407462375669
target distance 17.0
model initialize at round 292
at step 0:
{'scaleFactor': 20, 'currentTarget': array([36., 14.]), 'dynamicTrap': False, 'previousTarget': array([36., 14.]), 'currentState': array([20.58873149,  4.02157845,  0.98549527]), 'targetState': array([36, 14], dtype=int32), 'currentDistance': 18.359632176669216}
done in step count: 15
reward sum = 0.8600583546412884
running average episode reward sum: 0.7501311863733114
{'scaleFactor': 20, 'currentTarget': array([36., 14.]), 'dynamicTrap': False, 'previousTarget': array([36., 14.]), 'currentState': array([36.53628187, 13.43789087,  0.87610575]), 'targetState': array([36, 14], dtype=int32), 'currentDistance': 0.7768944034919124}
episode index:293
target Thresh 42.161818707843736
target distance 8.0
model initialize at round 293
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 7.90321099, 13.95421382,  1.45185726]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 9.979106933816281}
done in step count: 7
reward sum = 0.9320653479069899
running average episode reward sum: 0.7507500100519974
{'scaleFactor': 20, 'currentTarget': array([ 2., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 2., 22.]), 'currentState': array([ 2.26507387, 21.2556557 ,  1.45818567]), 'targetState': array([ 2, 22], dtype=int32), 'currentDistance': 0.7901345450229367}
episode index:294
target Thresh 42.22942743916049
target distance 10.0
model initialize at round 294
at step 0:
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'dynamicTrap': False, 'previousTarget': array([13., 14.]), 'currentState': array([21.22298999, 23.42653713,  2.16965616]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 12.509083365585166}
done in step count: 12
reward sum = 0.8863848717161292
running average episode reward sum: 0.7512097892440792
{'scaleFactor': 20, 'currentTarget': array([13., 14.]), 'dynamicTrap': False, 'previousTarget': array([13., 14.]), 'currentState': array([12.37770624, 14.68725409,  2.17196819]), 'targetState': array([13, 14], dtype=int32), 'currentDistance': 0.927128746301572}
episode index:295
target Thresh 42.296901088141965
target distance 33.0
model initialize at round 295
at step 0:
{'scaleFactor': 20, 'currentTarget': array([25.73325955, 21.42779698]), 'dynamicTrap': False, 'previousTarget': array([25., 22.]), 'currentState': array([ 5.75498325, 20.49587621,  6.18758917]), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 20.0}
done in step count: 22
reward sum = 0.8016305895390459
running average episode reward sum: 0.7513801297856163
{'scaleFactor': 20, 'currentTarget': array([38., 22.]), 'dynamicTrap': False, 'previousTarget': array([38., 22.]), 'currentState': array([38.81537146, 21.76701229,  1.19196346]), 'targetState': array([38, 22], dtype=int32), 'currentDistance': 0.848005830827849}
episode index:296
target Thresh 42.36423992468286
target distance 21.0
model initialize at round 296
at step 0:
{'scaleFactor': 20, 'currentTarget': array([33.27466854,  9.43537304]), 'dynamicTrap': False, 'previousTarget': array([33.45612429,  9.63241055]), 'currentState': array([14.26663813,  3.21486019,  5.43024588]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 20.0}
done in step count: 17
reward sum = 0.8429431933839268
running average episode reward sum: 0.7516884229290449
{'scaleFactor': 20, 'currentTarget': array([35., 10.]), 'dynamicTrap': False, 'previousTarget': array([35., 10.]), 'currentState': array([35.93050358,  9.27475454,  2.38913479]), 'targetState': array([35, 10], dtype=int32), 'currentDistance': 1.1797533159280673}
episode index:297
target Thresh 42.431444218138616
target distance 17.0
model initialize at round 297
at step 0:
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([3.13880567, 6.78750522, 1.84098518]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 15.479225765982363}
done in step count: 11
reward sum = 0.8953382542587164
running average episode reward sum: 0.7521704693429029
{'scaleFactor': 20, 'currentTarget': array([ 6., 22.]), 'dynamicTrap': False, 'previousTarget': array([ 6., 22.]), 'currentState': array([ 6.66841045, 22.06518603,  2.37573051]), 'targetState': array([ 6, 22], dtype=int32), 'currentDistance': 0.6715815238825489}
episode index:298
target Thresh 42.49851423732648
target distance 22.0
model initialize at round 298
at step 0:
{'scaleFactor': 20, 'currentTarget': array([22.29620629, 21.65206818]), 'dynamicTrap': False, 'previousTarget': array([20.88854382, 20.94427191]), 'currentState': array([ 4.39717543, 12.72880116,  5.75398261]), 'targetState': array([25, 23], dtype=int32), 'currentDistance': 20.0}
Process finished with exit code 0
